<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>False Positives at Fulltext Stage</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        .study {
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 20px;
            margin-bottom: 20px;
            background-color: #f9f9f9;
        }
        .metadata, .screening, .content {
            margin-bottom: 15px;
            padding: 10px;
            border-left: 3px solid #3498db;
        }
        .metadata {
            border-left-color: #3498db;
        }
        .screening {
            border-left-color: #e74c3c;
        }
        .content {
            border-left-color: #2ecc71;
        }
        .annotation {
            border-left-color: #f39c12;
            background-color: #fff8e1;
        }
        strong {
            color: #2c3e50;
        }
        .study-list {
            margin-top: 20px;
        }
        footer {
            margin-top: 40px;
            text-align: center;
            font-size: 0.9em;
            color: #7f8c8d;
        }
        
        /* Accordion styles */
        .accordion {
            background-color: #f1f1f1;
            color: #444;
            cursor: pointer;
            padding: 10px;
            width: 100%;
            border: none;
            text-align: left;
            outline: none;
            font-size: 14px;
            font-weight: bold;
            margin-top: 10px;
            margin-bottom: 10px;
            border-radius: 4px;
        }
        .accordion:hover {
            background-color: #ddd;
        }
        .accordion:after {
            content: ' \25BC'; /* Down arrow */
            font-size: 10px;
            color: #777;
            float: right;
        }
        .accordion.active:after {
            content: ' \25B2'; /* Up arrow */
        }
        .panel {
            padding: 0 18px;
            background-color: white;
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.2s ease-out;
            border: 1px solid #ddd;
            border-top: none;
            border-radius: 0 0 4px 4px;
        }
        .panel-content {
            padding: 15px;
        }
        .fulltext-content {
            white-space: pre-wrap;
            font-family: monospace;
            font-size: 12px;
            line-height: 1.4;
        }
    </style>
</head>
<body>
<button id="saveButton" onclick="saveAnnotations()" style="position: fixed; top: 10px; right: 10px; z-index: 1000; background-color: #27ae60; color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer;">Save Annotations</button>
<script>
    function toggleAccordion(btn) {
        btn.classList.toggle("active");
        var panel = btn.nextElementSibling;
        if (panel.style.maxHeight) {
            panel.style.maxHeight = null;
        } else {
            panel.style.maxHeight = panel.scrollHeight + "px";
        }
    }
    
    function saveAnnotations() {
        // Collect all annotations
        var annotations = [];
        var studies = document.getElementsByClassName('study');
        
        for (var i = 0; i < studies.length; i++) {
            var study = studies[i];
            var studyId = study.id;
            var pmid = study.querySelector('h2').textContent.split(':')[1].trim().split(' ')[0];
            
            // Get judgment
            var agreeRadio = document.getElementById('agree-' + (i+1));
            var disagreeRadio = document.getElementById('disagree-' + (i+1));
            var judgment = '';
            if (agreeRadio && agreeRadio.checked) {
                judgment = 'agree';
            } else if (disagreeRadio && disagreeRadio.checked) {
                judgment = 'disagree';
            }
            
            // Get comment
            var commentElement = document.getElementById('comment-' + (i+1));
            var comment = commentElement ? commentElement.value : '';
            
            annotations.push({
                'pmid': pmid,
                'judgment': judgment,
                'comment': comment
            });
        }
        
        // Create and download JSON file
        var dataStr = "data:text/json;charset=utf-8," + encodeURIComponent(JSON.stringify(annotations, null, 2));
        var downloadAnchorNode = document.createElement('a');
        downloadAnchorNode.setAttribute("href", dataStr);
        downloadAnchorNode.setAttribute("download", "annotations.json");
        document.body.appendChild(downloadAnchorNode);
        downloadAnchorNode.click();
        downloadAnchorNode.remove();
        
        // Show confirmation
        alert('Annotations saved successfully!');
    }
</script>
<h1>False Positives Papers at Fulltext Stage</h1>
<p>Total papers: 64</p>
<div class='study-list'>
<div class='study' id='study-1'>
<h2>1. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/34409940/' target='_blank'>34409940</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study in healthy adults (N=43, age range mean 26.7 within 17–65). The task probes social-related processing (empathy, perception/understanding of others’ pain and self–other distinction). The paper reports group-level, whole-brain mass-univariate analyses (contrasts: genuine pain – no pain; pretended pain – no pain; interaction) with cluster-level FWE correction and activation maps/coordinates across whole brain (bilateral aIns, aMCC, rSMG), and also presents brain–behavior regression results. Thus it provides univariate task-evoked whole-brain results for a healthy adult sample. It does not rely solely on ROI-only or connectivity-only findings. Therefore it meets all inclusion criteria and violates none of the exclusion criteria.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-1' name='judgment-1' value='agree'>
<label for='agree-1'>Agree</label>
<input type='radio' id='disagree-1' name='judgment-1' value='disagree'>
<label for='disagree-1'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-1' name='comment-1' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-2'>
<h2>2. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/24478377/' target='_blank'>24478377</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study in healthy adults (N=15, ages 20–28) that includes a social-distance task (names/photos of friends vs acquaintances) tapping social perception/understanding. The paper reports whole-brain analyses: a conventional univariate GLM and group-level voxelwise t tests (sooner vs later; closer vs farther; more vs less familiar) conducted across the whole brain; authors explicitly report the univariate results (no voxels survived FDR correction) and describe the group-level searchlight and RSA results. Although univariate effects were null, the study clearly performed and reported group-level, whole-brain task-evoked analyses for the healthy adult sample. It is not ROI-only, not connectivity- or resting-state-only, and participants fall within the eligible age range. Therefore it meets the inclusion criteria for fMRI studies of social-related processing in healthy adults.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-2' name='judgment-2' value='agree'>
<label for='agree-2'>Agree</label>
<input type='radio' id='disagree-2' name='judgment-2' value='disagree'>
<label for='disagree-2'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-2' name='comment-2' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-3'>
<h2>3. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/23142071/' target='_blank'>23142071</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is a task-based fMRI study of social perception (emotional facial expressions) including a healthy adult human sample (n=23, ages 24–34). The paper reports group-level, random-effects whole-brain univariate analyses with voxelwise maps and cluster correction, includes stereotactic coordinates and figures, and presents results for the healthy control group separately from the monkey data. Thus it meets all inclusion criteria (social-related fMRI task; healthy adult sample within 17–65; whole-brain group-level task activation maps).</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-3' name='judgment-3' value='agree'>
<label for='agree-3'>Agree</label>
<input type='radio' id='disagree-3' name='judgment-3' value='disagree'>
<label for='disagree-3'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-3' name='comment-3' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-4'>
<h2>4. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/21775952/' target='_blank'>21775952</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> The study reports task-based fMRI of observing virtual social interactions, addressing social processing. Participants (group-level results from N=15) are described as typical recruited subjects with informed consent, implying healthy adults. The paper presents group-level, univariate whole-brain activation maps/contrast results (t-value activation maps and time courses) showing activity in social cognition regions (STS, mPFC, amygdala), meeting the whole-brain evidence requirement. It is not limited to ROI, connectivity-only, or resting-state analyses. Therefore all inclusion criteria are met.</p>
<p><strong>Fulltext Confidence:</strong> 0.85</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-4' name='judgment-4' value='agree'>
<label for='agree-4'>Agree</label>
<input type='radio' id='disagree-4' name='judgment-4' value='disagree'>
<label for='disagree-4'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-4' name='comment-4' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-5'>
<h2>5. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/30624029/' target='_blank'>30624029</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study in healthy adults (ages 18–22) using an explicitly social task (iterated Prisoner’s Dilemma). The paper reports group-level, voxelwise whole-brain univariate GLM analyses (thresholded Z images and cluster-corrected results) and provides coordinates/tables/figures describing these effects in the healthy sample (OT vs placebo and genotype-restricted analyses). All inclusion criteria are met (social task, healthy adult sample, whole-brain task-evoked maps reported). No exclusion criteria apply (not ROI-only, not resting-state/connectivity only, healthy-group results clearly reported).</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-5' name='judgment-5' value='agree'>
<label for='agree-5'>Agree</label>
<input type='radio' id='disagree-5' name='judgment-5' value='disagree'>
<label for='disagree-5'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-5' name='comment-5' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-6'>
<h2>6. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/30639342/' target='_blank'>30639342</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is a task-based fMRI study of social processing (Taylor Aggression Paradigm involving social provocation and decision-making), conducted in healthy adult males (ages 18–35), satisfying the sample criterion. The paper reports group-level, voxelwise whole-brain univariate contrasts (e.g., high > low provocation during feedback and decision phases) with cluster-level FWE correction, tables of peak coordinates, and figures—fulfilling the whole-brain evidence requirement. Analyses are not limited to ROI-only or connectivity-only approaches, and healthy-group task activation maps/results are clearly reported and generalizable to healthy adults. No exclusion criteria are violated. Therefore the study meets all inclusion criteria for the meta-analysis of fMRI studies of social-related processing in healthy adults.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-6' name='judgment-6' value='agree'>
<label for='agree-6'>Agree</label>
<input type='radio' id='disagree-6' name='judgment-6' value='disagree'>
<label for='disagree-6'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-6' name='comment-6' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-7'>
<h2>7. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/25489093/' target='_blank'>25489093</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of social communication/mutual understanding in healthy adults (54 right-handed male participants, ages 18–27). The task is explicitly social (creation and negotiation of shared meanings between Communicator and Addressee). The paper reports group-level, whole-brain univariate analyses (model-based whole-brain search for BOLD dynamics matching behavioral mutual understanding; random-effects analysis with cluster-level correction, reported regional findings including rSTG), not limited to ROI-only or connectivity-only results. Healthy adult results are reported separately and are generalizable (random-effects, whole-brain inference). Therefore all inclusion criteria are met and no exclusion criteria are triggered.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-7' name='judgment-7' value='agree'>
<label for='agree-7'>Agree</label>
<input type='radio' id='disagree-7' name='judgment-7' value='disagree'>
<label for='disagree-7'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-7' name='comment-7' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-8'>
<h2>8. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/28540647/' target='_blank'>28540647</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study of social processing in healthy adults. The neuroimaging cohort included healthy participants aged 18–53 (n=62 after exclusions), within the 17–65 range. The authors conducted whole-brain, group-level univariate GLM analyses (random-effects), reported voxelwise results (thresholded p<.001, permutation correction), and described specific whole-brain activations (e.g., dACC, bilateral dlPFC, parietal cortex, insula). Statistical maps are available on NeuroVault. The task explicitly probes social influence/understanding of others in a decision-making paradigm. No exclusion criteria (e.g., only ROI, only connectivity, non-empirical) apply. Therefore the paper meets all inclusion requirements for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.94</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-8' name='judgment-8' value='agree'>
<label for='agree-8'>Agree</label>
<input type='radio' id='disagree-8' name='judgment-8' value='disagree'>
<label for='disagree-8'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-8' name='comment-8' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-9'>
<h2>9. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/30834300/' target='_blank'>30834300</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of real-time mutual eye contact—a social communication task—conducted in healthy adult participants (initial N=34; fMRI analyses on N=30; mean age ~21.7, within 17–65). The paper reports group-level, whole-brain univariate GLM results (LIVE > REPLAY), random-effects analyses with cluster-level FWE correction and coordinate information, and figures/tables of voxelwise activation maps generalizable to healthy adults. Although additional connectivity and interbrain synchronization analyses are included, the study provides the required whole-brain task-evoked statistical maps for the healthy control sample. No exclusion criteria apply (not ROI-only, not resting-state only, healthy adult group reported separately). Therefore it meets all inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-9' name='judgment-9' value='agree'>
<label for='agree-9'>Agree</label>
<input type='radio' id='disagree-9' name='judgment-9' value='disagree'>
<label for='disagree-9'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-9' name='comment-9' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-10'>
<h2>10. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/19739909/' target='_blank'>19739909</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study of social processing (inhibiting prejudice) in healthy adult participants (young and older adults). The abstract and paper text report group-level task-evoked activation differences (e.g., greater medial prefrontal cortex activity in young adults when viewing stigmatized faces; lateral prefrontal activity in older adults with preserved executive function), indicating whole-brain univariate task contrasts were conducted and reported. At least one healthy adult group falls within the 17–65 range (young adults), and results for healthy groups are reported separately. The study is not ROI-only, not resting-state or connectivity-only, and presents generalizable healthy-group task effects. Therefore it meets all inclusion criteria and no exclusion criteria appear violated.</p>
<p><strong>Fulltext Confidence:</strong> 0.85</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-10' name='judgment-10' value='agree'>
<label for='agree-10'>Agree</label>
<input type='radio' id='disagree-10' name='judgment-10' value='disagree'>
<label for='disagree-10'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-10' name='comment-10' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-11'>
<h2>11. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/29931116/' target='_blank'>29931116</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study in healthy adult samples (HCP and UCLA datasets; ages within 22–50) that includes a social cognition task (social interaction vs. random movement). The paper reports group-level task contrasts derived from voxel-wise GLMs and presents task activation results (e.g., Figure 1 and accompanying voxel-wise analyses), fulfilling the requirement for at least one whole-brain, univariate group-level task-evoked map for the healthy/control group. Although the paper also focuses on connectivity analyses, it explicitly reports and describes whole-brain activation contrasts for the social cognition condition versus control, and these are generalizable to healthy adults. Therefore all inclusion criteria are met and no exclusion criteria are triggered.</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-11' name='judgment-11' value='agree'>
<label for='agree-11'>Agree</label>
<input type='radio' id='disagree-11' name='judgment-11' value='disagree'>
<label for='disagree-11'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-11' name='comment-11' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-12'>
<h2>12. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/22563008/' target='_blank'>22563008</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of social-related processing (effects of expert/adult advice) during a decision-making task, explicitly probing social influence and social-cognition regions (e.g., TPJ, vmPFC, DLPFC). The sample includes a healthy adult group (ages 18–45) reported separately alongside adolescent groups. The paper reports group-level, whole-brain voxelwise task-evoked results (cluster-corrected maps with MNI coordinates and statistical tests) and presents adult-specific effects (e.g., vmPFC decreases in correlation with certainty equivalents in adults, t(23)=−2.28). Analyses are univariate task activation contrasts/parametric modulations rather than ROI-only or connectivity-only results. Participants are healthy and within the 17–65 range. Therefore the study meets all inclusion criteria and violates none of the exclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-12' name='judgment-12' value='agree'>
<label for='agree-12'>Agree</label>
<input type='radio' id='disagree-12' name='judgment-12' value='disagree'>
<label for='disagree-12'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-12' name='comment-12' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-13'>
<h2>13. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/27167401/' target='_blank'>27167401</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study of social cognition in healthy adults (final N=18, mean age 23.7 within 17–65). The task is explicitly social (inferring others’ emotions from visual/verbal cues with feedback). The paper reports whole-brain, group-level univariate analyses of task-evoked signals (value and prediction-error regressors) with tables of coordinates and conjunction/contrast maps, meeting the whole-brain evidence requirement. Results are presented for the healthy/control sample (no clinical subgroup confound). Analyses are task-based, not limited to ROI-only, connectivity-only, or resting-state approaches. Therefore all inclusion criteria are satisfied and no exclusion criteria apply.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-13' name='judgment-13' value='agree'>
<label for='agree-13'>Agree</label>
<input type='radio' id='disagree-13' name='judgment-13' value='disagree'>
<label for='disagree-13'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-13' name='comment-13' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-14'>
<h2>14. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/28169323/' target='_blank'>28169323</a></h2>
<div class='metadata'>
<h3>Metadata</h3>
<p><strong>Title:</strong> Social pain and social gain in the adolescent brain: A common neural circuitry underlying both positive and negative social evaluation</p>
<p><strong>Authors:</strong> N/A</p>
<p><strong>Journal:</strong> Sci Rep</p>
<p><strong>Publication Year:</strong> 2017</p>
<p><strong>DOI:</strong> 10.1038/srep42010</p>
<p><strong>PMCID:</strong> <a href='https://www.ncbi.nlm.nih.gov/pmc/articles/5294419/' target='_blank'>5294419</a></p>
</div>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of social evaluation (social inclusion/exclusion) using a task explicitly targeting social-related processing. The sample comprises healthy participants aged 17–20 (N=56 analyzed), which falls within the review’s adult age range (17–65) and are reported as a healthy group. The paper reports group-level, univariate whole-brain voxelwise task contrasts (positive>neutral, negative>neutral, conjunction analyses) with whole-brain FWE-corrected results and coordinates, satisfying the whole-brain evidence requirement. It is not limited to ROI-only, connectivity-only, patient-only, or non-empirical analyses. Therefore all inclusion criteria are met and no exclusion criteria are triggered.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p><strong>Abstract:</strong>  
Social interaction inherently involves the subjective evaluation of cues salient to social inclusion and exclusion. Testifying to the importance of such social cues, parts of the neural system dedicated to the detection of physical pain, the dorsal anterior cingulate cortex (dACC) and anterior insula (AI), have been shown to be equally sensitive to the detection of social pain experienced after social exclusion. However, recent work suggests that this dACC-AI matrix may index   any   socially pertinent information. We directly tested the hypothesis that the dACC-AI would respond to cues of   both   inclusion and exclusion, using a novel social feedback fMRI paradigm in a population-derived sample of adolescents. We show that the dACC and left AI are commonly activated by feedback cues of inclusion and exclusion. Our findings suggest that theoretical accounts of the dACC-AI network as a neural alarm system restricted within the social domain to the processing of signals of exclusion require significant revision. 
 </p>
<button class='accordion' onclick='toggleAccordion(this)'>Full Text Content (27902 characters)</button>
<div class='panel'>
<div class='panel-content'>
<div class='fulltext-content'> 
Humans are fundamentally social. We create and reside within a diversity of emergent social systems ranging from couples, families, and groups to cities, countries and civilizations. These social structures have evolved in tandem with biological and psychological mechanisms that support social behavior, and the consequent rich capacity for social interaction has enabled humans to survive, reproduce, and flourish. Central among these mechanisms is the ability to detect and respond to diverse signals of social inclusion and social exclusion – behavioral dynamics that are critical to the establishment and maintenance of relationships, groups and social hierarchies. Indeed, acceptance by our desired social partners is so fundamental that social exclusion has profound negative consequences for affect, health and well-being  and is particularly toxic during adolescence . 

There is burgeoning evidence to indicate that the mental ‘pain’ described by those experiencing such exclusion is more than just a metaphor. Brain imaging data suggest that the neural response to social rejection co-opts components of the well-established physical pain signature in the brain . Cues of rejection have reliably been shown to activate a network of so-called ‘social pain’ regions that overlaps with the neural response to nociceptive stimulation and primarily includes the dorsal anterior-cingulate-cortex (dACC) and the anterior insula (AI) . This account has been extended to suggest that the dACC in particular is involved in domain-general processing of pain information as it pertains to survival-relevant goal conflicts such as hunger or thirst and that social exclusion represents just one form of such survival threat . 

However, recent neuroimaging investigations within the social domain, have raised interesting questions about this social pain account of dACC-AI functioning . For example, multi-level kernel density meta-analyses  of the two prototypical social rejection paradigms used in neuroimaging studies – Cyberball (in which participants are excluded in a virtual ball-tossing game), and the reliving of memories of romantic rejection – have provided equivocal support for the claim that rejection activates the same neural matrix identified in studies of physical pain. Consistent with this, multivariate functional magnetic resonance imaging (fMRI) pattern analyses suggest that separate neural representations code physical pain and mental pain within this identified shared network . Parallel to this, some have argued the broad dACC-AI overlap between social pain and physical pain can be simply explained as salience, and hence trigger multimodal cognitive processes involved in detecting, orienting attention towards, and/or reacting to salient events . 

An alternative possibility is that this pattern of dACC-AI co-activation emergent from the social exclusion literature is not simply a form of ‘pain’, but instead a more sophisticated index of the social dynamic . One compelling candidate is that this network operates as a gauge of social inclusivity, a form of sociometer . If true, then this system would subserve the processing of any signal that provides salient information about social inclusivity, whether it indexes social pain or ‘social gain’. 

We therefore investigated the hypothesis that the dACC-AI matrix prototypically identified in studies of social rejection is in fact critically involved in processing signals of   both   social pain and social gain. We used a novel Social Feedback fMRI task that provides participants with comparably intense signals pertaining to either social exclusion or social inclusion, within the same paradigm, thus allowing us to identify their common and discrete neural substrates. 

Participants believed that they were competing with other contestants in a multi-round game. Participants were told that: at the end of each round, one contestant is excluded from the game while the others are included in the next round; each round involves each contestant individually performing a social task and performance is evaluated by a panel of judges; that these ratings form the basis of the inclusion/exclusion decisions; and that the game is played in a hyperscanning context  where each contestant is in a separate MRI scanner. In fact the judges and other contestants were confederates, only the participants were in a scanner, and only one round of the game, comprising our Social Feedback Task ( ; see also  ), was ever played. 

## Results 
  
Our results showed that participants rated negative social feedback as more upsetting than neutral feedback (  t   = 12.6, df = 55, p < 0.001) and positive feedback as less upsetting than neutral (  t   = 13.5, df = 55, p < 0.001), as expected ( ). Consistent with the social pain literature , the fMRI data (all whole brain,   p   < 0.05, FWE corrected) revealed greater activation in the bilateral dACC and left AI when receiving negative compared to neutral social feedback ( ). Critically, however, these same regions were also activated when receiving positive (relative to neutral) social feedback, along with the ventromedial prefrontal cortex (vmPFC) and ventral striatum bilaterally ( ). In fact, there were no regions that were significantly more activated for negative social feedback relative to positive (negative > positive contrast), even when we explored the data using lower activation thresholds (  p   < 0.005, uncorrected). Furthermore, the reverse contrast (positive > negative) simply revealed activations in the aforementioned ventral striatum and vmPFC regions, areas traditionally associated with reward processing ( )  ( ). These findings indicate that a common dACC-AI network subsumes the processing of information pertaining to both social exclusion and social inclusion. 

This was supported by a logical ‘AND’ conjunction analysis  of ‘negative > neutral social feedback ( )’ AND ‘positive > neutral social feedback ( )’, which revealed clusters in the left AI (peak voxel x = −28, y = 18, z = −10) and the dACC (peak voxel x = 2, y = 32, z = 24) that were significantly active across both conditions (whole brain   p   < 0.05, FWE corrected;  ). The conjunction contrast was masked inclusively using the contrasts ‘positive feedback > baseline’ and ‘negative feedback > baseline’ (see  ) to ensure activation was not a product of the neutral condition, though it should be noted the results were the same without masking the conjunction. Furthermore, separate psychophysiological interaction (PPI) functional connectivity analyses, seeded from these AI and dACC regions, showed comparable results for the positive and negative social feedback conditions (relative to neutral feedback) with significant (p < 0.05, FWE corrected) associations with activity in the right fusiform gyrus and inferior occipital lobe for both contrasts ( ,  , and  ). 

Is the common dACC-AI network identified here the same as that emerging from prior studies of social rejection ? An overlay of the results of our conjunction analysis ( ) on the clusters identified in the whole brain meta-analysis of social rejection studies  suggested that the conjunctive regions identified in the current data map closely onto the meta-analytic findings ( ), indicating that our Social Feedback Task is activating the same network as the Cyberball and romantic rejection paradigms reviewed therein. A similar overlay ( ), this time using just the results of our positive > neutral social feedback contrast, confirms that the network specifically underlying responses to positive evaluative information in the present data conjoins the social rejection network identified in the meta-analysis. In fact, if we extract the parameter estimates from our data that correspond to the peak dACC and AI coordinates from this whole-brain meta-analysis, they show the greatest activation during   positive  , rather than negative feedback in our data, suggesting that this network (hitherto associated with social pain) is actually more strongly activated in a social inclusion context. This is replicated when plotting the peak coordinates from a meta-analysis of social rejection tasks with a restricted focus on ACC activity  ( ,  ) and in structural and functional region of interest (ROI) analyses of the same regions ( ). 

Are there other potential accounts of the present data that merit consideration? One possibility (see   for a full discussion) is that the dACC-AI network activation found here in the context of signals of social inclusion simply occurs as a result of expectancy or carry-over effects from negative social feedback elsewhere in the task. However, these putative influences would also be present for the neutral feedback trials, for which the relevant activations were subtracted out in our critical positive social feedback contrast term, making this explanation less compelling. A related possibility is that positive and negative feedback activate a common neural network because they both involve some form of expectancy violation . However, again this seems unlikely because, if for illustration we focus on the critical positive feedback findings, the pattern of dACC-AI activation remains even for the subset of participants (  n   = 10) who (by their own ratings) expected to be consistently judged as best across all social domains and for whom the positive feedback was therefore unlikely to violate expectancies ( ). 

The shared dACC-AI activation also seems unlikely to be a simple function of emotional arousal as the effects remain after regressing out skin conductance responses (a reliable marker of psychophysiological arousal  recorded during the feedback epochs ( ). Similarly, applying an exclusive mask of the neural correlates of rating the affective impact of feedback (Rating Slide;  ) still revealed significant dACC-AI feedback conjunction clusters, suggesting that this shared activation is not simply attributable to affect processing ( ). Analogously, the pattern of dACC-AI activation in our feedback conjunction remained after applying either an exclusive meta-analysis mask of ‘salience’ (from neurosynth.org)  ( ), or a mask created by re-binning the feedback trials as a function of trial-by-trial stasis/change in social rank  ( ), suggesting that simple explanations based on general salience or social rank processing are also unlikely to account for the results. 


## Discussion 
  
The dACC and AI regions of the brain are implicated in a diverse range of psychological processes . Within social contexts, the dACC-AI network has hitherto been associated with experiences of social exclusion and rejection . However, our results show comparable patterns of involvement of this network in the processing of signals of social inclusion and of social acceptance. By comparable patterns, we mean we report significant independent contrasts of positive versus control conditions (either neutral or low-level baseline), and negative versus control conditions, and a significant conjunction (FWE corrected) for those separate effects. We are not intending to imply that those effects are identical in magnitude, although we failed to find any support for greater activation in this network in the face of negative social feedback relative to positive, or vice versa. These findings suggest that theoretical accounts of the dACC-AI network as a neural alarm system targeted at processing signals of exclusion within social contexts, and the resultant mental pain, require significant revision and extension. The current data are more consistent with a framework in which the dACC-AI matrix indexes signals of social inclusivity more generally within social contexts - a neural sociometer . This accords with functional level models emphasizing the integration of signals of social inclusion and exclusion as a gauge of fluctuating social status , and mirrors a similar theoretical shift concerning the brain’s so-called physical pain networks which have also been shown to be heavily implicated in the processing of physical pleasure . 

Several notable strengths of the current study bolster confidence in these conclusions, including the relatively large (for fMRI) and population-derived sample (  n   = 56), the use of a novel task targeted at the key research question, the application of a comprehensive analytic approach to address common potential confounds in the social pain literature , and the stringent use of familywise error-corrected statistics. 

Other findings from the wider social neuroscience literature are also consistent with this view that the prototypical dACC-AI social pain network is involved in the processing of inclusive social signals. Somerville   et al  . , in a study ostensibly examining social feedback in the context of expectancy violations, report comparable levels of dACC activation when subjects viewed pictures of people who reportedly dislike them   or   like them . Similarly, rostral ACC activation increases in tandem with increasing expectation of positive social feedback . Furthermore, μ-opioid receptors (MOR) that moderate physical pain appear to respond to positive social feedback in key social pain structures . Using Positron Emission Tomography (PET) combined with a social feedback task examining whether one is liked (social acceptance) or disliked (social rejection), MOR activation during social rejection was positively correlated with MOR activation during social acceptance in the anterior insula (left, r  = 0.79; right, r  = 0.62) and dACC (left, r  = 0.86; right, r  = 0.92) with no significant differences in levels of MOR activation in these structures between positive or negative social feedback conditions . 

Interestingly, potentially inconsistent data come from studies using the prototypical social rejection paradigm – Cyberball . Cyberball invariably contains a social ‘inclusion’ comparison condition where the participant is included in the virtual ball tossing game. Unlike social rejection, though, inclusion within Cyberball does not appear to activate the dACC-AI network . However, ‘inclusion’ here simply means not being excluded from the virtual ball tossing game. Such inclusion in this game playing context would be considered the social norm  and consequently is unlikely to be overt or salient enough to markedly activate any putative inclusivity-related brain network. To address this, some studies have adapted Cyberball by using an ‘over-inclusion’ condition in which participants receive the ball 80% of the time . However, if inclusion is the social norm as opposed to an overt, socially positive event, then increasing the number of social inclusion trials is likely to simply accentuate this. Indeed, although the fMRI results in these over-inclusion studies mirrored the usual Cyberball findings in showing that the dACC was more active during exclusion compared to over-inclusion, there was no behavioural difference in the level of subjective social pain reported between inclusion and over-inclusion conditions. This suggests that participants did not find over-inclusion any more socially rewarding than standard inclusion. Hence, while the Cyberball paradigm creates valid socially painful experiences through exclusion, it may be ill-suited to assess the social pleasure associated with inclusion. 

Some potential limitations merit comment. With the absence of a non-social comparison condition we were unable to evaluate the social specificity of our data, and hence rule out interpretations within the broader context of salience processing , although our findings remain even when exclusively masking brain regions prototypically associated with salience processing. Secondly, our GSR data suggest that the neutral condition was not equidistant from the negative and positive conditions in terms of elicited arousal, with the neutral condition being more similar to the negative condition. However, our findings remain the same when comparing positive and negative feedback to our low-level baseline (i.e., without the neutral condition) and when regressing out GSRs in the analyses, suggesting that this does not account for the results. Importantly, the neutral condition was well titrated in terms of its emotional impact relative to the positive and negative conditions ( ). Finally, as more resources and importance appear to be bestowed upon social evaluation in adolescence , our data may not be generalizable to the adult population. Future research into such questions would be beneficial. 

In summary, we show that the classic social pain network in the human brain, centered on the dACC and AI, shows similar patterns of sensitivity to signals of social inclusion as it does to social rejection. These findings have strong theoretical implications for our understanding of the role of this neural network in social cognition and are consistent with a neural sociometer that gauges the implications of all pertinent social information with respect to the organism’s social inclusion status. 


## Methods 
  
### Participants 
  
Participants were adolescents/young adults [  N   = 60; Mean (SD) age = 18 (0.7), range 17–20 years; 31 females] recruited from the population-representative ROOTS cohort (Total   N   = 1143) . We selected adolescents/young adults as we felt the Social Feedback Task would resonate strongly with that demographic. Inclusion criteria were: normal or corrected-to-normal vision; and English speaking. Exclusion criteria were: any history of neurological trauma resulting in loss of consciousness; current psychotropic medication use; current neurological disorder; current Axis 1 psychiatric disorder according to the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV; ref.  ); presence of metal in body; diagnosed specific learning disability; or IQ < 85 on the Wechsler Abbreviated Scale of Intelligence (WASI; ref.  ). 

Participants recruited to the study showed no significant selection bias compared to the total ROOTS sample in terms of gender ratio or socioeconomic status as assessed using the ACORN (A Classification Of Residential Neighbourhoods) geodemographic measure  (  http://www.caci.co.uk  ). 

One participant was removed from further analysis due to a failure of imaging acquisition. Additionally, in the post-scan questioning (see below) three subjects reported some disbelief concerning the veracity of the cover story and were removed from all subsequent analyses, leaving a total of 56 participants for analysis (See   for participant data). 

The study was carried out in accordance with the Declaration of Helsinki and Good Clinical Practice guidelines and approved by the Cambridgeshire Research Ethics Committee. All participants provided written informed consent. 


### The Social Feedback Task 
  
The paradigm was styled as a ‘Big Brother’ game, where participants competed against other contestants (in fact these were confederates) to impress a set of six judges (also confederates) on a series of tasks in order to win through successive rounds of the game (they were told that one contestant per round was rejected from the game) and to eventually win the game. During the study participants were told they would be competing against three other contestants who were each located in MRI scanners located across the U.K, electronically linked so they can play the game interactively (hyperscanning) (see also  ). 

Participants were told that there would be three rounds of the game in total, with one person being rejected on each round until there was one winner remaining. In fact, this was a cover story and only one round of the game was played with all participants being voted off at the end of round one before being fully debriefed following a series of post-scan questions. For this first (and only) round of the game, each participant made a one-minute video recording to be rated by the panel of six judges. Participants were told that these ratings decided which contestant would be rejected from the game and who would progress to the subsequent (fictitious) rounds. The participants were told the six judges were together in a room at a separate location where they could be e-mailed the video recordings and from where they could submit their ratings. The participants were shown pictures of the six judges who were all just a few years older than the participants and were told that the judges had been extensively trained in making social judgments from video recordings. 

For the video recording, participants were asked to describe themselves, talk about what they enjoy doing, say what is important to them in life, outline their aims and achievements and say what was the most important thing that has happened to them was. Participants were given time before the video was recorded to think about these issues, and were shown a video made by a previous participant as an example. A still photograph of each participant was taken at this point for subsequent use in the fMRI session. Participants were informed that, for round one of the game, the judges would be rating these videos on a series of social dimensions (social competence, motivation, self-confidence, personal strength, social attractiveness and emotional sensitivity) that had been reliably linked to social success, prosperity and satisfaction across the life course, and that had been reliably shown to be easily rated on the basis of short video clips. They were told that, for each attribute, each judge would rank the participant and the other three contestants in terms of who was the best on that attribute (positive social feedback), who was the worst on that attribute (negative social feedback), and who was intermediate (neutral feedback) on that attribute. The decisions of each judge for each attribute (36 sets of feedback) were then shown to each participant during the fMRI session, prior to the final decision about who was rejected from the game on Round 1. Participants were told the design of the game was intended to build tension, akin to the voting on ‘Big Brother’ style game shows. 

To encourage believability in the other contestants, having made their own video recording, participants were asked to rate their competitors’ videos along the same social dimensions as the judges were using. Participants were told that each of the other contestants would be doing the same with their (the participant’s) video in the other contestants’ separate locations. 

Participants were told that different recordings for Rounds 2 and 3 would be completed following potential success on Round 1 (which never in fact happened, but was described in order to maintain believability). 

In the MRI scanner (See  ), each judgment epoch began with an 8-second ‘Judge Slide’ showing which judge would be judging which attribute (e.g.   David will now be judging you on social attractiveness  ). This was followed by an 8-second anticipation period of fixation, and an 8-second ‘Feedback Slide’, showing whether each contestant was judged to be the best (positive feedback), intermediate (neutral feedback) or worst (negative feedback) on that particular attribute by that particular judge. Following this ‘Feedback Slide’, and a 2-second fixation, a 10-second ‘Rating Slide’ of how the participants felt about the feedback (ranging from 0 (disappointed) – 10 (pleased)) was completed. This sequence was repeated 36 times for each social attribute from each judge, resulting in 12 ‘best’ judgments, 12 ‘neutral/intermediate’ judgments and 12 ‘worst’ judgments. Attribute and judge orders were counterbalanced across participants. At the end of the 36 judgments, overall judgments were made by each judge detailing whether the participant had made it through to the next round, a total of 6 such final judgments was made (one by each judge); 5 of which were ‘worst’ and one ‘middle’ resulting in the participant being rejected on round one of the game. Following the scan, as a manipulation check, participants were asked a series of questions aimed at assessing believability of the task and of the hyperscanning environment. 

All personally identifiable information (videos and photographs) was deleted immediately following debriefing. 


### Data Acquisition and Analysis Approaches 
  
#### Image acquisition and preprocessing 
  
MRI scanning was conducted at the Medical Research Council Cognition and Brain Sciences Unit on a 3-Tesla Tim Trio Magnetic Resonance Imaging scanner (Siemens, Germany) by using a head coil gradient set. Whole-brain data were acquired with echoplanar T2*-weighted imaging (EPI), sensitive to BOLD signal contrast (48 sagittal slices, 3 mm thickness; TR = 2000 ms; TE = 30 ms; flip angle = 78°; FOV 192 mm; voxel size: 3 × 3 × 3 mm). To provide for equilibration effects the first 5 volumes were discarded. T1 weighted structural images were acquired at a resolution of 1 × 1 × 1 mm. 

SPM8 software (  www.fil.ion.ucl.ac.uk/spm/  ) was used for data analysis. The EPI images were sinc interpolated in time for correction of slice timing differences and realignment to the first scan by rigid body transformations to correct for head movements. Field maps were estimated from the phase difference between the images acquired at the short and long TE and unwrapped, employing the FieldMap toolbox. Field map and EPI imaging parameters were used to establish voxel displacements in the EPI image. Application of the inverse displacement to the EPI images served the correction of distortions. Utilising linear and non-linear transformations, and smoothing with a Gaussian kernel of full-width-half-maximum (FWHM) 8-mm, EPI and structural images were co-registered and normalised to the T1 standard template in Montreal Neurological Institute (MNI) space. Global changes were removed by proportional scaling and high-pass temporal filtering with a cut-off of 128 s was used to remove low-frequency drifts in signal. 


#### Statistical analysis approach to fMRI data 
  
After preprocessing, statistical analysis was performed using the general linear model. Analysis was carried out to establish each participant’s voxel-wise activation during the Feedback and Rating Slides (see  ). Activated voxels in each experimental context were identified using an epoch-related statistical model representing each of the three feedback trial types and subsequent affect ratings, convolved with a canonical haemodynamic response function and mean-corrected. Six head-motion parameters defined by the realignment were added to the model as regressors of no interest. Multiple linear regression modelling was then applied to generate parameter estimates for each regressor at every voxel. At the first level, the following feedback contrasts were generated; ‘positive feedback’; ‘neutral feedback’; ‘negative feedback’; ‘positive feedback’ minus ‘neutral feedback’; ‘negative feedback’ minus ‘neutral feedback’; ‘positive feedback’ minus ‘negative feedback’ and ‘negative feedback’ minus ‘positive feedback’. The same contrasts were also generated for the ratings of affect (Rating Slides) following each Feedback Slide. For group statistics, random effects analysis was utilized. A conservative voxel-wise statistical threshold of P < 0.05 familywise error (FWE) corrected for multiple comparisons across the whole-brain was used for all analyses. 




## Additional Information 
  
 How to cite this article:   Dalgleish, T.   et al  . Social pain and social gain in the adolescent brain: A common neural circuitry underlying both positive and negative social evaluation.   Sci. Rep.   7  , 42010; doi: 10.1038/srep42010 (2017). 

 Publisher's note:   Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. 


## Supplementary Material 
  
 </div>
</div>
</div>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-14' name='judgment-14' value='agree'>
<label for='agree-14'>Agree</label>
<input type='radio' id='disagree-14' name='judgment-14' value='disagree'>
<label for='disagree-14'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-14' name='comment-14' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-15'>
<h2>15. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/30389840/' target='_blank'>30389840</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of social cognition (mental state representation and prediction) in healthy adults (imaging sample n=28 after one exclusion; ages 18–22). The task required participants to judge others’ mental states (social-related task). The paper reports group-level, voxelwise whole-brain analyses: searchlight representational similarity mapping (whole-brain map with TFCE-corrected results in dorsal mPFC) and a univariate, voxelwise repetition-suppression analysis with permutation TFCE correction (posterior precuneus, p < 0.05). These are group-level, task-evoked, whole-brain results generalizable to the healthy adult sample. No exclusion criteria are met (not ROI-only, not resting-state/connectivity-only, includes appropriate age range and healthy participants). Therefore the study meets all inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-15' name='judgment-15' value='agree'>
<label for='agree-15'>Agree</label>
<input type='radio' id='disagree-15' name='judgment-15' value='disagree'>
<label for='disagree-15'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-15' name='comment-15' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-16'>
<h2>16. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/22507230/' target='_blank'>22507230</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study of affective speech comprehension (social communication / perception of others) in healthy adults (n=51, ages 18–53). The paper reports group-level, whole-brain univariate task contrasts (EMO vs GRAM) with FWE-corrected maps and coordinate tables, and describes event-related whole-brain analyses and activation maps. Results for the healthy sample are reported directly and are generalizable to healthy adults. The study does not rely solely on ROI, resting-state, or connectivity-only analyses, and does not report only between-group contrasts or clinical samples. All inclusion criteria are satisfied and no exclusion criteria are met.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-16' name='judgment-16' value='agree'>
<label for='agree-16'>Agree</label>
<input type='radio' id='disagree-16' name='judgment-16' value='disagree'>
<label for='disagree-16'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-16' name='comment-16' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-17'>
<h2>17. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/27978778/' target='_blank'>27978778</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study of social processing (face perception/emotional valence and attention) in healthy adult participants (final N=25, ages 21–33). The paper reports group-level, whole-brain univariate analyses: a random-effects faces>objects contrast from the localizer (group t-map, k>9, p<.001) used to define face-responsive ROIs, and whole-brain searchlight results (SnPM corrected). Group-level task-evoked statistical maps are clearly described and generalizable to healthy adults. The study therefore meets the inclusion criteria (social-related fMRI task, healthy adult sample, and reported whole-brain group-level task activation), and does not violate exclusion criteria.</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-17' name='judgment-17' value='agree'>
<label for='agree-17'>Agree</label>
<input type='radio' id='disagree-17' name='judgment-17' value='disagree'>
<label for='disagree-17'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-17' name='comment-17' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-18'>
<h2>18. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/24265613/' target='_blank'>24265613</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study in healthy adults (N=27, mean age 34.1, right-handed, no psychiatric/neurological history) that used a social communication task (multimodal emotional communication: facial expression, prosody, speech content). The paper reports group-level, voxelwise GLM whole-brain contrasts (channel-specific contrasts, conjunctions) and displays whole-brain activation maps and coordinates used to define ROIs. Although the reanalysis emphasizes FIR and DCM, whole-brain univariate task-evoked results for the healthy group are clearly reported and generalizable to healthy adults. No exclusion criteria (ROI-only, resting-state-only, clinical-only group) apply. Therefore the study meets all inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-18' name='judgment-18' value='agree'>
<label for='agree-18'>Agree</label>
<input type='radio' id='disagree-18' name='judgment-18' value='disagree'>
<label for='disagree-18'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-18' name='comment-18' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-19'>
<h2>19. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/30825583/' target='_blank'>30825583</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> The study uses task-based fMRI targeting perception of dynamic facial expressions, a social-cognition process (perception/understanding of others). The authors report a stringent localizer contrast (dynamic facial expressions vs. static neutral faces plus moving dots) that successfully identified a posterior STS cluster across subjects and reference Random Effects (RFX) analyses, indicating group-level univariate voxelwise activation mapping rather than ROI-only or connectivity-only results. The work targets healthy adult participants (results described “across subjects” with no indication that healthy adults are absent or that findings are limited to a special/clinical-only subgroup). The paper therefore meets: (1) social-related fMRI task, (2) includes healthy adult group-level analysis, and (3) reports whole-brain task-evoked group contrasts. Inclusion caveat: the provided excerpt does not list exact sample ages or full methods, so confidence is moderate (0.80).</p>
<p><strong>Fulltext Confidence:</strong> 0.8</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-19' name='judgment-19' value='agree'>
<label for='agree-19'>Agree</label>
<input type='radio' id='disagree-19' name='judgment-19' value='disagree'>
<label for='disagree-19'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-19' name='comment-19' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-20'>
<h2>20. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/30794869/' target='_blank'>30794869</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> The study directly examines a social cognitive process (theory of mind) using task-based fMRI in healthy younger and older adult groups. The abstract indicates a ‘task-defined’ theory-of-mind network derived from task data and reports intrinsic (resting-state) connectivity within that network; this implies they performed group-level task-evoked analyses to define the network rather than reporting only ROI or resting-state results. At least one healthy adult group (younger adults) is clearly included. The paper therefore meets the review’s requirements for fMRI during a social task, inclusion of healthy adult participants, and whole-brain task-defined results. Confidence is moderate (0.75) because the provided excerpt does not display the explicit whole-brain voxelwise tables/figures, but the description strongly suggests such analyses were performed and reported.</p>
<p><strong>Fulltext Confidence:</strong> 0.75</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-20' name='judgment-20' value='agree'>
<label for='agree-20'>Agree</label>
<input type='radio' id='disagree-20' name='judgment-20' value='disagree'>
<label for='disagree-20'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-20' name='comment-20' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-21'>
<h2>21. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/29396415/' target='_blank'>29396415</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This fMRI study includes a healthy adult group (final N=21 adults) within the target age range and uses a naturalistic, socially engaging movie clip (Toy Story) designed to elicit social-cognitive processing. The paper reports whole-brain, group-level voxel/nodewise statistical maps of inter-subject correlation (inter-SC) for the Adult group and provides peak coordinates and corrected cluster results. Although the analytic approach is inter-subject synchrony (rather than a conventional GLM contrast), it yields whole-brain task-evoked group-level maps generalizable to healthy adults and is not limited to ROIs or seed-based connectivity. Therefore it meets the inclusion criteria for whole-brain task-evoked fMRI of social processing in healthy adults.</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-21' name='judgment-21' value='agree'>
<label for='agree-21'>Agree</label>
<input type='radio' id='disagree-21' name='judgment-21' value='disagree'>
<label for='disagree-21'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-21' name='comment-21' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-22'>
<h2>22. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/28643894/' target='_blank'>28643894</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study of social cognition (implicit/explicit theory of mind) in healthy adults (final N=22, mean age 24.4). The paper reports group-level, whole-brain random-effects GLM results from an explicit general-belief localizer (false-belief vs false-photograph) thresholded at q(FDR)<0.05 and provides peak Talairach coordinates and figures. Although ROI analyses were used for the implicit task, the study includes univariate whole-brain task-evoked contrasts from the healthy/control group and reports coordinates, meeting the meta-analysis whole-brain evidence criterion. Age range and participant health criteria are satisfied and this is original empirical fMRI data. Therefore it meets all inclusion criteria and does not violate exclusion criteria.</p>
<p><strong>Fulltext Confidence:</strong> 0.92</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-22' name='judgment-22' value='agree'>
<label for='agree-22'>Agree</label>
<input type='radio' id='disagree-22' name='judgment-22' value='disagree'>
<label for='disagree-22'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-22' name='comment-22' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-23'>
<h2>23. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/26621704/' target='_blank'>26621704</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of social processing in healthy adults (N=20, age 18–27) using a mentalizing task (judging scenarios evoking 60 mental states). It reports task-evoked, group-level whole-brain analyses: univariate voxelwise regressions and corrected searchlight MVPA results, with whole-brain statistical maps and cluster-corrected coordinates (Tables S2–S3, Figs. 2–4, SI). Participants are healthy adults and results for the healthy group are reported separately. The paper is empirical, not ROI-only, and includes whole-brain task activation maps generalizable to healthy adults. No exclusion criteria are met, so the study should be included.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-23' name='judgment-23' value='agree'>
<label for='agree-23'>Agree</label>
<input type='radio' id='disagree-23' name='judgment-23' value='disagree'>
<label for='disagree-23'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-23' name='comment-23' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-24'>
<h2>24. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/25253279/' target='_blank'>25253279</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of social perception (biological motion) in healthy adults (N=26 analyzed; ages 18–39). The task is social-related (perception/understanding of others). The paper reports group-level whole-brain univariate analyses: whole-brain biological-motion-selectivity analyses projected to a FreeSurfer average surface with correction for multiple comparisons (Monte Carlo simulation) and a significant cluster in right pSTS is reported. Although ROI analyses are also performed, whole-brain task-evoked results for the healthy adult sample are clearly presented. No exclusion criteria apply (not resting-state-only, not clinical-only, and participants are within 17–65). Therefore it meets all inclusion criteria for the review.</p>
<p><strong>Fulltext Confidence:</strong> 0.94</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-24' name='judgment-24' value='agree'>
<label for='agree-24'>Agree</label>
<input type='radio' id='disagree-24' name='judgment-24' value='disagree'>
<label for='disagree-24'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-24' name='comment-24' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-25'>
<h2>25. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/22623534/' target='_blank'>22623534</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> Study used task-based fMRI in healthy adult participants watching emotional movies (naturalistic social-emotional stimulation) and examines neural synchrony underlying social/emotional processing. The paper reports voxelwise, whole-brain intersubject correlation (ISC) analyses and links moment-to-moment valence/arousal ratings to ISC, with group-level maps/localizations (sensory, corticolimbic, default-mode, attention networks) reported across the brain. This meets inclusion requirements: (1) social-related processing (empathy/emotional contagion/understanding others), (2) healthy adult sample, and (3) group-level whole-brain voxelwise task-evoked results (reported as ISC maps and region lists). No exclusion criteria apply (not ROI-only, not resting-state-only, healthy-group maps are provided). Therefore include.</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-25' name='judgment-25' value='agree'>
<label for='agree-25'>Agree</label>
<input type='radio' id='disagree-25' name='judgment-25' value='disagree'>
<label for='disagree-25'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-25' name='comment-25' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-26'>
<h2>26. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/29915004/' target='_blank'>29915004</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This paper reports task-based fMRI in healthy adult participants (N=59 across three unique participant samples) performing social-related tasks (person knowledge, famous faces/names, socially relevant concept words). Group-level, whole-brain, voxelwise contrasts are reported (e.g., social > non-social semantics) with thresholding and peak MNI coordinates and whole-brain maps described in the text and tables. The sample is healthy adults and results for the healthy groups are reported separately. The study is empirical and not ROI-only, and does not rely solely on connectivity/resting-state or between-group-only contrasts. Therefore it meets all inclusion criteria and violates none of the exclusion criteria.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-26' name='judgment-26' value='agree'>
<label for='agree-26'>Agree</label>
<input type='radio' id='disagree-26' name='judgment-26' value='disagree'>
<label for='disagree-26'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-26' name='comment-26' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-27'>
<h2>27. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/17055981/' target='_blank'>17055981</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study in healthy adults examining perception of faces (neutral and fearful)—a social-processing task (perception/understanding of others). The paper reports group-level brain responses (FFA, STS, amygdala) to visible and invisible faces, indicating task-evoked activations rather than only ROI or connectivity analyses. Results are described at the group level and are generalizable to healthy adults. No indication that participants were outside the 17–65 range or that only clinical groups were studied. Therefore it meets the inclusion criteria: social-related fMRI task in healthy adults with reported whole-brain task-evoked activations.</p>
<p><strong>Fulltext Confidence:</strong> 0.85</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-27' name='judgment-27' value='agree'>
<label for='agree-27'>Agree</label>
<input type='radio' id='disagree-27' name='judgment-27' value='disagree'>
<label for='disagree-27'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-27' name='comment-27' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-28'>
<h2>28. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/28583386/' target='_blank'>28583386</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is a task-based fMRI study in healthy adult participants (N=34 males, ages 19–31 and 22–29), well within the 17–65 range. The task probes social processing (vicarious embarrassment/fremdscham and schadenfreude) — directly relevant to social-affective/Perception and Understanding of Others constructs. The paper reports group-level, univariate whole-brain task contrasts (SIT>NEUT) with FWE-corrected results and tables of activations (Table 1), and describes within-group parametric modulators and second-level random-effects GLMs. It is not ROI-only, not resting-state/ connectivity-only, and healthy-group whole-brain effects are clearly reported. Therefore it meets all inclusion criteria and violates no exclusion criteria.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-28' name='judgment-28' value='agree'>
<label for='agree-28'>Agree</label>
<input type='radio' id='disagree-28' name='judgment-28' value='disagree'>
<label for='disagree-28'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-28' name='comment-28' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-29'>
<h2>29. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/31100434/' target='_blank'>31100434</a></h2>
<div class='metadata'>
<h3>Metadata</h3>
<p><strong>Title:</strong> Dyadic interaction processing in the posterior temporal cortex</p>
<p><strong>Authors:</strong> N/A</p>
<p><strong>Journal:</strong> Neuroimage</p>
<p><strong>Publication Year:</strong> 2019</p>
<p><strong>DOI:</strong> 10.1016/j.neuroimage.2019.05.027</p>
<p><strong>PMCID:</strong> <a href='https://www.ncbi.nlm.nih.gov/pmc/articles/6610332/' target='_blank'>6610332</a></p>
</div>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study of social interaction perception in healthy adults (N=21; ages 18–35). The task explicitly probes social interaction processing (arguing, celebrating, laughing) during fMRI. The manuscript describes GLM estimation and group-level whole-brain analyses used for functional localizers (interaction > scrambled; bodies > objects; faces > objects; mentalizing > pain) to define ROIs, and reports a whole-brain searchlight analysis. Thus the paper provides group-level, task-evoked whole-brain univariate contrasts (localizers) and multivariate whole-brain searchlight results appropriate for healthy adults. The sample is within the 17–65 range and healthy-group results are reported. The study does not meet any exclusion criteria (not limited to ROI-only, not resting-state, not between-group-only). Therefore it meets all inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p><strong>Abstract:</strong>  
Recent behavioural evidence shows that visual displays of two individuals interacting are not simply encoded as separate individuals, but as an interactive unit that is 'more than the sum of its parts'. Recent functional magnetic resonance imaging (fMRI) evidence shows the importance of the posterior superior temporal sulcus (pSTS) in processing human social interactions, and suggests that it may represent human-object interactions as qualitatively 'greater' than the average of their constituent parts. The current study aimed to investigate whether the pSTS or other posterior temporal lobe region(s): 1) Demonstrated evidence of a dyadic information effect - that is, qualitatively different responses to an interacting dyad than to averaged responses of the same two interactors, presented in isolation, and; 2) Significantly differentiated between different types of social interactions. 

Multivoxel pattern analysis was performed in which a classifier was trained to differentiate between qualitatively different types of dyadic interactions. Above-chance classification of interactions was observed in 'interaction selective' pSTS-I and extrastriate body area (EBA), but not in other regions of interest (i.e. face-selective STS and mentalizing-selective temporo-parietal junction). A dyadic information effect was not observed in the pSTS-I, but instead was shown in the EBA; that is, classification of dyadic interactions did not fully generalise to averaged responses to the isolated interactors, indicating that dyadic representations in the EBA contain unique information that cannot be recovered from the interactors presented in isolation. These findings complement previous observations for congruent grouping of human bodies and objects in the broader lateral occipital temporal cortex area. 
   Highlights  
  
pSTS and EBA classify between different dynamic interactions. 
  
EBA is sensitive to (uniquely) dyadic interaction information. 
  
These findings support previous evidence for grouping of interacting people/objects in LOTC. 
  
 </p>
<button class='accordion' onclick='toggleAccordion(this)'>Full Text Content (38608 characters)</button>
<div class='panel'>
<div class='panel-content'>
<div class='fulltext-content'> 
## Introduction 
  
Social interactions are ubiquitous, yet little research has investigated visual perceptual responses to these common social scenarios, relative to individual-person perception ( ). Interestingly, recent behavioural evidence demonstrates that visual responses to two human individuals that are positioned to imply an interaction evoke different responses than when not positioned in this manner. These effects are demonstrated most strikingly by the findings of  : In this study, subjects viewed pairs of briefly presented (30 ms) human bodies or control objects (i.e. chairs), that either faced   towards   or   away   from each other, in either upright or inverted orientation, and were instructed to respond to the stimulus category they saw (i.e. bodies or chairs). Greater recognition accuracy was shown for upright than inverted dyads when an interaction was implied by the two bodies facing towards each other, but crucially, not when facing away from each other. Similarly, visual search facilitation is shown for full body dyads that are positioned to   face towards   – rather than   away   from – each other ( ), while facing direction effects are shown to modulate the evaluation of facial emotion of a target face (i.e. the perceived emotional expression of a target face is modulated by the emotion of a simultaneously presented non-target face, but only when positioned to   face towards   the target;  ). 

Together, these behavioural findings demonstrate that interacting individuals are not merely perceived as separate individuals, but as an   interactive dyad  . Indeed, similar   non-linear   neural responses have been observed recently – that is,   that responses to dyadic interaction stimuli are not the same as a linear combination of responses to the isolated elements of an interaction  . Specifically,   demonstrated evidence of non-linear responses to human-object interaction stimuli in the posterior temporal cortex; the authors used a pattern classification approach to test whether responses to images of human-object interactions (e.g. a person pushing a shopping cart) are distinct from   the mean-averaged response   to the constituent parts of the interaction (i.e. the averaged response to an isolated human and isolated cart); it was found that voxel patterns for human-object interactions in the posterior superior temporal sulcus (pSTS) and lateral occipital cortex (LOC) were statistically distinct from the averaged patterns evoked by isolated ‘interaction parts’. These findings suggest that these regions are sensitive to   unique   interactive information that is accessed only through holistic processing of interactions, and not through part-wise analysis (i.e. processing of constituent ‘interaction parts’ in isolation). 

Interestingly, this response in the pSTS complements previous findings that this region plays an important role in the visual processing of dynamic social interactions; for example, greater pSTS responses are shown for interacting point-light human dyads relative to two non-interacting figures, as well as for similar stimuli depicted by moving geometric shapes that do not contain body information ( ;  ). This region also differentiates between types of interactions performed by live-action human stimuli ( ), and is sensitive to ‘interactive’ motion cues such as the movement contingency between two interacting human figures ( ), or the degree of correlated motion between interacting animate geometric shapes ( ;  ). These findings implicate the pSTS as a region that may be optimized for processing social interaction information. 

The main aim of the present study was to determine whether pSTS encodes dynamic human interactions between two individuals in a non-linear fashion, using a similar approach to  . We herein adopt the phrase ‘  dyadic information effect  ’ rather than ‘non-linear’ effect, to emphasize a   sensitivity to unique information that is only present in dyadic interactions and not the averaged responses evoked by each interactor, presented in isolation  . Specifically, we used support vector machine (SVM) classification to test whether voxel-pattern responses to dyadic stimuli in the pSTS were statistically differentiable from   averaged response patterns   of isolated interactors. Additionally, it was predicted that significantly differentiable responses to   different types   of dyadic interaction would be observed in the pSTS, replicating previous findings (e.g.  ;  ). Responses were also tested in 3 other functionally localized regions of interest (ROIs) that are selective for social information that likely contributes to social interaction processing, and therefore might also plausibly show the hypothesized effects: Extrastriate body area (EBA), mentalizing-selective temporo-parietal junction (TPJ-M), and face-selective STS (STS-F). 


## Material & methods 
  
### Participants 
  
21 right-handed adults (mean age = 23.40 years; SD = 3.74; range = 18–35; 12 females) participated in the study. Participants gave informed consent and received monetary compensation for taking part. Ethical procedures were approved by the Bangor University psychology ethics board. 


### Stimuli 
  
Stimuli consisted of 4 s (s) video clips that were taken from custom footage of paired actors engaging in semi-improvised interactions. Actors were instructed to improvise these scenarios while enacting scripted ‘  action-gestures  ’; for example, for a given arguing scenario, one actor might be instructed to   point angrily   at the other person while the other   shook their fists   in frustration. Therefore, each interaction depicted two individuals performing a given pair of complementary action-gestures that they were encouraged to enact in a natural, authentic way (see supplementary materials A for example videos). An initial set of   dyad stimuli   were created (along with a separate set of   alone stimuli  , as described below; see   for examples of both dyad and alone stimuli). Dyad stimuli depicted two actors engaging in one of 3   interactive scenarios  :   Arguing   (i.e. both actors engaging in an angry/frustrated confrontation),   celebrating   (i.e. both actors celebrating together, excitedly), and   laughing   (i.e. both actors were laughing together, or at each other). These specific scenarios were chosen for the ‘tonal consistency’ of actions performed by a given pair of interactors, such that the intentions, emotions, and valence information conveyed by both individuals in a given scenario were always similar (e.g. angry/frustrated) rather than contrasting (e.g. angry/sad). This ensured that successful classification of the different scenarios was not driven by systematic differences in intentional, emotional, or valence content   between   interactors. Therefore, these scenarios represented three interactive scenarios that were intended to be easily distinguishable.   
a. Example video frames from the dyad versions of the three interaction scenarios. Each row represents one of three unique female-male interactor pairs. b. Two example alone stimuli (created from a given dyad stimulus). 
  Fig. 1   

Within each interaction scenario (e.g. arguing), 4 exemplar videos were created, each using a unique pair of action-gestures, such that each video showed the two individuals performing a complementary pair of action-gestures (e.g. while arguing, interactor A accusatorily points at interactor B who is shaking their hands in frustration). Importantly, no gestures were ‘reused’ in any of the other action-gesture pairings (i.e. a total of 8 action gestures were used across the 4 exemplar videos for each scenario). Similarly, 3 different female-male   interactor pairs   enacted these scenarios, yielding a total of 36 dyad stimuli: 3 interaction scenarios (arguing, celebrating, laughing) x 4 unique action-gesture pairings x 3 interactor pairs. The final stimuli were chosen from a wider set of stimuli based on the highest ‘interactive-ness’ and ‘naturalness’ ratings from a pilot study (N = 10; see supplementary materials A). 

For these stimuli, the average horizontal distance between actors was closely matched – the visual angle between the centre of each actor's torso was approximately 4.80°, and actor height ranged between 3.73 and 4.26°. As dynamic facial information is known to activate the STS (e.g.  ), the presence of facial information was controlled such that classification could not be attributed to different facial expressions. Accordingly, these stimuli did not contain high spatial frequency face information, but body information was preserved. To achieve this, a circle-shaped Gaussian blur mask was placed on each of the actors' heads for each video frame. This preserved the overall shape of the head, preventing the potentially eerie appearance of headless interacting bodies. 

To test neural responses to the same interactive information – but without specifically   dyadic information   (i.e. information available from two interactors presented simultaneously) – a separate set of 72   alone stimuli   were created by removing either individual from each of the 36 dyad stimuli (see  b for examples of two alone stimuli). It is important to note that although these stimuli depicted an isolated interactor by themselves, they still conveyed interactive information (e.g. communicative gesturing towards an implied interactor). Two horizontally-flipped variants of these 108 unique stimuli (36 dyad ​+ ​72 alone stimuli) resulted in a final set of 216 stimuli. 


### Design & procedure 
  
A rapid event-related design was used, and each run was optimized using optseq2 (  http://surfer.nmr.mgh.harvard.edu/optseq  ), based on differentiating 6 conditions (i.e. both dyad and alone variants of the arguing, celebrating, and laughing interaction scenarios), with an inter-stimulus interval range between 0 and 10s (along with 8s fixation at the beginning of each run, and 16 s at the end to capture most of the haemodynamic response). The 6 designs with the highest detection sensitivity were selected to determine event timings for runs. 

Inside the scanner, participants viewed stimuli that were presented centrally on the screen within a 9.17 × 5.11° rectangular space. 6 runs were completed, each lasted exactly 7 minutes and contained 8 stimuli for each dyad version and 16 stimuli for each alone version of each of the 3 scenarios, resulting in 72 experimental stimuli per run. Three important stimulus ordering considerations are also noted here: Firstly, left and right horizontal presentations of each stimulus were balanced within the design, such that any resulting effects could not be attributed to low-level confounds in the horizontal position of interactors (i.e. left and right horizontally-flipped variants of the stimuli appeared equally often); secondly, that any given pair of alone stimuli (i.e. that originated from the same dyad stimulus) were always presented in the same run as each other so that classification of alone stimuli did not contain additional between-run variance that was not present for the dyad stimuli; thirdly, to minimize repetition effects (i.e. seeing the exact same action-gestures from a given dyad stimulus and the corresponding pair of alone stimuli), alone stimuli that appeared in any given run were always from dyad stimuli that were allocated to a different run. 

In addition to the stimuli already described, nine additional catch stimuli were presented (three dyad stimuli, and six alone stimuli) but were not later analysed. These trials contained a ‘frame-freeze’ in which 12 consecutive video frames (duration = 500 ms) were randomly removed from the video and replaced with one repeated frame for that period, creating the impression of a momentary video pause. Participants were instructed to simply watch the videos and to give a button-press response whenever a frame-freeze was detected, and to refrain from making explicit judgements about the interactors. 


### Localizer tasks & ROI creation 
  
Participants completed several localizer tasks in a separate scanning session, on a separate day (see supplementary materials B for full description of these tasks). Briefly explained, three different video tasks were used to localize brain regions that are sensitive to different types of social information: 1) A point-light figure social interaction task similar to that used previously ( ;  ) was used to localize interaction-selective pSTS (pSTS-I) regions of interest (ROI) with the interaction > scrambled interaction contrast (i.e. two intact human figures interacting vs. spatially scrambled versions of the same stimuli in which body and interactive information was disrupted). 2) A dynamic body and face localizer that was adapted from stimuli used previously ( ) – this served to localize body-selective EBA and face-selective STS cortex (i.e. STS-F), with the bodies > objects, and faces > objects contrasts, respectively. 3) A free-viewing animated film (‘Partly Cloudy’; Pixar Animation Studios:   https://www.pixar.com/partly-cloudy  ) identical to that used previously ( ) was used to localize mentalizing-selective TPJ-M with the mentalizing > pain contrast (i.e. mentalizing > pain time-points). 

These tasks allowed for the localization of 4 bilateral subject-specific ROIs (i.e. pSTS-I, EBA, STS-F, & TPJ-M; see supplementary materials C for a visualization of these ROIs). These ROIs were created with a group-constrained definition procedure (e.g.  ) as follows. For a given subject and contrast (e.g. interaction > scrambled interaction, for the pSTS-I), a 5 mm-radius ‘search sphere’ was created by running a whole-brain analysis for N-1 group subjects (i.e. with the ‘current’ subject excluded) and centring the sphere at the peak voxel (i.e. highest t-value) in the designated region. This relatively small sphere was chosen to ensure subject's ROIs did not deviate too far from a given designated anatomical region (e.g. pSTS). To determine the position of the final ROI, a whole-brain analysis for the current subject (for the same contrast) was run, and resulting activation was constrained to the search sphere. A 7 mm-radius sphere was then centred at the peak voxel in this search region; this ROI sphere size was chosen as an ideal compromise between capturing a relatively large number of voxels that would benefit classification performance (e.g.  ), and ensuring minimal overlap between neighbouring STS ROIs. 

All ROIs contained 179 voxels, with the exception of two subjects that had small regions of overlap between the right pSTS-I and right TPJ-M, and a further two subjects with similar overlap between the right pSTS-I and right STS-F. Across these four subjects, a mean overlap of 18 voxels (range: 12–24) was found. To ensure independence of ROI voxels within each of these four subjects, overlapping voxels were removed and ROIs were recreated (respective final ROI sizes for these four subjects were: 167, 161, 161, 155 voxels; all other ROIs for these subjects contained 179 voxels). 


### MRI parameters, pre-processing, & GLM estimation 
  
Scanning was performed with a Philips 3T scanner at Bangor University. Functional images were acquired with the following parameters: T2*-weighted gradient-echo single-shot EPI pulse sequence; TR ​= ​2000 ​ms, TE ​= ​30 ​ms, flip angle ​= ​83°, FOV(mm) ​= ​240 ​× ​240 x 108, acquisition matrix ​= ​80 ​× ​78 (reconstruction matrix ​= ​80); 36 contiguous axial slices were acquired, with a reconstructed voxel size of 3 mm . Four dummy scans were discarded prior to image acquisition for each run. Structural images were obtained with the following parameters: T1-weighted image acquisition using a gradient echo, multi-shot turbo field echo pulse sequence, with a five echo average; TR = 12 ms, average TE = 3.4 ms, in 1.7 ms steps, total acquisition time = 136s, FA = 8°, FOV = 240 × 240, acquisition matrix = 240 × 224 (reconstruction matrix = 240); 128 contiguous axial slices, acquired voxel size (mm) = 1.0 × 1.07 x 2.0 (reconstructed voxel size = 1 mm ). 

Pre-processing was performed with SPM12 (fil.ion.ucl.ac.uk/spm/software/spm12). This entailed slice-timing correction, re-alignment (and re-slicing), co-registration, segmentation, normalization, and smoothing. All default parameters were used except for a 6 mm FWHM Gaussian smoothing kernel. General linear model (GLM) estimation was performed in SPM12 on participants’ normalized images. For the main task, whole-brain beta maps were generated on a run-wise basis with events estimated as 6   classification conditions   – both dyad and alone variants of the arguing, celebrating, and laughing stimuli. One further set of maps were created where each event was modelled separately, to allow for stimulus-wise analyses (see supplementary materials D). 


### SVM classification analyses 
  
Leave-one-run-out linear support vector machine (SVM) classification was implemented with CoSMoMVPA ( ). Briefly explained, for a given subject, an SVM classifier was trained on ROI voxels (i.e. beta values) for the conditions of interest (e.g. dyad variants of the arguing, celebrating, and laughing conditions) in all but one run of data – with the ‘left-out’ run of data used to independently test classification performance on. This was iterated 6 times with each run serving as the left-out test run, and classification accuracy was averaged across iterations. These values were then entered into group level   t  -tests. All reported tests were significant at the corrected Bonferroni threshold (α) unless otherwise stated. A different threshold was calculated separately for each set of analyses (i.e. based on 8, 8, & 4 comparisons for dyad, alone, and cross-classification analyses, respectively), as stated in each sub-section in the results. All   t  -test   p-  values are one-tailed. 

This approach was almost identical for both ‘standard’ classification (e.g. between the three dyad conditions, or between the three alone conditions) and   cross-classification   analyses except that the allocation of training and test conditions differed; that is, for cross-classification, the classifier was trained on the three dyad conditions, but   tested   on the three alone conditions. Significant cross-classification demonstrates that the patterns underlying the two sets of conditions are similar to each other, and therefore are largely driven by the same information. However, we reasoned that if a region showed significantly greater dyad classification than cross-classification (i.e. between dyad and alone conditions), this would indicate sensitivity to dyadic information that could not be ‘recovered’ from the individual interactors presented in isolation (i.e. averaged responses to alone stimuli). As explained previously (see section  ) several stimulus ordering constraints were imposed within each run, and importantly, alone stimuli from a given dyad stimulus were always presented in a different run to minimize repetition effects. Notably, this likely resulted in a   more conservative   estimation of the dyadic information effect due to greater similarity between stimuli in test and train data splits for cross-classification, than for ‘standard’ classification (see supplementary materials E for further details). 



## Results 
  
### SVM classification analyses 
  
For each of the 8 functionally localized ROIs, a series of analyses were performed in which a linear SVM classifier was trained and tested on different variants of the 3 interaction scenarios (i.e. arguing, celebrating, and laughing). One-sample   t  -tests were used to determine whether classification accuracy was above chance level (i.e. 100% / 3 categories = 33.3% chance accuracy; Bonferroni corrected α = 0.006). 

Significant above-chance classification of the three interaction scenarios of dyad stimuli (see  ) was observed in the right pSTS-I (Classification accuracy (%):   M   = 41.39,   SD   = 9.10;   t   (19) = 3.96,   p   < .001) and both the right EBA (  M   = 49.38,   SD   = 12.19;   t   (17) = 5.59,   p   < .001) and left EBA (  M   = 50.88,   SD   = 13.00;   t   (18) = 5.88,   p   < .001), and at an uncorrected threshold in the left pSTS-I (  M   = 38.60,   SD   = 10.55;   t   (18) = 2.17,   p   = .022). None of the 4 other ROIs – bilateral STS-F and TPJ-M – showed above-chance classification of the dyad stimuli (all   ps   > .100; see  ; see supplementary materials F for full statistics).   
A bar chart showing classification accuracy values for dyad, alone, and cross-classification analyses for bilateral pSTS-I and EBA ROIs. Dashed line represents chance-level accuracy (33.3%). *** ​= ​  p   ​≤ ​.001; ** ​= ​  p   ​≤ ​.010; * ​= ​  p   ​≤ ​.05; +=   p = .  073. Error bars are SEM. 
  Fig. 2     
A bar chart showing classification accuracy values for dyad and alone classification for bilateral STS-F and TPJ-M ROIs. Dashed line represents chance-level accuracy (33.3%). No results were significant. Error bars are SEM. 
  Fig. 3   

It is possible that significant classification of dyad stimuli in the bilateral pSTS-I and EBA does not completely rely on inherently dyadic information, and may also encode information conveyed by isolated individuals (e.g. interactive gestures directed towards an implied – but physically absent – interaction partner). To test if this was true, another classification analysis (Bonferroni corrected α = 0.006) was run to see if these regions could differentiate the three interaction scenarios for the alone stimuli (see  ,  ). It is worth reiterating that the   same overall information   was present as in the dyad classification analysis (i.e. same scenarios, actors, & gestures). Above-chance classification was shown in right pSTS-I (  M   = 43.33,   SD   = 12.57;   t   (19) = 3.56,   p   = .001) but only marginally in left pSTS-I (  M   = 37.43,   SD   = 12.81;   t   (18) = 1.39,   p   = .090). Both right EBA (  M   = 46.30,   SD   = 7.86;   t   (17) = 7.00,   p   < .001), and left EBA (  M   = 46.49,   SD   = 6.73;   t   (18) = 8.52,   p   < .001) also showed significant classification. As for dyad classification, bilateral STS-F and TPJ-M ROIs did not show above-chance classification (all   ps   > .088), and therefore, these regions were excluded from further analyses. 

Together, these two classification analyses demonstrate interaction sensitive responses in the right pSTS-I and bilateral EBA regions, and to a marginal extent in the left pSTS-I; specifically, these regions were able to differentiate between the three different interaction scenarios, both when observing an intact dyad and when observing the same constituent interactors presented in isolation. However, although these regions are sensitive to both modes of presentation, this does not mean that the underlying information driving classification in both dyadic and alone scenarios is the same (e.g. information about the spatial-relations between interactors may contribute to classification of the dyad stimuli, but not the alone stimuli). Indeed, if voxel pattern classification in any region does not fully generalise from dyad stimuli to the alone stimuli, this would suggest that there is information encoded by these regions during dyadic interaction perception that cannot be recovered by the same information presented in the alone stimuli. 

Next, a cross-classification analysis was implemented (Bonferroni corrected α = 0.013) whereby an SVM classifier was trained to discriminate responses to the three interaction scenarios with the dyad stimuli, but performance was tested on responses to the alone stimuli. Significant cross-classification was shown for all 4 ROIs (right pSTS-I:   M   = 41.39,   SD   = 8.92;   t   (19) = 4.04,   p   < .001; left pSTS-I:   M   = 40.64,   SD   = 9.63;   t   (18) = 3.31,   p   = .002; right EBA:   M   = 43.21,   SD   = 7.75;   t   (17) = 5.40,   p   < .001; left EBA:   M   = 46.20,   SD   = 11.27;   t   (18) = 4.97,   p   < .001), demonstrating that these regions encode similar information in both the dyad and alone stimuli. 

To test for the main hypothesis (i.e. a dyadic information effect) paired   t  -tests were then performed (Bonferroni corrected α = 0.013) between dyad classification accuracy scores and cross-classification accuracy scores. No difference was observed for either the right pSTS-I (  t   (19) = 0.00,   p   = .500) or left pSTS-I (  t   (18) = −0.73,   p   = .763), showing no dyadic information effect, indicating that the main hypothesis was not supported. However, significantly greater accuracy for dyad classification than cross-classification was shown in the right EBA at an uncorrected level (  t   (17) = 2.07,   p   = .027). A similar, although weaker, marginal effect was also shown in the left EBA (  t   (18) = 1.52,   p   = .073). Therefore, evidence suggestive of a dyadic information effect was shown in the bilateral EBA only. 

To determine whether regions outside the functionally defined ROIs demonstrated a dyadic information effect, whole-brain searchlight analyses ( ) were performed (see supplementary materials G for a full description of searchlight methods and results). Peak classification accuracies (i.e. for dyad and alone classification separately, and also for cross-classification) were observed in the bilateral lateral occipito-temporal cortex (LOTC) and pSTS, along with weaker responses in other areas. However, no dyadic information effects were observed in the LOTC/EBA for this analysis (or in any other brain region), further demonstrating the subtle nature of the effect in the ROI analysis. 


### Reliability of the dyadic information effect in EBA 
  
Due to the marginal nature of these results in the EBA, several follow up tests were performed to determine the reliability of this effect. First, Cohen's   d   effect-sizes were calculated for both the right and left EBA. A medium effect-size was found for the right EBA (  d   = 0.60), and a small-to-medium effect was shown in the left EBA (  d   = 0.38). 

To ensure that these effects were not spuriously driven by the ‘direction’ of cross-classification training and testing roles, cross-classification was performed again, but with the training and testing roles reversed. That is, the classifier was now trained on the   alone   stimuli and tested on the   dyad   stimuli. Both right EBA (  M   = 43.83, SD = 7.60;   t   (17) = 5.86,   p   < .001) and left EBA (  M   = 46.20, SD = 10.32;   t   (17) = 5.43,   p   < .001) showed significant cross-classification. Crucially, dyadic information effects were replicated; greater accuracy for dyad classification than cross-classification was again shown in the right EBA (  t   (17) = 2.03,   p   = .029;   d   = 0.55) and marginally in the left EBA (  t   (18) = 1.41,   p   = .088;   d   = 0.40). 

One further test was performed to determine how reliable these effects were across different ROI sizes (i.e. in addition to the original 7 mm radius ROIs, 5, 6, 8, 9, 10, 11, & 12 mm radius ROIs were created). Consistent with the dyadic information effect in the in the original right EBA ROI, greater accuracy for dyad classification than cross-classification was shown across all ROI sizes, but was most pronounced in larger ROIs (i.e.   ps   < .05 for 8, 9, 11, & 12 mm radii; see supplementary materials H). By contrast, in the left EBA, the dyadic information trend was only shown for smaller ROI sizes (i.e. 5 mm radius:   p   < .05; 6 and 7 mm radii: marginal   ps   ≤ .073); indeed, these hemispheric differences appear to be consistent with larger regions of body selectivity in the right than left EBA as previously reported ( ). 


### Results summary 
  
In summary, although right pSTS-I – and marginally, left pSTS-I – differentiated between the three interaction scenarios, no evidence for specific   dyadic information   encoding was observed in these regions. Instead, this effect was observed in the right EBA at an uncorrected threshold (the data for this analysis are available to download; see supplementary materials I). Follow-up analyses demonstrated that this effect was reliable and interpretable, and is further supported by similar (although weaker) effects in left EBA. Control analyses revealed that these effects are not accounted for by low-level differences in stimulus motion energy between conditions (see supplementary materials J). Additionally, exploratory representational similarity analyses were also performed to further characterize EBA responses to dyad and alone stimuli (see supplementary materials D). 



## Discussion 
  
### Overview of results 
  
The present study aimed to determine whether the pSTS – or any other posterior temporal lobe region – showed sensitivity to   unique   dyadic information in visually observed interactive scenarios that is not present for isolated individual interactors. Two main findings were shown: 1) EBA – but not pSTS – showed evidence consistent with the encoding of unique dyadic information; 2) pSTS (and EBA) classified between three interaction scenarios (i.e. arguing, celebrating, & laughing) replicating similar differentiation of types of interactions between abstract moving shapes ( ;  ). 


### Interaction classification in the pSTS & EBA 
  
Specifically, which type of information might drive differentiation of interaction scenarios in the pSTS and EBA? The pSTS plays an important role in biological motion perception (e.g.  ;  ;  ), and is strongly responsive to movement contingencies between interacting figures (e.g.  ), as well as dynamic cues that imply interactive behaviour between animate moving shapes ( ;  ). Similarly, the pSTS is also sensitive to the intentional contents of actions ( ;  ;  ). It therefore seems plausible that classification in the pSTS is driven by differential intentional content between interaction scenarios that is extracted from different dynamic contingencies between interactors. 

Additionally, the EBA also classified between interaction scenarios. A direct interpretation of this result is that body posture information contributes strongly to the differentiation of these three scenarios. EBA is shown to be sensitive to dynamic postural information (i.e. continuous sequences of body postures that form coherent actions) and is suggested to encode body-based actions ( ). In the current study, distinctively different sequences of coherent body postures – or action-gestures – may have driven classification of interaction scenarios. Although distinct action-gestures were used within each interactive scenario, these tended to be relatively similar to each other (e.g. arguing gestures usually depicted short, sharp movements, while laughing gestures typically contained convulsive torso movements). Therefore, it seems possible that classification of interaction scenarios in the EBA was likely the result of similar action-gestures   within   each scenario, that were markedly different   across   the three scenarios. 


### No dyadic information effect in the pSTS 
  
Despite the pSTS classifying interactive scenarios, the main prediction was not supported; no   dyadic information effect   was observed for the pSTS. This contrasts with the findings of   that showed an analogous effect in the pSTS for static depictions of human-object (inter)actions compared to the averaged responses to isolated objects and humans. One possible explanation for this concerns STS sensitivity to implied biological motion in static images ( ;  ); static human-object interactions might imply greater biological motion or more effortful movement that is not ‘recoverable’ from isolated human and objects; for example, an image of a person pushing a cart implies greater movement than the same body pose and cart presented separately, by virtue of greater physical effort required to move the cart, along with the corresponding impression that the cart is moving. Additionally, pSTS sensitivity to causal contingencies (e.g. a billiard ball hitting another, causing a transfer in motion;  ) suggests the strong influence of physical contact in human-object interactions that was not present in the isolated stimuli. By contrast, the current study used dynamic stimuli that contained biological motion information but no physical contact, and as such, the dyad and alone stimuli were closely matched for these two sources of information that might have driven responses to the stimuli used by  . 

Although no dyadic information effect was found in the pSTS, it is important to note that interactive information was still conveyed in the alone stimuli (e.g. communicative gesturing to an unseen interactive partner was strongly implied). Therefore, successful classification of the alone stimuli does not necessarily reflect that pSTS responses are non-interactive. Indeed, in the context of the sorts of gestural interactions used in the current study, it is possible that classification of the alone and dyad stimuli relied on the same cues (i.e. communicative gestures). Similarly, the current data supports the possibility that representations of interactions in this region may encode the presence of two interactors in a linear fashion (i.e. dyad = the average of the two individuals). Alternatively, it is possible that the pSTS responses to both dyad and alone stimuli are driven by interactive gestures ‘directed’ at another individual, regardless of whether the other individual is present or not. 


### Dyadic information processing in the EBA 
  
Although not observed for the pSTS, a dyadic information effect was shown for the right EBA and to a lesser extent, the left EBA. Although not predicted, this does fit with previous findings observed in the wider LOTC area. Specifically,   observed differentiable responses to human-object interactions than averaged responses to humans and objects in object-selective LOTC (i.e. LOC – in close proximity to EBA); however, this trend did not quite reach significance in the EBA, likely due to weaker responses to object stimuli, suggesting that the currently observed EBA responses could be specific to human body information. Recent evidence also shows that object-selective LOTC is sensitive to ‘regular’ spatial configurations of objects that imply a congruent scene (e.g. different responses are shown for scenes that depict a sofa positioned in front of a television, rather than behind it;  ). Similarly, object-selective LOTC is sensitive to spatial configurations of objects that imply an action (e.g. a pitcher tilted towards an empty cup), relative to configurations that do not ( ). 

Broadly, these findings might suggest a converging role for configural processing of   distinct   objects and people in the LOTC. In relation to the present findings, it is conceivable that LOTC – and here the EBA specifically – performs similar configural processing or grouping based on the action-, body-, and movement information conveyed by interactors. If true, to what extent does   dynamic   information contribute to this effect? In contrast to previous work investigating LOTC grouping responses for static stimuli ( ;  ;  ), the current study used dynamic stimuli. Although the EBA is highly sensitive to static pose information, and may process body movements as a series of static ‘snapshots’ ( ;  ) body (and face) responses are shown to generalise across static and dynamic depictions in broad regions of the posterior temporal cortex ( ). Similarly, representations in the LOTC generalise across dynamic and static depictions of actions and are invariant to other low-level features such as movement direction, or the specific hand used to perform an action ( ;  ). 

In line with these findings, it is likely that dyadic representations of (inter)actions in the EBA generalise across static-dynamic depictions. While dynamic information may not be necessary to encode such scenarios, it may, potentially, allow for more elaborate encoding of body-based actions than similar, static depictions. Additionally, other spatial cues (e.g. interpersonal distance, physical contact, and facing direction), and temporal cues (e.g. movement contingencies and correlated motion) may also contribute to dyadic encoding in the EBA, and further research may directly clarify which cues contribute most prominently. 

It is also worth briefly considering the extent to which dyadic information processing is present for other types of interaction, for example, interactions depicted by moving geometric shapes that do not contain body information. These types of stimuli are known to drive responses in LOTC, ostensibly due to the presence of simple actions such as pushing and pulling movements ( ). As mentioned previously, the wider LOTC area shows some sensitivity to spatial-temporal relations between interacting or scene entities, and therefore cortex in close proximity to (and overlapping with) EBA might plausibly encode dyadic information for these abstract scenarios. 

The present stimuli consisted of interactions between individuals that did not involve physical contact, a potentially powerful interaction cue that is worthy of further investigation; indeed, stronger dyadic information effects might be predicted for contact-based interactions (e.g. two individuals shaking hands), by virtue of categorical differences in physical contact (i.e. presence of physical contact in dyadic interactions vs. absence of physical contact in ‘alone’ variants of these stimuli). 


### Conclusion 
  
In summary, the present results show that both EBA and pSTS differentiate between different types of social interactions. Crucially, representations of dyadic social interactions in the EBA are sensitive to information beyond that which is encoded by the simple average of two separate interactors presented in isolation. This so-called   dyadic information effect   suggests that the EBA is sensitive to unique interactive information that is present only when two individuals interact simultaneously. These findings complement previously observed sensitivity in the wider LOTC area to spatial configurations of objects or bodies that support the processing of holistic, congruent scenarios. 



## Author contributions 
  
J.W & K.K: study design, data-collection, analysis, writing, and editing. 


## Conflicts of interest 
  
None declared. 


## Funding 
  
This work has received funding from the   under the   (ERC starting grant: Becoming Social). 

 </div>
</div>
</div>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-29' name='judgment-29' value='agree'>
<label for='agree-29'>Agree</label>
<input type='radio' id='disagree-29' name='judgment-29' value='disagree'>
<label for='disagree-29'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-29' name='comment-29' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-30'>
<h2>30. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/24412687/' target='_blank'>24412687</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI task study of social processing (multiround Ultimatum Game). It includes a healthy adult group (n=15; ages 19–28) reported separately from child and adolescent groups. The paper reports group-level, whole-brain univariate analyses: one-sample t-tests per group and between-group full-factorial ANOVAs, and the primary contrast (unfair > fair offers) is presented with whole-brain statistical maps and coordinate tables (Table 1) thresholded at p<.001 uncorrected. Results for the healthy adult group are explicitly reported and generalizable to healthy adults (within the 17–65 range). The study is empirical and provides whole-brain task-evoked activation maps/coordinates for the healthy adult sample, so it meets all inclusion criteria and does not violate exclusion criteria.</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-30' name='judgment-30' value='agree'>
<label for='agree-30'>Agree</label>
<input type='radio' id='disagree-30' name='judgment-30' value='disagree'>
<label for='disagree-30'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-30' name='comment-30' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-31'>
<h2>31. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/20923708/' target='_blank'>20923708</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study using a social-related task (Cyberball social exclusion) in healthy participants and includes a distinct healthy adult group (ages ~23.9–38.8). The paper reports group-level, univariate, whole-brain task-evoked analyses (exclusion > inclusion) with cluster-level FWE-corrected results (Table 2) and voxel coordinates for significant clusters. It also reports a Condition × Age-group interaction and post-hoc within-adult t-tests (adult exclusion > inclusion; right vlPFC) with coordinates and statistics. Analyses are not limited to ROI or connectivity-only approaches. Thus the study provides whole-brain, univariate task activation results from healthy adults and meets the inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.85</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-31' name='judgment-31' value='agree'>
<label for='agree-31'>Agree</label>
<input type='radio' id='disagree-31' name='judgment-31' value='disagree'>
<label for='disagree-31'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-31' name='comment-31' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-32'>
<h2>32. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/31063817/' target='_blank'>31063817</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of healthy adults (N=78, age 18–40) that includes multiple social-related tasks (false belief/ToM, animacy/intentional triangles, eyes intention, trustworthiness, vocalizations) and reports group-level, voxelwise whole-brain random-effects analyses (RFX) with results thresholded at p<0.05 FWE and extensive statistical maps/tables/figures. Healthy-group results are reported and generalizable to the adult sample. The paper provides univariate task-evoked whole-brain contrasts (both main effects and conjunctions) for social cognition contrasts, satisfying all inclusion criteria and violating none of the exclusions (not ROI-only, not connectivity-only, includes within-range adults). Therefore it should be included.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-32' name='judgment-32' value='agree'>
<label for='agree-32'>Agree</label>
<input type='radio' id='disagree-32' name='judgment-32' value='disagree'>
<label for='disagree-32'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-32' name='comment-32' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-33'>
<h2>33. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/23267322/' target='_blank'>23267322</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI task study of social cognition contrasting interactive vs. observational conditions using dynamic video stimuli. Sample: 22 healthy adult participants (mean age 25) within the 17–65 range. Group-level, task-evoked analyses were conducted (two group RFX whole-brain ANOVAs) and reported; although ROI small-volume corrections were emphasized for main effects, the authors also report exploratory whole-brain results (early visual/V1) and describe group-level task effects thresholded at p<0.05 FWE. Thus the paper reports univariate, group-level task-evoked maps from healthy adults during a social-related task and meets the inclusion criteria (not limited to ROI/connectivity-only results).</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-33' name='judgment-33' value='agree'>
<label for='agree-33'>Agree</label>
<input type='radio' id='disagree-33' name='judgment-33' value='disagree'>
<label for='disagree-33'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-33' name='comment-33' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-34'>
<h2>34. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/27129794/' target='_blank'>27129794</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study in healthy adults (N=178, mean age 40.9, within 17–65). It uses a task-based social paradigm (EmpaToM) manipulating empathy and Theory of Mind, with participants performing video-based social tasks during fMRI. The paper reports group-level, whole-brain univariate task contrasts (emotional vs neutral videos; ToM vs non-ToM questions/videos) analyzed with random-effects one-sample t-tests and presents activation peaks and MNI coordinates and figures. Although connectivity/DCM and resting-state analyses are also reported, they do not replace the clear whole-brain task-evoked maps required by the inclusion criteria. Therefore all inclusion criteria are met and no exclusion criteria are violated.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-34' name='judgment-34' value='agree'>
<label for='agree-34'>Agree</label>
<input type='radio' id='disagree-34' name='judgment-34' value='disagree'>
<label for='disagree-34'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-34' name='comment-34' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-35'>
<h2>35. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/19630891/' target='_blank'>19630891</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of self-related social cognition (direct and reflected self-appraisals), involving a healthy adult sample (N=12, ages 22.6–30.4). The paper reports group-level, univariate whole-brain task contrasts and conjunction analyses for adults (e.g., direct and reflected appraisals vs. rest; reflected>direct), with coordinates and descriptions of whole-brain results. The study is empirical, not ROI-only or connectivity/resting-state only, and healthy adult results are reported separately alongside adolescent data. Therefore it meets all inclusion criteria.</p>
<p><strong>Fulltext Confidence:</strong> 0.98</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-35' name='judgment-35' value='agree'>
<label for='agree-35'>Agree</label>
<input type='radio' id='disagree-35' name='judgment-35' value='disagree'>
<label for='disagree-35'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-35' name='comment-35' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-36'>
<h2>36. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/28455517/' target='_blank'>28455517</a></h2>
<div class='metadata'>
<h3>Metadata</h3>
<p><strong>Title:</strong> Neural Activity while Imitating Emotional Faces is Related to Both Lower and Higher-Level Social Cognitive Performance</p>
<p><strong>Authors:</strong> N/A</p>
<p><strong>Journal:</strong> Sci Rep</p>
<p><strong>Publication Year:</strong> 2017</p>
<p><strong>DOI:</strong> 10.1038/s41598-017-01316-z</p>
<p><strong>PMCID:</strong> <a href='https://www.ncbi.nlm.nih.gov/pmc/articles/5430668/' target='_blank'>5430668</a></p>
</div>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> Meets inclusion criteria: (1) Uses task-based fMRI of social processing (imitate/observe emotional faces — social perception/affiliation/communication). (2) Sample comprises healthy adults (initial N=28, final N=20 after preprocessing; ages 18–55 within 17–65 range) with group-level analyses reported. (3) Provides whole-brain, voxelwise results: authors ran a GLM in SPM8 (reported as showing similar whole-brain activation patterns; Supplementary Figure 1) and present whole-brain voxel maps from the PLS analysis (voxelwise bootstrap ratio maps). Not ROI-only or connectivity-only. No exclusion criteria violated. Therefore include as an fMRI whole-brain study of social processing in healthy adults.</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p><strong>Abstract:</strong>  
Imitation and observation of actions and facial emotional expressions activates the human fronto-parietal mirror network. There is skepticism regarding the role of this low-level network in more complex high-level social behaviour. We sought to test whether neural activation during an observation/imitation task was related to both lower and higher level social cognition. We employed an established observe/imitate task of emotional faces during functional MRI in 28 healthy adults, with final analyses based on 20 individuals following extensive quality control. Partial least squares (PLS) identified patterns of relationships between spatial activation and a battery of objective out-of-scanner assessments that index lower and higher-level social cognitive performance, including the Penn emotion recognition task, reading the mind in the eyes, the awareness of social inference test (TASIT) parts 1, 2, and 3, and the relationships across domains (RAD) test. Strikingly, activity in limbic, right inferior frontal, and inferior parietal areas during imitation of emotional faces correlated with performance on emotion evaluation (TASIT1), social inference - minimal (TASIT2), social inference - enriched (TASIT3), and the RAD tests. These results show a role for this network in both lower-level and higher-level social cognitive processes which are collectively critical for social functioning in everyday life. 
 </p>
<button class='accordion' onclick='toggleAccordion(this)'>Full Text Content (34238 characters)</button>
<div class='panel'>
<div class='panel-content'>
<div class='fulltext-content'> 
## Introduction 
  
Social interactions are complex processes which are built upon the perception and understanding of the actions, intentions, and mental state of others. Social cognitive neuroscience has suggested a dichotomy between low-level processes such as understanding motor actions or emotional processing and higher-level processes such as inferring the mental states of others (e.g. theory of mind) or detecting deception and sarcasm. This apparent dichotomy between lower level and higher level social cognitive processes is supported by behavioral , neuroimaging  and lesion studies . Neuroimaging studies have primarily examined these processes with two approaches: using tasks relevant to the perception of and interaction with other people’s actions (action observation and imitation), and using tasks relevant to infer the beliefs and desires of the other person (mentalizing). Lower level social cognitive processes such as action interpretation have been associated with activation of a lateral frontal-parietal network, the putative human analogue to the mirror neuron system observed in monkeys . Higher-level social cognitive processes, such as theory of mind, have been associated with activation of cortical midline regions, including the medial prefrontal cortex, posterior cingulate cortex, and precuneus, as well as temporoparietal junction and temporal pole. This network is also known as the ‘mentalizing’ system. Dual processing models have been proposed in which the lateral fronto-parietal network supports relatively automatic processes such as identifying motor actions while the mentalizing network supports controlled processes of attributing actions to complex social causes . 

Most neuroimaging studies examining the lateral fronto-parietal network have made use of hand actions or other forms of simple motor stimuli . As such, the role of this system has been mainly considered in the context of perceiving gross motor actions. Some have taken the concept of mirroring one step further, by arguing that the lateral fronto-parietal network is important for emotional empathy by permitting humans to feel what others feel, and potentially to assess or interpret emotional cues and social behaviour . Investigating this issue may be best served by paradigms utilizing socially relevant stimuli as opposed to simple motor acts. One such paradigm is the imitation and observation of emotional faces, which has been shown to activate the lateral fronto-parietal ‘mirror’ network . This is consistent with the notion that activation of this network can play a role in allowing people to empathize by imitating emotions . Activity in the right IFG has been positively correlated with self-report measures of empathy in healthy children , and has been shown to be more active in normally developing children than those with autism , with activity in autistic children inversely correlated with social impairment. However, the IFG has also been implicated in some higher level social cognitive processes, specifically the inhibiting self-perspective in social judgements . Recent work has also shown structural changes in right fronto-parietal cortex in more socially impaired people with schizophrenia , a group of individuals in whom higher-level social cognitive processes such as theory of mind are more prominently affected . These findings raise the question of whether neural activation during basic mirroring tasks is related to higher-level social cognitive abilities. 

Meta-analysis of fMRI data has shown little overlap between activity in the brain regions associated with the mirroring and mentalizing systems across a range of tasks , and brain lesion studies have suggested a dissociation between mirroring and mentalizing regions . As such, mirroring and mentalizing were initially thought to represent dichotomous systems which function relatively independently, and the role of mirroring in higher-level social cognition is a contentious issue . However, there is a growing body of evidence that the mentalizing and mirroring systems may interact during more complex social cognitive processing, such as watching social videos . Co-activation of mentalizing and mirroring regions has also been noted when viewing agents performing irrational actions, suggesting aspects of the mentalizing system may facilitate interpreting unexpected actions . The mirroring and mentalizing system may work together during social cognitive processing, although a specific role of the mirror system in higher-level cognitive processing has yet to be established. 

The purpose of this study is to determine whether brain regions activated during a facial emotion mirroring task are related to lower-level social cognitive performance, higher-level social cognitive performance, or both. We utilized a process specific social mirroring task (the facial imitate/observe task ) inside the fMRI, along with a battery of objective out-of-scanner social tests ranging from basic emotional-perception tasks to complex higher level social cognitive tasks such as detection of lies or sarcasm. Brain-behavior relationships were examined using the spatio-temporal partial least squares (PLS), a non-parametric multivariate approach . PLS identifies latent variables (LVs), linear combinations of fMRI signal amplitude at each voxel across time and a design matrix which can consist of either experimental conditions or correlations across conditions with behavioral scores. Each LV expresses a pattern of the common covariance between the design and the fMRI data. PLS is designed to handle data with high collinearity while simultaneously capturing essential non-redundant relationships among the data , overcoming the confounding influence of collinear variables in traditional multiple regression . This makes PLS an excellent approach to uncover relationships between brain activity and a set of social cognitive scores which are likely highly interrelated. We hypothesized that a) consistent with previous studies, imitating emotional faces would activate the mirroring network, more prominently in the imitate than the observe condition, and b) neural activity during imitation of emotional faces specifically (rather than imitating neutral faces or observing faces) would be associated with both lower-level and higher-level social cognitive performance. 


## Results 
  
### Task Effects 
  
Task PLS is a multivariate model-free approach to identify spatial patterns which share relationships across the experimental conditions. A task PLS was run on the data to replicate previous task-based analysis, and examine the underlying pattern of task task-evoked spatial activity. PLS generated 6 LVs (as there were three trial types, emotional faces, neutral faces, and fixation, and two conditions, Imitate and Observe). Permutation analysis (see methods below) showed significance (p < 0.05) in the first three LVs, which accounted for 61.6%, and 14.5%, and 12.9% of crossblock covariance respectively (Fig.  ). LV 1 was related to viewing faces as opposed to fixation trials, and showed a pattern related to the lateral frontal-parietal network similar to the results of previous studies . LV 1 was related to viewing faces in general rather than distinguishing between emotional and neutral faces, demonstrating that this pattern of faces >fixation in the mirroring network is the dominant pattern of neural activity within this task analysis. LVs 2 and 3 explain some of the remaining variance in the data not accounted for the pattern presented in LV 1, representing interaction effects showing different patterns of activity across the Imitate and Observe conditions. LV 2 shows a large amount of overlap with LV 1, suggesting some of the variance within those regions is explained by aspects of both LVs 1 and 2. The majority of the variance explained by LV 1, but some additional variance within those voxels is explained by the pattern of emotional faces >neutral and fixation in Imitate and increased activity in fixation trials for Observe, as seen in LV 2. LV 3 represents an interaction in which the relative effects of neutral faces and fixation are different in the Imitate and Observe tasks.   
Results from the first latent variable using task PLS analysis. Data is shown for LV1, 2 and 3. The top panel shows the design pattern (the contribution of each condition to the LV). Error bars are 95% confidence interval derived from the bootstrap analysis. The bottom panel shows the voxel pattern from the first lag of the PLS analysis rendered on the cortex. Sagittal slices are shown for X = −10 (left) and X = 10 (right). Voxel intensity is displayed as bootstrap ratio, a measure of reliability of voxels within the LV. Red-yellow regions show the pattern described by the design pattern, while blue regions show the opposite pattern. 
  

For consistency and replication of previous studies, we also ran a general linear model (GLM) in SPM8. The GLM showed a similar pattern as in previous studies and LV1 of the task PLS (see supplementary methods and Supplementary Figure 1). 


### Behavioral PLS 
  
Behavioral PLS examined the relationships between patterns of activity in the brain and social cognitive test scores. Scores from six social cognitive tasks were included: the Emotion Recognition (ER40) , the Reading the Mind in the Eyes Test, RMET , the Relationships Across Domains test (RAD) , and the three subtests of the Awareness of Social Inference Test Revised (TASIT-R) . TASIT1 can be considered a test of lower-level social cognitive functioning, while TASIT2 and TASIT3 are high-level tests. The behavioral PLS generated 36 LVs, as there were six experimental conditions (emotional faces, neutral faces, and fixation, separately for Imitate and Observe, and six social cognitive tests). Overall permutation analysis showed significance in the first two LVs, which accounted for 23% and 13.2% of crossblock covariance respectively. However, the split-half permutation reliability analysis (see methods) demonstrated that the relationship between the design pattern and brain pattern was not reliable within LV 1 and as such it was not considered further. In LV 2, neural activity during imitation of emotional faces was positively correlated with RAD, TASIT1, TASIT2, and TASIT3 (Fig.  , top panel). Neural activity for Fixation and Neutral faces during Imitate, as well as Neutral faces during Observe, was negatively correlated with those same test scores. As such, LV 2 shows a pattern of correlations during the imitation of emotional faces related to performance on TASIT1, TASIT2, TASIT3, and RAD.   
LV 2 of the behavioral PLS results. (  A  ) Design pattern showing the pattern of correlations between brain signal for each during event type and each behavioral score within the voxel pattern. Error bars represent 95% confidence interval based off a bootstrapping analysis of 1000 iterations. (  B  ) Voxel patterns for each lag, displayed as bootstrap ratio on the MNI152 brain. Data is shown for two sagittal views as well as rendered onto the cortex (8 mm search depth) to better visualize cortical activity. Red-yellow shows a correlation pattern matching the design pattern (  A  ), while blue regions show the opposite pattern. Each lag represents a time point (TR) following stimuli onset (which occurred during ‘Lag0’). 
  

The pattern of brain activity in LV 2 (Fig.  , bottom panel) when imitating emotional faces in lag 1 showed a positive relationship with performance on TASIT1 TASIT2, TASIT3, and RAD. Activity was present in right pars opercularis (within inferior frontal gyrus), right and left supramarginal gyrus, right motor cortex, bilateral anterior temporal regions, fusiform gyrus, and parahippocampal cortex. Activity was also noted in right anterior/mid cingulate, right rolandic operculum, right putamen, and the cerebellum. Activity in lags 4 and 5, which positively correlated with social cognitive performance, was notably present in anterior temporal regions as well as in posterior middle and inferior right temporal gyri. Several regions showed negative relationships to the design pattern (thus greater fMRI activity while imitating emotional faces in individuals with poorer social cognitive performance), including the left inferior frontal gyrus (pars operculum and pars triangularis), and bilaterally in the basal ganglia (greatest in left putamen and right globus pallidus). 



## Discussion 
  
The behavioral PLS results of this study showed a robust relationship between neural activity during a process-specific imitate-observe task and both lower-level and higher-level social cognitive performance. While a growing number of studies using complex stimuli have noted co-activation of mirroring and mentalizing regions , co-activity cannot be taken to conclude that mirroring has a strong role in higher level social cognition. The novelty of our study is that we showed complex relationships of neural activity during a simple and process-specific mirroring task with performance on objective out-of-scanner assessments of higher-level social cognitive performance. In addition, the PLS approach allowed us to include several assessments in a single analysis, providing data driven evidence of which assessments best capture these brain-behaviour relationships. These behavioral relationships were specific to imitation of emotional faces, rather than neutral faces. Several regions outside the right fronto-parietal circuit, including left IFG and some mentalizing regions (e.g. TPJ), also showed negative correlations with social cognitive performance, suggesting either compensatory responses or over-activation in participants with poorer social cognition. 

These findings of a relationship between activity in a simple mirroring task and higher level social cognitive processing scores suggest a relationship for the mirror network in higher-order social cognitive processing including complex tasks. Effective mentalizing in humans although likely primarily reliant on classic mentalizing regions may be further subserved by mirroring regions . Given that the relationships elicited were noted in assessments involving interpretation of dynamic interactive video tasks including human actors, it may be that such relationships are only evoked when we consider social cognition as a process involving integrating information from multiple processing levels. The mirroring network may not be solely for understanding the intentions and physical goals of motor acts . Our findings add to the debate regarding the role of this network in social cognition . Several authors have argued that this network is important not only for imitation or action perception, but also for emotional empathy and possibly even in cognitive empathy (sometimes used interchangeably with theory of mind) , consistent with the theory of embodied simulation , despite debate to the contrary . Through the combination of a process-specific in-scanner task, more naturalistic and objective social cognitive tests, and the PLS multivariate framework, we were able to uncover essential brain-behavior relationships for which traditional task contrasts or linear regression analyses may be less sensitive. 

Higher level social cognitive processing involves concepts related to both theory of mind and cognitive empathy, both complex constructs which likely encompass several mental processes. To date, there is only limited evidence of overlap in neural activity between high-level tasks related to the mentalizing network and low-level tasks related to the mirroring network . However, task based analysis is often performed by contrasting conditions, and many mirroring tasks have utilized stimuli with little or no direct social relevance, or tasks designed to evaluate highly specific social cognitive processes which may not be well suited to capturing overlap within these systems . By moving away from an ‘overlapping activity’ perspective into an approach focused on brain-behavior relationships, we were able to demonstrate that activity in the mirroring network is correlated with higher-level social cognitive abilities. This combination of pairing a process-specific task paradigm with dynamic social cognitive measures may be a powerful approach for probing relationships between social cognitive networks and real-life social cognitive abilities. 

The behavioral PLS LV correlating neural activation during imitation of emotional faces with TASIT and RAD scores also included some regions which are often considered part of the mentalizing network. While these regions did not robustly activate during the task PLS, their presence in the behavioural PLS reflects activation during the imitate task that is relevant for social cognitive performance. Positive relationships with social cognition and emotional faces were observed in the temporal pole bilaterally, a region implicated in mentalizing and theory of mind  as well as in processing deception . Activity was also noted in the posterior region of the superior temporal sulcus, which may be of particular interest as this area has been implicated in both the mirroring/imitation network  and the mentalizing network . The superior temporal sulcus may serve as an integrative region, sharing information between these two networks  and/or as serving as a relay point for high level visual information . The superior temporal sulcus has also been implicated in analyzing dynamic facial features , perceiving biological motion , and gaze perception . As such, this region is involved with perceptual processes necessary for both low-level and high-level social cognition. It remains to be seen if this region may serve as an integrative hub between the mentalizing and mirroring networks, though the posterior superior temporal sulcus shares connectivity within both networks  and has been implicated as an important region in social cognitive impairment in autism . 

Studies showing increased connectivity between the right IFG (a critical hub of the mirroring network) and the mentalizing network provide evidence for functional communications between these networks . This connectivity between mirroring and mentalizing regions has been shown to be disrupted in autism , raising the possibility that disrupted social cognition in autism may be driven not only by localized effects within one of these systems but also as a network level disruption in how these systems interact. Likewise, social cognitive deficits are increasingly recognized as a core feature of schizophrenia, strongly related to overall functional outcomes . People with schizophrenia have been noted to show reduced right IFG activity when imitating emotional faces as well as opposite relationships between self-reported empathy and right IFG activity compared to controls . As social cognitive impairment is present at, or potentially preceding the onset of psychosis , it is possible this relatively brief imitate/observe in-scanner task may be useful as a potential early risk marker of the disease. 

We found that increased activity in the left IFG was associated with poorer social cognitive performance, as well as in clusters in the left medial frontal cortex and the TPJ . One study examining resting state network connectivity noted over-connectivity of the left IFG in the mirroring network in participants with autism, consistent with our finding that over activity in the left IFG is associated with poorer social cognitive performance . A number of regions outside the mirroring or mentalizing networks also showed a negative relationship with social cognitive performance, including the basal ganglia, thalamus, occipital cortex and superior colliculus. These regions may be involved in emotional face processing . We propose three possible explanations for the negative correlations between this pattern of neural activity when imitating emotional faces and social cognitive performance: 1) prolonged neural activity related to less efficient processing resulting in a greater magnitude of hemodynamic signal, as may be suggested by increased activity in regions associated with visual and face processing; 2) compensatory activity in which individuals with less efficient processing make greater use of the left IFG to compensate for lack of activity on the right (the compensatory hypothesis); or 3) over activity in the left IFG resulting in competition with right IFG, resulting in interference or an decrease in network coherence and deficits in social cognitive test performance (the dedifferentiation hypothesis). One possible explanation is that participants with poor performance may be attempting to compensate by activating aspects of the mentalizing system during lower-level social cognitive processing. It has been suggested that the anterior cingulate cortex, a region noted in the behavioral PLS, is involved in biasing activity towards either mentalizing or simulation . 

We did not find reliable relationships between the ER40 and RMET with neural activity to emotional faces. The ER40 and RMET rely on static images which fail to capture the full complexity of social cognitive processing, unlike the TASIT which requires interpretation of video scenarios of complex human interactions and the RAD which involves interpreting stories. It may be that more complex and process-general tests are required to more fully elicit relationships between activity in the simulation and mentalizing networks . Supporting this assertion, correlations between TASIT test scores and the observed voxel pattern in the PLS analysis were as high as 0.8, indicating a very strong relationship between mirroring network activity and social cognitive performance measured by TASIT1 (a lower-level task), as well as higher-level tasks TASIT2, and TASIT3. Some recent studies using more naturalistic higher-level social-cognitive paradigms, such as videos, have implicated IFG activity , and a small number of recent studies involving more complex social interaction tasks have noted co-activation in mirroring and mentalizing regions . 

Here we provide evidence that neural activation while imitating emotional faces is related to performance on dynamic and objective ‘real-life’ assessments of even the most complex social cognitive tasks. Given the relatively small sample size, further research expanding into a larger sample would be worthwhile to replicate and extend these findings. However, our findings suggest that patterns of neural activation during basic imitative behaviour may be a surrogate marker for highly complex social function in humans. The paradigm demonstrated here may be particularly useful for early identification studies in neurologic and psychiatric disorders with social cognitive impairment, and in intervention studies using ‘target engagement’ as a marker of early treatment response. 


## Methods 
  
### Participants 
  
Twenty-eight healthy adult participants were initially included in this study. Average age of participants was 34.3 ± 11.7 years, with 18 men and 10 women, 23 right-handed participants, and an average level of education of 15.1 ± 2.22 years. Participants aged 18 to 55 were included in the study. All participants completed the Edinburgh handedness Inventory and were administered the Structured Clinical Interview for DSM-IV disorders (SCID) to rule out possible psychiatric illness. Urine toxicology screening was performed to further ensure that no participant with a current substance use disorder was included in the study. Additionally, exclusion criteria included a first degree relative with a history of psychotic mental disorder, a history of head trauma resulting in unconsciousness, or a history of seizure or other neurological disorders. The protocol was approved by the research ethics board of the Centre for Addiction and Mental Health, University of Toronto, all research was conducted in accordance with the declaration of Helsinki and the tri-council policy statement on Ethic Conduct for Research Involving Humans. All participants gave informed consent, and signed an institutionally approved informed consent form, prior to any research procedures. 


### Social Cognitive Assessments 
  
The Emotion Recognition (ER40) task consists of 40 images of whole faces making emotional expressions, with participants selecting from four possible emotional responses . The Reading the Mind in the Eyes Test (RMET)  consists of 36 trials showing only the eyes of a black and white face, with four possible choices to describe what the person is feeling (e.g. amused, irritated, cautious, contemplative). The RMET is considered a test of empathic abilities as it measures the ability to judge emotional states from looking at the eyes. The Relationships Across Domains (RAD) test  presents 25 written vignettes of 2–4 lines followed by three statements which describe the behaviour of the male-female dyad from each vignette in domains of social life different from that vignette. Participants indicate if the behavior described in each statement is likely or unlikely to occur based on what was learned from the vignettes. The Awareness of Social Inference Test, Revised (TASIT) uses short video vignettes to measure emotional perception and theory of mind . The TASIT is divided into three parts. Part 1 (TASIT1) consists of 24 short videos of actors portraying different emotional states (happy, sad, fear, disgust, surprise, and anger). Part 2 (TASIT2; social inference – minimal) consists of 15 videos showing sincere or sarcastic interactions between two actors, followed by four questions relating to what the actors were thinking, doing, meaning to say, and feeling. TASIT2 is considered a test of theory of mind. TASIT part 3 (TASIT3; social inference – enriched) consists of 15 vignettes in which the speaker is making an assertion which is literally untrue, but can represent either sarcasm or an attempt at deception. Success in TASIT3 requires the ability to detect deception in social encounters. In total, six social cognitive scores were derived from these tests (ER40 reaction time, RMET, RAD TASIT1, TASIT2, and TASIT3). Age was regressed out of all social cognitive test scores. 


### MRI Scanning 
  
MRI scanning was conducted on a Discovery 3 T MR750 machine from General Electric at the Centre for Addiction and Mental Health. The Imitate/Observe task was part of a longer multimodal MRI protocol to which each participant consented. Each block of the task (Imitate and Observe) was collected in counter-balanced order as a separate echo-planar imaging scan, with TRs = 3 sec, TE 30 ms, voxel size 3 mm isotropic, 50 slices, 64 × 64 matrix with FOV = 192 mm, flip angle = 77, and 110 TRs per scan. The first 5 TRs were excluded prior to any preprocessing to allow for magnetic steady-state. 


### Imitate/Observe Task 
  
The participants performed an imitate/observe task as previously described  while being scanned. Participants were shown full-color photographs from an ethnically diverse set of 16 individuals (eight males and eight females) expressing five different facial expressions (fearful, sad, happy, angry, or neutral). During one scanning session, participants were instructed to imitate the expression on the faces (the   Imitate   session), while in the other session participants were instructed to observe the face (the   Observe   session). Each run consisted of 80 faces (16 per facial expression) plus 16 fixation trials presented in a pseudorandomized order determined using Optimize Design 11  to maximize contrast efficiency. Each trial lasted three seconds, with the faces presented for two seconds and a jittered ISI (500 ms to 1500 ms). The order of sessions (Imitate or Observe) was counter-balanced across participants. Participants practiced the task prior to MRI scanning and were instructed to minimize motion during the imitation, only using their facial muscles when matching the expressions. A video recording during the scan confirmed that all participants were following instructions (i.e. imitating during the imitate session but not during observe). 


### fMRI data analysis - Preprocessing 
  
The initial preprocessing stages were slice time corrected and motion corrected in SPM8 (Wellcome Department of Cognitive Neurology, London, UK). Individual sessions were subjected to single-session independent components analysis (ICA, from the MELODIC module in FSL 5.0.6). ICA reports were visually examined, and any ICAs which were clear artifacts were removed based on a set of established criteria. On a case by case basis where some large motions contaminated the ICAs, data ‘scrubbing’ was performed using a spline interpolation. Generally, two TRs were removed for each motion spike, and no more than three TRs were permitted per motion spike and a maximum of three motion spikes were removed from any scan. Of the 56 sessions (Observe and Imitate for the 28 participants) scrubbing was performed in 16 sessions. In nine of these cases (four Observe sessions and five Imitate sessions) scrubbing was not successful. Possible reasons for unsuccessful ‘data scrubbing’ include large movements exceeding three TRs, more than three large movements, or over 90% of ICA components classified as artifact after data scrubbing. These sessions were subsequently excluded from further analysis, leaving neuroimaging data from 20 of the original 28 individuals for final analysis. Data were then de-noised based on the selected noise ICA components using FSL 5.0.6 (reg_filt). Normalization into MNI space was done using SPM8, and images were smoothed with an 8 mm Gaussian kernel. 


### Partial Least Squares Analysis 
  
Patterns of task related activity and relationships between social cognitive test scores and BOLD signal were examined using spatio-temporal PLS , a multivariate approach which allows for detection of spatial patterns and dependant variables across the brain without the need for a-priori selected contrasts across experimental conditions. PLS is designed to handle data with high collinearity while capturing essential non-redundant relationships among the data , overcoming the confounding influence of collinear variables (e.g. cognitive tests) in traditional multiple regression. Additionally, PLS is also well suited to studying the relationships between numerous variables even in the presence of a relatively small sample, which is ideal for our purposes as it allows us to examine a range of social cognitive batteries. PLS produces latent variables (  LVs  ) relating patterns of experiment task activity or behavioral measures with spatial patterns of neural activity across time points (scans, referred to as    lags   , normalized to the scan in which the stimuli was presented, labeled as lag0). As a model free non-parametric approach, PLS is ideal to examine complex relationships amongst the battery of social cognitive scores and neural activity when imitating and observing emotional faces. 

A task-PLS analysis was conducted to replicate the regions of activity in the SPM8 GLM. Brain data from an 18 second window (corresponding to 6 lags) was normalized to the first lag (lag0) to create a data matrix for each condition, stacked across participants. Cross covariance was calculated between the data matrix and a design matrix consisting of vector of experimental conditions (in the task PLS, emotional faces, neutral faces, and fixation crosses, separately for Imitate and Observe). The resulting cross-covariance matrix was then decomposed using singular value decomposition, which created a set of orthogonal latent variables (  LVs  ) which optimally represent spatio-temporal relationships between voxels and experimental conditions. For each LV, a pattern of voxels at each lag value (the ‘brain pattern’) demonstrates the relationship with activity in the voxel at that lag and the ‘design pattern’ (representing the weights of each experimental event). Voxel weight is expressed as salience, which is proportional to the covariance of activity in that voxel and the design pattern expressed by that LV. 

Behavioral PLS was used to examine relationships between social cognition and neural activity. Behavioral PLS examines patterns of covariance between scores and neural activity for each trial type across lags. As such the design pattern is the correlation between the overall pattern of neural activity in each condition and each social cognitive score. For the behavioral PLS, neural activity to emotional faces, neutral faces, and fixation (separately for imitate and observe) was related to the six social cognitive test scores from our battery, deterministically producing 36 LVs. 

Statistical evaluation of each LV was performed using split-half permutation testing. A total of 500 permutations were run, with 100 split-half permutations within each. The overall permutation score determines if the effect represented by the LV is sufficiently strong to be differentiated from random noise, while the split-half analysis provides a measure of the stability of relationships between voxel patterns and design patterns in the data for each latent variable. As the permutations are performed at the level of the entire PLS analysis (rather than on individual LVs), multiple comparisons for the number of LVs is not necessary. In the split-half, the ‘BrainCorr’ value tests the reliability of the voxel pattern for a given design pattern associated with an LV, while ‘DesignCorr’ tests the reliability of a given voxel pattern for the behavioral pattern associated with that LV. Results are expressed as p-values. LV’s were considered significant if the overall permutation and at least one of the BrainCorr or DesignCorr was less than 0.05. A bootstrapping procedure with 1000 iterations was used to test if specific voxels were reliably related to the LV. A bootstrap ratio for each voxel was calculated as the voxel salience divided by its bootstrap standard error. A bootstrap ratio of 2.5 (corresponding to >95% reliability) was used to threshold all voxel pattern maps in PLS. 


 </div>
</div>
</div>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-36' name='judgment-36' value='agree'>
<label for='agree-36'>Agree</label>
<input type='radio' id='disagree-36' name='judgment-36' value='disagree'>
<label for='disagree-36'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-36' name='comment-36' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-37'>
<h2>37. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/24795436/' target='_blank'>24795436</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study of social influence in healthy adults (mean age ~24; right-handed students; N=20 scanned). The task is explicitly social/interactive (pair-based communication of arousal ratings) and addresses social-related processing (social appraisal/conformity). The paper reports group-level, whole-brain univariate analyses (random-effects), with voxelwise results surviving clusterwise FWE correction reported in tables and figures (insula, DLPFC, ventral striatum, amygdala), including parametric modulators and whole-brain contrasts. Results for the healthy/control group are presented and generalizable to healthy adults. No exclusion criteria apply (not ROI-only, not connectivity-only, not between-group-only, participants within 17–65). Therefore it meets all inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-37' name='judgment-37' value='agree'>
<label for='agree-37'>Agree</label>
<input type='radio' id='disagree-37' name='judgment-37' value='disagree'>
<label for='disagree-37'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-37' name='comment-37' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-38'>
<h2>38. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/31028922/' target='_blank'>31028922</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study in healthy adults (participants aged 19–36; fMRI sample N≈38) that explicitly investigates perception of animacy — a social-relevant process (perception/understanding of others). The paper reports group-level, whole-brain univariate task-evoked results (e.g., conjunction whole-brain contrasts, voxelwise FWE-corrected maps reported and shown in figures/tables) rather than ROI-only or connectivity-only analyses. Healthy adult results are reported and generalizable (random-effects group analyses). The task probes social-related processing (animacy perception as precursor to social cognition). No exclusion criteria are violated. Therefore the study meets all inclusion criteria for the review.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-38' name='judgment-38' value='agree'>
<label for='agree-38'>Agree</label>
<input type='radio' id='disagree-38' name='judgment-38' value='disagree'>
<label for='disagree-38'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-38' name='comment-38' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-39'>
<h2>39. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/26302673/' target='_blank'>26302673</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of social cognition (race-based impression formation) using a task-evoked impression-formation paradigm assessing perception/understanding of others. Sample comprises healthy adult participants (Caucasian Americans aged 19–34; N=44 after exclusions), meeting the age and health criteria. The paper reports group-level, univariate whole-brain task contrasts and tables of peak coordinates (see Table 2 and Table 3) and describes whole-brain exploratory regression results (thresholds, cluster-correction via AlphaSim), satisfying the whole-brain evidence requirement. The study also includes ROI analyses but crucially provides univariate, group-level whole-brain activation maps/contrasts for the healthy/control group. No exclusion criteria (ROI-only, connectivity-only, non-healthy sample, or non-empirical) apply. Therefore it meets all inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-39' name='judgment-39' value='agree'>
<label for='agree-39'>Agree</label>
<input type='radio' id='disagree-39' name='judgment-39' value='disagree'>
<label for='disagree-39'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-39' name='comment-39' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-40'>
<h2>40. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/29366950/' target='_blank'>29366950</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> The study used task-based fMRI in healthy adult participants (N=19, mean age 25.9) during a social-related auditory perception task (judgement/perception of affective vocalisations—social communication/perception of others). The authors report group-level, whole-brain univariate analyses (mixed-effects FLAME) and present a voxelwise species contrast (human > chimpanzee > macaque) with coordinates, statistical maps/figures and tables (Table 2; whole-brain threshold p < .001). The sample is within the 17–65 range and results for the healthy group are reported separately. The study is empirical and not ROI-only or connectivity-only. Therefore it meets all inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-40' name='judgment-40' value='agree'>
<label for='agree-40'>Agree</label>
<input type='radio' id='disagree-40' name='judgment-40' value='disagree'>
<label for='disagree-40'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-40' name='comment-40' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-41'>
<h2>41. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/24760733/' target='_blank'>24760733</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of face perception (Perception and Understanding of Others), a social-related process. Participants: 25 healthy adults (mean age 25.24), within 17–65. The paper reports group-level, voxelwise whole-brain task contrast (natural-colored face vs natural-scrambled) with peak coordinates, cluster sizes (Table 1) and figures, i.e., univariate whole-brain activation maps generalizable to healthy adults. Although ROI analyses are also presented, whole-brain results are explicitly reported. No exclusion criteria are met (not ROI-only, not connectivity/resting-state only, healthy adult group reported separately). Therefore the study meets all inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-41' name='judgment-41' value='agree'>
<label for='agree-41'>Agree</label>
<input type='radio' id='disagree-41' name='judgment-41' value='disagree'>
<label for='disagree-41'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-41' name='comment-41' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-42'>
<h2>42. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/26825440/' target='_blank'>26825440</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of social processing (perception of others’ facial expressions) in healthy adults (n=24; participants aged ~25). The paper reports group-level, mixed-effects fMRI analyses and explicitly generated parameter-estimate maps for each expression condition and used a group-level whole-brain localiser contrast (faces > scrambled faces) from a large healthy sample (n=83) to define face-selective ROIs. The task is clearly social/communication-related (facial expression perception). Whole-brain, univariate group-level contrasts are described (localiser and group mixed-effects maps), and results from healthy adult participants are reported. It does not rely solely on ROI or connectivity-only analyses. Therefore the study meets the inclusion criteria for fMRI of social-related processing in healthy adults.</p>
<p><strong>Fulltext Confidence:</strong> 0.85</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-42' name='judgment-42' value='agree'>
<label for='agree-42'>Agree</label>
<input type='radio' id='disagree-42' name='judgment-42' value='disagree'>
<label for='disagree-42'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-42' name='comment-42' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-43'>
<h2>43. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/29408539/' target='_blank'>29408539</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This study reports task-based fMRI activation for social cognition (TOM vs Random) in a large sample of healthy adults (Human Connectome Project, n=787; ages 22–>35). The paper provides group-level, univariate whole-brain task-evoked statistical maps (Cohen's d and level-3 z maps) and describes cerebellar and whole-brain activation clusters for the social contrast. Healthy-control results are reported clearly and are generalizable to the healthy adult population. Although resting-state and seed-based analyses are also included, the paper contains the required task activation whole-brain maps (not ROI-only, connectivity-only, nor between-group-only contrasts). Therefore it meets all inclusion criteria and violates none of the exclusion criteria for the review.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-43' name='judgment-43' value='agree'>
<label for='agree-43'>Agree</label>
<input type='radio' id='disagree-43' name='judgment-43' value='disagree'>
<label for='disagree-43'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-43' name='comment-43' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-44'>
<h2>44. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/31844272/' target='_blank'>31844272</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study in healthy adult participants (N=24; ages 22–39) that investigates social-related processing (collective memory, social knowledge, representation of group/narrative schemas). The task is a task-evoked recall task probing social/collective schema influences on memory (relevant constructs: perception/understanding of others, social knowledge). The paper reports group-level whole-brain analyses: a surface-based searchlight with second-level non-parametric random-effects analysis and TFCE correction (Pcorrected < 0.05) identifying clusters related to collective schema (Extended Data Fig.1), in addition to ROI results. Thus it meets all inclusion criteria (healthy adult sample, social-related fMRI task, and group-level whole-brain task-evoked statistical maps). No exclusion criteria (ROI-only, connectivity-only, non-empirical, out-of-range ages) apply.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-44' name='judgment-44' value='agree'>
<label for='agree-44'>Agree</label>
<input type='radio' id='disagree-44' name='judgment-44' value='disagree'>
<label for='disagree-44'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-44' name='comment-44' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-45'>
<h2>45. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/20439736/' target='_blank'>20439736</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This study used task-based fMRI during an explicitly social communication task (charades/gestural communication) in healthy adult participants (final N=18; mean age 27.5). The paper reports group-level, voxelwise whole-brain analyses: a traditional GLM was used to identify mirror-system ROIs (seed regions defined from group GLM contrasts), and whole-brain differential Granger-causality (bbGCM) maps are presented and thresholded at the group level. Results and statistical maps are described for the healthy adult sample and are generalizable to healthy adults (ages within 17–65). Although connectivity analyses are a primary focus, univariate whole-brain GLM results (used to define seed ROIs and discussed relative to bbGCM) are reported/used, so the study is not limited to ROI-only or connectivity-only reporting. Therefore it meets the inclusion criteria for fMRI studies of social processing in healthy adults.</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-45' name='judgment-45' value='agree'>
<label for='agree-45'>Agree</label>
<input type='radio' id='disagree-45' name='judgment-45' value='disagree'>
<label for='disagree-45'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-45' name='comment-45' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-46'>
<h2>46. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/23850465/' target='_blank'>23850465</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an original, task-based fMRI study of social cognition (impression formation) in healthy adults (N=19, mean age 19.1, within 17–65). The paper reports group-level, voxelwise univariate task contrasts and provides whole-brain results with Talairach coordinates and cluster correction (Tables 1 and 2), not solely ROI or connectivity analyses. The social task directly targets perception/understanding of others and uses an established functional localizer plus main task analyses generalizable to healthy adults. No exclusion criteria (e.g., only ROI, only connectivity, non-healthy sample, or non-empirical) apply. Therefore it meets all inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.92</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-46' name='judgment-46' value='agree'>
<label for='agree-46'>Agree</label>
<input type='radio' id='disagree-46' name='judgment-46' value='disagree'>
<label for='disagree-46'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-46' name='comment-46' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-47'>
<h2>47. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/22371086/' target='_blank'>22371086</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of social perception (affiliation vs isolation) in healthy adults. It reports whole-brain, voxelwise, group-level univariate analyses (GLM, random-effects) with thresholded results and coordinate tables/figures for task contrasts (social vs nonsocial; affiliation vs isolation) and age-group contrasts. The sample includes a healthy younger adult group (mean age 20.75), which falls within the 17–65 inclusion range; results for healthy younger adults are analyzed and reported (e.g., conjunctions and Younger > Older contrasts), satisfying the requirement for a healthy adult group. The study is not ROI-only and does not report only connectivity or non-empirical data. Although an older group (>65) is included, that does not preclude inclusion because a qualifying healthy adult subgroup is present and reported separately.</p>
<p><strong>Fulltext Confidence:</strong> 0.88</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-47' name='judgment-47' value='agree'>
<label for='agree-47'>Agree</label>
<input type='radio' id='disagree-47' name='judgment-47' value='disagree'>
<label for='disagree-47'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-47' name='comment-47' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-48'>
<h2>48. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/26162239/' target='_blank'>26162239</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of social-related processing (mentalizing/theory-of-mind) using an interactive Prisoner’s Dilemma task. The sample includes 164 healthy adult subjects and reports task-evoked brain activation (group-level univariate results) across the canonical mentalizing network (bilateral precuneus, cingulate cortices, temporal poles, IFG, TPJ, medial frontal gyri, amygdala, etc.), indicating whole-brain task effects generalizable to healthy adults. The paper reports correlations of attachment dimensions with task-associated neural activity, not limited to ROI-only or connectivity-only analyses, and presents whole-brain findings suitable for inclusion. No exclusion criteria are violated (not ROI-only, not resting-state/seed-connectivity-only, not only between-group contrasts). Therefore the study meets all inclusion criteria for healthy adult whole-brain fMRI social task studies.</p>
<p><strong>Fulltext Confidence:</strong> 0.93</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-48' name='judgment-48' value='agree'>
<label for='agree-48'>Agree</label>
<input type='radio' id='disagree-48' name='judgment-48' value='disagree'>
<label for='disagree-48'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-48' name='comment-48' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-49'>
<h2>49. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/27109357/' target='_blank'>27109357</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study in healthy adult participants (age range 19–38; fMRI subsample n=163) using a social interactive task (Prisoner’s Dilemma) that elicits mentalizing/social cognition. The paper reports whole-brain, group-level univariate task contrasts (ment > cont) and provides whole-brain activation results (tables/figures and cluster-level FWE-corrected maps) for the healthy sample. Results are not limited to ROI-only or connectivity/resting-state analyses. All inclusion criteria are met and no exclusion criteria apply (healthy adults, task-evoked whole-brain maps reported).</p>
<p><strong>Fulltext Confidence:</strong> 0.92</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-49' name='judgment-49' value='agree'>
<label for='agree-49'>Agree</label>
<input type='radio' id='disagree-49' name='judgment-49' value='disagree'>
<label for='disagree-49'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-49' name='comment-49' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-50'>
<h2>50. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/30842248/' target='_blank'>30842248</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> The study uses task-based fMRI examining perception and recognition of faces (a social ‘Perception and Understanding of Others’ construct) in healthy adult participants (main sample N=20, plus an independent localizer sample N=83). The paper reports group-level univariate whole-brain contrast data used as an independent localizer (faces vs scrambled) to define ROIs, satisfying the requirement for a group-level, task-evoked whole-brain map generalizable to healthy adults. Participants are within the 17–65 age range and results for healthy groups are reported. Although primary analyses emphasize MVPA/ROI results, a whole-brain localizer contrast is explicitly reported and used. No exclusion criteria (e.g., only ROI/ connectivity analyses, non-healthy sample, or non-empirical study) are violated.</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-50' name='judgment-50' value='agree'>
<label for='agree-50'>Agree</label>
<input type='radio' id='disagree-50' name='judgment-50' value='disagree'>
<label for='disagree-50'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-50' name='comment-50' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-51'>
<h2>51. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/31747689/' target='_blank'>31747689</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of action perception—clearly within ‘Perception and Understanding of Others’ (social-related processing). It reports task-evoked, group-level voxelwise whole-brain analyses (e.g., ActionOBS–CtrlOBS and weight-discrimination contrasts) with thresholded statistical maps and localization, described for healthy adult participants across three experiments (total N=79) and separate analyses comparing healthy controls to SCA6 patients. Healthy adult results are reported independently of clinical groups. The paper therefore meets: (1) social-related fMRI task, (2) healthy adult sample in the 17–65 range, (3) whole-brain univariate group-level voxelwise maps, and (4) no exclusion criteria (not ROI-only, not connectivity-only, and includes healthy-group whole-brain effects).</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-51' name='judgment-51' value='agree'>
<label for='agree-51'>Agree</label>
<input type='radio' id='disagree-51' name='judgment-51' value='disagree'>
<label for='disagree-51'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-51' name='comment-51' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-52'>
<h2>52. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/23639347/' target='_blank'>23639347</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study of social-related processing in healthy adults (n=22, mean age 23.6). The task manipulated social context (holding a friend’s hand, a stranger’s hand, or alone) during threat-of-shock anticipation — a social-affiliation/attachment-related paradigm. The paper reports group-level, voxelwise whole-brain analyses (threat vs. safe contrast) used to identify functional ROIs via cluster-wise tests (Z>2.3, cluster p<.05) and provides coordinates/tables and figures of activations for the healthy sample. Participants are within the 17–65 age range and results for the healthy group are presented. No exclusion criteria (e.g., ROI-only, connectivity-only, or non-healthy-only samples) are violated. Therefore the study meets all inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-52' name='judgment-52' value='agree'>
<label for='agree-52'>Agree</label>
<input type='radio' id='disagree-52' name='judgment-52' value='disagree'>
<label for='disagree-52'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-52' name='comment-52' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-53'>
<h2>53. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/31798816/' target='_blank'>31798816</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> The study uses task-based fMRI during observation of social (interpersonal) touch, targeting social-affective processing. It includes a healthy neurotypical adult control group (N=21) reported separately. The paper reports group-level univariate whole-brain task effects (social vs. non-social touch) with mean group effects and peak coordinates presented in supplemental materials (Additional file 1: Figure S2 and Table S2), satisfying the whole-brain evidence requirement. The design is task-evoked (not resting-state or connectivity-only), and healthy-group results are clearly described and generalizable to healthy adults. No exclusion criteria (ROI-only, connectivity-only, age outside 17–65, or non-empirical) apply. Therefore the study meets all inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-53' name='judgment-53' value='agree'>
<label for='agree-53'>Agree</label>
<input type='radio' id='disagree-53' name='judgment-53' value='disagree'>
<label for='disagree-53'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-53' name='comment-53' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-54'>
<h2>54. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/24412433/' target='_blank'>24412433</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> The study reports task-based fMRI comparing responses to familiar (partner, child, friend) versus unfamiliar faces in a healthy control group (n=13). The task probes social/person-related processing (affiliation/attachment and perception/understanding of others). The paper provides group-level, whole-brain univariate contrasts for the control group (e.g., partner > unfamiliar faces with FWE-corrected whole-brain results and coordinates). Although the sample is older adults (mean age 64.5 ±5.7), a healthy control group is reported separately from the clinical case, satisfying the criterion that healthy-group whole-brain task effects are available. Therefore it meets the inclusion criteria for fMRI studies of social-related processing in healthy adults.</p>
<p><strong>Fulltext Confidence:</strong> 0.85</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-54' name='judgment-54' value='agree'>
<label for='agree-54'>Agree</label>
<input type='radio' id='disagree-54' name='judgment-54' value='disagree'>
<label for='disagree-54'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-54' name='comment-54' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-55'>
<h2>55. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/30949039/' target='_blank'>30949039</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study using a socioemotional Face Processing Task (fearful vs neutral faces) in a sample of healthy adults (N=49, ages 18–33). The paper reports group-level, whole-brain univariate task activation results (F> N and N>F contrasts) with FWE-corrected voxelwise analyses (TFCE, 5,000 permutations) and provides coordinates and figures for these effects. Although ROI and FC analyses are included, whole-brain task-evoked maps for the healthy/control sample are clearly reported and generalizable to healthy adults. No exclusion criteria apply (not ROI-only, not resting-state-only, and participants are within the 17–65 age range). Therefore the study meets all inclusion criteria for the review.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-55' name='judgment-55' value='agree'>
<label for='agree-55'>Agree</label>
<input type='radio' id='disagree-55' name='judgment-55' value='disagree'>
<label for='disagree-55'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-55' name='comment-55' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-56'>
<h2>56. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/22439896/' target='_blank'>22439896</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of social evaluation (impression formation) in healthy adults, directly addressing social processing. It includes a healthy younger adult group (ages 20–29) within the 17–65 inclusion range and reports group-level, whole-brain univariate task contrasts (social vs non-social; social-meaningful vs social-irrelevant) using random-effects analyses, corrected cluster thresholds, and coordinate tables/figures. Results for healthy groups are reported separately (young and older), and whole-brain maps/peaks are described, satisfying the whole-brain evidence requirement. Although an older group extends beyond 65, the presence of a healthy adult subgroup within 17–65 with reported whole-brain results makes the study eligible. The study is empirical original fMRI research (not ROI-only or connectivity-only). Therefore it meets all inclusion criteria.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-56' name='judgment-56' value='agree'>
<label for='agree-56'>Agree</label>
<input type='radio' id='disagree-56' name='judgment-56' value='disagree'>
<label for='disagree-56'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-56' name='comment-56' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-57'>
<h2>57. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/27579051/' target='_blank'>27579051</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> The study reports task-based fMRI of social-related processing (Face Matching Task with fearful and neutral faces — perception/understanding of others). The sample comprises healthy adults (n=10, mean age 25), within the 17–65 range. The paper includes group-level, univariate whole-brain GLM analyses (first-level contrasts and second-level one-sample t-tests for contrasts such as fear>neutral and neutral+fear>control) with reported thresholding and results presented (Table S2 and Figures S1–S3), satisfying the requirement for a whole-brain task-evoked statistical map from the healthy group. Although the paper also presents ICA (connectivity) analyses, this does not conflict with inclusion since univariate whole-brain results are provided. Therefore all inclusion criteria are met and no exclusion criteria are triggered.</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-57' name='judgment-57' value='agree'>
<label for='agree-57'>Agree</label>
<input type='radio' id='disagree-57' name='judgment-57' value='disagree'>
<label for='disagree-57'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-57' name='comment-57' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-58'>
<h2>58. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/22982277/' target='_blank'>22982277</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> Meets all inclusion criteria: (1) Task-based fMRI probing a social/attachment construct (maternal neural response to infant cry); (2) Sample includes healthy adult participants (22 mothers, mean age 24.1; screened with SCID and none met current MDE; results reported for this healthy group; clinical history and a few medicated participants were analyzed and retained with sensitivity checks); (3) Reports whole-brain, voxelwise, group-level univariate task contrasts (own cry > control; own cry > other) analyzed with mixed-effects FLAME and cluster-wise thresholding, with coordinates and tables/figures provided. Does not rely solely on ROI, connectivity, or resting-state analyses. No exclusion criteria are violated. Although sample size is modest and includes a high-risk recruitment strategy, the reported healthy adult whole-brain task activation maps are clearly described and generalizable to healthy adults, so the study should be included.</p>
<p><strong>Fulltext Confidence:</strong> 0.92</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-58' name='judgment-58' value='agree'>
<label for='agree-58'>Agree</label>
<input type='radio' id='disagree-58' name='judgment-58' value='disagree'>
<label for='disagree-58'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-58' name='comment-58' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-59'>
<h2>59. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/22308468/' target='_blank'>22308468</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical task-based fMRI study of social cognition (social working memory) in healthy adults (n=16, mean age 20). The paper reports group-level, whole-brain univariate analyses (parametric load regressors) with voxelwise results and coordinate tables/figures, random-effects group inference, and thresholding procedures. The task explicitly probes social constructs (perspective-taking, mentalizing). It is not ROI-only, not connectivity-only, includes healthy adult behavioral and neural results generalizable to the healthy group, and falls within the 17–65 age range. Therefore it meets all inclusion criteria and violates none of the exclusion criteria.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-59' name='judgment-59' value='agree'>
<label for='agree-59'>Agree</label>
<input type='radio' id='disagree-59' name='judgment-59' value='disagree'>
<label for='disagree-59'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-59' name='comment-59' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-60'>
<h2>60. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/27358450/' target='_blank'>27358450</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of healthy adult participants (N=34, mean age 22.2) who performed a social-related task (naturalistic movie viewing emphasizing social interactions). The paper reports group-level task-evoked analyses including whole-brain voxelwise reverse-correlation results (χ2 goodness-of-fit maps and corrected statistical parametric maps visualized in Figure 3) and intersubject correlation statistics, all derived from the healthy adult sample. Results are not limited to ROI-only or connectivity-only analyses; whole-brain, group-level univariate results are clearly described and corrected for multiple comparisons. Participants fall within the 17–65 age range and the healthy group results are reported directly. No exclusion criteria are violated. Therefore the study meets all inclusion criteria for the meta-analysis.</p>
<p><strong>Fulltext Confidence:</strong> 0.92</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-60' name='judgment-60' value='agree'>
<label for='agree-60'>Agree</label>
<input type='radio' id='disagree-60' name='judgment-60' value='disagree'>
<label for='disagree-60'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-60' name='comment-60' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-61'>
<h2>61. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/19384602/' target='_blank'>19384602</a></h2>
<div class='metadata'>
<h3>Metadata</h3>
<p><strong>Title:</strong> Audiovisual Non-Verbal Dynamic Faces Elicit Converging fMRI and ERP Responses</p>
<p><strong>Authors:</strong> N/A</p>
<p><strong>Journal:</strong> Brain Topogr</p>
<p><strong>Publication Year:</strong> 2009</p>
<p><strong>DOI:</strong> 10.1007/s10548-009-0093-6</p>
<p><strong>PMCID:</strong> <a href='https://www.ncbi.nlm.nih.gov/pmc/articles/2707948/' target='_blank'>2707948</a></p>
</div>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an empirical fMRI study of social-related processing (integration of non-verbal vocalizations and facial movements — social communication/perception of others) in healthy adults (fMRI sample: 10 right-handed males, ages 24–37). The paper reports group-level, voxelwise univariate fMRI analyses (voxel-by-voxel t tests, percent signal change maps, corrected with AlphaSim) with group activation maps and difference maps (AV, VIS, AUD vs rest and contrasts), presented in figures/tables. Whole-brain voxelwise results are clearly reported for the healthy group (even though slice coverage emphasized temporal cortex), and healthy adult results are reported separately. No exclusion criteria apply (not ROI-only, not connectivity-only, not between-group-only). Therefore it meets all inclusion criteria for the review.</p>
<p><strong>Fulltext Confidence:</strong> 0.9</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p><strong>Abstract:</strong>  
In an everyday social interaction we automatically integrate another’s facial movements and vocalizations, be they linguistic or otherwise. This requires audiovisual integration of a continual barrage of sensory input—a phenomenon previously well-studied with human audiovisual speech, but not with non-verbal vocalizations. Using both fMRI and ERPs, we assessed neural activity to viewing and listening to an animated female face producing non-verbal, human vocalizations (i.e. coughing, sneezing) under audio-only (AUD), visual-only (VIS) and audiovisual (AV) stimulus conditions, alternating with Rest (R). Underadditive effects occurred in regions dominant for sensory processing, which showed AV activation greater than the dominant modality alone. Right posterior temporal and parietal regions showed an AV maximum in which AV activation was greater than either modality alone, but not greater than the sum of the unisensory conditions. Other frontal and parietal regions showed Common-activation in which AV activation was the same as one or both unisensory conditions. ERP data showed an early superadditive effect (AV > AUD + VIS, no rest), mid-range underadditive effects for auditory N140 and face-sensitive N170, and late AV maximum and common-activation effects. Based on convergence between fMRI and ERP data, we propose a mechanism where a multisensory stimulus may be signaled or facilitated as early as 60 ms and facilitated in sensory-specific regions by increasing processing speed (at N170) and efficiency (decreasing amplitude in auditory and face-sensitive cortical activation and ERPs). Finally, higher-order processes are also altered, but in a more complex fashion. 

## Electronic supplementary material 
  
The online version of this article (doi:10.1007/s10548-009-0093-6) contains supplementary material, which is available to authorized users. 

 </p>
<button class='accordion' onclick='toggleAccordion(this)'>Full Text Content (41009 characters)</button>
<div class='panel'>
<div class='panel-content'>
<div class='fulltext-content'> 
## Introduction 
  
Everyday social interactions involve the integration of auditory and visual information from speech and non-verbal social cues. These latter cues are often underemphasized in humans, as most attention tends to focus on the spoken word (Campbell et al.  ; Capek et al.  ; Frith and Frith  ; Kawashima et al.  ; Macaluso et al.  ; MacSweeney et al.  ). Humans generate many non-verbal vocalizations that are accompanied by readily identifiable stereotypical facial gestures (Howell  ). Non-verbal vocalizations likely engage higher-order processing, and can be overlooked, misused, or misinterpreted by those with social cognition disorders (Golarai et al.  ; Luyster et al.  ; Sarfati et al.  ; Troisi et al.  ). Non-verbal vocalizations can be communicative as one may purposely vocalize or exaggerate non-verbal cues to send a message, such as burp to signal the deliciousness of a meal, or one may purposely suppress a sign or yawn to conceal dissatisfaction or boredom. Social and other advantages may thus come from the ability to interpret information about the mental, emotional, or homeostatic state of individuals as conveyed through multisensory non-verbal cues. This is supported by studies that show greater activation to human non-verbal stimuli versus other non-human categories in multiple regions including STS, frontal parietal regions, and insula (Fecteau et al.  ; Lewis et al.  ). 

In a normal context, the accurate interpretation of socially related non-verbal information requires appropriate integration of multisensory input, usually visual and auditory information, which can change based on incoming information quality. In a noisy situation like a crowded bar, one observes lip and face movements more than in a quiet setting, as the visual information can effectively amplify the audio by up to 11 dB (MacLeod and Summerfield  ). Behavioral studies of both speech and non-speech stimuli indicate that multiple (congruent) stimulus modalities lead to improved processing, with both shorter reaction times and increased accuracy compared to either modality alone (Grant and Walden  ; Miller  ; Sumby and Pollack  ). 

These behavioral advantages for multisensory stimuli manifest as differences in timing, amount and type of brain activity compared to unisensory stimuli. However, studies have revealed conflicting results including both facilitation, in the form of faster and   decreased   brain responses (for fMRI Martuzzi et al.  ; Wright et al.  ; for ERPs Besle et al.  ; van Wassenhove et al.  ), and enhancement, or   increased   activation, for multisensory versus unisensory stimuli (Hubbard et al.  ; Kayser et al.  ). The reasons for these differences in multisensory effects are not understood, although some studies suggest that they may be related to factors such as congruency (Puce et al.  ; Saint-Amour et al.  ), whether one modality predicts the other (Ghazanfar et al.  ; Stekelenburg and Vroomen  ), or neuronal population properties (Laurienti et al.  ; Stevenson et al.  ). Even more complex results have been seen for higher-order regions, with effects (in speech-related studies) seen in posterior superior temporal sulcus (pSTS), inferior parietal lobule (IPL), and inferior frontal cortex (IFC) (Calvert et al.  ; Kawashima et al.  ). In the current study we were particularly interested in multisensory effects in pSTS due to its postulated role in social related processes (Redcay  ), and links to different visual, auditory, and motor processes (Beauchamp et al.  ). 

To investigate multisensory effects related to human non-verbal vocalizations and accompanying facial movements, we studied neural responses elicited to an animated synthetic female face producing various non-verbal vocalizations (i.e. coughing, yawning), using both functional magnetic resonance imaging (fMRI) and event-related potentials (ERPs). We presented stimuli under three conditions. In the audiovisual (AV) condition participants saw the animated face and heard congruent human vocalizations. In the visual (VIS) condition, only the animated face was seen, whereas in the auditory (AUD) condition only the vocalizations were heard. Randomized blocked presentations of AV, VIS and AUD conditions were alternated with rest (R). Two participant groups (  n   = 10 for fMRI,   n   = 13 for ERPs) responded to infrequent unisensory targets (animated face blinking, or uttering “mmm” without a visual change to the face). Our hypothesis predicted that sensory-specific regions specialized for a given unisensory condition, would show facilitated processing (faster times to peak and reduced amplitudes) in the presence of a multisensory stimulus. Specifically, for the fMRI experiment, we predicted a reduced BOLD signal for the AV versus either unimodal condition in sensory regions. For the ERP experiment, we predicted reduced amplitudes and faster latencies for early ERP components. In addition, we predicted that higher-order regions, especially right pSTS, would show greater AV activation (versus unisensory conditions) due to specialization in multisensory and/or social processes. 


## Materials and Methods 
  
### Participants 
  
For the fMRI study 10 right-handed healthy males participated (ages: 24–37 years, mean 28 years). For the ERP study, there were 13 right-handed participants (18 originally collected, 5 excluded, for the 13 included participants: 7 males, ages: 19–43 years, mean 29 years). All participants had either normal or corrected-to-normal vision and gave informed consent in a study approved by the Institutional Review Board for the Protection of Human Participants at West Virginia University. 


### Stimuli and Task 
  
Participants viewed 4 × 4 degree videos of a synthetic female face producing facial movements and vocalizations. Stimuli were seven non-speech vocalizations with accompanying face movements consisting of a cough, sneeze, burp, yawn, laugh, sigh and whistle. Two infrequently presented unisensory target stimuli, a blink (visual) and an uttered ‘mmm’ (auditory), made participants focus on visual and auditory sensory input equally. In the Audiovisual condition, there was a 33 ms (or 1 video frame) delay between the peak movement (i.e. fully opened mouth) and sound. Animations were based on filmed real life movements associated with the seven non-verbal vocalizations of three different actors. 

Stimulus type was pseudorandomly ordered within 20 s stimulus blocks consisting of 10 trials each of combined Audiovisual stimulation (AV), Auditory stimulation only (AUD), and Visual stimulation only (VIS) (Fig.  ). In the AV participants saw the face making the facial movements and heard the associated non-speech vocalizations. In VIS, participants observed the face making movements without hearing the vocalizations. In AUD, participants viewed a neutral colored plain background (RGB = 140, 132, 127) and heard the vocalizations. The absence of the face for the AUD condition prevented an ‘incongruent’ stimulus (face still but vocalization present), but made an event-related design difficult due to onset effects. Visual motion duration and sound duration was 600 and 567 ms respectively, for all non-target stimulus types. Participants maintained their gaze on an ever-present green fixation cross and pressed a single response button when either of one of the two specified unisensory targets were seen or heard. Behavioral responses were monitored to ensure attentional alertness. Minor variations in timing occurred for the fMRI versus ERP paradigms.   
Example of a stimulus still frame depicted at the middle of an animation.   a   In the AV condition, the face is present and the non-verbal vocalization accompanies the visual stimulus. Here a yawn is depicted and the open mouth and narrowing eyes can be clearly seen.   b   In the VIS condition only the moving face is present.   c   In the AUD condition only the vocalizations are heard. For all conditions a green fixation cross was located in same position on the screen throughout scans (between the eyes when the face present in the AV and VIS conditions) 
  


### Data Acquisition 
  
#### Functional MRI Study 
  
Data were acquired on a 3 Tesla GE Horizon LX MRI scanner and quadrature birdcage headcoil. We used a 14 slice split-sagittal acquisition (Puce et al.  ), where 7 sagittal slices (3 mm thickness + 1 mm gap) were taken in each hemisphere to maximally visualize the cortex of the STS and STG (see Supplementary Fig.  ). A series of 125 gradient echo echoplanar volumes were acquired over each of the three, 4 min 10 s stimulation periods (after before and after rest period removal, total = 375 volumes) using the following parameters: TE = 35, TR = 2000, α = 70°, NEX = 1, BW = 125, FOV = 24 mm, matrix = 128 × 128 (in-plane resolution of 1.875 mm), slice thickness = 3 mm, gap = 1 mm. In the Talaraich   x   plane, sagittal slice coverage was from   x   = −34 to −67, and   x   = 34 to 67. 

A T1-weighted whole brain volume which was acquired as a high-resolution spoiled gradient-recalled acquisition in a steady state (SPGR) (voxel size = 1.2 × 0.9375 × 0.9375 mm; FOV = 240; matrix = 256 × 256; 124 slices). 


#### EEG/ERP Study 
  
Participants were seated comfortably in an armchair in a dimly lit room with a white noise generator. A continuous 128-channel recording of 124 channels of scalp EEG (QuikCap, Compumedics Neuroscan, El Paso, TX, USA) and 4 channels of horizontal and vertical electrooculograph (EOG) was taken using Neuroscan 4.3 software (Compumedics, Neuroscan, El Paso, TX, USA). Data were sampled at 250 Hz/channel and bandpass filtered from 0.1 to 100 Hz and amplified with a gain of 5,000. A reference consisted of two electrodes placed either side of the nose or on the cheek close to the nose. The midline frontal ground electrode was sited on the electrode cap itself. Electrode impedances were kept below 10 kΩ. 



### Data Analysis 
  
#### Functional MRI 
  
Data reconstruction was implemented via Analysis of Functional Neural Images (AFNI), version 2.31 software (Cox  ). Data processing steps included offline image reconstruction in conjunction with smoothing in Fourier space via a Fermi window (full width at half maximum = 1 voxel), correction for differences in slice-timing, and 6-parameter rigid-body motion correction. The motion estimates over the course of the scan for translation (inferior–superior, right–left, and anterior–posterior) and rotation (yaw, pitch, roll) parameter estimates were used as covariates in further analyses. 

Each image time series was spatially registered to the volume closest in time to the high-resolution structural scan both within-plane and then in all three planes using an iterative linear least squares method, to reduce the effects of head motion. AUD, VIS, and AV blocks were analyzed with a least-squares general linear model (GLM) fit that modeled each activation block and head motion parameters. Each regressor consisted of an ideal hemodynamic response function for the specified block type, obtained by convolving the event time file (across 3 concatenated imaging runs) with a γ-variate function. The beta-weights resulting from the GLM analysis were converted to percent signal change using the mean overall baseline and spatially smoothed using a 4 mm Gaussian filter. These percentage signal change maps were transformed into standardized Talaraich space. 

A voxel-by-voxel parametric two-tailed   t   test was used on the percent signal change maps for a group comparison of each condition (versus rest) separately, plus AV versus AUD + VIS.   P  -value correction for multiple comparisons was based on a combination of threshold cutoff and cluster extent using 3dmerge (AFNI). Minimal cluster size was calculated using Monte Carlo simulation program AlphaSim (AFNI). For a masked AFNI image, AlphaSim ran 1,000 iterations, with a radius connectivity of 4.1 (since slice thickness + gap was 4 mm) and image defined Gaussian filters with FWHM determined with 3dFWHM. The minimal cluster size to avoid false cluster detection was 57 voxels for   P   < 0.05, 16 voxels for   P   < 0.01, 11 voxels for   P   < 0.005, 6 voxels for   P   < 0.001. Alpha maps were overlaid on inflated PALS atlas cortical model brains (Van Essen  ; Van Essen et al.  ). 

Regions of interest (ROIs) were based on significant activation from the analyses above. The average time course of the MR BOLD response in select ROIs was generated using the AFNI 3dDeconvolve program with the iresp option. The average time courses for each condition (AUD, VIS, AV, ApV) were averaged within each ROI and normalized across datasets. For a given hemisphere, we took voxels showing significant activation from that hemisphere plus its mirror opposite correlate (using 3dLRflip in AFNI), such that each ROI had equivalent right and left hemisphere volumes. 


#### Event-Related Potentials 
  
ERP analysis was performed using Neuroscan 4.3 Software (Neurosoft, Inc., Sterling, VA, USA). EEG data were first segmented into 1500 ms epochs with 100 ms pre-stimulus baseline based on the event markers which identified each trial type. The EEG data of the target trials were not included in subsequent analyses (similar to the fMRI study). Epochs containing artifact registering greater than ±100 μV, due primarily to eye blinks, or electromyographic activity due to face or head motion, were excluded from subsequent analyses. We excluded data from five of the eighteen participants (three participants had technically suboptimal studies due to excessive eye blinks/muscle activity in their EEG data, and two participants were deemed to be overly familiar with the stimuli and showed low alertness levels during the study), leaving thirteen participants in the final ERP analyses. 

The zero time point was the start of the audio, visual or simultaneous audiovisual stimulus. Individual epochs were normalized relative to a 100 ms prestimulus baseline, and linear trend was calculated and removed across the entire epoch, based on the prestimulus baseline. Stimulus types for each condition (AUD, VIS, AV) were averaged across all 6 runs. Each participant’s averaged ERP data were then digitally smoothed with a zero phase-shift low pass filter (cut-off 30 Hz, 6 dB/octave). 

Group averages were constructed and the averaged ERPs were scrutinized to identify ERP peaks and troughs. P100, N170, P250, and P500 ERP components were identified in the group average waveforms. Area under the curve (AUC) ranges were also selected for certain broader peaks. Latency ranges (windows) were selected for each grand average ERP peak or trough, and an automated peak picking routine was then run on the averaged ERP data of each individual subject. Area under the curve (AUC) measures were also taken for selected ERP components. Each subject’s ERP waveforms and ERP peak amplitude and latency measures and AUCs were exported as sets of ASCII files. 

Topographic voltage maps were created from the grand average ERP data at peak and trough timepoints to examine the regional distribution of ERP activity. Data from multiple sensors showing similar ERP behavior were then averaged as noted in the results section, with location of sensor markers determined by averaging Polhemus digitizer locations. 

Data at eye channels were also displayed in order to determine whether ERP signals may have been influenced by systematic, but subtle, eye movements. The signal excursion for the artifact free data in the eye channels was small (on the order of μV) and therefore did not appear to be due to actual eye movements which typically generate signals on the order of mV. In addition, the lower horizontal EOG channels did not show an equal and opposite negativity, suggesting that the positivity in the upper vertical EOG channel, located on the forehead, was likely due to frontal brain activity and not to eye related activity per se. 


#### Statistical Analysis of ERP Data 
  
Student’s   t   tests, one-way (Condition) and two-way (Condition by Hemisphere) ANOVAs of peak amplitudes, latencies, and AUC for particular ERP components were analyzed using SPSS V15. In order to objectively determine the timepoints for AUC measures, we calculated timepoint by timepoint values for the   t   test difference for AV versus AUD plus VIS. Thus, we set as time regions for AUC, periods of sustained (20 ms, 5 timepoints) significant differences (  t   > 1.67 for   n   = 60 epochs) between the multisensory and sum of the unisensory conditions. We used similar calculations to measure the time elapsed, after which no significant peaks occurred, an effective Return to Baseline (RTB). To determine the RTB we first calculated the   t   value versus zero for each point on the waveform. The RTB was defined as the end of the last significant peak of sustained (20 ms, 5 timepoints) significance. 




## Results 
  
### fMRI Data 
  
Ten participants completed the non-verbal unisensory target detection task in a 3T MRI scanner. A split sagittal slice acquisition optimized sampling of temporal cortex, but excluded medial regions as well as more medial aspects of frontal parietal cortex, fusiform and early visual cortex. 

All three conditions produced robust activation in sensory and higher-order cognitive regions. AUD and AV conditions produced additional and extensive activation of mid- to anterior STS and mid-insula (Fig.  a, c), whereas VIS and AV conditions produced activation of lateral occipital and posterior middle temporal gyrus (LO/pMTG) and lateral fusiform gyrus (Fig.  b, c). Brain regions showed multisensory relationships that fell into four main categories (Fig.  ):   
Group fMRI activation maps for each stimulus condition, AUD (  a  ) VIS (  b  ) and AV (  c  ) versus REST.   Warm colors   represent net positive BOLD signal,   cool colors   represent net negative BOLD signal.   d   Difference maps for VIS versus AUD. Regions more active in the VIS condition are represented by   warm colors   (  P   < 0.01 corrected).   e   Common activation maps (  black  ) for AV, VIS and AUD (  P   < 0.001 corrected). Overlaid regions show mathematical superadditivity (  solid white lines  ) and underadditivity (  dashed white lines   P   < 0.05 corrected) 
    
Group fMRI data ROI analyses: underadditive BOLD responses. Histograms depict relative fMRI percent signal change for all three conditions in   underadditive   ROIs (  a  –  c  )   AV maximum   (  d  –  e  ) and   common-activation   ROIs (  f  ).   Asterisks   indicate pairwise   t   test significance: *   P   < 0.05, **   P   < 0.01, ***   P   < 0.001 
    
 superadditive  , defined as audiovisual greater than the sum of auditory alone and visual alone i.e. AV > ApV; 
  
 underadditive  , defined as audiovisual less than the sum of auditory and visual alone, and audiovisual less than the dominant sensory modality e.g. AV < ApV and AV < AUD or VIS; 
  
 AV maximum  , defined as audiovisual greater than either unisensory condition along (AV > VIS and AV > AUD, and VIS > 0, AUD > 0). 
  
 Common activation  , defined as AV activation equal to one or both conditions (AV = AUD and/or VIS. Note that for both AV maximum and Common activation, audiovisual would be less than the sum of the unisensory conditions (AV < ApV). 
  

Two regions showed mathematical   superadditivity,   the right insula/frontal operculum and left angular gyrus. However, this resulted from negative activation versus baseline in one or in all three conditions, and neither region showed significant positive activation for the AV condition (solid white outlines, Fig.  d). 

Several regions showed significant or near-significant   underadditive   effects including MTG, LO/pMTG, and lateral fusiform gyri, with the AV condition showing decreased activation compared with either unisensory condition or the sum of the unisensory conditions, ApV (Table  A). The AUD-preferred region, left mid-MTG, showed a trend (  P   < 0.1) of AV < AUD. Similarly, for the VIS-preferred regions, LO and fusiform gyrus, there was a significant difference and trend, respectively, of AV < VIS. LO and fusiform also showed a right-hemisphere bias (Table  A).   
Summary of significant and trend effects resulting from 2-way ANOVA comparisons for (A) fMRI and (B) ERPs 
  
The middle columns show the results of AV versus ApV, while the right columns show the effects for AV versus VIS and AV versus AUD. B has an additional set of columns for ERP latency effects.   Asterisks   indicate   P  -value of   F   statistic: *   P   < 0.05, **   P   < 0.01, ***   P   < 0.001, ****   P   < 0.00001. All effects were tested, but non-significant interactions are listed as n.s. or are omitted. In several cases, noteworthy   t   tests are listed 
  

 AV maximum   and Common activation   effects were seen in frontal, parietal and temporal regions (Black overlay at   P   < 0.001, Fig.  d). The pSTS, TPJ, IFG, and DLPFC all showed strong condition effects for AV < ApV (Table  B). Portions of these regions were also revealed in a voxelwise   t   test of AV versus ApV (dashed white lines in Fig.  d). The pSTS showed a significant hemisphere effect (right > left), with the VIS condition showing the strongest lateralization (  t   = 4.98,   P   < 0.005).   AV maximum   activation was seen in several of these regions, including right posterior pSTG (  P   < 0.01 versus VIS,   P   < 0.001 versus AUD), and TPJ (  P   < 0.01, versus VIS,   P   < 0.05 versus AUD). IFG and DLPFC showed   common activation,   with the AUD condition showing the least activation in DLPFC. 

The STS and IFG regions, in addition to showing at least a trend towards hemisphere effects for amplitude of activation (Table  A), also showed a greater number of active voxels in the right versus left hemisphere (Right STS: 134 versus Left STS: 0; Right IFG: 1871 mm  versus Left IFG: 157 mm ), thus showing right hemisphere dominance in both magnitude and extent of activation. 

A separate group of 13 participants participated in the ERP version of the experiment. Since the neutral, eyes forward face was present as a baseline for the duration of VIS and AV blocks (except during movements), the zero time point for ERP measurements is at the onset of the facial movement and/or simultaneous vocalization (for AUD, onset of vocalization). In general, ERP waveforms revealed modality-specific early components with characteristic topographies and morphologies, in addition to a late positivity (Fig.  ). Stimuli containing auditory stimulation (AUD and AV conditions) showed the typical auditory N140 with an amplitude maximum at midline central electrodes. Stimuli containing visual stimulation (VIS and AV conditions) showed the typical face-specific N170 at bilateral temporo-occipital electrodes, showing delayed latencies typical of dynamic visual stimuli. In addition to showing both typical auditory and face-related components, the AV condition also elicited a unique early positivity in left parieto-occipital electrodes. All three conditions showed a diffuse late positivity which lasted up to 1500 ms, and varied in amplitude between conditions. A   t   test of AV versus ApV was used to search for regions of potential multisensory effects. Again, effects were grouped into the four categories of   superadditive  ,   underadditive, AV maximum   and   common-activated  .   
Group average topographic ERP maps as a function of condition and time. Topographic maps are depicted at post-stimulus time points of 80, 140, 270 ms, and then every 100 ms for all three stimulus conditions (  top three rows  ). The   bottom row   shows difference maps for the AV condition minus the sum of the unisensory conditions (ApV).   Red-yellow   shows positive ERP activity,   blue-aqua   show negative ERP activity. Topographic maps showing timepoints analyzed in subsequent figures are labeled 
  

#### Superadditivity at 60–148 ms 
  
At this relatively early post-stimulus time range, the AV condition elicited an early positivity in bilateral temporo-occipital electrodes that was not seen in either unisensory condition (Figs.  ,  a). We performed a timepoint-by-timepoint   t   test analysis to determine the time range showing significant differences between AV and AUD plus VIS (i.e. AV versus ApV), and performed an AUC analysis for this time range (60–148 ms). A two-way ANOVA revealed superadditivity, with main effects of Hemisphere [  F  (2,24) = 16.67,   P   < 0.01].   
Superadditivity in group ERP averaged data at bilateral temporo-occipital electrodes (from 60 to 148 ms). Histograms for area under the curve (AUC) analysis for time range 60–148 ms. Topographic map shows 12 sampled electrodes (  white dots  ). Waveform for 6 averaged right temporal occipital electrodes shows unique peak for AV (  black circle  ).   Asterisks   indicate pairwise   t   test significance 
  


#### Trend Towards Underadditivity for N140 
  
AUD and AV, but not VIS, conditions elicited a central negativity at 144.7 ms, typical for auditory stimuli (Fig.  a). Peak amplitude analysis revealed a trend for AUD > AV (Table  B), but found no significant super- or underadditivity.   
Underadditivity in group averaged ERP data in relatively early post-stimulus timeranges. Histograms for peak amplitude for   a   N140 and both peak amplitude and latency for   b   N170. Averaged ERP waveforms for sampled electrode sites (  white dots   on topographic maps), appear at the right of the histograms.   Asterisks   indicate pairwise   t   test significance.   P  -values listed for non-significant trends 
  


#### Underadditivity for N170 
  
All three conditions produced a negativity at an average 276.7 ms, with preference for conditions including visual stimuli (Fig.  b). The waveform was characteristic of the N170 which is elicited by dynamic faces, peaking at temporo-occipital electrodes, with a right hemisphere bias (Fig.  c, white dots). As the stimulus was dynamic, the N170 was considerably delayed relative to the 170 ms typical for the presentation of static face stimuli (see Puce et al.  ). Face movement began at 0 ms and was generally identifiable as a particular “vocalization” by 33 ms. Although the AV and VIS N170 s were larger, the AUD condition also elicited a negativity that had a similar timecourse at these electrodes (274.8 ms). The N170 was underadditive, such that AV < ApV, and showed condition and hemisphere main effects (Table  B). Peak analysis revealed reduced amplitude and decreased latency for the AV versus VIS condition, as well as a right hemisphere bias for amplitude (Table  B). 


#### AV Maximum and Common Activation for Late Positivities 
  
All three conditions elicited widely distributed late positivities (Fig.  ). A strict AV versus ApV   t   test revealed an underadditivity at bilateral temporo-parietal electrodes at 230–304 ms (timepoint-by-timepoint   t   test, Fig.  a). Even though the peaks were broad, we used a semi-automated peak analysis with verification of peaks in individual subjects data. We wanted to examine the data for latency differences, and more strictly apply multisensory criteria (at the time of the AV and AUD peaks, the VIS peak has yet to appear). The temporo-parietal peak occurred at 240 and 244 ms respectively for the AUD and AV conditions, but was delayed at 328 ms for the VIS condition. This peak was considered a   common activation,   as analysis, using the homologous peaks for all three conditions, did not show any significant amplitude differences between conditions (Table  B).   
Later ERPs histograms and waveforms. Charts showing peak amplitude and latency analysis for Common activation   a   12 temporo-parietal electrodes (6 in each hemisphere) in the post-stimulus timerange, 230–304 ms and AV maximum,   b   8 frontal-temporal electrodes in the timerange 460–616 ms. Rightmost panel in each row shows ERP waveforms for each condition along with AV-ApV topographic maps (sampled electrodes are   white dots  ).   Asterisks   indicate pairwise   t   test significance. Very late timeranges show common activation in occipital and frontal electrode sites (  c  –  e  ) 
  

An underadditivity was seen at right fronto-temporal electrodes at 460–616 ms (timepoint-by-timepoint   t   test, Fig.  b). Latency of this broad peak was also greatest for the VIS condition (492 ms) and was similar across the AUD and AV conditions (384 and 400 ms, respectively). Analysis of homologous peaks revealed a Condition effect, with the AUD condition showing the smallest amplitude (Table  B). This peak was considered an   AV maximum  , as the AV condition was significantly greater than either unisensory condition in the right hemiscalp, and a similar trend was seen in the mirror opposite electrodes (Fig.  b). 

 Common activation   was seen at several other electrode sites, include occipital and frontal electrodes. Occipital electrodes showed equivalent sustained activation for all three conditions in the 700–800 ms range (Fig.  c). In contrast, for frontal electrodes, F5 and FPZ, late sustained activation was only seen for AV and VIS conditions and not the AUD condition (rectangles, Fig.  d, e). Note at F5 the   AV maximum   peaks at the 250 ms and 450 ms ranges (circle, Fig.  d). 



### Results Summary 
  
The ERP data showed a unique early positivity for the AV condition starting at 60 ms. In sum, however, there was convergence of ERP and fMRI data. Both the N140 (generated in the superior temporal plane) (Giard et al.  ; Godey et al.  ; Ponton et al.  ), and the auditory STS showed non-significant trends towards underadditivity. For VIS related processing, AV activation was significantly reduced (smaller amplitude ERPs and BOLD responses) compared to the preferred unisensory stimuli (VIS). Finally, multiple “higher-order” cortical regions, as well as late time ranges typically associated with more higher-order processes showed significant or near significant trends of AV maximum activation (AV greater than either condition alone). Other regions/timeranges showed common activation in which AV and one or both conditions showed similar degrees of activation. Several of these regions and timeranges showed a right hemisphere bias. 



## Discussion 
  
Using an animated synthetic face and associated real human non-verbal vocalizations we elicited reliable fMRI activation to unisensory and multisensory stimulation in an imaging study designed to optimally image the STS/STG in its entirety, while still including face-sensitive and auditory regions in lateral sensory cortex. In a second group of subjects we elicited reproducible and consistent ERPs to the relatively long durations of the facial motion and associated vocalizations. Subjects were asked to detect unisensory target stimuli (a blink and an “mmm” sound) so that we could study audiovisual integration without a bias to a particular sensory modality. We discuss our results, summarized in Fig.  , in the context of other audiovisual non-biological and speech integration studies, and in more general terms of social cognition.   
Summary of main fMRI and ERP findings in terms of time of occurrence relative to stimulus onset and type of multisensory phenomenon 
  

### Multisensory Effects in Sensory-Related Processes 
  
Multisensory effects were seen in early regions (fMRI data) and ERP components which supported our hypotheses predicting facilitation effects. Interestingly, we also observed a unique early AV ERP component. This AV positivity peaking around 60–80 ms is similar to that seen in recent audiovisual integration studies (Giard and Peronnet  ; Shams et al.  ). Somewhat surprisingly, these studies, which used   non-biological   stimuli, showed early AV integration effects whose laterality was opposite to ours. Giard and Perronet ( ) proposed that their early ERP response may stem from the recruitment of specific multisensory cells in or near striate cortex, where bisensory cells have been seen observed in animals (Fishman and Michael  ; Morrell  ). Due to our slice selection in our fMRI study, we could not confirm whether multisensory effects occurred in early visual cortex, however, such early effects in humans have been seen in other studies (Martuzzi et al.  ). 

Aside from the unique AV ERP signal, AUD and VIS-related sensory-related regions and ERP signals showed multisensory effects characteristic of facilitation, as predicted by our hypothesis. Significant effects were seen in VIS-related regions (LO) and ERP components (N170), and trends in the same direction were seen in fMRI activated AUD-related regions and ERP components (mid-MTG and N140). The strongest case of fMRI and ERP convergence was at mid-level visual processes, characterized by a right hemisphere bias and decrease in both amplitude (fMRI and ERP) and speed (ERP) of AV versus VIS. 

The auditory trend towards facilitation was consistent with the role of the centrally located N140 in multisensory integration (Besle et al.  ; Puce et al.  ; van Wassenhove et al.  ). A study by Puce et al. ( ) showed the largest N140s were elicited when a dynamic human face (relative to house and primate face stimuli) was paired with incongruous sounds, suggesting that the context provided by a conspecific (human) face influences associated auditory processing. Additionally, when congruous sounds were presented, the N140 was largest to both human and primate faces when paired with species-appropriate vocalizations relative to a house stimulus whose front door opened with a creaking door sound (Puce et al.  ). However, unlike previous ERP studies using speech stimuli (Besle et al.  ; van Wassenhove et al.  ), our results here did not reach significance, perhaps due to differences in timing of facial movements relative to vocalizations. In our paradigm, face movement and audio were simultaneous, although there was a natural delay in the movement peaking for our non-verbal stimuli (e.g. fully open mouth, upturned eyes in the sigh in Fig.  ), which is opposite to speech stimuli. Ghazanfar et al. (Ghazanfar et al.  ) also have shown that timing plays a critical role in multisensory effects in an experiment in which monkeys were presented conspecific coos and grunts along with images of primate faces, however, in this experiment static faces of primates were utilized. Multisensory neurons in the auditory core and belt regions showed more enhancement when the face versus audio delay was less than 100 ms, and facilitation when the delay was greater than 200 ms. The variance of these studies based on timing underscores the importance of subtle audio versus visual onset time differences in multisensory processing. 

Reduced activation in the AV condition could be due to various causes, including less energy demands brought about by facilitated processing. Alternatively, the relative decrease in amplitude in the multisensory relative to the unisensory conditions may be due a smaller population of neurons with exclusively multisensory versus unisensory preferences (Beauchamp  ; Laurienti et al.  ). Alternatively, the distribution of resources available to process these stimuli might be limited over multiple sensory cortical regions. We favor the increased efficiency explanation in the light of the behavioral facilitation effects observed in many studies (Bolognini et al.  ; Gondan et al.  ; Grant and Walden  ; Miller  ; Sumby and Pollack  ), although all three explanations are plausible and cannot be differentiated in the current dataset. Multisensory optimization may also take the form of synchronization of the phase of oscillatory stimuli (such as gamma band activity) (Engel et al.  ; Schroeder et al.  ; Senkowski et al.  ), which can produce important behavioral sequelae (Schroeder et al.  ). Future studies quantifying ERP amplitudes and latencies, oscillatory activity and behavior in a combined manner might better clarify the underlying nature of these processes. 

#### Higher-Order Underadditive Effects 
  
We saw significant activation by all three conditions in putative higher-order cognitive processes (based on latencies (ERP data) and origins (fMRI data); Doeller et al.  ; van Herten et al.  ; Vuilleumier and Pourtois  ). Here a clear case for convergence is more difficult to make since later ERPs are typically diffusely distributed, making source localization challenging, as they can potentially come from multiple sources (Siedenberg et al.  ; Soltani and Knight  ). However in our study, later ERPs, and activation in higher-order brain regions as revealed by fMRI showed spatially distributed responses and a combination of   AV maximum   and   Common activation   responses. From the fMRI side, this network of frontal, temporal and parietal regions has been implicated in other studies as playing an important role in multisensory perception (for review see Ghazanfar and Schroeder  ), as well as for understanding speech and socially related stimuli (Calvert et al.  ; Moll and de Oliveira-Souza  ). For the ERPs, there were multiple distributed late peaks that showed an AV response with properties from both unisensory conditions (AV elicited larger amplitudes like the VIS condition and faster latencies like the AUD condition). 

It is much more difficult to attribute higher-order activation as being specifically related to multisensory processing, as these regions did not show superadditivity, perhaps due to ceiling effects from these robust stimuli (Stevenson et al.  ). In addition, higher order processes can be non-specific and can be very sensitive to other factors such as attention. The target stimuli were always unisensory, and therefore there were two possible unisensory targets in the AV blocks. It is possible that in the AV and VIS conditions responding to the corresponding unisensory target stimulus may have resulted in potentially greater stimulus-driven attentional effects. Having said that, the STS, IFG, and TPJ have shown potential multisensory behavior in other studies where the task requirements did not involve such contingencies (Calvert et al.  ; Kawashima et al.  ). Additionally, the blocked-event design in this study could conceivably have produced some refractoriness effects in the data, and in other studies (Calvert et al.  ; Kawashima et al.  ), albeit unlikely. Ideally, an event-related design would circumvent these kinds of issues. 

Notably, all three conditions in our study activated right pSTS, a region previously shown to be important in social-related multisensory processing (Redcay  ). Right pSTS along with right TPJ, were the only regions in which there was maximum activation in the AV condition. Further evidence of pSTS importance in both multisensory and social processing come from prior research on a possible pSTS homologue in monkeys, the Anterior Superior Temporal Polysensory Area (STPa), which responds to visual biological motion, faces, and head and body view and direction (Jellema et al.  ; Oram and Perrett  ), and projects to higher order cognitive and emotional processing regions such as the amygdala and prefrontal regions (Oram and Perrett  ). 

In addition, we saw right-hemisphere dominated effects in STS and frontal regions, which is typically not seen in most speech related studies (Campbell et al.  ; Capek et al.  ; Hubbard et al.  ; Kawashima et al.  ; Macaluso et al.  ; MacSweeney et al.  ; Skipper et al.  ). However, left lateralized activation has been less strongly observed in multisensory studies of simple speech, syllables, and emotional prosody (Kreifelts et al.  ; Olson et al.  ; Wright et al.  ). It is possible that higher-order regions in both hemispheres have multisensory properties and are recruited based on verbal versus non-verbal relevance. 



### Summary 
  
Both imaging modalities produced datasets that were very complex, yet there was a surprising degree of convergence between the ERP and fMRI data (Fig.  ). Underadditivity dominated the multisensory effects in earlier regions as supported by the significant (VIS areas) and trend towards (AUD) smaller and faster responses for AV versus unisensory stimuli. These data, along with previous behavioral studies, suggest that early or mid-sensory regions may be optimized to process multisensory stimuli, if information from multiple modalities is available (Foxe and Schroeder  ). Multiple “higher-order” cortical regions, as well as late ERP activity typically associated with more higher-order processes showed underadditive effects driven by common activation for all conditions of non-verbal human stimuli, with a dominance of the AV condition in temporal regions. In particular the unique right pSTS effects confirm the important role of pSTS in social cognition, and again show the tendency toward right lateralization for social-related stimuli. 



## Electronic supplementary material 
  
 </div>
</div>
</div>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-61' name='judgment-61' value='agree'>
<label for='agree-61'>Agree</label>
<input type='radio' id='disagree-61' name='judgment-61' value='disagree'>
<label for='disagree-61'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-61' name='comment-61' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-62'>
<h2>62. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/25987597/' target='_blank'>25987597</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI study of social working memory (social cognition) in a healthy adult sample (N=25; mean age 21.6). The task explicitly manipulates social content (trait-based reordering) versus a non-social control and reports group-level, whole-brain, voxelwise univariate analyses (parametric modulation, contrasts SWM vs CWM, conjunctions) with cluster and voxelwise thresholds, tables of activated clusters, and surface-rendered maps. Results for the healthy participant group are reported separately and are generalizable to healthy adults. No exclusion criteria (e.g., ROI-only, connectivity-only, non-empirical, or out-of-range ages) are violated. Therefore the study meets all inclusion criteria for the review.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-62' name='judgment-62' value='agree'>
<label for='agree-62'>Agree</label>
<input type='radio' id='disagree-62' name='judgment-62' value='disagree'>
<label for='disagree-62'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-62' name='comment-62' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-63'>
<h2>63. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/18234518/' target='_blank'>18234518</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI task study of social cognition in healthy adults (N=16, ages 18–29). The task investigates evaluating others’ actions vs intentions (social-related processing). The paper reports group-level, univariate voxelwise whole-brain analyses (voxelwise ANOVAs with reported thresholds and coordinates and voxelwise results described in text/figures), not only ROI results. Healthy adult sample results are reported and generalizable via random-effects analyses. No exclusion criteria are violated (not connectivity-only, not ROI-only, participants within age range). Therefore it meets all inclusion criteria for the review.</p>
<p><strong>Fulltext Confidence:</strong> 0.95</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-63' name='judgment-63' value='agree'>
<label for='agree-63'>Agree</label>
<input type='radio' id='disagree-63' name='judgment-63' value='disagree'>
<label for='disagree-63'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-63' name='comment-63' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
<div class='study' id='study-64'>
<h2>64. PMID: <a href='https://pubmed.ncbi.nlm.nih.gov/24550800/' target='_blank'>24550800</a></h2>
<div class='screening'>
<h3>Screening Results</h3>
<p><strong>Fulltext Decision:</strong> included</p>
<p><strong>Fulltext Reasoning:</strong> This is an fMRI task study of affective (social) touch including a healthy adult group (N=22, ages 19–35). The paper reports group-level, whole-brain random-effects GLM analyses for adults (Arm>Rest, Palm>Rest) with cluster-corrected thresholds and tables/figures of activations (including pSTS, insula), satisfying whole-brain evidence. The task probes social/affiliative processing (affective-motivational touch) relevant to the review constructs. No exclusion criteria apply (not ROI-only, not connectivity-only, includes healthy adult subgroup within 17–65). Therefore it meets all inclusion criteria.</p>
<p><strong>Fulltext Confidence:</strong> 0.92</p>
</div>
<div class='content'>
<h3>Fulltext Content</h3>
<p>Fulltext not available</p>
</div>
<div class='annotation'>
<h3>Annotation</h3>
<p><strong>Do you agree with the LLM's judgment?</strong></p>
<input type='radio' id='agree-64' name='judgment-64' value='agree'>
<label for='agree-64'>Agree</label>
<input type='radio' id='disagree-64' name='judgment-64' value='disagree'>
<label for='disagree-64'>Disagree</label>
<br><br>
<label for='comment'><strong>Comments:</strong></label>
<textarea id='comment-64' name='comment-64' rows='4' cols='50' placeholder='Add your comments here...'></textarea>
</div>
</div>
</div>

<footer>
    <p>Generated by Qualitative Review Tool for Meta-Analysis Pipeline</p>
</footer>
</body>
</html>
