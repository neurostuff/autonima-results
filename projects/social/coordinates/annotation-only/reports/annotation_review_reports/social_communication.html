<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>social_communication review report</title>
  <style>
    :root {
      --bg: #f7f6f2;
      --panel: #ffffff;
      --ink: #1d2730;
      --line: #d8dde3;
    }
    body { margin: 0; padding: 1.25rem; font-family: "IBM Plex Sans", "Segoe UI", sans-serif; background: var(--bg); color: var(--ink); }
    header { background: var(--panel); border: 1px solid var(--line); border-radius: 10px; padding: 1rem; margin-bottom: 1rem; }
    .top-nav { position: sticky; top: 0; z-index: 10; display: flex; flex-wrap: wrap; gap: 0.5rem; background: #eef3f2; border: 1px solid var(--line); border-radius: 10px; padding: 0.6rem; margin-bottom: 1rem; }
    .top-nav a { display: inline-block; padding: 0.35rem 0.6rem; border: 1px solid var(--line); border-radius: 999px; background: #fff; text-decoration: none; font-size: 0.9rem; color: #0e4f85; }
    section { margin-bottom: 1rem; }
    .bucket > summary, .doc-card > summary, .inner-accordion > summary { cursor: pointer; }
    .doc-card { background: var(--panel); border: 1px solid var(--line); border-radius: 10px; padding: 0.85rem; margin-bottom: 0.85rem; }
    .table-wrap, .table-html { overflow-x: auto; }
    .inner-accordion { margin-top: 0.6rem; border-top: 1px dashed var(--line); padding-top: 0.4rem; }
    .paper-text { white-space: pre-wrap; max-height: 26rem; overflow-y: auto; background: #fbfcfe; border: 1px solid var(--line); border-radius: 8px; padding: 0.6rem; font-size: 0.88rem; line-height: 1.35; }
    table { width: 100%; border-collapse: collapse; font-size: 0.9rem; }
    th, td { border: 1px solid var(--line); padding: 0.45rem; vertical-align: top; text-align: left; }
    th { background: #edf2f5; }
    .decision-cell, .confusion-cell { text-align: center; vertical-align: middle; }
    .decision-pill, .confusion-pill {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      min-width: 1.55rem;
      padding: 0.12rem 0.45rem;
      border-radius: 999px;
      font-weight: 700;
      font-size: 0.82rem;
      border: 1px solid transparent;
    }
    .decision-include { background: #e9f8ef; color: #1f7a3d; border-color: #b7e4c6; }
    .decision-exclude { background: #fdecec; color: #9b1c1c; border-color: #f6caca; }
    .decision-none { background: #f2f4f7; color: #5b6775; border-color: #dde3ea; }
    .confusion-good { background: #e9f8ef; color: #166534; border-color: #b7e4c6; }
    .confusion-bad { background: #fdecec; color: #991b1b; border-color: #f6caca; }
    .confusion-na { background: #f2f4f7; color: #5b6775; border-color: #dde3ea; }
    a { color: #0e4f85; }
  </style>
</head>
<body>
  <header>
    <a id="top"></a>
    <h1>social_communication report</h1>
    <p>Manual benchmark is sliced to the auto PMID universe from <code>outputs/nimads_annotation.json</code>. Analysis-level row is evaluated only on manual analyses with an assigned auto match (truth positives are accepted fuzzy matches only).</p>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Level</th>
            <th>TP</th>
            <th>FP</th>
            <th>FN</th>
            <th>Precision</th>
            <th>Recall</th>
            <th>F1</th>
            <th>Manual Positives</th>
            <th>Predicted Positives</th>
            <th>Universe</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Document bucket overlap</td>
            <td>45</td>
            <td>55</td>
            <td>6</td>
            <td>0.450</td>
            <td>0.882</td>
            <td>0.596</td>
            <td>51</td>
            <td>100</td>
            <td>106</td>
          </tr>
          <tr>
            <td>Study inclusion</td>
            <td>52</td>
            <td>49</td>
            <td>5</td>
            <td>0.515</td>
            <td>0.912</td>
            <td>0.658</td>
            <td>57</td>
            <td>101</td>
            <td>134</td>
          </tr>
          <tr>
            <td>Analysis inclusion (matched manual universe; accepted=positive)</td>
            <td>158</td>
            <td>22</td>
            <td>19</td>
            <td>0.878</td>
            <td>0.893</td>
            <td>0.885</td>
            <td>177</td>
            <td>180</td>
            <td>202</td>
          </tr>
        </tbody>
      </table>
    </div>
  </header>
  <nav class="top-nav">
    <a href="#bucket-correct">Correct (45)</a>
    <a href="#bucket-false-positive">False Positive (55)</a>
    <a href="#bucket-false-negative">False Negative (6)</a>
    <a href="#missing-manual">Missing PMIDs (0)</a>
    <a href="#top">Top</a>
  </nav>
  <section id="bucket-correct"><details class="bucket"><summary><h2>Correct (45)</h2></summary><p><strong>Match status totals:</strong> accepted=168 | uncertain=7 | unmatched=18</p>
<details class="doc-card">
  <summary><strong>PMID 14980212</strong> | Pred included: 4 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/14980212/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>14980212_analysis_0</td><td>Affective-Neutral Faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task involves viewing faces and implicit social judgments (gender judgments on faces previously encountered in social interactions). Viewing and evaluating faces falls under social communication (receptive facial communication).</td></tr>
<tr><td>14980212_analysis_1</td><td>Cooperator-Neutral Faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants view faces and implicitly evaluate socially relevant cues (cooperators). This fits reception of facial communication.</td></tr>
<tr><td>14980212_analysis_2</td><td>Defector-Neutral Faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis involves viewing faces learned through social interaction and implicitly evaluating them (receptive facial communication), satisfying social communication criteria.</td></tr>
<tr><td>14980212_analysis_3</td><td>Cerebral Foci of Activation for Intentional versus Nonintentional Cooperator Faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast involves viewing faces and implicit social inference about intentionality and cooperation, engaging reception of facial/social cues and thus social communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>14980212_1</td><td>Affective &gt; Neutral faces; socialcommunication</td><td>14980212_analysis_0</td><td>Affective-Neutral Faces</td><td>0.917</td><td>1.000</td><td>0.975</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>14980212_2</td><td>Cooperator &gt; Neutral faces; socialcommunication</td><td>14980212_analysis_1</td><td>Cooperator-Neutral Faces</td><td>0.920</td><td>1.000</td><td>0.976</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>14980212_3</td><td>Defector &gt; Neutral faces; socialcommunication</td><td>14980212_analysis_2</td><td>Defector-Neutral Faces</td><td>0.913</td><td>1.000</td><td>0.974</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 15808992</strong> | Pred included: 2 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/15808992/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>15808992_analysis_0</td><td>self-other contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task consists of viewing faces and making identity judgements (receptive facial communication). Viewing and discriminating faces is a canonical social communication/reception paradigm and matches the examples (Viewing pictures of faces).</td></tr>
<tr><td>15808992_analysis_1</td><td>other-self contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Viewing and evaluating familiar faces is a receptive facial-communication task (face perception and social information extraction). The contrast (Other &gt; Self) thus measures social communication processes.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>15808992_1</td><td>Other - Self; socialcommunication</td><td>15808992_analysis_1</td><td>other-self contrast</td><td>0.645</td><td>1.000</td><td>0.894</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>15808992_2</td><td>Self - Other; socialcommunication</td><td>15808992_analysis_0</td><td>self-other contrast</td><td>0.645</td><td>1.000</td><td>0.894</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16035037</strong> | Pred included: 5 | Manual included (accepted matches only): 5 | Correct overlaps: 5 | Match statuses: accepted=5, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16035037/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16035037_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Viewing faces (faces &gt; scrambled) taps reception of facial communication and face-processing relevant to social communication.</td></tr>
<tr><td>16035037_analysis_1</td><td>familiar minus distracter contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast involves viewing and discriminating faces (familiar vs unknown), which is reception of facial communication and fits social communication criteria.</td></tr>
<tr><td>16035037_analysis_2</td><td>Local maxima of CBF change during self minus distracter contrast corrected at P = 0.001 and cluster at P = 0.05</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Viewing and classifying faces (including self) engages reception of facial communication and face recognition processes relevant to social communication.</td></tr>
<tr><td>16035037_analysis_3</td><td>Local Maxima of CBF change during familiar minus self contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast involves face viewing and discrimination (familiar vs self), which engages reception of facial communication and face-recognition processes.</td></tr>
<tr><td>16035037_analysis_4</td><td>CBF change during self minus familiar contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Viewing and discriminating self versus familiar faces engages reception of facial communication and face-recognition processes relevant to social communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>16035037_1</td><td>distracter &gt; null; socialcommunication</td><td>16035037_analysis_0</td><td>analysis_0</td><td>0.148</td><td>1.000</td><td>0.744</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>16035037_2</td><td>familiar face &gt; distracter; socialcommunication</td><td>16035037_analysis_1</td><td>familiar minus distracter contrast</td><td>0.667</td><td>1.000</td><td>0.900</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>16035037_3</td><td>familiar face &gt; self face; socialcommunication</td><td>16035037_analysis_3</td><td>Local Maxima of CBF change during familiar minus self contrast</td><td>0.370</td><td>1.000</td><td>0.811</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>16035037_4</td><td>self face &gt; distracter; socialcommunication</td><td>16035037_analysis_2</td><td>Local maxima of CBF change during self minus distracter contrast corrected at P = 0.001 and cluster at P = 0.05</td><td>0.312</td><td>1.000</td><td>0.794</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>16035037_5</td><td>self face &gt; famililar face; socialcommunication</td><td>16035037_analysis_4</td><td>CBF change during self minus familiar contrast</td><td>0.444</td><td>1.000</td><td>0.833</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16055351</strong> | Pred included: 7 | Manual included (accepted matches only): 7 | Correct overlaps: 7 | Match statuses: accepted=7, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16055351/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16055351_analysis_0</td><td>Anger vs. Neutral (AN + NA) vs. (NN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This contrast tests recognition of emotional prosody (auditory emotion), which is a canonical Social Communication task (reception of non-facial communication). Meets Social Communication inclusion I1.</td></tr>
<tr><td>16055351_analysis_1</td><td>Anger to-be-attended vs. Neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This contrast measures reception of emotional prosody (auditory emotion) while attended — directly fits Social Communication (reception of non-facial communication). Meets Social Communication I1.</td></tr>
<tr><td>16055351_analysis_2</td><td>Anger to-be-ignored vs. Neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Even when to-be-ignored, the contrast probes reception of emotional prosody (auditory emotion), fitting Social Communication inclusion I1.</td></tr>
<tr><td>16055351_analysis_3</td><td>Anger to-be-attended vs. to-be-ignored</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast assesses differential processing of emotional prosody (auditory reception of non-facial social signals) under attention — fits Social Communication I1.</td></tr>
<tr><td>16055351_analysis_4</td><td>Anger to-be-ignored vs. to-be-attended</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast evaluates reception of emotional vocal signals under different attention conditions — aligns with Social Communication (reception of non-facial communication) inclusion I1.</td></tr>
<tr><td>16055351_analysis_5</td><td>Spatial attention towards right vs. left ear</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Even though this is a spatial attention contrast, it is derived from a voice/gender-decision task involving emotional prosody and voice perception, which falls under Social Communication (reception of non-facial communication). Meets Social Communication I1.</td></tr>
<tr><td>16055351_analysis_6</td><td>Spatial attention towards left vs. right ear</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Although a spatial attention contrast, it stems from a task involving reception of vocal emotional signals (social communication). Therefore it meets Social Communication inclusion I1.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>16055351_1</td><td>Anger &gt; Neutral (AN + NA) &gt; (NN); socialcommunication</td><td>16055351_analysis_0</td><td>Anger vs. Neutral (AN + NA) vs. (NN)</td><td>0.882</td><td>0.889</td><td>0.887</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>16055351_2</td><td>Anger to be attended &gt; to be ignored; socialcommunication</td><td>16055351_analysis_3</td><td>Anger to-be-attended vs. to-be-ignored</td><td>0.838</td><td>0.800</td><td>0.811</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>16055351_3</td><td>Anger to-be-attended &gt; Neutral; socialcommunication</td><td>16055351_analysis_1</td><td>Anger to-be-attended vs. Neutral</td><td>0.935</td><td>1.000</td><td>0.981</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>16055351_4</td><td>Anger to-be-ignored &gt; Neutral; socialcommunication</td><td>16055351_analysis_2</td><td>Anger to-be-ignored vs. Neutral</td><td>0.933</td><td>1.000</td><td>0.980</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>16055351_5</td><td>anger to be ignored &gt; to be attended; socialcommunication</td><td>16055351_analysis_4</td><td>Anger to-be-ignored vs. to-be-attended</td><td>0.838</td><td>1.000</td><td>0.951</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>16055351_6</td><td>spatial attention to left ear &gt; right ear; socialcommunication</td><td>16055351_analysis_6</td><td>Spatial attention towards left vs. right ear</td><td>0.847</td><td>0.889</td><td>0.876</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>16055351_7</td><td>spatial attention to right ear&gt; left ear; socialcommunication</td><td>16055351_analysis_5</td><td>Spatial attention towards right vs. left ear</td><td>0.847</td><td>1.000</td><td>0.954</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16171833</strong> | Pred included: 6 | Manual included (accepted matches only): 6 | Correct overlaps: 6 | Match statuses: accepted=6, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16171833/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16171833_analysis_0</td><td>a) Common activations of social interaction (SOC &gt; ARB)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>SOC&gt;ARB isolates reception of socially communicative facial gestures (greetings/approach) versus non-communicative movements, matching the Social Communication construct (reception of facial communication).</td></tr>
<tr><td>16171833_analysis_1</td><td>b) Common activations of arbitrary facial movements (ARB &gt; SOC)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>ARB&gt;SOC involves perception of facial movements and distinguishes communicative (SOC) from non-communicative (ARB) signals, directly relevant to Social Communication (reception of facial communication and related distinctions).</td></tr>
<tr><td>16171833_analysis_2</td><td>c) Common activations of self-involvement (ME &gt; OTHER)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>ME&gt;OTHER involves directed gaze (an ostensive communicative signal) and thus indexes a form of social communication (production/reception of gaze-based communicative cues).</td></tr>
<tr><td>16171833_analysis_3</td><td>d) Common activations of other-related activity (OTHER &gt; ME)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>OTHER&gt;ME involves perceiving gaze and facial behavior directed to another, which is part of Social Communication (reception of face/gaze cues and their communicative significance).</td></tr>
<tr><td>16171833_analysis_4</td><td>e) Common activations of the statistical interaction SOC × ME</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>SOC×ME concerns when communicative facial expressions (SOC) are directed at the self, directly indexing social-communication processes (reception of facial/gaze communication in an interpersonal context).</td></tr>
<tr><td>16171833_analysis_5</td><td>f) Common activations of the statistical interaction SOC × OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>SOC×OTHER concerns reception/analysis of socially communicative facial expressions when they are directed to another person, fitting the Social Communication construct (reception of facial/gaze signals).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>16171833_1</td><td>ARB &gt; SOC; socialcommunication</td><td>16171833_analysis_1</td><td>b) Common activations of arbitrary facial movements (ARB &gt; SOC)</td><td>0.250</td><td>1.000</td><td>0.775</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>16171833_2</td><td>ME &gt; OTHER; socialcommunication</td><td>16171833_analysis_2</td><td>c) Common activations of self-involvement (ME &gt; OTHER)</td><td>0.312</td><td>1.000</td><td>0.794</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>16171833_3</td><td>OTHER &gt; ME; socialcommunication</td><td>16171833_analysis_3</td><td>d) Common activations of other-related activity (OTHER &gt; ME)</td><td>0.286</td><td>1.000</td><td>0.786</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>16171833_4</td><td>SOC &gt; ARB; socialcommunication</td><td>16171833_analysis_0</td><td>a) Common activations of social interaction (SOC &gt; ARB)</td><td>0.281</td><td>1.000</td><td>0.784</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>16171833_5</td><td>SOC x ME&gt;Other; socialcommunication</td><td>16171833_analysis_4</td><td>e) Common activations of the statistical interaction SOC × ME</td><td>0.245</td><td>1.000</td><td>0.773</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>16171833_6</td><td>SOC x OTHER&gt; ME; socialcommunication</td><td>16171833_analysis_5</td><td>f) Common activations of the statistical interaction SOC × OTHER</td><td>0.250</td><td>1.000</td><td>0.775</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16759672</strong> | Pred included: 3 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16759672/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> faces associated with nice behaviors &gt; faces associated with neutral behaviors; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16759672_analysis_0</td><td>Areas showing significantly greater activity for faces associated with behaviors than for novel faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast involves viewing faces and assessing socially relevant information bound to faces (reception of facial communication), fitting Social Communication criteria.</td></tr>
<tr><td>16759672_analysis_1</td><td>Faces associated with disgusting behaviors greater than faces associated with aggressive behaviors</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast is based on face stimuli and decoded social-affective information (reception of facially bound social information), so meets Social Communication criteria.</td></tr>
<tr><td>16759672_analysis_2</td><td>Faces associated with aggressive behaviors greater than faces associated with disgusting behaviors</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast uses face stimuli and contrasts socially meaningful information (traits inferred from behaviors), aligning with Social Communication inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>16759672_1</td><td>face associated with behavior &gt; novel faces; socialcommunication</td><td>16759672_analysis_0</td><td>Areas showing significantly greater activity for faces associated with behaviors than for novel faces</td><td>0.583</td><td>1.000</td><td>0.875</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>16759672_2</td><td>faces associated with aggressive behaviors &gt; faces associated with disgusting behaviors; socialcommunication</td><td>16759672_analysis_2</td><td>Faces associated with aggressive behaviors greater than faces associated with disgusting behaviors</td><td>0.930</td><td>1.000</td><td>0.979</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>16759672_3</td><td>faces associated with disgusting behaviors &gt; faces associated with aggressive behaviors; socialcommunication</td><td>16759672_analysis_1</td><td>Faces associated with disgusting behaviors greater than faces associated with aggressive behaviors</td><td>0.930</td><td>0.944</td><td>0.940</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>16759672_4</td><td>faces associated with nice behaviors &gt; faces associated with neutral behaviors; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 17071110</strong> | Pred included: 3 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=2, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/17071110/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Retaliation interaction with low CU &gt; High CU; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>17071110_analysis_0</td><td>Receiving aversive stimuli; parametric modulation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Subjects view social cues (opponent’s actions/scale and later pain-related videos) and emotional signals; this involves reception of socially relevant information (facial/behavioral cues), consistent with social communication measures.</td></tr>
<tr><td>17071110_analysis_1</td><td>Retaliation; parametric modulation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Although primarily decision-oriented, the retaliation condition constitutes a communicative social act (sending a socially meaningful signal to the opponent) and involves processing of socially relevant cues; it therefore fits the social communication inclusion (exchange/reception of social information).</td></tr>
<tr><td>17071110_analysis_2</td><td>Conjunction; retaliation and watching the opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Both components involve exchange and perception of socially relevant signals (observing opponent’s suffering, adjusting punishment)—this involves receptive and productive aspects of social communication (facial/behavioral cues and communicative retaliation).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>17071110_1</td><td>Retaliation &gt; Watching opponent; socialcommunication</td><td>17071110_analysis_1</td><td>Retaliation; parametric modulation</td><td>0.554</td><td>0.750</td><td>0.691</td><td>uncertain</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>17071110_2</td><td>Retaliation interaction with low CU &gt; High CU; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>17071110_3</td><td>Watching opponent &gt; Retaliation; socialcommunication</td><td>17071110_analysis_0</td><td>Receiving aversive stimuli; parametric modulation</td><td>0.375</td><td>0.842</td><td>0.702</td><td>uncertain</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>17071110_4</td><td>Watching opponent &gt; Retaliation (conjunction); socialcommunication</td><td>17071110_analysis_2</td><td>Conjunction; retaliation and watching the opponent</td><td>0.484</td><td>0.900</td><td>0.775</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 17408704</strong> | Pred included: 3 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/17408704/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>17408704_analysis_0</td><td>EMO</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>EMO involves reception of facial communication and affect recognition (selecting the emotion displayed), matching the Social Communication construct.</td></tr>
<tr><td>17408704_analysis_1</td><td>AGE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Although implicit, AGE involves viewing faces and processing facial information; it taps receptive aspects of facial communication (face perception) and so meets Social Communication inclusion criteria.</td></tr>
<tr><td>17408704_analysis_2</td><td>EMO-AGE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast isolates modulation of facial affect perception (reception of facial communication) by task, fitting the Social Communication construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>17408704_1</td><td>explicit (emotion) &gt; baseline; socialcommunication</td><td>17408704_analysis_0</td><td>EMO</td><td>0.188</td><td>1.000</td><td>0.756</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>17408704_2</td><td>explicit (emotion) &gt; implicit (age); socialcommunication</td><td>17408704_analysis_2</td><td>EMO-AGE</td><td>0.286</td><td>1.000</td><td>0.786</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>17408704_3</td><td>implicit (age) &gt; baseline; socialcommunication</td><td>17408704_analysis_1</td><td>AGE</td><td>0.214</td><td>1.000</td><td>0.764</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 17627852</strong> | Pred included: 3 | Manual included (accepted matches only): 4 | Correct overlaps: 3 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/17627852/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>17627852_analysis_0</td><td>Communication—control</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast isolates production of communicative speech (socially directed verbal exchange) relative to a control naming task, matching the Social Communication construct.</td></tr>
<tr><td>17627852_analysis_1</td><td>Description—control</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although verbal, the description task is non-interactive (describing the situation rather than producing social/communicative exchange); the contrast versus control indexes descriptive speech and action perception rather than social communicative production.</td></tr>
<tr><td>17627852_analysis_2</td><td>Communication— description</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This contrast isolates production of social communicative speech (communication) relative to non-communicative description, directly indexing Social Communication production processes.</td></tr>
<tr><td>17627852_analysis_3</td><td>Familiar communication – unfamiliar communication</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast examines production of communicative speech as modulated by familiarity (production of social communication toward familiar vs unfamiliar recipients), fitting Social Communication production.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>17627852_1</td><td>Communication &gt; Description; socialcommunication</td><td>17627852_analysis_2</td><td>Communication— description</td><td>0.943</td><td>0.714</td><td>0.783</td><td>accepted</td><td>coord_count_mismatch</td></tr><tr><td>17627852_2</td><td>Communication &gt; control; socialcommunication</td><td>17627852_analysis_0</td><td>Communication—control</td><td>0.909</td><td>1.000</td><td>0.973</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>17627852_3</td><td>Description &gt; control; socialcommunication</td><td>17627852_analysis_1</td><td>Description—control</td><td>0.900</td><td>1.000</td><td>0.970</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>17627852_4</td><td>Familiar communication &gt; unfamiliar communication; socialcommunication</td><td>17627852_analysis_3</td><td>Familiar communication – unfamiliar communication</td><td>0.980</td><td>1.000</td><td>0.994</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18501639</strong> | Pred included: 15 | Manual included (accepted matches only): 8 | Correct overlaps: 8 | Match statuses: accepted=8, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18501639/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18501639_analysis_0</td><td>Main effect [(Sf + Sn)–(Ff + Fn) masked by Sf–Ff and Sn–Fn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task involves perception of social stimuli (faces and written names) relevant to social communication; contrast assesses differential processing of social cues for self vs friend.</td></tr>
<tr><td>18501639_analysis_1</td><td>Face (simple effect) [Sf–Ff]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Viewing faces and discriminating self vs friend taps receptive facial social-communication processes (face perception).</td></tr>
<tr><td>18501639_analysis_2</td><td>Name (simple effect) [Sn–Fn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Recognition of written names is a non-facial form of social communication/receptive processing; contrast addresses differential social-information processing.</td></tr>
<tr><td>18501639_analysis_3</td><td>Face specific (interaction) [(Sf–Ff)–(Sn–Fn) masked by Sf–Ff]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Face-specific effects implicate receptive facial communication processes; contrast is relevant to social-communication subconstructs.</td></tr>
<tr><td>18501639_analysis_4</td><td>Name specific (interaction) [(Sn–Fn)–(Sf–Ff) masked by Sn–Fn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Written-name recognition is a form of non-facial social communication; name-specific self effects relate to receptive non-facial communication.</td></tr>
<tr><td>18501639_analysis_5</td><td>Main effect [(Ff + Fn)–(Sf + Sn) masked by Ff–Sf and Fn–Sn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast involves face/name stimuli and differential processing for a familiar person, engaging social-communication systems.</td></tr>
<tr><td>18501639_analysis_6</td><td>Face (simple effect) [Ff–Sf]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Face perception of a friend engages receptive facial communication processes (gaze, identity cues).</td></tr>
<tr><td>18501639_analysis_7</td><td>Name (simple effect) [Fn–Sn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Processing written names of a friend implicates non-facial social communication/reception mechanisms.</td></tr>
<tr><td>18501639_analysis_8</td><td>Face specific (interaction) [(Ff–Sf)–(Fn–Sn) masked by Ff–Sf]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Face-specific friend effects engage receptive facial communication processes for familiar others.</td></tr>
<tr><td>18501639_analysis_9</td><td>Name specific (interaction) [(Fn–Sn)–(Ff–Sf) masked by Fn–Sn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Processing names of friends involves non-facial social communication (reception of identity cues).</td></tr>
<tr><td>18501639_analysis_10</td><td>Main effect [(Sf + Ff + Sn + Fn)–2(Cf + Cn) masked by Sf–Cf, Ff–Cf, Sn–Cn, and Fn–Cn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Familiar vs unfamiliar person stimuli (faces/names) engage social-communication networks involved in person recognition.</td></tr>
<tr><td>18501639_analysis_11</td><td>Face (simple effect) [Sf + Ff–2Cf masked by Sf–Cf, and Ff–Cf]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Face familiarity modulation is a core receptive facial communication process (identity and social cue processing).</td></tr>
<tr><td>18501639_analysis_12</td><td>Name (simple effect) [Sn + Fn–2Cn masked by Sn–Cn, and Fn–Cn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Processing familiar vs unfamiliar names is a non-facial form of social communication related to person identity recognition.</td></tr>
<tr><td>18501639_analysis_13</td><td>Face specific (interaction) [(Sf + Ff–2Cf)–(Sn + Fn–2Cn) masked by Sf + Ff–2Cf]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Face-specific familiarity effects involve receptive facial communication and social-cue processing.</td></tr>
<tr><td>18501639_analysis_14</td><td>Name specific (interaction) [(Sn + Fn–2Cn)–(Sf + Ff–2Cf) masked by Sn + Fn–2Cn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Name-specific familiarity effects relate to non-facial receptive social-communication processing (person identity by name).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>18501639_1</td><td>Face (simple effect) [Ff–Sf]; socialcommunication</td><td>18501639_analysis_6</td><td>Face (simple effect) [Ff–Sf]</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18501639_2</td><td>Face (simple effect) [Sf+Ff–2Cf masked by Sf–Cf, and Ff–Cf]; socialcommunication</td><td>18501639_analysis_11</td><td>Face (simple effect) [Sf + Ff–2Cf masked by Sf–Cf, and Ff–Cf]</td><td>0.983</td><td>1.000</td><td>0.995</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18501639_3</td><td>Face (simple effect) [Sf–Ff]; socialcommunication</td><td>18501639_analysis_1</td><td>Face (simple effect) [Sf–Ff]</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18501639_4</td><td>Face specific (interaction) [(Sf+Ff–2Cf)–(Sn+Fn–2Cn) masked by Sf+Ff–2Cf]; socialcommunication</td><td>18501639_analysis_13</td><td>Face specific (interaction) [(Sf + Ff–2Cf)–(Sn + Fn–2Cn) masked by Sf + Ff–2Cf]</td><td>0.961</td><td>1.000</td><td>0.988</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18501639_5</td><td>Face specific (interaction) [(Sf–Ff)–(Sn–Fn) masked by Sf–Ff]; socialcommunication</td><td>18501639_analysis_3</td><td>Face specific (interaction) [(Sf–Ff)–(Sn–Fn) masked by Sf–Ff]</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18501639_6</td><td>Main effect [(Ff+Fn)–(Sf+Sn) masked by Ff–Sf and Fn–Sn]; socialcommunication</td><td>18501639_analysis_5</td><td>Main effect [(Ff + Fn)–(Sf + Sn) masked by Ff–Sf and Fn–Sn]</td><td>0.965</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18501639_7</td><td>Main effect [(Sf+Ff+Sn+Fn)–2(Cf+Cn) masked by Sf–Cf, Ff–Cf, Sn–Cn, and Fn–Cn]; socialcommunication</td><td>18501639_analysis_10</td><td>Main effect [(Sf + Ff + Sn + Fn)–2(Cf + Cn) masked by Sf–Cf, Ff–Cf, Sn–Cn, and Fn–Cn]</td><td>0.951</td><td>0.667</td><td>0.752</td><td>accepted</td><td></td></tr><tr><td>18501639_8</td><td>Name (simple effect) [Sn+Fn–2Cn masked by Sn–Cn, and Fn–Cn]; socialcommunication</td><td>18501639_analysis_12</td><td>Name (simple effect) [Sn + Fn–2Cn masked by Sn–Cn, and Fn–Cn]</td><td>0.983</td><td>1.000</td><td>0.995</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18514546</strong> | Pred included: 6 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18514546/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18514546_analysis_0</td><td>SELF versus high-level baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants view emotional faces and select emotional descriptors (receptive facial communication and emotion recognition), meeting criteria for social communication (face/emotion processing).</td></tr>
<tr><td>18514546_analysis_1</td><td>OTHER versus high-level baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>OTHER vs baseline involves viewing faces and judging others&#x27; emotional states (receptive facial communication), satisfying social communication criteria.</td></tr>
<tr><td>18514546_analysis_2</td><td>Correlations between mirror neuron activation in the SELF/OTHER task and individual empathy scores (BEES)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The correlated activations are in response to viewing and evaluating emotional faces (facial communication) and thus pertain to social communication processes (reception of facial communication).</td></tr>
<tr><td>18514546_analysis_3</td><td>Interaction (SELF_f – B_f) – (SELF_m – B_m)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The underlying SELF task involves viewing emotional faces and making judgments; the interaction reflects differences in processing facial/emotional communication (social communication).</td></tr>
<tr><td>18514546_analysis_4</td><td>Interaction (SELF_m – B_m) – (SELF_f – B_f)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>As with the other SELF-based contrasts, this interaction concerns processing of emotional facial stimuli and their social communicative content; thus it meets social communication criteria.</td></tr>
<tr><td>18514546_analysis_5</td><td>Interaction (OTHER_f – B_f) – (OTHER_m – B_m)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast concerns processing of emotional facial expressions and gender modulation of that processing (reception of facial communication), meeting social communication inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>18514546_1</td><td>Other &gt; high-level baseline; socialcommunication</td><td>18514546_analysis_1</td><td>OTHER versus high-level baseline</td><td>0.881</td><td>1.000</td><td>0.964</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18514546_2</td><td>Self &gt; high-level baseline; socialcommunication</td><td>18514546_analysis_0</td><td>SELF versus high-level baseline</td><td>0.877</td><td>1.000</td><td>0.963</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18633834</strong> | Pred included: 5 | Manual included (accepted matches only): 6 | Correct overlaps: 5 | Match statuses: accepted=6, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18633834/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18633834_analysis_0</td><td>STEP1
F+STEP2 - REST</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Auditory stimuli (footstep sounds) convey socially relevant information (number of walkers, identity cues); this matches Social Communication (receptive non-facial auditory social cues).</td></tr>
<tr><td>18633834_analysis_1</td><td>NOISE1
F+NOISE2 - REST</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>NOISE1+NOISE2 are non-social control sounds; the contrast vs REST does not measure reception of social communication (it is a non-social auditory baseline).</td></tr>
<tr><td>18633834_analysis_2</td><td>(STEP2 - NOISE2) - (STEP1 - NOISE1)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This enhanced contrast isolates differences in processing socially informative auditory cues (two vs one walker) and thus indexes reception of non-facial social communication in the auditory domain.</td></tr>
<tr><td>18633834_analysis_3</td><td>STEP1 - NOISE1</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast evaluates receptive auditory social cues (footsteps conveying identity/gender/motion), matching Social Communication (reception of non-facial auditory social information).</td></tr>
<tr><td>18633834_analysis_4</td><td>STEP2 - NOISE2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Two-person walking sounds provide richer socially communicative auditory cues (interpersonal dynamics, identity cues); this contrast measures reception of non-facial social communication.</td></tr>
<tr><td>18633834_analysis_5</td><td>STEP2 - STEP1</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Comparing two vs one walker examines differences in processing socially informative auditory cues (group dynamics, identity cues), consistent with Social Communication (reception of non-facial auditory social information).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>18633834_1</td><td>(STEP2 - NOISE2) - (STEP1 - NOISE1); socialcommunication</td><td>18633834_analysis_2</td><td>(STEP2 - NOISE2) - (STEP1 - NOISE1)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633834_2</td><td>NOISE1 + NOISE2 - REST; socialcommunication</td><td>18633834_analysis_1</td><td>NOISE1
F+NOISE2 - REST</td><td>0.955</td><td>1.000</td><td>0.986</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633834_3</td><td>STEP1 + STEP2 - REST; socialcommunication</td><td>18633834_analysis_0</td><td>STEP1
F+STEP2 - REST</td><td>0.950</td><td>1.000</td><td>0.985</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633834_4</td><td>STEP1 - NOISE1; socialcommunication</td><td>18633834_analysis_3</td><td>STEP1 - NOISE1</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633834_5</td><td>STEP1 - STEP1; socialcommunication</td><td>18633834_analysis_5</td><td>STEP2 - STEP1</td><td>0.923</td><td>1.000</td><td>0.977</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633834_6</td><td>STEP2 - NOISE2; socialcommunication</td><td>18633834_analysis_4</td><td>STEP2 - NOISE2</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18633846</strong> | Pred included: 3 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18633846/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18633846_analysis_0</td><td>Main effect of type of observed action</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Observing interactions between agents involves exchange of socially relevant information (gaze, action-directedness, inferred intentions). The task engages receptive social communication processes (interpreting actors&#x27; actions and communicative intent).</td></tr>
<tr><td>18633846_analysis_1</td><td>Main effect of gaze</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Gaze direction/availability is a prototypical social-communication signal (eye contact, gaze following). The contrast directly measures responses to gaze cues (GAZE vs NO-GAZE), matching Social Communication inclusion criteria.</td></tr>
<tr><td>18633846_analysis_2</td><td>Interaction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The interaction explicitly involves gaze (a communicative signal) and social vs individual action context; it measures how communicative gaze cues modulate processing of others&#x27; actions and intentions (fits Social Communication criteria).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>18633846_1</td><td>Interaction [( Individual gaze + Social No-gaze) - (Individual No-gaze + Social gaze)];socialcommunication</td><td>18633846_analysis_2</td><td>Interaction</td><td>0.227</td><td>1.000</td><td>0.768</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>18633846_2</td><td>[( Social gaze + Individual gaze) - ( Social No-gaze + Individual No-gaze)]; socialcommunication</td><td>18633846_analysis_1</td><td>Main effect of gaze</td><td>0.149</td><td>1.000</td><td>0.745</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>18633846_3</td><td>[(Individual gaze + Social No-gaze) - (Individual No-gaze + Social Gaze); socialcommunication</td><td>18633846_analysis_0</td><td>Main effect of type of observed action</td><td>0.183</td><td>1.000</td><td>0.755</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18783371</strong> | Pred included: 1 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18783371/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> interaction between observed (hand and foot) and group (Compatible and Incompatible) &gt; baseline during Action Observation task; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18783371_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The paradigm centers on action observation and sensorimotor matching/mimicry (mirror and counter-mirror training). This is directly relevant to social communication processes (examples in the criteria include hand mimicking), so the analysis meets the inclusion criterion for Social Communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>18783371_1</td><td>interaction between observed (hand and foot) and group (Compatible and Incompatible) &gt; baseline during Action Observation task; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>18783371_2</td><td>observation of both hands and feet &gt; baseline and action execution (hands and feet) &gt; baseline (conjunction analysis); socialcommunication</td><td>18783371_analysis_0</td><td>analysis_0</td><td>0.126</td><td>1.000</td><td>0.738</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 19048432</strong> | Pred included: 4 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19048432/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Interaction between feedback condition and variance in pupil size (positive &gt; negative); socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19048432_analysis_0</td><td>Main effect of viewing eyes</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants viewed eye-region stimuli and dynamic pupil changes (receptive facial communication, eye signals); analysis directly probes reception of facial communication.</td></tr>
<tr><td>19048432_analysis_1</td><td>Main effect of change in observed and observer&#x27;s pupil size</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>ROI analysis targets regions (FFA, STS, amygdala, insula) engaged by facial/eye-based communication and responses to observed pupil changes, meeting reception of facial communication criteria.</td></tr>
<tr><td>19048432_analysis_2</td><td>Region of interest analysis</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis examines reception of facial/eye communication (dynamic pupil changes, coherence/incoherence), matching social communication inclusion criteria.</td></tr>
<tr><td>19048432_analysis_3</td><td>Interaction between feedback condition and variance in pupil size</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>Interaction (positive vs negative feedback) concerns receptive aspects of facial communication (eye region dynamics) and their neural encoding, matching social communication criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>19048432_1</td><td>Interaction between feedback condition and variance in pupil size (positive &gt; negative); socialcommunication</td><td>19048432_analysis_3</td><td>Interaction between feedback condition and variance in pupil size</td><td>0.855</td><td>0.125</td><td>0.344</td><td>unmatched</td><td>coord_count_mismatch, low_coord_high_name, low_total_score</td></tr><tr><td>19048432_2</td><td>Main effect of change in observed and observer’s pupil size; socialcommunication</td><td>19048432_analysis_1</td><td>Main effect of change in observed and observer&#x27;s pupil size</td><td>0.983</td><td>1.000</td><td>0.995</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>19048432_3</td><td>Main effect of viewing eyes; socialcommunication</td><td>19048432_analysis_0</td><td>Main effect of viewing eyes</td><td>1.000</td><td>0.833</td><td>0.883</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 19347874</strong> | Pred included: 5 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19347874/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Coorelation -- attachment avoidance with brain responses to masked sad faces; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19347874_analysis_0</td><td>Brain regions exhibiting significantly increased activation in response to masked sad faces compared to neutral faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis involves viewing facial expressions (receptive facial communication), which meets the Social Communication inclusion criteria.</td></tr>
<tr><td>19347874_analysis_1</td><td>Brain regions exhibiting significantly increased activation in response to masked happy faces compared to neutral faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Viewing facial expressions (happy faces) is a prototypical social communication receptive task, so this contrast meets the criterion.</td></tr>
<tr><td>19347874_analysis_2</td><td>Table III. Negative correlations of attachment avoidance with brain responses to masked sad faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast involves perception of facial emotional expressions (receptive facial communication) and how it varies with attachment, meeting Social Communication criteria.</td></tr>
<tr><td>19347874_analysis_3</td><td>Positive correlations</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>Task involves receptive processing of facial expressions (happy faces) and how it varies with attachment, fitting Social Communication inclusion criteria.</td></tr>
<tr><td>19347874_analysis_4</td><td>Negative correlations</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Examines reception of facial emotional cues (happy faces) and their modulation by attachment, meeting Social Communication criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>19347874_1</td><td>Coorelation -- attachment avoidance with brain responses to masked sad faces; socialcommunication</td><td>19347874_analysis_3</td><td>Positive correlations</td><td>0.227</td><td>0.500</td><td>0.418</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>19347874_2</td><td>Masked sad faces &gt; neutral faces; socialcommunication</td><td>19347874_analysis_0</td><td>Brain regions exhibiting significantly increased activation in response to masked sad faces compared to neutral faces</td><td>0.416</td><td>1.000</td><td>0.825</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>19347874_3</td><td>Negative coorelation -- attachment avoidance with brain responsed to masked sad faces; socialcommunication</td><td>19347874_analysis_2</td><td>Table III. Negative correlations of attachment avoidance with brain responses to masked sad faces</td><td>0.890</td><td>0.857</td><td>0.867</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>19347874_4</td><td>masked happy faces &gt; neutral faces; socialcommunication</td><td>19347874_analysis_1</td><td>Brain regions exhibiting significantly increased activation in response to masked happy faces compared to neutral faces</td><td>0.431</td><td>1.000</td><td>0.829</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20056152</strong> | Pred included: 2 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20056152/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20056152_analysis_0</td><td>Places &gt; bodies</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The paradigm explicitly uses gaze following and joint attention (seeing where the avatar looks), which are core aspects of social communication (production/reception of gaze). The Places &gt; Bodies contrast arises from this gaze-based communicative scenario.</td></tr>
<tr><td>20056152_analysis_1</td><td>Bodies &gt; places</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Bodies &gt; Places arises from gaze-following and joint-attention behavior (avatar looking at occluded stimuli), which are key social communication processes (gaze perception and following).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>20056152_1</td><td>Bodies &gt; Places; socialcommunication</td><td>20056152_analysis_1</td><td>Bodies &gt; places</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>20056152_2</td><td>Places &gt; Bodies; socialcommunication</td><td>20056152_analysis_0</td><td>Places &gt; bodies</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20096792</strong> | Pred included: 4 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20096792/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20096792_analysis_0</td><td>Live&gt;Recorded</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task requires reciprocal, face-to-face communication (gaze, instructions, showing/matching events), matching the social communication construct (receptive and productive aspects).</td></tr>
<tr><td>20096792_analysis_1</td><td>Recorded &gt;Live</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The underlying paradigm involves face-to-face communicative episodes (showing, matching); the Recorded&gt;Live contrast still indexes differences in social-communication contexts (albeit showing stronger non-social motor responses).</td></tr>
<tr><td>20096792_analysis_2</td><td>JA&gt;SA</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Joint attention and gaze-following are prototypical social communication behaviors (production and reception of gaze cues); contrast explicitly isolates gaze-based communication.</td></tr>
<tr><td>20096792_analysis_3</td><td>SA&gt;JA</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Solo Attention still engages social-communicative elements (visible partner, eye movements) and is contrasted with Joint Attention to index communication-related processes; thus it meets the criterion.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>20096792_1</td><td>Live &gt; Recorded; socialcommunication</td><td>20096792_analysis_0</td><td>Live&gt;Recorded</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>20096792_2</td><td>Recorded &gt; Live; socialcommunication</td><td>20096792_analysis_1</td><td>Recorded &gt;Live</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>20096792_3</td><td>JA &gt; SA; socialcommunication</td><td>20096792_analysis_2</td><td>JA&gt;SA</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20188190</strong> | Pred included: 7 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20188190/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20188190_analysis_0</td><td>Preference</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task involves viewing faces and responding to socially meaningful facial stimuli (receptive facial communication). This fits the Social Communication inclusion examples (viewing faces, affect recognition).</td></tr>
<tr><td>20188190_analysis_1</td><td>Gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Gender identification uses facial stimuli and face-processing mechanisms (receptive facial communication), matching the Social Communication inclusion examples (viewing faces).</td></tr>
<tr><td>20188190_analysis_2</td><td>Preference &gt; Gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Preference&gt;Gender isolates processing of socially meaningful facial information and evaluative reception of faces, consistent with Social Communication inclusion examples.</td></tr>
<tr><td>20188190_analysis_3</td><td>Gender &gt; Preference</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast emphasizes face-processing and related social perceptual processing (e.g., TPJ, STG), consistent with Social Communication (receptive face processing).</td></tr>
<tr><td>20188190_analysis_4</td><td>aMFC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This connectivity analysis examines interactions among regions engaged while processing faces and social evaluative information (receptive facial communication), aligning with Social Communication inclusion criteria.</td></tr>
<tr><td>20188190_analysis_5</td><td>vMFC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>vMFC connectivity during face evaluation involves processing socially relevant facial information (receptive facial communication), consistent with Social Communication inclusion.</td></tr>
<tr><td>20188190_analysis_6</td><td>PCC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>PCC PPI assesses interactions among regions engaged in evaluating faces and social information, consistent with Social Communication (receptive facial processing).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>20188190_1</td><td>Gender &gt; Preference; socialcommunication</td><td>20188190_analysis_3</td><td>Gender &gt; Preference</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>20188190_2</td><td>Preference &gt; Gender; socialcommunication</td><td>20188190_analysis_2</td><td>Preference &gt; Gender</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23298748</strong> | Pred included: 3 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23298748/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23298748_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants view faces and make trait judgments (receptive facial processing), matching the inclusion examples (Viewing pictures of faces; reception of facial communication).</td></tr>
<tr><td>23298748_analysis_1</td><td>analysis_1</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis involves viewing and evaluating faces (receptive facial communication), consistent with inclusion examples such as Faces &gt; non-faces or affect/face perception tasks.</td></tr>
<tr><td>23298748_analysis_2</td><td>analysis_2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Faces are presented and participants make trait-based judgments (receptive facial communication), meeting the social communication inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>23298748_1</td><td>interaction between sociality and working memory load; socialcommunication</td><td>23298748_analysis_2</td><td>analysis_2</td><td>0.127</td><td>1.000</td><td>0.738</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>23298748_2</td><td>sociality &gt; working memory load; socialcommunication</td><td>23298748_analysis_0</td><td>analysis_0</td><td>0.146</td><td>1.000</td><td>0.744</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>23298748_3</td><td>working memory load &gt; sociality; socialcommunication</td><td>23298748_analysis_1</td><td>analysis_1</td><td>0.195</td><td>1.000</td><td>0.759</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23378834</strong> | Pred included: 10 | Manual included (accepted matches only): 12 | Correct overlaps: 7 | Match statuses: accepted=12, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23378834/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23378834_analysis_0</td><td>FAMOUS FACES vs. BASELINE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Viewing and recognizing faces (famous) fits Social Communication (reception of facial communication); contrast measures faces vs baseline.</td></tr>
<tr><td>23378834_analysis_1</td><td>FAMILIAR FACES vs. BASELINE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task involves viewing faces and social judgments (reception of facial communication); meets social communication criteria.</td></tr>
<tr><td>23378834_analysis_2</td><td>FAMOUS ∩ FAMILIAR FACES</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Conjunction derives from face-related contrasts (reception of facial information) and thus is relevant to social communication.</td></tr>
<tr><td>23378834_analysis_3</td><td>Faces &gt; Landmarks</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Faces &gt; landmarks is a canonical social communication/reception contrast (viewing faces).</td></tr>
<tr><td>23378834_analysis_4</td><td>Landmarks &gt; Faces</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>This contrast is not measuring reception or production of facial/social communication — it emphasizes landmarks over faces.</td></tr>
<tr><td>23378834_analysis_5</td><td>Famous faces &gt; Novel faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast compares face categories (familiarity) and involves reception of facial information — fits social communication criteria.</td></tr>
<tr><td>23378834_analysis_6</td><td>Novel faces &gt; Famous faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast isolates face-related perceptual processing (reception of facial information), meeting social communication criteria.</td></tr>
<tr><td>23378834_analysis_7</td><td>Novel faces &gt; Novel landmarks</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Faces vs landmarks (novel) is a prototypical social communication/reception contrast (viewing faces).</td></tr>
<tr><td>23378834_analysis_8</td><td>Novel landmarks &gt; Novel faces</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Contrast does not measure reception/production of facial/social communication (focus is landmarks).</td></tr>
<tr><td>23378834_analysis_9</td><td>Famous faces &gt; Novel landmarks</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast involves viewing and recognizing famous faces (reception of facial information), meeting social communication criteria.</td></tr>
<tr><td>23378834_analysis_10</td><td>Novel landmarks &gt; Famous faces</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Contrast does not measure facial/social communication; it emphasizes landmarks over faces.</td></tr>
<tr><td>23378834_analysis_11</td><td>Famous landmarks &gt; Novel landmarks</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Not a face/communication task; landmarks contrasts are non-social perceptual comparisons.</td></tr>
<tr><td>23378834_analysis_12</td><td>Novel landmarks &gt; Famous landmarks</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Contrast does not involve faces or social communication processes.</td></tr>
<tr><td>23378834_analysis_13</td><td>Familiar faces &gt; Novel faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast measures reception of facial information for social judgments about friends (reception of facial communication).</td></tr>
<tr><td>23378834_analysis_14</td><td>Famous ∩ Familiar faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Conjunction reflects face-related activations (reception of facial information) relevant to social communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>23378834_1</td><td>Faces &gt; Landmarks; socialcommunication</td><td>23378834_analysis_3</td><td>Faces &gt; Landmarks</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23378834_10</td><td>Novel landmarks &gt; Famous faces; socialcommunication</td><td>23378834_analysis_10</td><td>Novel landmarks &gt; Famous faces</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23378834_11</td><td>Novel landmarks &gt; Famous landmarks; socialcommunication</td><td>23378834_analysis_12</td><td>Novel landmarks &gt; Famous landmarks</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23378834_12</td><td>Novel landmarks &gt; Novel faces; socialcommunication</td><td>23378834_analysis_8</td><td>Novel landmarks &gt; Novel faces</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23378834_2</td><td>Familiar faces &gt; Novel faces; socialcommunication</td><td>23378834_analysis_13</td><td>Familiar faces &gt; Novel faces</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23378834_3</td><td>Famous faces &gt; Novel faces; socialcommunication</td><td>23378834_analysis_5</td><td>Famous faces &gt; Novel faces</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23378834_4</td><td>Famous faces &gt; Novel landmarks; socialcommunication</td><td>23378834_analysis_9</td><td>Famous faces &gt; Novel landmarks</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23378834_5</td><td>Famous landmarks &gt; Novel landmarks; socialcommunication</td><td>23378834_analysis_11</td><td>Famous landmarks &gt; Novel landmarks</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23378834_6</td><td>Famous ∩ Familiar faces; socialcommunication</td><td>23378834_analysis_14</td><td>Famous ∩ Familiar faces</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23378834_7</td><td>Landmarks &gt; Faces; socialcommunication</td><td>23378834_analysis_4</td><td>Landmarks &gt; Faces</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23378834_8</td><td>Novel faces &gt; Famous faces; socialcommunication</td><td>23378834_analysis_6</td><td>Novel faces &gt; Famous faces</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23378834_9</td><td>Novel faces &gt; Novel landmarks; socialcommunication</td><td>23378834_analysis_7</td><td>Novel faces &gt; Novel landmarks</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23599165</strong> | Pred included: 8 | Manual included (accepted matches only): 7 | Correct overlaps: 7 | Match statuses: accepted=7, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23599165/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23599165_analysis_0</td><td>Yes (Match + Unrequited) &gt; No (Rejection + Disinterest)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants are presented with partners&#x27; faces and their decisions (social signals). The contrast measures reception of socially communicative signals (interest/rejection), matching the Social Communication construct.</td></tr>
<tr><td>23599165_analysis_1</td><td>No (Rejection + Disinterest) &gt; Yes (Match + Unrequited)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast probes reception of communicative signals (partners&#x27; ‘no’ responses) conveyed via faces/labels—fits social communication inclusion criteria.</td></tr>
<tr><td>23599165_analysis_2</td><td>Match &gt; Unrequited</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast involves reception of partners&#x27; affirmative decisions (social communicative signals) in face-presentations—matches Social Communication criteria (faces, social signals).</td></tr>
<tr><td>23599165_analysis_3</td><td>Rejection &gt; Disinterest</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Both conditions involve reception of partners&#x27; communicative signals (no responses) but differ in social context (desired vs undesired); the contrast probes social communication processing.</td></tr>
<tr><td>23599165_analysis_4</td><td>Mismatched (Rejection + Unrequited) &gt; matched (Match + Disinterest)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This contrast reflects processing of discrepant communicative signals (partner decision mismatches participant expectation) conveyed via faces/labels, fitting Social Communication.</td></tr>
<tr><td>23599165_analysis_5</td><td>Unsigned prediction errors from RL model</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The prediction-error signal is derived from partners&#x27; communicative decisions about the participant (yes/no), reflecting reception and interpretation of social signals.</td></tr>
<tr><td>23599165_analysis_6</td><td>Partners who were given a yes &gt; those given a no</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This contrast uses faces and associated social decisions (participant&#x27;s own prior decision) to probe anticipatory processing of communicative outcomes—fits Social Communication criteria.</td></tr>
<tr><td>23599165_analysis_7</td><td>Partners who were given a no &gt; those given a yes</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This contrast concerns perception of faces labeled with the participant’s prior ‘no’ decision—processing communicative social cues (reception of social information).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>23599165_1</td><td>Match &gt; Unrequited; socialcommunication</td><td>23599165_analysis_2</td><td>Match &gt; Unrequited</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23599165_2</td><td>Mismatched (Rejection + Unrequited) &gt; matched (Match + Disinterest); socialcommunication</td><td>23599165_analysis_4</td><td>Mismatched (Rejection + Unrequited) &gt; matched (Match + Disinterest)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23599165_3</td><td>Partners who were given a no &gt; those given a yes; socialcommunication</td><td>23599165_analysis_7</td><td>Partners who were given a no &gt; those given a yes</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23599165_4</td><td>Partners who were given a yes &gt; those given a no; socialcommunication</td><td>23599165_analysis_6</td><td>Partners who were given a yes &gt; those given a no</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23599165_5</td><td>Rejection &gt; Disinterest; socialcommunication</td><td>23599165_analysis_3</td><td>Rejection &gt; Disinterest</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23599165_6</td><td>Unsigned prediction errors from RL model; socialcommunication</td><td>23599165_analysis_5</td><td>Unsigned prediction errors from RL model</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23599165_7</td><td>Yes (Match + Unrequited) &gt; No (Rejection + Disinterest); socialcommunication</td><td>23599165_analysis_0</td><td>Yes (Match + Unrequited) &gt; No (Rejection + Disinterest)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23667619</strong> | Pred included: 4 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23667619/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23667619_analysis_0</td><td>CSL&gt;TIC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast examines perception of affective, nonverbal auditory communication (laughter types) and uses an explicit categorization task of vocal social signals, directly indexing social communication (receptive non-facial communication).</td></tr>
<tr><td>23667619_analysis_1</td><td>TIC&gt;CSL</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Tickling laughter is an affective nonverbal auditory signal; the contrast evaluates receptive non-facial social communication (auditory) and how different laughter types are communicated and processed, meeting the social communication criteria.</td></tr>
<tr><td>23667619_analysis_2</td><td>CAT&gt;COU</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This contrast explicitly manipulates processing of socially communicative vocal signals (categorization of laughter types) versus an implicit auditory task, directly targeting social communication (receptive auditory social signals).</td></tr>
<tr><td>23667619_analysis_3</td><td>HAP&lt;sub&gt;CAT&lt;/sub&gt; ∩ TAU&lt;sub&gt;CAT&lt;/sub&gt; ∩ TIC&lt;sub&gt;CAT&lt;/sub&gt; ∩ HAP&lt;sub&gt;COU&lt;/sub&gt; ∩ TAU&lt;sub&gt;COU&lt;/sub&gt; ∩ TIC&lt;sub&gt;COU&lt;/sub&gt;</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The conjunction highlights bilateral primary and association auditory cortex and IFG regions commonly engaged by laughter perception — directly indexing receptive social communication (auditory non-facial signals).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>23667619_1</td><td>CAT &gt; COU; socialcommunication</td><td>23667619_analysis_2</td><td>CAT&gt;COU</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23667619_2</td><td>CSL &gt; TIC; socialcommunication</td><td>23667619_analysis_0</td><td>CSL&gt;TIC</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23667619_3</td><td>TIC &gt; CSL; socialcommunication</td><td>23667619_analysis_1</td><td>TIC&gt;CSL</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23684882</strong> | Pred included: 2 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=3</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23684882/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> [(S-IMIO &gt; NS-IMIO) &gt; (S-CTO &gt; NS-CTO)] &gt; [(S-IMIE &gt; NS-IMIE) &gt; (S-CTE &gt; NS- CTE)]; socialcommunication, [(S-IMIO &gt; S-CTO) + (S-IMIE &gt; S-CTE)] &gt; [(NS-IMIO &gt; NS-CTO) + (NS-IMIE &gt; NS-CTE)]; socialcommunication, [S (IMIO + CTO + IMIE + CTE) &gt; NS (IMIO + CTO + IMIE + CTE)]; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23684882_analysis_0</td><td>(IMI_O &gt; CT_O) ∩ (IMI_E &gt; CT_E)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task requires viewing and imitating hand/arm gestures (including communicative gestures) and contrasts imitation versus a motor control. This directly indexes production/reception of non‑facial social communication (hand mimicking/gesture imitation), matching the Social Communication inclusion examples.</td></tr>
<tr><td>23684882_analysis_1</td><td>(S-IMI_O &gt; NS-IMI_O) &gt; (S-CT_O &gt; NS-CT_O)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The comparison isolates processing of communicative (social) gestures versus non-communicative gestures during an imitation-related observation task, directly indexing reception and interpretation of non-facial social communicative signals (hand gestures).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>23684882_1</td><td>(IMIO &gt; CTO) ∩ (IMIE &gt; CTE); socialcommunication</td><td>23684882_analysis_0</td><td>(IMI_O &gt; CT_O) ∩ (IMI_E &gt; CT_E)</td><td>0.931</td><td>0.950</td><td>0.944</td><td>accepted</td><td>high_coord_match</td></tr><tr><td>23684882_2</td><td>(S-IMIO&gt;NS-IMIO) &gt; (S-CTO &gt; NS-CTO); socialcommunication</td><td>23684882_analysis_1</td><td>(S-IMI_O &gt; NS-IMI_O) &gt; (S-CT_O &gt; NS-CT_O)</td><td>0.949</td><td>1.000</td><td>0.985</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23684882_3</td><td>[(S-IMIO &gt; NS-IMIO) &gt; (S-CTO &gt; NS-CTO)] &gt; [(S-IMIE &gt; NS-IMIE) &gt; (S-CTE &gt; NS- CTE)]; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>23684882_4</td><td>[(S-IMIO &gt; S-CTO) + (S-IMIE &gt; S-CTE)] &gt; [(NS-IMIO &gt; NS-CTO) + (NS-IMIE &gt; NS-CTE)]; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>23684882_5</td><td>[S (IMIO + CTO + IMIE + CTE) &gt; NS (IMIO + CTO + IMIE + CTE)]; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23722983</strong> | Pred included: 3 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23722983/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23722983_analysis_0</td><td>Intention effect</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Interns evaluated whether they would recommend ideas (a communicative intent) and later produced videotaped recommendations; the analysis links neural responses during encoding to communicative intentions, which fits social communication (production/reception of social information).</td></tr>
<tr><td>23722983_analysis_1</td><td>Salesperson effect</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The salesperson effect reflects interns&#x27; communicative effectiveness (how well their communicated recommendations influenced producers), directly mapping onto social communication (production of non-facial communication, persuasive messaging).</td></tr>
<tr><td>23722983_analysis_2</td><td>Buzz effect</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Buzz indexes which ideas the communicators encode that later spread via their communications (videotaped recommendations), tying directly to social communication processes (production and impact of communicative acts).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>23722983_1</td><td>Buzz effect; socialcommunication</td><td>23722983_analysis_2</td><td>Buzz effect</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23722983_2</td><td>Intention effect; socialcommunication</td><td>23722983_analysis_0</td><td>Intention effect</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23722983_3</td><td>Salesperson effect; socialcommunication</td><td>23722983_analysis_1</td><td>Salesperson effect</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24243619</strong> | Pred included: 3 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24243619/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24243619_analysis_0</td><td>Main effects of social judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Tasks involve reception and evaluation of auditory (vocal) social signals (affect and trait information), matching the Social Communication construct (reception of non‑facial communication).</td></tr>
<tr><td>24243619_analysis_1</td><td>Overlapping effects during social and emotional judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis assesses reception and evaluation of auditory social signals (voices conveying emotion and trait information), fitting Social Communication (reception of non‑facial communication).</td></tr>
<tr><td>24243619_analysis_2</td><td>Overlapping effects during social and cognitive judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast involves evaluation of vocal stimuli for trait and age information (auditory social communication), matching Social Communication (reception of non‑facial cues).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24243619_1</td><td>[(TR−AG)∩(AT−AG)∩(HA−AG)]; socialcommunication</td><td>24243619_analysis_1</td><td>Overlapping effects during social and emotional judgments</td><td>0.291</td><td>1.000</td><td>0.787</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>24243619_2</td><td>[(TR−HA)∩(AT−HA)∩(AG−HA)]; socialcommunication</td><td>24243619_analysis_2</td><td>Overlapping effects during social and cognitive judgments</td><td>0.330</td><td>1.000</td><td>0.799</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>24243619_3</td><td>[(TR−HA)∩(AT−HA)∩(TR−AG)∩(AT−AG)]; socialcommunication</td><td>24243619_analysis_0</td><td>Main effects of social judgments</td><td>0.256</td><td>1.000</td><td>0.777</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24414614</strong> | Pred included: 7 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24414614/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24414614_analysis_0</td><td>Imitating</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Task is hand-gesture imitation (production of non-facial communicative gestures), matching the example domain (hand mimicking) and directly measures social communication exchanges.</td></tr>
<tr><td>24414614_analysis_1</td><td>Being imitated</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Being imitated involves non-facial communicative hand gestures and reciprocal exchange (receptive/interpretive aspect), matching social communication criteria (hand mimicking).</td></tr>
<tr><td>24414614_analysis_2</td><td>Observation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Observation of hand gestures probes reception of non-facial social communication (action perception, gesture processing), consistent with the social communication inclusion examples.</td></tr>
<tr><td>24414614_analysis_3</td><td>Imitating-IFG</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Seed in IFG (mirror system) during imitation probes neural substrates of producing/processing communicative hand actions and their social exchange dynamics (fits social communication, e.g., hand mimicking).</td></tr>
<tr><td>24414614_analysis_4</td><td>Being imitated-IFG</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Being imitated engages reception and interpretation of non-facial communicative gestures; IFG-seed connectivity during this condition addresses social communication processing (reception of non-facial gestures).</td></tr>
<tr><td>24414614_analysis_5</td><td>Imitating-IPL*</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>IPL is a posterior mirror-system region implicated in action perception/means; its increased coupling during imitation reflects processing of non-facial communicative gestures (hand mimicking), meeting social communication criteria.</td></tr>
<tr><td>24414614_analysis_6</td><td>Being imitated-IPL</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Being imitated engages reception of non-facial communicative gestures; IPL connectivity during this condition indexes perception/processing of those gestures consistent with social communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24414614_1</td><td>being imitated &gt; rest; socialcommunication</td><td>24414614_analysis_1</td><td>Being imitated</td><td>0.800</td><td>1.000</td><td>0.940</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24414614_2</td><td>imitating &gt; rest; socialcommunication</td><td>24414614_analysis_0</td><td>Imitating</td><td>0.720</td><td>1.000</td><td>0.916</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24414614_3</td><td>observation &gt; rest; socialcommunication</td><td>24414614_analysis_2</td><td>Observation</td><td>0.759</td><td>1.000</td><td>0.928</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24462962</strong> | Pred included: 1 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24462962/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24462962_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast is faces &gt; houses (viewing pictures of faces), and the study links neural responses to measures of affective/social competence. This fits Social Communication (reception of facial communication) per the inclusion examples.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24462962_1</td><td>house + faces; socialcommunication</td><td>24462962_analysis_0</td><td>analysis_0</td><td>0.182</td><td>1.000</td><td>0.755</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24814646</strong> | Pred included: 5 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=2, unmatched=6</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24814646/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> A&amp;B; socialcommunication, p2&gt;n2; socialcommunication, D&amp;E; socialcommunication, a1&gt; n1; socialcommunication, a2&gt; n2; socialcommunication, p1&gt; n1; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24814646_analysis_0</td><td>fMRI contrasts on the basis of the neutral condition as baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>Stimuli contain communicative body and facial actions (dynamic social interactions) and the contrasts examine responses to these communicative signals (receptive processing of non‑verbal/gestural social communication).</td></tr>
<tr><td>24814646_analysis_1</td><td>A</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis A interrogates neural responses to dynamic, communicative social signals (gestures, body language, facial cues) and thus meets Social Communication criteria.</td></tr>
<tr><td>24814646_analysis_2</td><td>B</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Dynamic communicative signals (body movement, gestures, facial cues) are central to these contrasts, meeting Social Communication inclusion criteria.</td></tr>
<tr><td>24814646_analysis_3</td><td>C</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrasts assess neural responses to communicative, non‑verbal social signals in the videos (body language, facial expressions), meeting Social Communication criteria.</td></tr>
<tr><td>24814646_analysis_4</td><td>D</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Final-phase clips contain communicative gestures and emotional expressions; contrasts measure receptive social communication processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24814646_1</td><td>A&amp;B; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>24814646_10</td><td>p2&gt;n2; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>24814646_2</td><td>D&amp;E; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>24814646_3</td><td>a1&gt; n1; socialcommunication</td><td>24814646_analysis_0</td><td>fMRI contrasts on the basis of the neutral condition as baseline</td><td>0.304</td><td>0.282</td><td>0.289</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>24814646_4</td><td>a1&gt;p1; socialcommunication</td><td>24814646_analysis_1</td><td>A</td><td>0.250</td><td>1.000</td><td>0.775</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>24814646_5</td><td>a2&gt; n2; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>24814646_6</td><td>a2&gt;p2; socialcommunication</td><td>24814646_analysis_3</td><td>C</td><td>0.069</td><td>1.000</td><td>0.721</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>24814646_7</td><td>p1&gt; n1; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>24814646_8</td><td>p1&gt;a1; socialcommunication</td><td>24814646_analysis_2</td><td>B</td><td>0.000</td><td>0.938</td><td>0.656</td><td>uncertain</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>24814646_9</td><td>p2&gt;a2; socialcommunication</td><td>24814646_analysis_4</td><td>D</td><td>0.000</td><td>0.923</td><td>0.646</td><td>uncertain</td><td>coord_count_mismatch, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24842782</strong> | Pred included: 3 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24842782/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24842782_analysis_0</td><td>Angry &gt;neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast contrasts viewing angry versus neutral facial expressions (receptive facial communication), fitting Social Communication (reception of facial signals).</td></tr>
<tr><td>24842782_analysis_1</td><td>Angry &gt;neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast isolates neural response to angry vs neutral facial communication (reception of facial emotion), matching Social Communication criteria.</td></tr>
<tr><td>24842782_analysis_2</td><td>Parametric modulation angry &gt;neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The modulation examines neural sensitivity to another&#x27;s facial communication (angry expression) as it relates to communicative/affective behavior (punishment selection), fitting Social Communication reception.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24842782_1</td><td>Decision phase: Angry &gt; neutral; socialcommunication</td><td>24842782_analysis_1</td><td>Angry &gt;neutral</td><td>0.652</td><td>1.000</td><td>0.896</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24842782_2</td><td>Decision phase: Parametric modulation angry &gt; neutral; socialcommunication</td><td>24842782_analysis_2</td><td>Parametric modulation angry &gt;neutral</td><td>0.822</td><td>1.000</td><td>0.947</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24842782_3</td><td>Outcome phase: Angry &gt; neutral; socialcommunication</td><td>24842782_analysis_0</td><td>Angry &gt;neutral</td><td>0.667</td><td>1.000</td><td>0.900</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24936688</strong> | Pred included: 3 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24936688/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24936688_analysis_0</td><td>Status Type by Status Level interaction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants viewed faces and formed impressions; the task involves reception of facial communication (viewing faces) and the contrasts examine responses to face stimuli with social information, meeting Social Communication criteria.</td></tr>
<tr><td>24936688_analysis_1</td><td>Status Type main effect</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis examines neural responses to faces conveying socially relevant information (status cues); therefore it falls under Social Communication (reception of facial communication).</td></tr>
<tr><td>24936688_analysis_2</td><td>Status Level main effect</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This analysis involves viewing faces and measuring neural responses as a function of status level; thus it constitutes Social Communication (reception of facial information) processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24936688_1</td><td>Status Level &gt; baseline; socialcommunication</td><td>24936688_analysis_2</td><td>Status Level main effect</td><td>0.723</td><td>1.000</td><td>0.917</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24936688_2</td><td>Status Type &gt; baseline; socialcommunication</td><td>24936688_analysis_1</td><td>Status Type main effect</td><td>0.711</td><td>1.000</td><td>0.913</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24936688_3</td><td>Status Type x Status Level &gt; baseline; socialcommunication</td><td>24936688_analysis_0</td><td>Status Type by Status Level interaction</td><td>0.763</td><td>1.000</td><td>0.929</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25281889</strong> | Pred included: 12 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25281889/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25281889_analysis_0</td><td>HA&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast presents faces and requires affect recognition (receptive facial communication), fitting the Social Communication construct (Reception of Facial Communication).</td></tr>
<tr><td>25281889_analysis_1</td><td>AN&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Task involves receptive facial communication (recognizing/labeling angry faces) and thus meets Social Communication criteria.</td></tr>
<tr><td>25281889_analysis_2</td><td>FE&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task assesses reception of facial communication (recognition/labeling of fearful expressions), matching Social Communication inclusion criteria.</td></tr>
<tr><td>25281889_analysis_3</td><td>HA&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrasting emotional faces and relating activation to alexithymia indexes reception and interpretation of facial communication, so it fits Social Communication.</td></tr>
<tr><td>25281889_analysis_4</td><td>AN&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis concerns reception of facial communication (anger recognition) and its relation to alexithymia, fitting the Social Communication construct.</td></tr>
<tr><td>25281889_analysis_5</td><td>FE&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Fearful face perception and its neural correlates (and relation to alexithymia) target receptive facial communication, meeting Social Communication criteria.</td></tr>
<tr><td>25281889_analysis_6</td><td>HA&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Correlating brain responses to faces with TAS-20-DDF concerns reception of facial communication and its neural basis, fitting Social Communication.</td></tr>
<tr><td>25281889_analysis_7</td><td>AN&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Relates reception of facial communication (angry faces) to an alexithymia measure, meeting Social Communication inclusion criteria.</td></tr>
<tr><td>25281889_analysis_8</td><td>FE&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Fearful face perception and its relation to TAS-20-DDF involve reception of facial communication, satisfying Social Communication criteria.</td></tr>
<tr><td>25281889_analysis_9</td><td>HA&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Relates reception of facial communication (happy faces) to an observer-rated alexithymia subscale, matching Social Communication.</td></tr>
<tr><td>25281889_analysis_10</td><td>AN&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Concerns reception/recognition of facial communication (anger) and its neural correlates as a function of TSIA-DDF, meeting Social Communication criteria.</td></tr>
<tr><td>25281889_analysis_11</td><td>FE&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Examines reception of facial communication (fear) and its neural correlates as a function of TSIA-DDF, fulfilling Social Communication criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25281889_1</td><td>AN &gt; NE; socialcommunication</td><td>25281889_analysis_1</td><td>AN&gt;NE</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25281889_2</td><td>FE &gt; NE; socialcommunication</td><td>25281889_analysis_2</td><td>FE&gt;NE</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25281889_3</td><td>HA &gt; NE; socialcommunication</td><td>25281889_analysis_0</td><td>HA&gt;NE</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25640962</strong> | Pred included: 5 | Manual included (accepted matches only): 5 | Correct overlaps: 5 | Match statuses: accepted=5, uncertain=0, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25640962/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> (IC &gt; NG ∩ ego &gt; allo) (conjunction analysis); socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25640962_analysis_0</td><td>IC&gt;NG</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast compares presence vs absence of co-speech gestures, directly indexing social communicative signals (receptive non-facial communication) used in an explicit communication judgment task.</td></tr>
<tr><td>25640962_analysis_1</td><td>ego&gt;allo</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Actor orientation modulates communicative intent and signaling (who is addressed). This is directly relevant to social communication (reception of non-facial communicative cues).</td></tr>
<tr><td>25640962_analysis_2</td><td>allo&gt;ego</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Allocentric presentation changes communicative signalling and how messages are received, directly relevant to social communication and reception of non-facial cues and speech-gesture integration.</td></tr>
<tr><td>25640962_analysis_3</td><td>(IC-ego&gt;IC-allo)&gt;(NG-ego&gt;NG-allo)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast tests how communicative cues (gesture) interact with orientation to affect perceived addressment—directly indexing social communication processes (reception of non-facial cues and communicative intent).</td></tr>
<tr><td>25640962_analysis_4</td><td>(IC-allo&gt;NG-allo)&gt;(IC-ego&gt;NG-ego)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This interaction directly tests the role of co-speech gestures (production/reception of non-facial communicative behavior) across orientations—central to social communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25640962_1</td><td>(IC &gt; NG ∩ ego &gt; allo) (conjunction analysis); socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>25640962_2</td><td>(IC-allo&gt;NG-allo)&gt;(IC-ego&gt;NG-ego); socialcommunication</td><td>25640962_analysis_4</td><td>(IC-allo&gt;NG-allo)&gt;(IC-ego&gt;NG-ego)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25640962_3</td><td>(IC-ego&gt;IC-allo)&gt; (NG-ego&gt;NG-allo); socialcommunication</td><td>25640962_analysis_3</td><td>(IC-ego&gt;IC-allo)&gt;(NG-ego&gt;NG-allo)</td><td>1.000</td><td>0.800</td><td>0.860</td><td>accepted</td><td>high_coord_match</td></tr><tr><td>25640962_4</td><td>IC &gt; NG; socialcommunication</td><td>25640962_analysis_0</td><td>IC&gt;NG</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25640962_5</td><td>allo &gt; ego; socialcommunication</td><td>25640962_analysis_2</td><td>allo&gt;ego</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25640962_6</td><td>ego &gt; allo; socialcommunication</td><td>25640962_analysis_1</td><td>ego&gt;allo</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25716010</strong> | Pred included: 11 | Manual included (accepted matches only): 10 | Correct overlaps: 10 | Match statuses: accepted=10, uncertain=1, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25716010/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25716010_analysis_0</td><td>A. Fear&gt;happy+neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task involves receptive social communication (perception of non-facial bodily expressions and group emotional signals), matching the Social Communication construct.</td></tr>
<tr><td>25716010_analysis_1</td><td>B. Fear+happy&gt;neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrasts emotional group displays (body motion, affect) vs neutral involve reception of socially relevant non-facial communication.</td></tr>
<tr><td>25716010_analysis_2</td><td>C. Fear+neutral&gt;happy</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Comparing emotional (fear/neutral) to happy group displays involves reception of social, non-facial communicative cues (body expressions, dynamics).</td></tr>
<tr><td>25716010_analysis_3</td><td>D. Happy&gt;fear+neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Happy crowd displays convey socially communicative non-facial signals (body movement, group affect), matching Social Communication (receptive non-facial).</td></tr>
<tr><td>25716010_analysis_4</td><td>E. Neutral&gt;fear+happy</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Even neutral crowd displays engage perception of social scenes and non-facial communication (context, implied actions), so Social Communication applies.</td></tr>
<tr><td>25716010_analysis_5</td><td>Main effect of dynamics.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Interactive vs individual behavior conveys communicative, non-facial social signals (interaction dynamics), fitting Social Communication (receptive non-facial).</td></tr>
<tr><td>25716010_analysis_6</td><td>A. Interactive fear&gt;individual fear</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast focuses on reception of non-facial communicative cues (interactive fearful body language), satisfying Social Communication inclusion criteria.</td></tr>
<tr><td>25716010_analysis_7</td><td>B. Interactive fear&gt;individual fear+interactive happy&gt;individual happy</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Comparing interactive vs individual across emotions assesses reception of social communicative cues (body motion/dynamics) — fits Social Communication.</td></tr>
<tr><td>25716010_analysis_8</td><td>C. Interactive fear&gt;individual fear+interactive neutral&gt;individual neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>By contrasting interactive vs individual in fear and neutral, the analysis probes reception of communicative, non-facial cues and action perception — Social Communication applies.</td></tr>
<tr><td>25716010_analysis_9</td><td>D. Interactive fear&gt;individual fear+individual neutral&gt;interactive neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This compound contrast examines how communicative interactive signals (body/dynamics) vary with emotion, fitting Social Communication reception criteria.</td></tr>
<tr><td>25716010_analysis_10</td><td>E. Interactive neutral&gt;individual neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Even neutral interactive behavior conveys social communicative cues (movement/dynamics); this contrast targets receptive non-facial social communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25716010_1</td><td>Fear +  happy &gt; neutral; socialcommunication</td><td>25716010_analysis_1</td><td>B. Fear+happy&gt;neutral</td><td>0.889</td><td>1.000</td><td>0.967</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25716010_10</td><td>Neutral &gt; fear +  happy; socialcommunication</td><td>25716010_analysis_4</td><td>E. Neutral&gt;fear+happy</td><td>0.889</td><td>1.000</td><td>0.967</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25716010_11</td><td>interactive &gt; individual; socialcommunication</td><td>25716010_analysis_5</td><td>Main effect of dynamics.</td><td>0.348</td><td>0.882</td><td>0.722</td><td>uncertain</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>25716010_2</td><td>Fear + neutral &gt; happy; socialcommunication</td><td>25716010_analysis_2</td><td>C. Fear+neutral&gt;happy</td><td>0.889</td><td>1.000</td><td>0.967</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25716010_3</td><td>Fear &gt; happy +  neutral; socialcommunication</td><td>25716010_analysis_0</td><td>A. Fear&gt;happy+neutral</td><td>0.889</td><td>1.000</td><td>0.967</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25716010_4</td><td>Happy &gt; fear + neutral; socialcommunication</td><td>25716010_analysis_3</td><td>D. Happy&gt;fear+neutral</td><td>0.889</td><td>1.000</td><td>0.967</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25716010_5</td><td>Interactive fear &gt; individual fear; socialcommunication</td><td>25716010_analysis_6</td><td>A. Interactive fear&gt;individual fear</td><td>0.958</td><td>1.000</td><td>0.987</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25716010_6</td><td>Interactive fear &gt; individual fear +  individual neutral &gt; interactive neutral; socialcommunication</td><td>25716010_analysis_9</td><td>D. Interactive fear&gt;individual fear+individual neutral&gt;interactive neutral</td><td>0.968</td><td>1.000</td><td>0.990</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25716010_7</td><td>Interactive fear &gt; individual fear + interactive happy &gt; individual happy; socialcommunication</td><td>25716010_analysis_7</td><td>B. Interactive fear&gt;individual fear+interactive happy&gt;individual happy</td><td>0.966</td><td>1.000</td><td>0.990</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25716010_8</td><td>Interactive fear &gt; individual fear + interactive neutral &gt; individual neutral; socialcommunication</td><td>25716010_analysis_8</td><td>C. Interactive fear&gt;individual fear+interactive neutral&gt;individual neutral</td><td>0.968</td><td>1.000</td><td>0.990</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25716010_9</td><td>Interactive neutral &gt; individual neutral; socialcommunication</td><td>25716010_analysis_10</td><td>E. Interactive neutral&gt;individual neutral</td><td>0.964</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25929599</strong> | Pred included: 5 | Manual included (accepted matches only): 5 | Correct overlaps: 5 | Match statuses: accepted=5, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25929599/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25929599_analysis_0</td><td>Incongruent &gt; Congruent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task presents faces and hand movements (receptive and production aspects of social communication) and this contrast isolates neural control of imitative tendencies during social stimulus presentation.</td></tr>
<tr><td>25929599_analysis_1</td><td>Happy (Incongruent &gt; Congruent) masked inclusively with Happy (Incongruent &gt; Congruent) &gt; Angry (Incongruent &gt; Congruent)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast involves processing facial expressions (receptive facial communication) while measuring imitative responding (production-related aspects); fits social communication criteria.</td></tr>
<tr><td>25929599_analysis_2</td><td>Angry (Incongruent &gt; Congruent) masked inclusively with Angry (Incongruent &gt; Congruent) &gt; Happy (Incongruent &gt; Congruent)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast involves viewing facial emotional expressions (reception) and their impact on imitative behavior (production aspects), meeting social communication criteria.</td></tr>
<tr><td>25929599_analysis_3</td><td>Out-group (Incongruent &gt; Congruent) masked inclusively with Out-group (Incongruent &gt; Congruent) &gt; In-group (Incongruent &gt; Congruent)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast involves processing face-based group cues (receptive facial communication) and imitative behavior (production), consistent with social communication processes.</td></tr>
<tr><td>25929599_analysis_4</td><td>In-group (Incongruent &gt; Congruent) masked inclusively with In-group (Incongruent &gt; Congruent) &gt; Out-group (Incongruent &gt; Congruent)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast involves viewing in-group faces and effects on imitative responding (face reception and action production), consistent with social communication constructs.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25929599_1</td><td>Angry (Incongruent &gt; Congruent) masked inclusively with Angry (Incongruent &gt; Congruent) &gt; Happy (Incongruent &gt; Congruent); socialcommunication</td><td>25929599_analysis_2</td><td>Angry (Incongruent &gt; Congruent) masked inclusively with Angry (Incongruent &gt; Congruent) &gt; Happy (Incongruent &gt; Congruent)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25929599_2</td><td>Happy (Incongruent &gt; Congruent) masked inclusively with Happy (Incongruent &gt; Congruent) &gt; Angry (Incongruent &gt; Congruent); socialcommunication</td><td>25929599_analysis_1</td><td>Happy (Incongruent &gt; Congruent) masked inclusively with Happy (Incongruent &gt; Congruent) &gt; Angry (Incongruent &gt; Congruent)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25929599_3</td><td>In-group (Incongruent &gt; Congruent) masked inclusively with In-group (Incongruent &gt; Congruent) &gt; Out-group (Incongruent &gt; Congruent); socialcommunication</td><td>25929599_analysis_4</td><td>In-group (Incongruent &gt; Congruent) masked inclusively with In-group (Incongruent &gt; Congruent) &gt; Out-group (Incongruent &gt; Congruent)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25929599_4</td><td>Incongruent &gt; Congruent; socialcommunication</td><td>25929599_analysis_0</td><td>Incongruent &gt; Congruent</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25929599_5</td><td>Out-group (Incongruent &gt; Congruent) masked inclusively with Out-group (Incongruent &gt; Congruent) &gt; In-group (Incongruent &gt; Congruent);socialcommunication</td><td>25929599_analysis_3</td><td>Out-group (Incongruent &gt; Congruent) masked inclusively with Out-group (Incongruent &gt; Congruent) &gt; In-group (Incongruent &gt; Congruent)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25996424</strong> | Pred included: 6 | Manual included (accepted matches only): 5 | Correct overlaps: 5 | Match statuses: accepted=5, uncertain=1, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25996424/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25996424_analysis_0</td><td>Eye Contact &gt; Averted Gaze</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Eye contact and gaze are canonical social-communication signals; this contrast directly addresses processing of gaze/face cues and thus measures social communication (receptive aspects).</td></tr>
<tr><td>25996424_analysis_1</td><td>Averted Gaze &gt; Eye Contact</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast involves processing of face/gaze cues (communication signals) and therefore indexes social communication processes.</td></tr>
<tr><td>25996424_analysis_2</td><td>Congruent Gaze Cues &gt; Incongruent Gaze Cues</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Gaze following and congruency are core social-communication phenomena (production/reception of gaze signals); the contrast directly measures social communication.</td></tr>
<tr><td>25996424_analysis_3</td><td>Incongruent Gaze Cues &gt; Congruent Gaze Cues</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrasting incongruent vs congruent gaze cues addresses processing of communicative gaze signals and thus falls under social communication.</td></tr>
<tr><td>25996424_analysis_4</td><td>Eye Contact congruent &gt; Averted Gaze congruent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This interaction contrast assesses communicative value of eye contact and subsequent gaze (receptive/social-communicative processing), meeting social_communication criteria.</td></tr>
<tr><td>25996424_analysis_5</td><td>Averted Gaze incongruent &gt; Averted Gaze congruent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrasting incongruent vs congruent gaze when faces looked away still examines communicative gaze cues and social attention, satisfying social_communication inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25996424_1</td><td>averted gaze &gt; eye contact; socialcommunication</td><td>25996424_analysis_1</td><td>Averted Gaze &gt; Eye Contact</td><td>1.000</td><td>0.941</td><td>0.959</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>25996424_2</td><td>averted gaze x incongruent &gt; averted  gaze x congruent; socialcommunication</td><td>25996424_analysis_5</td><td>Averted Gaze incongruent &gt; Averted Gaze congruent</td><td>0.961</td><td>1.000</td><td>0.988</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25996424_3</td><td>congruent gaze cues &gt; incongruent gaze cues; socialcommunication</td><td>25996424_analysis_2</td><td>Congruent Gaze Cues &gt; Incongruent Gaze Cues</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25996424_4</td><td>eye contact &gt; averted gaze; socialcommunication</td><td>25996424_analysis_0</td><td>Eye Contact &gt; Averted Gaze</td><td>1.000</td><td>0.500</td><td>0.650</td><td>uncertain</td><td>coord_count_mismatch</td></tr><tr><td>25996424_5</td><td>eye contact x congruent &gt; averted gaze x congruent; socialcommunication</td><td>25996424_analysis_4</td><td>Eye Contact congruent &gt; Averted Gaze congruent</td><td>0.958</td><td>0.800</td><td>0.848</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>25996424_6</td><td>incongruent gaze cues &gt; congruent gaze cues; socialcommunication</td><td>25996424_analysis_3</td><td>Incongruent Gaze Cues &gt; Congruent Gaze Cues</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26417673</strong> | Pred included: 3 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=2</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26417673/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Disgust &gt; Neutral; socialcommunication, Out-group &gt; In-group; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26417673_analysis_0</td><td>Brain activation of the racial prejudice in disgust perception during the passive viewing task ((disgusted out-group-neutral out-group)-(disgusted in-group-neutral in-group))</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis involves viewing facial expressions (disgust vs neutral) and group membership, directly indexing reception of facial communication (emotion recognition). This matches criteria for social communication tasks (viewing faces/emotion).</td></tr>
<tr><td>26417673_analysis_1</td><td>Positive connectivity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>Connectivity was assessed in the context of viewing facial expressions (disgusted faces) and group membership, directly tied to reception of facial communication and emotion processing, meeting the social communication inclusion criteria.</td></tr>
<tr><td>26417673_analysis_2</td><td>Negative connectivity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>The connectivity is modulated by viewing facial expressions of disgust from in-group vs out-group members, which is a form of reception of facial communication (emotion cues), satisfying social communication criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26417673_1</td><td>Disgust &gt; Neutral; socialcommunication</td><td>26417673_analysis_1</td><td>Positive connectivity</td><td>0.316</td><td>0.023</td><td>0.111</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>26417673_2</td><td>Disgusted out-group-neutral out-group &gt; Disgusted In-group-neutral in-group; socialcommunication</td><td>26417673_analysis_0</td><td>Brain activation of the racial prejudice in disgust perception during the passive viewing task ((disgusted out-group-neutral out-group)-(disgusted in-group-neutral in-group))</td><td>0.578</td><td>1.000</td><td>0.873</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26417673_3</td><td>Out-group &gt; In-group; socialcommunication</td><td>26417673_analysis_2</td><td>Negative connectivity</td><td>0.194</td><td>0.067</td><td>0.105</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26505303</strong> | Pred included: 4 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26505303/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26505303_analysis_0</td><td>Words &gt; Faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis isolates responses to words (speech acts) versus faces, directly indexing social communication (receptive language/face processing in interactive contexts).</td></tr>
<tr><td>26505303_analysis_1</td><td>Request &gt; Naming</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This contrast isolates communicative function (directive Requests vs referential Naming) using identical utterances in different contexts, directly indexing social communication processes.</td></tr>
<tr><td>26505303_analysis_2</td><td>Request &gt; Naming</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>As with analysis_1, this contrast isolates communication type (Requests vs Naming) and therefore meets the social communication inclusion criteria.</td></tr>
<tr><td>26505303_analysis_3</td><td>Naming &gt; Request</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Naming is a communicative speech act and the contrast examines communicative processing (linking words to referents), meeting the social communication criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26505303_1</td><td>Request &gt; Naming; socialcommunication</td><td>26505303_analysis_1</td><td>Request &gt; Naming</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26505303_2</td><td>Words &gt; Faces; socialcommunication</td><td>26505303_analysis_0</td><td>Words &gt; Faces</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26908320</strong> | Pred included: 5 | Manual included (accepted matches only): 5 | Correct overlaps: 5 | Match statuses: accepted=5, uncertain=0, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26908320/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Black &gt; White faces: A. Increased activation with more racial trust disparities; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26908320_analysis_0</td><td>A. Increased activation with more racial trust disparity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Task involves viewing faces and decoding social signals (trustworthiness), matching Social Communication inclusion (reception of facial communication).</td></tr>
<tr><td>26908320_analysis_1</td><td>B. Increased connectivity with orbitofrontal cortex (18, 66, 0) with less racial trust disparity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Connectivity during viewing of faces examines reception/processing of facial cues relevant to trustworthiness, matching Social Communication.</td></tr>
<tr><td>26908320_analysis_2</td><td>A. Increased activation with more differentiation disparity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Task involves viewing faces and assessing perceptual differentiation (reception of facial information), consistent with Social Communication inclusion criteria.</td></tr>
<tr><td>26908320_analysis_3</td><td>Increased activation with less differentiation disparity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Involves viewing faces and the reception of facial information relevant to differentiation, aligning with Social Communication inclusion.</td></tr>
<tr><td>26908320_analysis_4</td><td>B. Increased connectivity with dorsolateral prefrontal seed (24, 42, 21) with less differentiation disparity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Connectivity during face perception involves reception of facial cues and individuation relevant to social communication constructs.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26908320_1</td><td>Black &gt; White faces: A. Increased activation with less differentiation disparity; socialcommunication</td><td>26908320_analysis_3</td><td>Increased activation with less differentiation disparity</td><td>0.824</td><td>1.000</td><td>0.947</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26908320_2</td><td>Black &gt; White faces: A. Increased activation with less racial trust disparities; socialcommunication</td><td>26908320_analysis_0</td><td>A. Increased activation with more racial trust disparity</td><td>0.770</td><td>0.750</td><td>0.756</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>26908320_3</td><td>Black &gt; White faces: A. Increased activation with more differentiation disparity; socialcommunication</td><td>26908320_analysis_2</td><td>A. Increased activation with more differentiation disparity</td><td>0.849</td><td>1.000</td><td>0.955</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26908320_4</td><td>Black &gt; White faces: A. Increased activation with more racial trust disparities; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>26908320_5</td><td>Black &gt; White faces: B. Increassed connectivity with dorsolateral prefrontal seed (24, 42, 21) with less differentiation disparity; socialcommunication</td><td>26908320_analysis_4</td><td>B. Increased connectivity with dorsolateral prefrontal seed (24, 42, 21) with less differentiation disparity</td><td>0.908</td><td>1.000</td><td>0.972</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26908320_6</td><td>Black &gt; White faces: B. Increased connectivity with orbitofrontal cortex (18, 66, 0) with less racial trust disparity; socialcommunication</td><td>26908320_analysis_1</td><td>B. Increased connectivity with orbitofrontal cortex (18, 66, 0) with less racial trust disparity</td><td>0.901</td><td>1.000</td><td>0.970</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27375449</strong> | Pred included: 7 | Manual included (accepted matches only): 4 | Correct overlaps: 4 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27375449/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27375449_analysis_0</td><td>Angry &gt; Joyful session</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task involves perception of facial expressions and socially relevant actions (faces and grasping) and therefore taps receptive social communication (affect/face perception).</td></tr>
<tr><td>27375449_analysis_1</td><td>Grasping &gt; Faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast involves perception of non-facial actions (grasping) versus faces; fits within social communication (reception of non-facial communicative/action cues).</td></tr>
<tr><td>27375449_analysis_2</td><td>Faces &gt; Grasping</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Faces &gt; Grasping directly targets reception of facial communication (emotion/face perception), matching social communication criteria.</td></tr>
<tr><td>27375449_analysis_3</td><td>Interaction analysis between grasping and neutral grasping</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The interaction involves facial expressions and action contexts, relevant to reception of facial/non-facial social cues (social communication).</td></tr>
<tr><td>27375449_analysis_4</td><td>Angry &gt; Joyful run</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analyses probe connectivity between insula and STG during perception of emotional faces/actions — relevant to reception of facial/non-facial social cues (social communication).</td></tr>
<tr><td>27375449_analysis_5</td><td>Angry grasping &gt; Joyful grasping</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Comparison of emotional action conditions (angry vs joyful grasping) pertains to reception of socially communicative emotional cues in faces/actions.</td></tr>
<tr><td>27375449_analysis_6</td><td>PPI: seed in the right Insula</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Seed-based connectivity with STG during emotional face/action perception addresses receptive social communication processes (facial/non-facial cues).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27375449_1</td><td>Angry &gt; Joyful session; socialcommunication</td><td>27375449_analysis_0</td><td>Angry &gt; Joyful session</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27375449_2</td><td>Faces &gt; Grasping; socialcommunication</td><td>27375449_analysis_2</td><td>Faces &gt; Grasping</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27375449_3</td><td>Grasping &gt; Faces; socialcommunication</td><td>27375449_analysis_1</td><td>Grasping &gt; Faces</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27375449_4</td><td>Interaction analysis between grasping and neutral grasping; socialcommunication</td><td>27375449_analysis_3</td><td>Interaction analysis between grasping and neutral grasping</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 28477977</strong> | Pred included: 8 | Manual included (accepted matches only): 5 | Correct overlaps: 5 | Match statuses: accepted=5, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/28477977/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>28477977_analysis_0</td><td>SELF minus OTHER [(SELF_No + SELF_Single + SELF_ Group) - (OTHER_No + OTHER_Single + OTHER_Group)] (Fig. 3)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task involves production (reading/telling jokes) and perception of non-facial auditory social signals (laughter). The SELF minus OTHER contrast distinguishes self-related communicative context and therefore pertains to social communication.</td></tr>
<tr><td>28477977_analysis_1</td><td>Group minus No Laughter [(SELF_ Group - SELF_No) + (OTHER_ Group - OTHER_No)] (Supplemental Fig. S1)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This contrast isolates neural responses to auditory social communication (laughter), i.e., reception of non-facial communicative signals, matching the Social Communication inclusion criteria.</td></tr>
<tr><td>28477977_analysis_2</td><td>Single minus No Laughter [(SELF_ Single - SELF_No) + (OTHER_Single - OTHER_No)]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast isolates reception of non-facial auditory social communication (single laughter) and therefore meets Social Communication criteria.</td></tr>
<tr><td>28477977_analysis_3</td><td>Group minus Single Laughter [(SELF_ Group - SELF_Single) + (OTHER_ Group - OTHER_Single)]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast involves reception of non-facial auditory communicative signals (different laughter types) and therefore falls under Social Communication.</td></tr>
<tr><td>28477977_analysis_4</td><td>[(SELF_ Group - SELF_No) - (OTHER_Group - OTHER_No)] (Fig. 4)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast operationalizes how communicative outcomes (laughter) contingent on agent (self vs other) modulate brain responses — a core Social Communication process (reception of non-facial cues within interaction).</td></tr>
<tr><td>28477977_analysis_5</td><td>[(SELF_ Single - SELF_No) - (OTHER_Single - OTHER_No)]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast tests processing of communicative auditory feedback (single laughter) contingent on agent (self vs other) and therefore pertains to social communication.</td></tr>
<tr><td>28477977_analysis_6</td><td>Physio-physiological interaction (PPI) seeded on the right auditory cortex and mPFC.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis explicitly examines interaction between a self-related region (mPFC) and an auditory region processing communicative laughter — directly addressing reception/processing of non-facial social communication and its integration.</td></tr>
<tr><td>28477977_analysis_7</td><td>Physio-physiological interaction (PPI) analysis in the left auditory cortex and mPFC.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis tests interactions between self-related and auditory social-signal regions (left AC), addressing reception and integration of non-facial communicative cues — a Social Communication measure.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>28477977_1</td><td>SELFGroup−SELFNo &gt; OTHER Group–OTHERNo; socialcommunication</td><td>28477977_analysis_4</td><td>[(SELF_ Group - SELF_No) - (OTHER_Group - OTHER_No)] (Fig. 4)</td><td>0.485</td><td>1.000</td><td>0.845</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>28477977_2</td><td>group &gt; no laughter; socialcommunication</td><td>28477977_analysis_1</td><td>Group minus No Laughter [(SELF_ Group - SELF_No) + (OTHER_ Group - OTHER_No)] (Supplemental Fig. S1)</td><td>0.343</td><td>1.000</td><td>0.803</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>28477977_3</td><td>group &gt; single laughter; socialcommunication</td><td>28477977_analysis_3</td><td>Group minus Single Laughter [(SELF_ Group - SELF_Single) + (OTHER_ Group - OTHER_Single)]</td><td>0.406</td><td>1.000</td><td>0.822</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>28477977_4</td><td>self &gt; other; socialcommunication</td><td>28477977_analysis_0</td><td>SELF minus OTHER [(SELF_No + SELF_Single + SELF_ Group) - (OTHER_No + OTHER_Single + OTHER_Group)] (Fig. 3)</td><td>0.271</td><td>1.000</td><td>0.781</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>28477977_5</td><td>single &gt; no laughter; socialcommunication</td><td>28477977_analysis_2</td><td>Single minus No Laughter [(SELF_ Single - SELF_No) + (OTHER_Single - OTHER_No)]</td><td>0.450</td><td>1.000</td><td>0.835</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29221830</strong> | Pred included: 2 | Manual included (accepted matches only): 5 | Correct overlaps: 2 | Match statuses: accepted=5, uncertain=1, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29221830/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29221830_analysis_0</td><td>Collaborative &gt; Arbitrary</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task requires producing labels in a communicative referential game; the contrast isolates retrieval of labels established in an interactive communicative context, directly measuring social communication production and its neural correlates.</td></tr>
<tr><td>29221830_analysis_1</td><td>Individual &gt; Arbitrary</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although retrieval occurred in a communicative scanner task, the contrast isolates effects of self-generated semantic associations versus arbitrary associations rather than social communication per se; it does not test reception/production of social communicative signals.</td></tr>
<tr><td>29221830_analysis_2</td><td>Collaborative &gt; Individual</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The manipulation contrasts labels created in an interactive communicative setting versus a solo setting; the contrast taps communicative production and partner-oriented message formation, matching Social Communication.</td></tr>
<tr><td>29221830_analysis_3</td><td>Arbitrary &gt; Individual</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TN</span></td><td></td><td>Although the scanning task involved communicative output, this contrast isolates differences in hippocampal/effortful retrieval for arbitrary associations versus self-generated ones, not social communication per se.</td></tr>
<tr><td>29221830_analysis_4</td><td>Individual &gt; Collaborative</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast does not specifically measure social communicative processes; it identifies areas relatively more active for individual encoding compared to collaborative encoding, interpreted as non-social or semantic effects.</td></tr>
<tr><td>29221830_analysis_5</td><td>Arbitrary &gt; Collaborative</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although behavior occurred in a communicative context, this contrast isolates retrieval differences driven by arbitrary associations and hippocampal demand, not social communication processes per se.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29221830_1</td><td>Arbitrary &gt; Collaborative; socialcommunication</td><td>29221830_analysis_5</td><td>Arbitrary &gt; Collaborative</td><td>1.000</td><td>0.946</td><td>0.962</td><td>accepted</td><td>high_coord_match</td></tr><tr><td>29221830_2</td><td>Arbitrary &gt; Individual; socialcommunication</td><td>29221830_analysis_3</td><td>Arbitrary &gt; Individual</td><td>1.000</td><td>0.617</td><td>0.732</td><td>uncertain</td><td>coord_count_mismatch</td></tr><tr><td>29221830_3</td><td>Collaborative &gt; Arbitrary; socialcommunication</td><td>29221830_analysis_0</td><td>Collaborative &gt; Arbitrary</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29221830_4</td><td>Collaborative &gt; Individual; socialcommunication</td><td>29221830_analysis_2</td><td>Collaborative &gt; Individual</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29221830_5</td><td>Individual &gt; Arbitrary; socialcommunication</td><td>29221830_analysis_1</td><td>Individual &gt; Arbitrary</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29221830_6</td><td>Individual &gt; Collaborative; socialcommunication</td><td>29221830_analysis_4</td><td>Individual &gt; Collaborative</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29740753</strong> | Pred included: 3 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29740753/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29740753_analysis_0</td><td>Main effect of modality</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The modality main effect involves reception of facial and vocal emotional signals (viewing faces, listening to prosody) and thus directly measures Social Communication (receptive communication).</td></tr>
<tr><td>29740753_analysis_1</td><td>Main effect of emotion</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Emotion category effects arise from perception of facial and vocal emotional signals (receptive aspects of social communication), so this analysis measures Social Communication.</td></tr>
<tr><td>29740753_analysis_2</td><td>The 3 (Emotion)×3 (Modality) interaction effects generated by repeated measures ANOVA.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The interaction identifies differences in receptive social communication (face vs voice vs bimodal) for emotion recognition, meeting Social Communication criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29740753_1</td><td>Emotion &gt; Modality; socialcommunication</td><td>29740753_analysis_1</td><td>Main effect of emotion</td><td>0.350</td><td>1.000</td><td>0.805</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>29740753_2</td><td>Modality &gt; Emotion; socialcommunication</td><td>29740753_analysis_0</td><td>Main effect of modality</td><td>0.390</td><td>1.000</td><td>0.817</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>29740753_3</td><td>Modality X Emotion; socialcommunication</td><td>29740753_analysis_2</td><td>The 3 (Emotion)×3 (Modality) interaction effects generated by repeated measures ANOVA.</td><td>0.320</td><td>1.000</td><td>0.796</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 30272134</strong> | Pred included: 3 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/30272134/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>30272134_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Tasks involve exchanging socially relevant information (face and voice emotional signals, audiovisual emotion perception and congruency), matching the Social Communication construct (receptive facial and non-facial communication).</td></tr>
<tr><td>30272134_analysis_1</td><td>(BIMODAL &gt; UNIMODAL FACES) ∩ (BIMODAL &gt; UNIMODAL VOICES)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast probes reception and integration of facial and vocal communicative signals (emotional expressions), matching Social Communication (receptive facial/non-facial communication).</td></tr>
<tr><td>30272134_analysis_2</td><td>INCONGRUENT &gt; CONGRUENT</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Incongruency between face and voice taps receptive social communication processes (processing and integration of facial and vocal emotional cues), meeting Social Communication criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>30272134_1</td><td>(Bimodal &gt; Unimodal faces) ∩ (Bimodal &gt; Unimodal Voices); socialcommunication</td><td>30272134_analysis_1</td><td>(BIMODAL &gt; UNIMODAL FACES) ∩ (BIMODAL &gt; UNIMODAL VOICES)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>30272134_2</td><td>Incongruent &gt; Congruent; socialcommunication</td><td>30272134_analysis_2</td><td>INCONGRUENT &gt; CONGRUENT</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 31090126</strong> | Pred included: 8 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/31090126/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>31090126_analysis_0</td><td>Stimulus type (expression&gt;mosaic)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Viewing and discriminating dynamic emotional faces is a canonical social communication reception task (facial affect recognition); the contrast isolates face-related communication (faces &gt; mosaics).</td></tr>
<tr><td>31090126_analysis_1</td><td>Stimulus type (expression&gt;mosaic)×emotion</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Emotion×stimulus-type interaction still pertains to reception of facial communication (how different emotions modulate face processing); meets social communication criteria.</td></tr>
<tr><td>31090126_analysis_2</td><td>Stimulus type (expression&gt;mosaic)×gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Investigates whether reception of facial communication differs by sex; falls within social communication (faces &gt; non-faces) domain.</td></tr>
<tr><td>31090126_analysis_3</td><td>Stimulus type (expression&gt;mosaic)×emotion×gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The interaction examines modulation of facial communication reception by emotion and sex—still a social communication measure (faces &gt; mosaics).</td></tr>
<tr><td>31090126_analysis_4</td><td>Stimulus type (expression&gt;mosaic)×laterality</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Laterality of face-expression processing pertains to reception of facial communication (which hemisphere is dominant for processing facial signals).</td></tr>
<tr><td>31090126_analysis_5</td><td>Stimulus type (expression&gt;mosaic)×emotion×laterality</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Evaluates whether reception of facial communication (face &gt; mosaic) lateralization differs by emotion; relevant to social communication reception.</td></tr>
<tr><td>31090126_analysis_6</td><td>Stimulus type (expression&gt;mosaic)×sex×laterality</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Tests whether reception of facial communication lateralization differs by participant sex; fits social communication reception category.</td></tr>
<tr><td>31090126_analysis_7</td><td>Stimulus type (expression&gt;mosaic)×emotion×sex×laterality</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This comprehensive interaction remains within reception of facial communication (how emotion, sex, and laterality modulate face processing).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>31090126_1</td><td>(expression &gt; mosaic) × laterality; socialcommunication</td><td>31090126_analysis_4</td><td>Stimulus type (expression&gt;mosaic)×laterality</td><td>0.800</td><td>1.000</td><td>0.940</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>31090126_2</td><td>expression &gt; mosaic; socialcommunication</td><td>31090126_analysis_0</td><td>Stimulus type (expression&gt;mosaic)</td><td>0.704</td><td>0.959</td><td>0.882</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>
</details></section><section id="bucket-false-positive"><details class="bucket" open><summary><h2>False Positive (55)</h2></summary><p><strong>Match status totals:</strong> accepted=0 | uncertain=3 | unmatched=17</p>
<details class="doc-card">
  <summary><strong>PMID 29079809</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29079809/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29079809_analysis_0</td><td>clusters genetic &gt; non-genetic</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The movie stimulus contains interpersonal dialogue, emotional exchanges and nonverbal cues; ISC of these viewings captures shared processing of communicative social signals (receptive social communication), so the analysis engages social communication processes.</td></tr>
<tr><td>29079809_analysis_1</td><td>clusters non-genetic &gt; genetic</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Even where ISC is stronger in occipital cortex for the non-genetic condition, the underlying task is a communicative movie (dialogue, social signals). The analysis derives from viewing social interactions and thus involves receptive social communication processes.</td></tr>
<tr><td>29079809_analysis_2</td><td>moral dilemma decision task</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The moral decision task is primarily about making evaluative choices (moral/affiliation preference) rather than measuring receptive or productive social-communication processes (e.g., face/gaze or communicative signal perception). The paradigm localizes moral/decision regions rather than social-communication per se.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 5660240)</summary><p><strong>Title:</strong> Differential inter-subject correlation of brain activity when kinship is a variable in moral dilemma</p><details><summary>Abstract</summary><pre class="paper-text">Previous behavioural studies have shown that humans act more altruistically towards kin. Whether and how knowledge of genetic relatedness translates into differential neurocognitive evaluation of observed social interactions has remained an open question. Here, we investigated how the human brain is engaged when viewing a moral dilemma between genetic vs. non-genetic sisters. During functional magnetic resonance imaging, a movie was shown, depicting refusal of organ donation between two sisters, with subjects guided to believe the sisters were related either genetically or by adoption. Although 90% of the subjects self-reported that genetic relationship was not relevant, their brain activity told a different story. Comparing correlations of brain activity across all subject pairs between the two viewing conditions, we found significantly stronger inter-subject correlations in insula, cingulate, medial and lateral prefrontal, superior temporal, and superior parietal cortices, when the subjects believed that the sisters were genetically related. Cognitive functions previously associated with these areas include moral and emotional conflict regulation, decision making, and mentalizing, suggesting more similar engagement of such functions when observing refusal of altruism from a genetic sister. Our results show that mere knowledge of a genetic relationship between interacting persons robustly modulates social cognition of the perceiver.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Evaluating and predicting social interactions of others is an integral part of social cognition, one of the most fundamental of human cognitive functions. Indeed, the evolution of social cognition may best explain why humans have a more developed neocortex than other species . So far, social cognition has been predominantly studied with stimuli depicting interactions between strangers, however, most of the significant interactions evaluated in daily life are between one’s family members, friends, and acquaintances 

More importantly, most of our social interactions (and social effort) is directed to a very small number of familiar individuals, 60% of our social effort is directed to just 15 close friends and family . There is also considerable experimental and observational evidence for a “kinship premium” in our interactions with others, especially when those interactions involve altruistic behaviour . We are more likely to help our genetic relatives compared to unrelated individuals, and to do so implicitly, without conscious elaboration . In a trolley dilemma , subjects have to decide if they would push a handle to set a trolley to another track so that instead of killing five people when left without intervention, it will kill a single person on an alternative track. If only strangers are considered, the study subjects favoured the survival of the five over one life; however, their judgement changed if the single person was genetically related to the subject. 

On the other hand it has been shown, that subjects judged incest to be equally morally wrong for a sibling, irrespective of whether this was a genetic, adoptive or step sibling . In these studies, rather co-residence with the sibling in the family, irrespective of genetic status, was the most relevant factor in decisions about the moral reprehensibility of incest. 

These and many other studies, have shown differences in multiple aspects of moral perception/processing, evaluation, judgement and decision making at the behavioural level when processing information about kin vs. non-kin. However, much less is known about the neuronal underpinnings of these effects. Recently, Wlodarski and Dunbar  have shown that different brain regions are active when subjects judge moral dilemmas about kin vs. friends. They found the sensorimotor cortex, ventromedial prefrontal cortex and posterior cingulate cortex to be more strongly activated when the subjects processed social information about their friends than about their kin. These differences imply that the brain processes kinship information differently than information on unrelated individuals. 

We explored this further by comparing subjects’ brain responses to a moral dilemma involving a pair of genetic versus adoptive (  i  .  e  ., unrelated) sisters. During functional magnetic resonance imaging (fMRI) the subjects viewed the same movie involving two sisters, but one group was primed with the information that they were genetic sisters and the other group with the information that they were sisters by adoption. The case of sisters related genetically vs. by adoption is especially suitable for testing whether knowledge of genetic relationship influences perception of a moral dilemma between kin given that there is no potential for shared genetic interest in future generations for adopted siblings . Note, however, that the current study examined perception of a kin relationship that subjects were seeing in a movie, while in Wlodarski and Dunbars  analysis the subjects answered questions about their own kin members and friends. 

In the present study, we utilized inter-subject correlation (ISC) of brain hemodynamic activity as a model-free analysis approach that makes it possible to use movies as ecologically valid stimuli during fMRI. Due to improvements in fMRI acquisition methods and data analysis algorithms , it has become possible to study specific aspects of social cognition between subjects using ecologically valid fMRI paradigms. The ecological validity is particularly important when studying moral dilemmas in order to engage the subjects and make the dilemma as credible and perceptible as possible in order to get authentic reactions. To investigate the degree of similarity in how individual brains respond to the common movie stimulus, the brains of individual subjects are aligned and ISC between the hemodynamic activity time courses for each voxel are calculated across all subject pairs. ISC can be interpreted as reflecting synchronized neural activity and thus similarity of cerebral information processing across individuals . It has been shown that when viewing a feature film during brain scanning, both “higher-order” prefrontal cortical as well as basic sensory cortex regions become synchronized across subjects . Further, ISC may not only reflect mutual neuronal responses, but could provide the basis of inducing a specific common mind set, e.g. built by context information or perspective taking as well as predicting the actions of others . 

The model-free approach of ISC does not require any a priori, pre-designed modeling of the fMRI signal to carry out the analysis and thus provides a powerful tool to investigate neuronal mechanisms as the correlations are exclusively based on similarities between the subjects’ brain activities when they react to the various aspects of the complex movie . At the same time, ISC has been shown to reliably detect involved brain regions in complex experimental setups almost as sensitively as a model-based analysis . 

In study 1, we asked whether people discriminate behaviourally between relatives with genetic vs. non-genetic backgrounds: In an implicit association test (IAT ) the subjects’ reaction time when associating the words “sister” and “adopted sister” to positive or negative connoted adjectives was measured. Further, after watching the movie, the subjects were asked whether genetic vs. non-genetic relationship status mattered to them in the moral dilemma that they observed. 

In study 2, we tested how the subjects perceived moral dilemmas involving genetically related vs. unrelated individuals during fMRI. In a first task, the subjects watched the movie depicting the moral dilemma between two sisters after being primed that they were either genetically related sisters or sisters related only by adoption. Should knowledge of genetic relationship matter, we expect to see differences in the behavioural tests (IAT; questionnaires) as well as the neuronal mechanisms: we predict that brain regions known to be involved in processing of mentalizing , conflict resolution , emotion regulation , and moral dilemmas  would be activated differently under the two viewing conditions. 

Second, following the hypothesis that moral processing is the most relevant factor to distinguish between watching the movie when believing the sisters to be genetic or adopted (i.e., non-genetic), each subject underwent a moral decision task during fMRI scanning to evaluate specifically which brain areas are associated with the perception and processing of moral dilemmas during movie-watching. In this task the subjects had to decide whom to save from a dangerous area and had different choices including their own sister, best friend, and strangers, an experimental design similar to the classical moral trolley dilemma. Again, if the genetic relationship had an effect on the viewers, as has previously been shown behaviourally , we hypothesize that the subjects will show kinship preference by saving their sister over others and that similar brain areas are engaged both in the decision task and when watching the movie believing that the sisters are genetic. 


## Results 
  
### Study 1: Implicit association test (IAT) 
  
To examine if a possible general implicit bias against adopted sisters (that potentially modulates brain functions during movie viewing) underlies the subjects’ perception of the movie, we asked 30 subjects in a behavioural experiment to undergo an IAT . In this test, reaction times during assignment of positive and negative connoted words to the categories of sister and adopted sister showed that there is no such implicit bias: Out of 30 subjects, nine favoured a genetic sister, 13 a non-genetic sister, and eight had no preference (one sampled t-test t = −0.9564 p = 0.3468). The TOST procedure  indicated that the ratings of emotional closeness were significantly similar (observed effect size (d = −0.17) was significantly within the equivalent bounds of d = −0.68 and d = 0.68; t(29) = 2.77, p = 0.005). 


### Study 2: Inter-subject correlation (ISC) of fMRI during movie watching 
  
#### Inter-subject correlation (ISC) across all conditions 
  
During fMRI scanning the subjects watched a movie depicting a moral dilemma between two sisters, either believing the sisters are genetic sisters or that the younger sister was adopted at birth. In a first step, the overall ISC (22) of hemodynamic activity of an independent set of 30 subjects was calculated during first viewing of the movie (Fig.  ). Significant ISC was observed extensively in occipital lobes, posterior parietal areas, and temporal cortices. In the frontal cortex, areas in the lateral inferior frontal gyrus (IFG), lateral middle frontal gyrus (MFG), dorsolateral prefrontal cortex (DLPFC), dorsomedial prefrontal cortex (DMPFC) and ventromedial prefrontal cortex (VMPFC) showed ISC between all subjects. The location of all brain areas were defined using anatomical brain atlases as specifically the Harvard-Oxford Cortical Structural Atlas and the Juelich Histological Atlas.   
Inter-subject correlation (ISC) of all 30 subjects during the first viewing of the movie. On top row are shown lateral and on bottom row medial surfaces of left and right cerebral hemispheres. Red-yellow colours indicate areas of significant ISC during movie watching (FDR q &lt; 0.05). Abbreviations: ACC = anterior cingulate cortex, ANG = angular gyrus, CAL = calcarine gyrus, DLPFC = dorsolateral prefrontal cortex, DMPFC = dorsomedial prefrontal cortex, IFG = inferior frontal gyrus, IOG = inferior occipital gyrus, MFG = middle frontal gyrus, MOG = middle occipital gyrus, MTG = middle temporal gyrus, PCC = posterior cingulate cortex, SOG = superior occipital gyrus, SPL = superior parietal lobe, STS/STG = superior temporal sulcus and gyrus, VMPFC = ventromedial prefrontal cortex, a = anterior, d = dorsal, p = posterior, v = ventral. 
  


#### Differences in ISC between conditions 
  
In a second step, the ISC of all subjects (N = 30) were contrasted between the genetic vs. non-genetic relationship viewing conditions. As each participant watched the movie in the genetic and in the non-genetic condition on two different scanning days in a counterbalanced order, this is a within-subject design. There were robust differences between the two conditions in the ISC of hemodynamic activity of the subjects, despite 90% of the subjects self-reporting that it did not matter to them whether the sisters were related genetically or not. When the subjects watched the movie believing that they were seeing genetically related sisters, the ISC was significantly stronger in the superior temporal sulcus and gyrus (STS/STG), VMPFC, DLPFC, anterior cingulate cortex (ACC) and posterior cingulate cortex (PCC), IFG, insula, cuneus, precuneus, and superior parietal lobule (SPL) (Fig.  , Table  ).   
Differential ISC between the conditions of an assumed genetic and non-genetic sisters and BOLD time series from two exemplary single voxels. (  A  ) Significant differences in brain activity when all subjects watched the movie thinking that the sisters were genetically vs. non-genetically related (FDR q &lt; 0.05, t = 2.1447, for consistent illustration purposes, the figures shows t-values from 3 to 9 and −3 to −9) (N = 30, within subject design). Red-yellow colours indicate areas of significantly higher ISC when the subjects watched the movie as depicting genetically related, as compared to non-genetically related, sisters. Blue colour indicates areas showing significantly higher ISC in the reverse contrast. Abbreviations: ACC = anterior cingulate cortex, CAL = calcarine gyrus, DLPFC = dorsolateral prefrontal cortex, IFG = inferior frontal gyrus, IOG = inferior occipital gyrus, MOG = middle occipital gyrus, PCC = posterior cingulate cortex, SPL = superior parietal lobe, STS/STG = superior temporal sulcus and gyrus, VMPFC = ventromedial prefrontal cortex, a = anterior, d = dorsal, p = posterior, v = ventral. (  B  ) Across subjects averaged BOLD time series of two voxels, one in the area VMPFC that showed significantly higher ISC when the subjects were viewing the sisters as genetic and one time series of a voxel in area V1 (primary visual cortex) that did not show significant between-condition ISC differences. The red line plots the group mean BOLD in the genetic sisters condition and the blue line plots the group mean BOLD in non-genetic sisters condition over the whole length of the movie. Red and blue shades indicate the 25  and 75  percentile of the variance. 
    
Clusters size, peak coordinates and t value of all clusters of Experiment 1 (movie watching task). 
  

When the subjects thought that the sisters were non-genetic, higher ISC was observed mainly in the occipital cortex. Importantly, the movie stimulus was identical in both viewing conditions. 

To illustrate blood oxygenation level dependent (BOLD) time series of specific voxels, in Fig.  , the panel B shows the time series of two exemplary voxels, over the whole length of the movie, from: i) VMPFC that showed higher ISC when the subjects viewed the sisters as genetic and ii) from a voxel in the area V1 (primary visual cortex), which is an early sensory brain area that did not show any between condition differences in ISC. 

Self-ratings of emotional valence and arousal obtained after the scans were not significantly different between the conditions (Valence: r = 0.0075, p = 0.3458, Arousal: r = −0.0189, p = 0.6081) (Fig.  ). Further, the mean ISC of eye-movements (eISC) over time windows showed no significant difference between the groups of participants believing in genetic or non-genetic sisters (p = 0.3918) (see Fig.  ). Likewise, no significant difference could be found in the heart and breathing rate comparing the conditions of assumed genetic versus non-genetic sisters (with bootstrap over 5000 permutations, breathing rate: t-value = 0.430, p = 0.335: heart rate: t-value = −1.12, p = 0.129) (see Fig.  ).   
Experienced emotional valence and arousal as well as physiological parameters when perceiving the sisters in the movie as genetic vs. non-genetic. (  A  ) Shown are dynamic self-ratings of emotional valence and arousal over the whole time course of the movie obtained during re-viewing of the movie after the fMRI sessions when the sisters were viewed as genetically related (red) or non-genetic (blue). The ratings were highly similar and there were no time periods where significant between-condition differences could have been observed. Note that half of the subjects (N = 2 × 15) rated experienced arousal and the rest rated experienced valence after the first fMRI session followed by rating the other emotional dimension after the second fMRI session. Plotted are means for all subjects in the red line for assumed genetic sisters and the blue line for non-genetic sisters condition. Red and blue dashed lines show the 25  and 75  percentile of the variance. (  B  ) Eye gaze behavior (N = 29) in the movies when the sisters were perceived as either genetic (right) or non-genetic (left) shown as a violin plot with the red cross depicting the means and green squares the medians. There were no significant differences between the conditions. (  C  ) Breathing and heart rates (N = 30) when the sisters were perceived as either genetic (red) or non-genetic (blue). There were no significant differences between the conditions. Red line plots the condition with assumed genetic sisters and the blue line non-genetic sisters. Red and blue shade show the 25  and 75  percentile of the variance. 
  



### Comparison with the moral dilemma decision experiment 
  
To further examine which neurocognitive processes might be involved, we studied whether the brain areas showing higher ISC in the genetic condition overlap with those engaged during a modified moral dilemma task  analysed with general linear modelling (GLM) (Fig.  , Tables   and  ). Naturally, it should be kept in mind that while ISC and GLM analyses of brain hemodynamic activity can yield converging results , this is not necessarily the case, as high ISC can be observed also when the BOLD signals are small.   
Having to decide in a simulation between saving one’s sister, friend, and others from crisis regions elicited significant brain activity in the VMPFC, ACC, precuneus, DLPFC, IFG, insula, TPJ and MTG. These activations of subjects (N = 30) were obtained by contrasting the decision phases (from the point of revealing the individuals involved until the decision signaled by the subject’s button press) against non-decision phases (subjects watching the background story depicting the two crisis regions and how the subjects only have resources to save individuals from one of the crisis regions) (FDR q &lt; 0.05, t = 2.0384, for consistent illustration purposes, the figures shows t-values from 3 to 9) Left-lateralized motor and supplementary motor are probably explained by the button press that the subjects performed to announce their decision. Abbreviations: ACC = anterior cingulate cortex, DLPFC = dorsolateral prefrontal cortex, IFG = inferior frontal gyrus, IOG = inferior occipital gyrus, ITG = inferior temporal gyrus, MC = motor cortex, MFC = medial frontal cortex, MOG = middle occipital gyrus, MTG = middle temporal gyrus, PCC = posterior cingulate cortex. SMA = supplementary motor area, SPL = superior parietal lobe, TPJ = temporo-parietal junction,VMPFC = ventromedial prefrontal cortex, a = anterior, d = dorsal, p = posterior, v = ventral. 
    
Clusters size, peak coordinates and t value of all clusters of and Experiment 2 (moral dilemma decision task). 
  

When the same subjects who watched the movie had to decide between saving their sister, best friend, vs. stranger(s), in various combinations, from a crisis region, 93% of the subjects showed a clear kin preference by choosing their sister (even when associated with some strangers) rather than their best female friend (chi-squared test ×  = 43.4 p &lt; 4 × 10 ). Further, as can be seen in Fig.   the VMPFC, ACC, IFG, MTG, SPL, PCC, precuneus, DLPFC, and anterior insula were consistently involved in both making choices of whom to prefer in a morally dilemmatic situation and when observing the moral dilemma between a genetic vs. non-genetic sister. Importantly, ratings of emotional closeness were not significantly different for the subjects’ sisters and their best friends with an average of 9.28 (sisters) and 8.80 (friends) on a 1–10 scale (Wilcoxon signed rank test = 0.12, t-test, t = 1.64 p = 0.11). The TOST procedure  indicated that the ratings of emotional closeness were significantly similar (observed effect size (d = 0.33) was significantly within the equivalent bounds of d = −0.68 and d = 0.68, or in raw scores: −1.07 and 1.07, t(29) = −1.92, p = 0.032).   
Activity during moral dilemma decision making as disclosed by GLM analysis (red) and the ISC when the subjects believed in a genetic relationship between the sisters in the movie (blue), along with the overlap of these two maps (violet), as well as the more strict overlap with a conjunction test (yellow);  , (FDR q &lt; 0.05, t = 2.0384 for GLM and t = 2.1447 for ISC). Abbreviations: ACC = anterior cingulate cortex, DLPFC = dorsolateral prefrontal cortex, IFG = inferior frontal gyrus, MFC = medial frontal cortex, MOG = middle occipital gyrus, MTG = midddle temporal gyrus, PCC = posterior cingulate cortex. SPL = superior parietal lobe, VMPFC = ventromedial prefrontal cortex, a = anterior, d = dorsal, p = posterior, v = ventral. 
  

As measured with an independent group of subjects outside the scanner, reaction times for the moral-dilemma decision were significantly longer in the case of a decision between a group comprised of their friend and four strangers on one side and their sister alone on the other side as in the case that only comprised strangers (on both sides) (paired Wilcoxon rank sum test p = 0.0011973). 



## Discussion 
  
In the present study, we investigated whether refusing altruism from a sister is perceived differently when the viewers think that the sisters are genetically related vs. when they think that one of the sisters has been adopted at young age. The results of the IAT in study 1 suggest that the subjects do not show an implicit bias against adoptive sisters compared to genetic sisters in general. Also, when explicitly asked if the relationship (as genetic or adopted) would matter in the decision of an organ donation, most subjects (90%) report that this knowledge would not affect the decision. Further, heart rate and breathing rate exhibited no significant differences between the two conditions, and self-reported emotional valence and arousal was likewise similar between the conditions (Fig.  ), suggesting that there were no robust differences in experienced emotions between the conditions. 

In contrast, the ISC of the hemodynamic brain activity show a different picture: robust differences were observed in patterns of brain activity due to the mere knowledge of the genetic relationship between the sisters in the movie. Specifically, there were multiple brain regions showing significantly higher ISC when the subjects thought that they are seeing a young girl refusing to donate her organ to save her genetic, as opposed to non-genetic, sister (Fig.  ). These areas included VMPFC, DLPFC, ACC, PCC, insula, precuneus, and SPL. While we would caution against drawing conclusions of specific cognitive functions involved on the basis of observed differences in brain activity, these brain regions have been previously shown to be associated with moral and emotional conflict regulation , decision making , mentalizing , and perspective taking , thus tentatively suggesting more uniform engagement of such neurocognitive functions when observing the dilemma of organ donation between genetic sisters. 

In the reverse contrast (i.e. when the subjects thought the sisters were non-genetic), higher ISC was observed in brain areas in the occipital cortex, conventionally associated mainly with visual perception . One possible explanation could potentially be that in the case of a non-genetic relationship between the sisters, the processing of complex social conflict associated with the moral dilemma is less demanding, and therefore leaves room for the subjects to focus on the visual aspects of the movie. However, eye-movements were not significantly different between the two viewing conditions, suggesting that differences in attention to movie events did not explain the observed robust differences in ISC. 

To specifically test for the possibility that the differences in ISC between the conditions reflected differences in moral evaluation, we compared the areas showing differences in ISC with areas activated when the subjects engaged in a separate moral-decision making control task. In this control task, the subjects had to make choices about saving people (including their sister, best friend, and strangers, in various combinations) from disaster. As each subject made only one decision that contrasted saving the friend over strangers and one decision of saving the friend over the own sister plus four strangers (as well as four decisions that contrasts the sister to groups of others), the statistical power in this experimental design was unfortunately not sufficient to differentiate directly between brain responses during decisions to save the sister vs. the friend. Rather, the results should be viewed as localization of brain regions involved in making moral decisions, yet also modulated by differences in, e.g., executive control, readiness for action, and attention, between the passive perception of the story and active decision making. However, significantly longer reaction times suggested increased difficulty when having to choose between a sister and the friend together with four strangers while areas in the VMPFC, DLPFC, ACC, PCC, precuneus, IFG, MTG, SPL and anterior insula were consistently involved in both making choices of whom to prefer in a morally dilemmatic situation and when observing the moral dilemma between a genetic vs. non-genetic sister (Fig.  ). This overlap in engaged brain regions suggests that processing of moral dilemmas took place during movie watching when the sisters were understood to be genetically related. It is significant that the brain regions flagged up in this analysis are those known to be involved in processing moral dilemmas and mentalizing. The DLPFC has been reported to play a role in overcoming a primary moral judgment in favour of greater welfare  and in cortical emotional processing , while the MTG has been implicated in attributing mental states as well as ingroup/outgroup distinctions , the ACC has been reported to be engaged in resolving conflicts , and the SPL, precuneus, and PCC have been implicated in mentalizing and perspective taking . Further, the VMPFC has been associated with viewing moral conflicts, making moral decisions, attributing mental states to self and others, adopting another person’s perspective, and evaluating their beliefs . 

Thus, the differences in ISC between the conditions appear to have arisen due to the knowledge about the sisters’ relationship influencing cognitive evaluation of the moral dilemma depicted in the movie. The contrast to the behavioural results assessed in study 1, where we find that behavioural decisions were not influenced by knowledge of the relationship, is particularly interesting since it suggests that differential processing is taking place under the surface. 

There could be at least two possible explanations for these findings. First, the study subjects might have purposely been hiding their “real” honest opinions as they might have not been willing to reveal these to the researchers, presumably because of social pressure against discriminating between genetic and adoptive siblings. However, the IAT is an implicit test for biases (it uses differences in reaction times for associations of a specific term with positively and negatively connoted words), so subjects are not aware of their performance on this task. Further, they do not know the exact measures which are used to calculate the IAT score, thus making it difficult to engineer potential biases; hence, it is very difficult, even impossible, to manipulate an IAT response in a desired direction . Thus, while a conscious manipulation of reported opinions would be possible in the open-format questionnaire (when asked if the relationship of the sisters matters in the situation of organ donation), it is very unlikely in the IAT. However, Liberman   et al  .  showed that, when judging incest reprehensibility, the coresidence of siblings is a stronger factor than the assumed relationship status and in when the two parameters are in conflict, the time spent in coresidence outweighs the belief of kin relation. As in this study the subjects were told that the apparent adoption took place as the younger sister was a newborn (implying coresidence of the sisters in both the genetic and the adoption case), we suggest, in accordance with Liberman   et al  ., that the factor of coresidence was given greater account than the kin relationship and thus the subjects’ explicit answers in the questionnaire could probably be seen/taken as truthful i.e. reporting authentic, honest thoughts. 

A second possible explanation for our findings is that, as the results of the implicit testing show, the study subjects indeed did not show any biases behaviourally and still pursued different ways of considering the case of a relationship by genes or by adoption, with resulting differences in brain activity patterns. These results show that an event that is behaviourally counter-intuitive (e.g. refusing to help a sister to prolong her life) needs different and potentially more intensive mental processing when the sisters are related genetically compared to adoptive sisters. As the differences in the brain activity patterns between the genetic and the adopted condition particularly comprise areas known to be involved in processing moral dilemmas and mentalizing as the VMPFC, DLPFC, ACC, PCC, precuneus, IFG, MTG, SPL, and anterior insula, we suggest that the study subjects’ expectations of morality are more strongly violated when close genetic relatives refuse to help each other than when unrelated individuals behave this way, despite their close social relation (adoption). 

Notwithstanding these points, we wish to caution the reader to keep in mind the caveats associated with reverse inference  (although see ). Specifically, even as we are suggesting that an activation observed in a certain region is indicative of a specific cognitive process based on results of previous research documented in the literature, it is possible that the activation of that region in the present study was due to some other cognitive process. This is because in general any given brain region is involved in multiple cognitive functions, thus making it difficult to infer with certainty the cognitive functions involved in a task based on brain regions that are activated. 

An alternative possibility is that the different measures operate at different levels of cognition: The null result in the IAT could be relying on a more basic level of attention to the social knowledge, whereas the questionnaire requires high level explicit cognition and the ISC during movie perception reflects some intermediate level of cognition. Had we thought to include them, other behavioural tests might have revealed more detail and background information on the subjects. Finally, it is always possible that something other than moral considerations could underlie the differences in brain patterns that we found, although, given the brain areas that show differences, this is unlikely. 

In summary, we observed robust differences in brain activity when subjects viewed a movie depicting refusal to donate an organ to a genetic vs. non-genetic sister. These differences in brain activity were observed despite the subjects self-reporting that the relational status of the sisters did not make any difference to them. Areas of increased synchrony in the case of genetic sisters overlapped with those activated in a separate moral dilemma decision task. Taken together, our results suggest that the precuneus, MTG, insula, SPL, and the VMPFC, along with the associated cognitive processes (i.e., moral and emotional conflict regulation), decision making, mentalizing and perspective taking are synchronized across subjects more robustly when they are viewing refusal of altruism from genetic as opposed to non-genetic, kin. Overall, these findings are fundamentally important for understanding social cognition, a pivotal ability that makes us human and, among other things, enables the existence of societies. Our findings point out that the perceived relationship of interacting persons robustly modulates how the brains of spectators process third-party interactions. This is highly significant given that majority of research to date on social cognition has been on strangers, whereas most of our social interactions take place between family members, friends, and acquaintances. 


## Material and Methods 
  
### Subjects 
  
We studied 33 healthy female subjects  (19–39 years, mean age of 26 years, one left-handed, laterality index of right handed 84.5%). None of the subjects reported any history of neurological or psychiatric disorders. When asked, all subjects reported either normal vision or corrected to normal vision by contact lenses. Three subjects were excluded due to discomfort in the scanner, so that the final analysis included 30 subjects. 27 of them were native Finnish speakers and three were native Russian speakers. All subjects were sufficiently proficient in English to follow the dialogue in the movie without subtitles. The experimental protocols were approved by the research ethics committee of the Aalto University and the study was carried out with their permission (Lausunto 9 2013 Sosiaalisen kognition aivomekanismit, 8.10.2013) and in accordance with the guidelines of the declaration of Helsinki . Written informed consent was obtained from each subject prior to participation. 


### Stimuli and Procedure 
  
The study consisted of two experiments. In the first experiment, the feature film   My Sister’s Keeper”   (dir. Nick Cassavetes, 2009, Curmudgeon Films), edited to 23 minutes and 44 s, (of which 14 min 17 s (60%) portray the theme of refusal of the organ donation),with the main story line retained, was shown to the subjects during fMRI. This shortened version of the movie focuses on the moral dilemma of the protagonist Anna to donate one of her kidneys to her sister Kate, who is fatally ill from cancer. In the course of the movie, Anna refuses to donate and Kate dies. The reason for Anna refusing to donate the kidney was not revealed to the subjects until after the experiment. The movie was shown to the subjects in the scanner four times in two separate scanning sessions on two different days. For each viewing of the movie the instructions were varied regarding the information about the sister’s relationship and the perspective to take in this viewing (Fig.  ). Each subject thus watched the movie assuming that the sisters were genetic sisters or that the younger sister Anna had been adopted as a newborn. In addition each subject was asked to take either the perspective of the potential donor (Anna) or the perspective of the potential recipient (Kate) on separate viewings (and both under the condition of a genetic or non-genetic relation background). The order of the different viewing conditions was counterbalanced between the subjects.   
Experimental procedure and ISC analysis in the movie watching task. (  A  ) Every subject watched the movie four times, in a 2 × 2 design assuming that the movie characters are either genetic sisters or not genetically related and taking the perspective of the to-be-donor sister Anna or the to-be-recipient sister Kate. The order of all the conditions were counter-balanced. (  B  ) Time series from each voxel from all the fMRI recordings are compared across subjects in pairwise correlations to obtain the mean inter-subject-correlation (ISC). 
  

In the second experiment, each subject carried out a moral-dilemma decision task during fMRI in order to localize brain regions that are related to moral decision making. For this purpose, a modified version of the classical trolley dilemma , was shown to the subjects. Each subject had to choose between rescuing different individuals, including unknown individuals, their sister and a best female friend. A presentation showing text and pictures told a story about civil unrest in a fictive distant country. This country had two parts: one part very dangerous and the other much less dangerous. Different people are in both parts of the country. Subjects were also told that as they were very rich and owned an airplane, they could go there and rescue some of the people. However, due to the circumstances in the country they had to decide which group of people to rescue. The two choices were always a group of five individuals on one side and a single person on the other. In seven runs the identity of the involved individual(s) was varied using the real names of the subject’s sister and best female friend. The 7 runs were: 1. All persons are unknown; 2. Sister is with four others in the dangerous part of the country, the single person is unknown; 3. Five persons are in the dangerous part of the country, the single person is the sister; 4. Five persons are in the dangerous part of the country, the single person is the friend; 5. Sister is with four others in the dangerous part of the country, the single person is the friend; 6. Friend is with four others in the dangerous part of the country, the single person is the sister; 7. Sister is with four others in the less dangerous part of the country, the single person is unknown. Responses in the moral dilemma decision task were recorded with a button press on a LUMItouch keypad (Photon Control Inc.8363, Canada). For the all questions, it was calculated with which percentage the sister was chosen over the friend and the stranger(s); statistical significance was tested with a Chi  test. 


### fMRI acquisition 
  
Before each scan the subjects were informed about the scanning procedures and asked to avoid bodily movements during the scans. All stimuli were presented to the subject with the Presentation software (Neurobehavioral Systems Inc., Albany, CA, USA), synchronizing the onset of the stimuli with the beginning of the functional scans. The movie was back-projected on a semitransparent screen using a data projector (PT-DZ8700/DZ110X Series, Panasonic, Osaka, Japan). The subjects viewed the screen at 33–35 cm viewing distance   via   a mirror located above their eyes. The audio track of the movie was played to the subjects with a Sensimetrics S14 audio system (Sensimetrics Corporation Malden, USA). The intensity of the auditory stimulation was individually adjusted to be loud enough to be heard over the scanner noise. The brain-imaging data were acquired with a 3T Siemens MAGNETOM Skyra (Siemens Healthcare, Erlangen, Germany), at the Advanced Magnetic Imaging center, Aalto University, using a standard 20-channel receiving head-neck coil. Anatomical images were acquired using a T1-weighted MPRAGE pulse sequence (TR 2530 ms, TE 3.3 ms, TI 1100 ms, flip angle 7°, 256 × 256 matrix, 176 sagittal slices, 1-mm3 resolution). Whole-brain functional data were acquired with T2*-weighted EPI sequence sensitive to the BOLD contrast (TR 2000 ms, TE 30 ms, flip angle 90, 64 × 64 matrix, 35 axial slices, slice thickness 4 mm, 3 × 3 mm in plane resolution). 

A total of 712 whole-brain EPI volumes were thus acquired for each movie viewing. The number of whole-brain EPI volumes for the moral dilemma decision task varied individually depending on the decision made by each subject (median 267 whole-brain EPI volumes). Heart pulse and respiration were monitored with the Biopac system (Biopac Systems Inc., Isla Vista, California, USA) during fMRI. Instantaneous values of heart rate and breathing rate were estimated with Drifter software package  (  http://becs.aalto.fi/en/research/bayes/drifter/  ). 


### fMRI preprocessing 
  
Standard fMRI preprocessing steps were applied using the FSL software (  www.fmrib.ox.ac.uk  ) and custom MATLAB code (available at   https://version.aalto.fi/gitlab/BML/bramila/  ). Briefly, EPI images were corrected for head motion using MCFLIRT. 

Then they were coregistered to the Montreal Neurological Institute 152 2 mm template in a two-step registration procedure using FLIRT: from EPI to subject’s anatomical image after brain extraction (9 degrees of freedom) and from anatomical to standard template (12 degrees of freedom). Further, spatial smoothing was applied with a Gaussian kernel of 6 mm full width at half maximum. High pass temporal filter at a cut-off frequency of 0.01 Hz was used to remove scanner drift. To further control for motion and physiological artefacts, BOLD time series were cleaned using 24 motion-related regressors, signal from deep white matter, ventricles and cerebral spinal fluid locations (see ) for details, cerebral spinal fluid mask from SPM8 file csf.nii, white matter and ventricles masks from Harvard Oxford atlas included with FSL). As a measure of quality control we computed framewise displacement to quantify instantaneous head motion. Out of all the 120 runs (30 subjects, 4 sessions each), 97.5% of the runs (117 runs) had 90% of time points (640 volumes) with framewise displacement under the 0.5 mm threshold suggested in . For the remaining three runs, the number of time points under 0.5 mm were 639 (89.7%), 633 (88.9%), 489 (68.7%), i.e. only one session had a considerable amount of head motion. While head motion is a concern in connectivity studies as it can increase spurious BOLD time series correlations that are affected by the same amount of instantaneous head motion, with across-brain time series correlation, head motion is expected to reduce the SNR. However, to make sure that head motion similarity did not explain any group difference, we computed the same permutation test for the ISC also for average framewise displacement by estimating the similarity of two subjects as the distance between their average framewise displacement value. We found that similarity in average head motion was not different between the two viewing conditions (t-value = 0.255; p = 0.398 obtained with 5000 permutations). 


### Inter-subject correlation (ISC) analysis of brain activity during movie watching 
  
To investigate how similar the brain activity was across subjects in the different experimental conditions, we performed inter-subject correlation (ISC) using the isc-toolbox (  https://www.nitrc.org/projects/isc-toolbox/  ) . For each voxel the toolbox computes a similarity matrix between subject pairs and within same subject in all conditions, with the conditions being: (i) shared assumption that the movie’s sisters are genetically related, (ii) shared assumption that the younger sister was adopted, (iii) shared perspective of the to-be-organ-donor, and (iv) shared perspective of the to-be-organ-recipient. The total size of the similarity matrix is then 120 × 120 (4 conditions × 30 subjects) with each subject having two viewings for the genetic and two viewings for the non-genetic condition. The comparison between the conditions of the sisters to be perceived as either genetic sisters or non-genetic sisters results thus in a total of 1740 pairs per condition, as the similarity of BOLD time series during the two viewings (in either the genetic or the non-genetic condition) of each subject is compared with the two respective viewings of the other N-1 subjects. As the order of subjects does not matter, the final number of pairs in same conditions will be 2*2*(N-1)*N/2 = 1740 with N = 30. Each value of the correlation matrix is a result of the correlation between the BOLD time series of the pair of subjects considered for the selected voxel. We computed differences between experimental conditions by first transforming the correlation values into z-scores with the Fisher Z transform and then computing t-values and corresponding p-values using a permutation based approach . 

The Fisher-Z transformed correlations of the two perspectives were pooled for either the genetic or the non-genetic sisterhood. 

Correction for the multiple comparison was performed with Benjamini-Hochberg false discovery rate (BH-FDR) correction at a q &lt; 0.05, corresponding to a t-value threshold of 2.133. For visualization purposes, all results were also cluster corrected by removing any significant cluster smaller than 4 × 4 × 4 voxels. Summary tables were generated with an increased t-value threshold of 3. For the conjunction or “intersection–union test”  the p values of the ISC and GLM results are pulled together by considering the maximum p-value at each voxel. Then, multiple comparisons correction is performed with the Benjamini-Hochberg false discovery rate procedure with an FDR threshold equal to q &lt; 0.05. 

Unthresholded statistical parametric maps can be found in neurovault:   http://neurovault.org/collections/WGSQZWPH/  . 

#### Perspective taking 
  
In the movie-viewing experiment, in addition to having the subjects to watch the movie in the conditions of sisters related by birth or by adoption, we had altogether four runs, so that on two of the runs the subjects were asked to view the movie from the perspective of the sister who was expected to donate the organ, and on two of the runs from the perspective of the to-be-recipient sister. Thus, there was one run wherein the subjects viewed the movie from the perspective of the to-be- donor thinking that the sisters were genetic, one run wherein the subjects viewed the movie from the perspective of the to-be- donor thinking that the sisters were non-genetic, one run wherein the subjects viewed the movie from the perspective of the to-be- recipient thinking that the sisters were genetic, and one run wherein the subjects viewed the movie from the perspective of the to-be- recipient thinking that the sisters were non-genetic. 

As the results of this task open up a completely other aspect of the experiment with various results to discuss, which go beyond the scope and the space limitation of this article, they will be reported separately elsewhere. These conditions are mentioned here for reasons of describing the experimental procedures thoroughly so that it would be possible for others to replicate the study should they wish to do so. 



### General linear model analysis of the fMRI data acquired during the control task 
  
A moral dilemma decision task was performed by all subjects to localize regions involved in moral processing. The moral dilemma decision task was analyzed with a general linear model approach using the SPM12 software (  www.fil.ion.ucl.ac.uk/spm  ). To distinguish between moments of decision in the moral dilemma and the simple perception of the presentation, we created a temporal model of the occurrence of decision moments during the experiment. The decision regressor included time points from the revelation of the identity of involved individuals to the moment of decision indicated by button press. The activity during these time points was compared to the activity in all other time points of the task, including telling the background story of the moral dilemma in the presentation. Regressors were convolved with canonical hemodynamic response function to account for hemodynamic lag. From the preprocessed input data (see above) low-frequency signal drifts were removed by high-pass filtering (cutoff 128 s). First, individual contrast images were generated for the main effects of the regressors, then first level analyses were subjected to second-level analyses in MATLAB using one-sample   t  -test to test which brain areas showed significant activations in decision vs. no decision moments in a one-sample   t  -test over subjects. Statistical threshold was set at   p   &lt; 0.05 (cluster-corrected using the threshold free cluster enhancement approach implemented by FSL randomize with 5000 permutations). 


### Recording of eye-movements 
  
Eye movements were recorded during fMRI scanning from all subjects with an EyeLink 1000 eye tracker (SR Research, Mississauga, Ontario, Canada; sampling rate 1000 Hz, spatial accuracy better than 0.5°, with a 0.01° resolution in the pupil-tracking mode). Due to technical problems, 4 subjects had to be excluded from the final data analysis (with the rejection criteria of blinks maximum 10% of the duration of the scan and majority of blinks and saccades less than 1 second in duration). In addition, a part of recordings from some additional subjects had to de discarded due to the same criteria mentioned above, resulting in 61 recorded files with sufficient quality, with 35 files remaining in the genetic condition and 26 remaining files for the non-genetic condition. Prior to the experiment the eye tracker was calibrated once with a nine-point calibration. 

Saccade detection was performed using a velocity threshold of 30°/s and an acceleration threshold of 4000°/s2. Because the experiment was relatively long and no intermediate drift correction was performed, we retrospectively corrected the mean effect of the drift. We first calculated the mean of all fixation locations over the entire experiment for each subject, and then rigidly shifted the fixation distributions so that the mean fixation location coincided with the grand mean fixation location over all subjects. 


### Eye-movement analysis 
  
Subject-wise gaze fixation distributions were compared across the genetic vs. non-genetic conditions in the movie viewing task. Individual heat maps were generated by modelling each fixation as a Gaussian function using a Gaussian kernel with a standard deviation of 1degree of visual angle and a radius of 3 standard deviations. The heat maps were generated in time windows of 2 seconds corresponding to the TR used in the fMRI measurements. Spatial similarities between each pair of heat maps across the eye-tracking sessions were calculated using Pearson’s product-moment correlation coefficient (inter-subject correlation of eye gaze, eyeISC ). In the end a similarity matrix was obtained with correlations between each pair for each of the 712 time windows. 

First, the mean eISC scores over all 712 time windows were examined. These mean scores were acquired by extracting the mean of Fisher’s Z-transformed correlation scores and then transforming these mean values back to the correlation scale before the statistical analysis. The statistical significance of the group differences was analysed by contrasting pairs in which both subjects assumed a genetic relationship with pairs in which both subjects assumed the younger sister to be adopted. Non-parametric permutation tests with a total of 100000 permutations were used to avoid making assumptions about the data distribution. In this procedure the data were mixed randomly to change groupings and differences in the resulting new randomised groups were used to form an estimated distribution of the data. A comparison of how many of the permuted random partitions into groups build a more extreme group mean difference that the one observed with the original grouping yielded the final p-values. 


### Behavioral Measurements and Self-reports 
  
#### Valence and Arousal measurements 
  
The subjects self-reported emotions they had experienced during movie viewing. This was carried out after the fMRI experiment by viewing the movie again (Full procedures have been described in an earlier publication ). Two aspects of emotional experience were rated: emotional valence (positive-negative scale) and arousal which were acquired on separate runs. While watching the movie in the middle of the screen, the subjects could move a small cursor on the right side of the screen up and down on a scale using the computer mouse to report their current state of valence or arousal using a web tool   https://version.aalto.fi/gitlab/eglerean/dynamicannotations  . The self-ratings were collected at 5 Hz sampling rate. 


#### Behavioral questionnaires 
  
The subjects were asked after the first fMRI session five short freeform questions about their perception of the movie, specifically about how easy it was to take one or the other perspective, and whether they would have donated their kidney if in place of the movie protagonist. After the second fMRI session all subjects were debriefed by showing them the ending of the original movie, where it is revealed that the sick sister had wished for the healthy sister to refuse donating her kidney. Afterwards they were asked if seeing the real ending changed their opinion on the roles of the two movie protagonists. 

As an additional self-report measure, the subjects’ disposition for catching emotions from others was assessed with two emotional empathy questionnaires: Hatfield’s Emotional Contagion Scale  and the BIS/BAS scale . Every subject also filled in a questionnaire quantifying their social network , including their emotional closeness to their sister and best friend. The names of the sister and best friend were obtained from this questionnaire for the moral dilemma task. 



### Analysis of behavioral measurements 
  
#### Valence and arousal measurements 
  
To test whether dynamic valence and arousal were different between the genetic and non-genetic condition, we first computed inter-subject similarity matrices using valence and arousal rating time-series. These were compared against a similarity matrix for the experimental conditions of the viewing preceding the valence/arousal rating, i.e. the model tests for the case where individuals are more similar within the same condition (genetic or non-genetic), but dissimilar between conditions. Tests were performed using Mantel test with 5000 permutations. We also performed a test to see if subjects who were rating arousal and valence for the genetic condition had a stronger group similarity than subjects who rated arousal and valence for the non-genetic condition. Tests were performed using permutation based t-tests. As dynamic ratings can also be different in specific time points, we also performed a permutation-based t-test on valence and arousal values at each time point corrected for multiple comparisons across time. 


#### Heart rate and breathing rate analysis 
  
Differences between experimental conditions were computed in the same way as in the ISC analysis: Correlation values were first transformed into z-scores with the Fisher Z’s transform and then a permutation based approach was used to compute t-values and corresponding p-values . Correction for the multiple comparisons was performed with Benjamini-Hochberg false discovery rate (BH-FDR) correction at a q &lt; 0.05, corresponding to a t-value threshold of 2.133. 



### Behavioral measurements with a new group of subjects 
  
Subsequent to the fMRI experiments a new group of 30 subjects (all female, and having a sister, 18–33 years, mean age 25.5 years, right handed) were recruited for two further behavioral measurements. The subjects first performed an implicit association test (IAT). The IAT measures attitudes and beliefs that might not be consciously self-recognized by the subject or attitudes that the subjects are unwilling to report. By asking the subjects to sort, as quickly as possible, positively and negatively connoted words into categories, the IAT can measure the reaction times of the association process between the categories and the evaluations (e.g., good, bad). It has been shown in previous studies that making a response is easier and thus faster if the category is matching the implicit evaluation the subject bears in mind . In this study the two categories were “genetic sister” (sisko) and “adopted sister” (adoptiosisko). The two categories were paired in different randomized runs with positive or negative words, thus the experiment comprised separate runs asking the subjects to either match the positive words with the category “genetic sister” and negative words with the category “adopted sister” or vice versa to match positive words with the category “adopted sister” and negative words with the category “genetic sister”. The order in which the runs are presented counter-balanced across subjects and categories switched their localization on the screen in different runs to be on the left or right side of the screen to the same extent. Subjects were asked to press a key with either the right or the left hand and thus assign the evaluation word to one category on either the left or right hand side of the computer screen. With the experiment going on, the number of trials in this part of the IAT is increased in order to minimize the effects of practice. The IAT score is based on how long it takes a person to sort the words with the condition associating positive words and genetic (and negative and adopted) in contrast to negative words and genetic (and positive words and adopted). If an implicit preference exist for one of the categories subjects would be faster to match positive words to that category relative to the reverse. Data were analysed using Matlab. Similarity between subjects’ scores were examined TOST testing . As a second task, reaction times for the moral decision task were measured with the same group of subjects that underwent the IAT. As a difference to the decision task performed during fMRI scanning the order of the decisions was randomized (with easy decision including only strangers and difficult decisions including the sister on one side and the friend on the other). Reaction times were measured as the time between the onset of the slide revealing the identity of the involved individuals and the button press of the subject that related her decision. 


### Data availability 
  
The data that support the findings of this study are available on request from the corresponding author MBT. The data are not publicly available due to a prohibition by the Finnish law: Juridical restrictions set by the Finnish law prevent public access to the collected data, be it anonymized or non-anonymized, when data are recorded from human individuals. As the consent given by the subjects only applies to the specific study reported in our manuscript, no portion of the data collected could be used or released for use by third parties.</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (2)</summary><details class="inner-accordion"><summary>Table 1 (Tab1) - Clusters size, peak coordinates and t value of all clusters of Experiment 1 (movie watching task).</summary><div class="table-html"><table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Clusters size, peak coordinates and t value of all clusters of Experiment 1 (movie watching task).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Cluster Label: movie watching task</th><th colspan="1" rowspan="1">Cluster extent (voxels)</th><th colspan="1" rowspan="1">x MNI (mm)</th><th colspan="1" rowspan="1">y MNI (mm)</th><th colspan="1" rowspan="1">z MNI (mm)</th><th colspan="1" rowspan="1">Peak value (T-value)</th></tr></thead><tbody><tr><td colspan="6" rowspan="1">
<bold>clusters genetic &gt; non-genetic</bold>
</td></tr><tr><td colspan="1" rowspan="1">Superior parietal lobule (R)</td><td colspan="1" rowspan="1">8794</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">−86</td><td colspan="1" rowspan="1">44</td><td colspan="1" rowspan="1">14.6272</td></tr><tr><td colspan="1" rowspan="1">Ventromedial prefrontal cortex (R)</td><td colspan="1" rowspan="1">4099</td><td colspan="1" rowspan="1">0</td><td colspan="1" rowspan="1">54</td><td colspan="1" rowspan="1">−8</td><td colspan="1" rowspan="1">8.3312</td></tr><tr><td colspan="1" rowspan="1">Superior temporal gyrus (L)</td><td colspan="1" rowspan="1">2182</td><td colspan="1" rowspan="1">−42</td><td colspan="1" rowspan="1">−26</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">16.974</td></tr><tr><td colspan="1" rowspan="1">Superior temporal gyrus (R)</td><td colspan="1" rowspan="1">1138</td><td colspan="1" rowspan="1">62</td><td colspan="1" rowspan="1">−26</td><td colspan="1" rowspan="1">4</td><td colspan="1" rowspan="1">13.0493</td></tr><tr><td colspan="1" rowspan="1">Putamen (R)</td><td colspan="1" rowspan="1">616</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">16</td><td colspan="1" rowspan="1">−10</td><td colspan="1" rowspan="1">7.2211</td></tr><tr><td colspan="1" rowspan="1">Insula (L)</td><td colspan="1" rowspan="1">266</td><td colspan="1" rowspan="1">−40</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">−10</td><td colspan="1" rowspan="1">6.9642</td></tr><tr><td colspan="1" rowspan="1">Inferior temporal gyrus (R)</td><td colspan="1" rowspan="1">145</td><td colspan="1" rowspan="1">52</td><td colspan="1" rowspan="1">−46</td><td colspan="1" rowspan="1">−30</td><td colspan="1" rowspan="1">7.432</td></tr><tr><td colspan="1" rowspan="1">Middle temporal gyrus (R)</td><td colspan="1" rowspan="1">110</td><td colspan="1" rowspan="1">66</td><td colspan="1" rowspan="1">−4</td><td colspan="1" rowspan="1">−16</td><td colspan="1" rowspan="1">10.4489</td></tr><tr><td colspan="1" rowspan="1">Inferior frontal gyrus (L)</td><td colspan="1" rowspan="1">89</td><td colspan="1" rowspan="1">−36</td><td colspan="1" rowspan="1">34</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">5.3232</td></tr><tr><td colspan="1" rowspan="1">Superior frontal gyrus (R)</td><td colspan="1" rowspan="1">89</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">62</td><td colspan="1" rowspan="1">26</td><td colspan="1" rowspan="1">6.2544</td></tr><tr><td colspan="1" rowspan="1">Inferior parietal lobule (L)</td><td colspan="1" rowspan="1">86</td><td colspan="1" rowspan="1">−52</td><td colspan="1" rowspan="1">−60</td><td colspan="1" rowspan="1">48</td><td colspan="1" rowspan="1">5.0215</td></tr><tr><td colspan="1" rowspan="1">Postcentral gyrus (R)</td><td colspan="1" rowspan="1">81</td><td colspan="1" rowspan="1">56</td><td colspan="1" rowspan="1">−6</td><td colspan="1" rowspan="1">38</td><td colspan="1" rowspan="1">7.0252</td></tr><tr><td colspan="1" rowspan="1">Superior parietal lobule (L)</td><td colspan="1" rowspan="1">73</td><td colspan="1" rowspan="1">−14</td><td colspan="1" rowspan="1">−58</td><td colspan="1" rowspan="1">72</td><td colspan="1" rowspan="1">7.4174</td></tr><tr><td colspan="6" rowspan="1">
<bold>clusters non-genetic &gt; genetic</bold>
</td></tr><tr><td colspan="1" rowspan="1">Inferior occipital gyrus (R)</td><td colspan="1" rowspan="1">1938</td><td colspan="1" rowspan="1">40</td><td colspan="1" rowspan="1">−66</td><td colspan="1" rowspan="1">−8</td><td colspan="1" rowspan="1">−12.3679</td></tr><tr><td colspan="1" rowspan="1">Middle occipital gyrus (L)</td><td colspan="1" rowspan="1">604</td><td colspan="1" rowspan="1">−32</td><td colspan="1" rowspan="1">−84</td><td colspan="1" rowspan="1">4</td><td colspan="1" rowspan="1">−9.7755</td></tr><tr><td colspan="1" rowspan="1">Cerebellar crus II (R)</td><td colspan="1" rowspan="1">318</td><td colspan="1" rowspan="1">30</td><td colspan="1" rowspan="1">−86</td><td colspan="1" rowspan="1">−32</td><td colspan="1" rowspan="1">−7.3011</td></tr><tr><td colspan="1" rowspan="1">Temporal pole (middle) (R)</td><td colspan="1" rowspan="1">171</td><td colspan="1" rowspan="1">38</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">−34</td><td colspan="1" rowspan="1">−6.6407</td></tr><tr><td colspan="1" rowspan="1">Inferior temporal gyrus (L)</td><td colspan="1" rowspan="1">165</td><td colspan="1" rowspan="1">−54</td><td colspan="1" rowspan="1">−64</td><td colspan="1" rowspan="1">−4</td><td colspan="1" rowspan="1">−8.2858</td></tr><tr><td colspan="1" rowspan="1">Angular gyrus (L)</td><td colspan="1" rowspan="1">147</td><td colspan="1" rowspan="1">−42</td><td colspan="1" rowspan="1">−54</td><td colspan="1" rowspan="1">28</td><td colspan="1" rowspan="1">−5.5752</td></tr><tr><td colspan="1" rowspan="1">Superior temporal gyrus (R)</td><td colspan="1" rowspan="1">95</td><td colspan="1" rowspan="1">64</td><td colspan="1" rowspan="1">−28</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">−8.3381</td></tr><tr><td colspan="1" rowspan="1">Inferior frontal gyrus (R)</td><td colspan="1" rowspan="1">94</td><td colspan="1" rowspan="1">52</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">16</td><td colspan="1" rowspan="1">−5.7279</td></tr><tr><td colspan="1" rowspan="1">Superior temporal gyrus (R)</td><td colspan="1" rowspan="1">87</td><td colspan="1" rowspan="1">64</td><td colspan="1" rowspan="1">0</td><td colspan="1" rowspan="1">−4</td><td colspan="1" rowspan="1">−9.6944</td></tr><tr><td colspan="1" rowspan="1">Precentral gyrus (L)</td><td colspan="1" rowspan="1">86</td><td colspan="1" rowspan="1">−44</td><td colspan="1" rowspan="1">10</td><td colspan="1" rowspan="1">52</td><td colspan="1" rowspan="1">−6.1405</td></tr><tr><td colspan="1" rowspan="1">Superior frontal gyrus (L)</td><td colspan="1" rowspan="1">86</td><td colspan="1" rowspan="1">−8</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">60</td><td colspan="1" rowspan="1">−7.8522</td></tr></tbody></table></table-wrap>
</div></details><details class="inner-accordion"><summary>Table 2 (Tab2) - Clusters size, peak coordinates and t value of all clusters of and Experiment 2 (moral dilemma decision task).</summary><div class="table-html"><table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Clusters size, peak coordinates and t value of all clusters of and Experiment 2 (moral dilemma decision task).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Cluster Label: moral dilemma decision task</th><th colspan="1" rowspan="1">Cluster extent (voxels)</th><th colspan="1" rowspan="1">x MNI (mm)</th><th colspan="1" rowspan="1">y MNI (mm)</th><th colspan="1" rowspan="1">z MNI (mm)</th><th colspan="1" rowspan="1">Peak value (T-value)</th></tr></thead><tbody><tr><td colspan="1" rowspan="1">Cerebellar lobule VI (R)</td><td colspan="1" rowspan="1">10417</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">−54</td><td colspan="1" rowspan="1">−30</td><td colspan="1" rowspan="1">10.1921</td></tr><tr><td colspan="1" rowspan="1">Superior frontal gyrus (L)</td><td colspan="1" rowspan="1">3631</td><td colspan="1" rowspan="1">−8</td><td colspan="1" rowspan="1">10</td><td colspan="1" rowspan="1">48</td><td colspan="1" rowspan="1">8.4473</td></tr><tr><td colspan="1" rowspan="1">Precentral gyrus (L)</td><td colspan="1" rowspan="1">2476</td><td colspan="1" rowspan="1">−34</td><td colspan="1" rowspan="1">−18</td><td colspan="1" rowspan="1">58</td><td colspan="1" rowspan="1">7.2107</td></tr><tr><td colspan="1" rowspan="1">Inferior frontal gyrus (L)</td><td colspan="1" rowspan="1">1207</td><td colspan="1" rowspan="1">−52</td><td colspan="1" rowspan="1">12</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">6.3788</td></tr><tr><td colspan="1" rowspan="1">Superior parietal lobule (L)</td><td colspan="1" rowspan="1">815</td><td colspan="1" rowspan="1">−28</td><td colspan="1" rowspan="1">−56</td><td colspan="1" rowspan="1">48</td><td colspan="1" rowspan="1">6.0945</td></tr><tr><td colspan="1" rowspan="1">Amygdala (L)</td><td colspan="1" rowspan="1">720</td><td colspan="1" rowspan="1">−14</td><td colspan="1" rowspan="1">−12</td><td colspan="1" rowspan="1">−12</td><td colspan="1" rowspan="1">6.2453</td></tr><tr><td colspan="1" rowspan="1">Putamen (R)</td><td colspan="1" rowspan="1">257</td><td colspan="1" rowspan="1">24</td><td colspan="1" rowspan="1">10</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">5.4598</td></tr><tr><td colspan="1" rowspan="1">Superior parietal lobule (R)</td><td colspan="1" rowspan="1">199</td><td colspan="1" rowspan="1">26</td><td colspan="1" rowspan="1">−62</td><td colspan="1" rowspan="1">54</td><td colspan="1" rowspan="1">4.8181</td></tr><tr><td colspan="1" rowspan="1">Angular gyrus (L)</td><td colspan="1" rowspan="1">156</td><td colspan="1" rowspan="1">−48</td><td colspan="1" rowspan="1">−52</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">4.9727</td></tr><tr><td colspan="1" rowspan="1">Insula (R)</td><td colspan="1" rowspan="1">128</td><td colspan="1" rowspan="1">40</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">0</td><td colspan="1" rowspan="1">5.3664</td></tr><tr><td colspan="1" rowspan="1">Middle temporal gyrus (L)</td><td colspan="1" rowspan="1">123</td><td colspan="1" rowspan="1">−62</td><td colspan="1" rowspan="1">−24</td><td colspan="1" rowspan="1">−12</td><td colspan="1" rowspan="1">4.7632</td></tr><tr><td colspan="1" rowspan="1">Temporal pole (middle) (L)</td><td colspan="1" rowspan="1">77</td><td colspan="1" rowspan="1">−56</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">−24</td><td colspan="1" rowspan="1">5.4375</td></tr></tbody></table></table-wrap>
</div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 29330483</strong> | Pred included: 12 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29330483/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29330483_analysis_0</td><td>Female &gt; Male Pictures</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Viewing pictures of other peoples&#x27; faces/bodies and evaluating them engages receptive social-communication processes (facial/body perception and implicit communicative cues). The contrast (female&gt;male pictures) indexes differential social-communicative perception of actors.</td></tr>
<tr><td>29330483_analysis_1</td><td>Male &gt; Female Pictures</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Differential responses to pictured actors’ faces/bodies implicate social-communication processes (receptive facial/body information); the contrast indexes such processing.</td></tr>
<tr><td>29330483_analysis_2</td><td>Female &gt; Male Pictures</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Viewing and differentially responding to pictured actors involves receptive social-communication processes (faces/bodies), satisfying Social Communication criteria.</td></tr>
<tr><td>29330483_analysis_3</td><td>Male &gt; Female Pictures</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Differential neural responses to pictured actors imply processing of faces/bodies and other communicative cues—meeting Social Communication inclusion criteria.</td></tr>
<tr><td>29330483_analysis_4</td><td>Female &gt; Male Pictures</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Viewing actors’ faces/bodies and differentially responding (female vs male pictures) taps receptive social-communication processes (face/body perception).</td></tr>
<tr><td>29330483_analysis_5</td><td>Male &gt; Female Pictures</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Differential responses to pictured actors engage receptive social-communication processes (faces/bodies), so Social Communication applies.</td></tr>
<tr><td>29330483_analysis_6</td><td>Heterosexual Women: Female &gt; Male Videos</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Videos include vocalizations and facial/body motion; receptive social-communication (auditory and visual cues) is strongly engaged, matching Social Communication inclusion criteria.</td></tr>
<tr><td>29330483_analysis_7</td><td>Heterosexual Women: Male &gt; Female Videos</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Video stimuli include auditory cues and body motion that convey social-communicative information; the contrast indexes such receptive communication processes.</td></tr>
<tr><td>29330483_analysis_8</td><td>Bisexual Women: Female &gt; Male Videos</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Video stimuli contain vocalizations and facial/body movements that engage receptive social-communication processes; contrast meets Social Communication criteria.</td></tr>
<tr><td>29330483_analysis_9</td><td>Bisexual Women: Male &gt; Female Videos</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Differential responses to actors in videos involve receptive social-communication (auditory and visual cues), satisfying Social Communication inclusion criteria.</td></tr>
<tr><td>29330483_analysis_10</td><td>Homosexual Women: Female &gt; Male Videos</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Videos include vocal and facial/body cues that convey social-communicative information; the contrast indexes these receptive communication processes.</td></tr>
<tr><td>29330483_analysis_11</td><td>Homosexual Women: Male &gt; Female Videos</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Video-based auditory and visual social cues (vocalizations, facial/body motion) engage receptive social-communication processes; the analysis measures such processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 5766543)</summary><p><strong>Title:</strong> Neural Correlates of Sexual Orientation in Heterosexual, Bisexual, and Homosexual Women</p><details><summary>Abstract</summary><pre class="paper-text">We used fMRI to investigate neural correlates of responses to erotic pictures and videos in heterosexual (N = 26), bisexual (N = 26), and homosexual (N = 24) women, ages 25–50. We focused on the ventral striatum, an area of the brain associated with desire, extending previous findings from the sexual psychophysiology literature in which homosexual women had greater category specificity (relative to heterosexual and bisexual women) in their responses to male and female erotic stimuli. We found that homosexual women’s subjective and neural responses reflected greater bias towards female stimuli, compared with bisexual and heterosexual women, whose responses did not significantly differ. These patterns were also suggested by whole brain analyses, with homosexual women showing category-specific activations of greater extents in visual and auditory processing areas. Bisexual women tended to show more mixed patterns, with activations more responsive to female stimuli in sensory processing areas, and activations more responsive to male stimuli in areas associated with social cognition.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Studies using physiological measures have found that women tend to have non-specific patterns of genital arousal . That is, in contrast to men, women tend to show similar degrees of arousal to erotic stimuli depicting either sex. For example, heterosexual women have generally shown equivalent arousal to both erotic stimuli featuring men and erotic stimuli featuring women. This has been repeatedly demonstrated with vaginal photoplethysmography . This pattern has also been found using less direct measures such as looking time , pupil dilation , and fMRI . Notably, homosexual women’s arousal patterns are more category-specific than heterosexual women’s, although less so than men’s . 

The fact that women’s sexual arousal patterns are less category-specific than men’s has been interpreted as a potential contributor to gender differences in “erotic plasticity” , which Baumeister has defined as “the extent to which sex drive is shaped by social, cultural, and situational factors.” 

Baumeister offered three lines of evidence when he initially proposed that women may have greater erotic plasticity compared with men: (1) women show larger effects of social and cultural factors on sexual attitudes, desire, and behavior; (2) sexual attitude-behavior consistency is lower in women than in men; (3) individual women exhibit more variation in sexual behavior across time than men. Women’s less specific arousal patterns may also contribute to their increased “sexual fluidity” , which Diamond has defined as an individual’s “capacity for situation-dependent flexibility in sexual responsiveness, which allows individuals to experience changes in same-sex or other-sex desire across both short-term and long-term time periods” . 

One might hypothesize that arousal patterns of bisexual women should be similar to the non-specific arousal patterns of heterosexual women; however, studies of women’s arousal patterns have mostly neglected to include bisexual women. Heterosexual women’s arousal does not appear to favor erotic stimuli of either sex, and thus may be considered to reflect a bisexual pattern. (We do not mean to imply that heterosexual women are confused or in denial about their “real preferences”; rather, the findings in need of explanation are why heterosexual women show non-heterosexual arousal patterns in the laboratory). The implication of women’s non-specific arousal patterns for their sexual orientations is difficult to interpret. Most women, like most men, behave and identify heterosexually . However, men are more likely than women to identify as completely heterosexual or completely homosexual, and women are more likely than men to identify as bisexual or “mostly heterosexual” . 

If arousal patterns are similar between heterosexual and bisexual women, the question remains what distinguishes the two groups. One possibility, supported by some research, is that bisexual women tend to have greater sexual motivation, which may increase the likelihood of exploring a capacity for attraction to both sexes . Or, bisexual women may be more aware than heterosexual women of their non-specific arousal , which could partially contribute to bisexual sexual motivation. Alternatively, bisexual women may be more likely than heterosexual women to interpret their non-specific arousal states in sexual or romantic terms. 

It is also possible that bisexual women’s arousal patterns differ from those observed in heterosexual women. Consistent with this possibility, recent studies suggest that women with bisexual interests tend to be more aroused by female than by male erotic stimuli . Perhaps for some women with female-biased arousal patterns, this bias can motivate non-heterosexual feelings, behavior, and identity. 

Interpretations of non-specific arousal patterns in women are further complicated by the fact that female genital arousal exhibits relatively low correlations with subjectively reported sexual arousal, in contrast to the high correlations observed in men . Discrepancies between existing genital and subjective measures indicate that some women may report substantial subjective arousal without substantial genital arousal, and vice versa. It has also been suggested that non-specific arousal patterns may not indicate affective responses to erotic stimuli, but may instead reflect a kind of protective preparatory response . 

Neuroimaging assessments may shed light on the neural systems that are involved in responding to a given paradigm. Functional magnetic resonance imaging (fMRI) is a neuroimaging approach that allows for the indirect assessment of brain activity by tracking ratios of oxygenated and deoxygenated blood a proxy for neural firing. When used in the context of presenting erotic stimuli, this non-invasive neural measure could provide a converging line of evidence for interpreting the genital and subjective arousal findings described above. In this study, we used fMRI to specifically focused on the “reward system” in order to address the question: to what extent is there an affective significance to findings from the literature on women’s sexual orientation and genital arousal? 

The part of the “reward system” that we focused on is the ventral striatum, a dopamine-sensitive area of the brain that is a reliable measure of reward-related processing–and in particular, wanting and “incentive motivation” –including with respect to sexual orientation . Most neuroimaging studies of sexual response have focused on men , but the ventral striatum has also been found to reliably activate in studies of women’s responses to erotic stimuli . However, until now, no studies have measured neural responses to erotic stimuli in bisexual women. 

The present investigation primarily focused on two hypotheses: (1) Homosexual women may show greater category-specificity than non-homosexual women in brain activity, as suggested by the genital arousal literature; (2) Bisexual women may show larger biases towards female stimuli, compared with heterosexual women. We tested these hypotheses with respect to subjective and neural responses to erotic pictures and erotic videos. We used two different kinds of erotic stimuli because of their potentially non-overlapping strengths and weaknesses. Erotic pictures may be particularly well-suited for assessing the initial appraisal of sexual stimuli, but their brevity may not reflect the kinds of experiences that drive sexuality in the real world. Erotic videos may allow for the measurement of more intense states, but their extended duration may also provide opportunities for self-regulatory efforts to modify erotic responses. 


## Method 
  
### Participants 
  
Participants were 26 heterosexual women, 26 bisexual women, and 24 homosexual women, recruited from a variety of publicly-posted and online advertisements seeking (paid) volunteers for a neuroimaging study of sexual orientation and arousal. Bisexual women were required to have had at least two previous sexual partners and one romantic partner (of three months or greater duration) of each sex. Homosexual and heterosexual participants all met these criteria with respect to their respective preferred sexes. 

After responding to advertisements, participants were screened for inclusion using online questionnaires. Participants provided information about sexual orientation, sexual interests, and personality, in addition to answering screening questions relevant to medical eligibility for fMRI research. Participants were required to be right handed, non-claustrophobic, free from ferromagnetic implants, and not currently taking psychiatric medications. Participants were informed of the risks and nature of the study and agreed to participate in all portions of the research. This study was approved by the Institutional Review Board of Northwestern University and carried out in accordance with its guidelines. Informed consent was obtained from each participant for every portion of the study in which they participated. 

Participants’ sexual orientation was assessed using self-reported identities (i.e. “Homosexual”/“Gay”, “Bisexual”/“Bi”, “Heterosexual”/“Straight”), as well as with a modified Kinsey score, which asked participants about their sexual fantasies throughout adulthood as well as in the past year. The scale ranged from 0 to 6, with 0 corresponding to an exclusively heterosexual orientation and 6 corresponding to an exclusively homosexual orientation. Responses to questions about adulthood and about the past year were averaged to create a Kinsey score for each participant. The average Kinsey score was 0.8 for heterosexual women (  SD   = 0.7,   range   = 0–2), 2.63 for bisexual women (  SD   = 0.7,   range   = 2–4.5), and 5.2 for homosexual women (  SD   = 0.68,   range   = 4–6). 

Participants’ ages ranged from 21 to 46 years old. Mean ages were 29.7 for heterosexual women (  SD   = 5.86,   range   = 25–46), 30.27 for bisexual women (  SD   = 6.41,   range   = 21–48), and 29 for homosexual women (  SD   = 3.12,   range   = 25–38). The sample of 76 participants was racially and ethnically diverse, with 23 non-Caucasian participants including two Latina participants, ten African-American participants, four Asian-American participants, and seven participants who identified otherwise or who identified as multiethnic/multiracial. Groups did not significantly differ either with respect to age (  F  (2,73) = 0.348,   p   = 0.708) or ethnicity (c (2, N = 76) = 2.94, p = 0.23). We also confirmed that ethnicity did not significantly impact responses to the erotic stimuli. 


### Stimuli and Procedure 
  
Subjects experienced two experimental paradigms in the scanner: first erotic pictures were shown (over a period of ~21 minutes), and then erotic videos were shown (over a period of ~19 minutes) after a brief rest period. Picture stimuli were shown before video stimuli for all participants in an attempt to promote stimulus engagement. That is, it was assumed that potentially less intense stimuli might be better presented earlier in the experimental session while attentional resources are highest. Further, there was concern that first showing more intense stimuli would reduce engagement with subsequent stimuli. As such, pictures and videos stimuli were not counterbalanced with respect to each other. 

Participants watched stimuli while laying down with a combination of earplugs (to minimize scanner noise) and over-ear headphones (for video sound and communication with experimenters). Images were displayed via projector onto a wall, which was made viewable to participants via an angled mirror placed above the eyes. 

#### Erotic pictures paradigm 
  
The present study employed a subset of the picture stimuli used in Safron   et al  .  and Sylva   et al  . . Pictures depicted a nude man, a nude woman, or a same-sex couple (i.e., either two men or two women) engaged in explicit sexual contact. Erotic stimuli featuring both individual nudes and same-sex pairs engaging in explicit sexual interaction is common in research on sexual arousal and sexual orientation , which is not the case when stimuli featuring male-female couples is presented. However, erotic stimuli featuring explicit sexual activity in same-sex couples tends to be substantially more arousing compared with pictures of single nudes . Such stimuli are similar to pictures of nude individuals, in the sense that only men or women, but not both, are depicted in a given picture. Thus, sexual arousal induced by them is relatively unambiguous in terms of the gender to which participants are responding. 

In each of two 10.5-minute runs (ordering counterbalanced), participants viewed 40 erotic pictures featuring male models and 40 erotic pictures featuring female models. Each picture was shown for 3.5 seconds, followed by a variable-duration fixation cross presented for either 1.5, 6.5, or 11.5 seconds. Variable-duration baselines were utilized for superior deconvolution of the BOLD signal in a rapid event-related design for fMRI (in which evoked signals are never allowed to return to baseline levels). During the presentation of each picture, participants used buttons held in their right hands to rate that image on a scale of −2 to +2 (respectively: “strongly disliked,” “disliked,” “liked,” “strongly liked”), with no option of 0 for neutral ratings. Neutral options for ratings were not provided for the sake of consistency with previous research using the same stimuli. Note: Subjective ratings of pictures were lost for some participants due to a button-box equipment error. 


#### Erotic videos paradigm 
  
Following picture assessment, participants were shown six video clips depicting individual masturbating men and six video clips depicting individual masturbating women. Depicted individuals appeared sexually aroused but did not reach orgasm. To estimate baseline responses, six natural landscape videos were shown. 

In each of two 9.25-minute runs (ordering counterbalanced), videos were presented for 15 seconds each, followed by a 15-second distraction task requiring participants to indicate via button-press when a number in a series decreased by an interval other than seven. This task was intended to facilitate a return to emotional and physiological baseline. 15-second stimulus presentations were chosen as a desirable stimulation period in an fMRI block design, which can potentially be more sensitive than event-related designs . 

After leaving the scanner, participants viewed the videos once more and provided ratings of each clip. Videos were rated using a 5-point scale for degree of sexual appeal, ranging from “not at all” (0) to “very much” (4), with a midpoint of “somewhat” (2).’ 



### fMRI signal extraction methods 
  
#### Image acquisition 
  
A Siemens Trio 3 T magnet and 12-channel RF head coil were used to collect T2*-weighted gradient-recalled EPI images from the whole brain (32 3-mm slices with a 0.99-mm interslice gap; TR = 2500 ms; TE = 20 ms; flip angle = 80°; FOV = 200 × 220 mm, 120 × 128 acquisition matrix). Slices were taken along the plane connecting the anterior and posterior commissures, with a 1.72 mm × 1.72 mm × 3.99 mm resolution, with more refined axial dimensions intended to produce less distortion and signal dropout in sub-cortical areas, although possibly at the expense of signal-to-noise ratio. During each picture run, 250 whole-brain volumes were collected, and during each video run, 220 whole-brain volumes were collected, with the first four volumes discarded to account for initial magnetization effects. For anatomical localization, a structural MRI scan consisting of T1-weighted images was conducted after the testing runs (160 1-mm axial slices; TR = 2.1 ms; TE = 4.38 ms; flip angle = 15°; FOV = 220 mm; 256 × 192 matrix). 


#### Image pre-processing 
  
Image pre-processing and analysis was performed using SPM 12b (Wellcome Trust Centre for Neuroimaging, London, UK), and implemented in Matlab v 8.1.604 (The MathWorks Inc., MA, USA). 

Functional (EPI) volumes were first corrected for slice timing. Each participant’s volumes were then registered to the mean slice, after which the registered volumes were resliced, used to create a mean resliced image, and then co-registered to the structural (T1) image. All EPI images, including the mean resliced image, as well as the structural (T1) scans were then spatially normalized to Montreal Neurological Institute (MNI) space, and re-sampled to 3 × 3 × 3 mm (27 mm ) resolution. Normalized functional images were then smoothed to an 8 mm full-width-at-half-maximum Gaussian kernel. 


#### Signal to noise ratio and head coverage exclusions 
  
To exclude participants with poor signal due to either head motion or scanner conditions, average signal-to-noise ratio (SNR) over time was calculated for each subject (after preprocessing, using a mask that included only voxels with appreciable EPI signal). The SNR ratio for each voxel (mean divided by standard deviation) was averaged across all voxels in the brain . Participants whose picture data SNR was less than one standard deviation below the mean were excluded from picture analyses. Similarly, participants whose video data SNR was less than one standard deviation below the mean were excluded from video analyses. 

Based on these criteria, fourteen participants (five heterosexual, five bisexual, and four homosexual) were excluded from fMRI and subjective picture analyses, and sixteen participants (six heterosexual, six bisexual, and four homosexual) were excluded from fMRI and subjective video analyses. After exclusions were performed for SNR, we included a total of twenty-one heterosexual women, twenty-one bisexual women, and twenty homosexual women in fMRI picture analyses. Video analyses after SNR exclusion included eighteen heterosexual women, eighteen bisexual women, and twenty homosexual women. To check the validity of our SNR criterion, head motion plots were visually inspected for all participants (Parrish,   et al  . ). Excluded participants had highly variable head positions as compared to included participants. An additional validity-check was performed using evoked responses to erotic pictures minus a fixation-cross baseline. Excluded participants had substantially reduced activity in visual cortices as compared to included participants. 

An additional thirty-two participants (twelve heterosexual, twelve bisexual, and eight homosexual) were excluded from subjective picture rating analyses due to insufficient subjective data resulting from a button-box equipment error. Five participants (three bisexual and two homosexual) were excluded from subjective video analyses for the same reason. Thus, after exclusions were performed for insufficient subjective data, we included a total of nine heterosexual women, nine bisexual women, and twelve homosexual women in subjective picture analyses, and twenty heterosexual women, seventeen bisexual women, and eighteen homosexual women in subjective video analyses. 

For whole-brain analyses, mean functional scans were individually examined to identify participants with substantial cutoffs in head coverage. As a result, one heterosexual female who had substantial frontal lobe cutoff was excluded from whole-brain analyses in addition to those participants excluded for SNR. 


#### First-level analyses 
  
For both the video and picture assessments, a standard general linear model (GLM)  was used in identifying hemodynamic changes for each participant, and a high-pass filter (cutoff 128 s) was used to remove low-frequency temporal noise. 

Estimated average activity was calculated for each participant’s separate responses to male pictures, female pictures, male videos, and female videos (contrasted with fixation cross for pictures and neutral nature scenes for videos). These estimates were used for region of interest analyses. For whole-brain analyses, estimated average activity was also calculated for each participant’s response to male compared with female pictures and videos. For both the picture and video assessments, each participant’s responses to each stimulus contrast of interest were concatenated within stimulus type, using data from both the 1  and 2  runs. 

Ventral striatum region of interest analyses. An a priori region of interest (ROI) analysis was performed on the ventral striatum—centered on the nucleus accumbens—as this was the area most likely to indicate desire. The ventral striatum and hypothalamus are the only two areas that have been shown to be specifically associated with sexual (as opposed to general) arousal . We focused on the ventral striatum because it likely has higher validity for reflecting sexual incentive value compared with the hypothalamus, which contains a variety of nuclei with heterogeneous functions (including sexual arousal) that would be difficult to disambiguate with the limited spatial resolution of 3 T fMRI. 

The ventral striatum ROI mask used in the present study was drawn on an MNI template brain using the WFU PickAtlas toolbox for SPM 8 . It was anatomically defined as a dilated intersection of the ventral anterior caudate and putamen. The resulting ventral striatum ROI is shown in Fig.  .   
Mask used as the ventral striatum (VS) ROI, drawn using an average brain in the WFU PickAtlas toolbox for SPM 8. MNI coordinates displayed: x = 0, y = 17, z = −8. 
  

Estimates of average ventral striatum activity for each participant were extracted using the MarsBar toolbox for SPM8 . Extracted ventral striatum ROI data were analyzed using JMP Pro v11 (SAS Institute, Cary, NC). 



### Planned contrasts and within-group tests 
  
We constructed separate dependent variables for each combination of stimulus type (i.e. picture or video) and response type (i.e., subjective or ventral striatum activation) by subtracting response to female stimuli from response to male stimuli. That is, we constructed dependent variables for 1) subjective response to pictures, 2) subjective response to videos, 3) ventral striatum activation to pictures, and 4) ventral striatum responses to videos, each of which reflected responses to male stimuli minus responses to female stimuli. We refer to this as the Male-Female contrast. 

Because there were three groups (i.e., heterosexual, homosexual, and bisexual women), two orthogonal between-groups contrasts were constructed to examine what we believe to be the most interesting pair of independent questions based on previous literature . The first question was whether homosexual women differed from the other two groups in their Male-Female contrasts. The second question was whether bisexual women differed from heterosexual women in their Male-Female contrasts. The use of orthogonal planned contrasts allowed us to test these hypotheses with maximum statistical power while simultaneously minimizing the number of overall comparisons. 

Within-group t-tests were also performed separately in each group in order to characterize relative responding to male and female stimuli. 


### Whole-brain analyses 
  
Finally, we examined overall patterns of differential activation in response to male compared with female erotic stimuli across the entire brain. If bisexual and heterosexual women have less specific arousal patterns, then they are likely to exhibit less extensive differential activity between male and female stimuli compared with the activity patterns expected for homosexual women. 

Tests of average group responses to stimulus conditions were performed using one-sample contrasts. Each group (heterosexual women, bisexual women, and homosexual women) was tested individually for clusters of greater activity for male stimuli compared with female stimuli, and female stimuli compared with male stimuli, using a corrected statistical threshold (p &lt; 0.05 FWE). 

For these analyses, cluster reports were generated in SPM. Peak activations and cluster extents (extent threshold k = 5) were visually examined as overlays on slice and render maps. Neuroanatomical descriptions were determined based on agreement between two trained investigators, and checked against designations from the WFU Atlas (Maldjian   et al  ., 2003). 


### Data availability statement 
  
The datasets generated and analyzed during the current study are available from the corresponding author on request. 



## Results 
  
### Between-group planned contrasts 
  
As previously described, planned comparisons for the ventral striatum ROI were conducted via multiple regression using two orthogonal between-groups contrasts: one comparing homosexual women with heterosexual and bisexual women, and one comparing heterosexual with bisexual women. Separate analyses were conducted for each of the Male-Female contrasts (i.e., responses to female stimuli subtracted from responses to male stimuli). Results are presented in Table  .    
Planned contrasts comparing women of different orientation groups. 
  
*Significant p-value &lt; 0.05. **Signifcant p-value &lt; 0.01. 

***Significant p-value &lt; 0.001. 
    
Within-group male – female (male minus female) stimuli difference scores for subjective ratings and ventral striatum (VS) responses, by sexual orientation. Difference scores are defined as a participant’s average response to stimuli depicting males minus average response to stimuli depicting females. Points represent individual participants. Horizontal bars indicate group means and 95% confidence intervals of the means. Horizontal lines at 0 indicate no difference between ratings to erotic stimuli depicting each sex. (  a  ) Difference scores for subjective ratings of picture stimuli. (b) Difference scores for VS activation evoked by picture stimuli. (c) Difference scores for subjective ratings of video stimuli. (d) Difference scores for VS activation evoked by video stimuli. ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05. 
  

#### Homosexual versus non-homosexual women 
  
Subjective ratings. Compared with non-homosexual women, homosexual women had significantly more negative (i.e., gynephilic) Male-Female contrasts for both pictures (p = 0.015) and videos (p &lt; 0.001). That is, homosexual women showed a greater preference for pictures and videos of females relative to males, compared with both bisexual and heterosexual women. 

Ventral striatum activation patterns. Homosexual women had significantly more female-biased ventral striatum responses compared to non-homosexual women for pictures (p = 0.002), but not videos. 


#### Bisexual versus heterosexual women 
  
We compared heterosexual and bisexual women’s subjective and ventral striatum responses to erotic pictures and videos, and observed only one significant difference: for video stimuli, bisexual women had significantly more female-preferring subjective responses than did heterosexual women (p = 0.026). 



### Within-group tests comparing responses to male and female erotic stimuli 
  
Figure   (showing the distribution of Male-Female contrasts for the three groups) shows that heterosexual women exhibited a non-significant trend (p = 0.079) towards favoring female erotic pictures compared with male erotic pictures, and had no differentiation between stimulus sex for other tests. Bisexual women also did not subjectively differentiate among stimulus types based on sex, although they did exhibit (non-significant) marginal female-favoring ventral striatum scores for picture (p = 0.063) and video (p = 0.054) stimuli. Homosexual women, in contrast to heterosexual and bisexual women, showed clear favoring of female stimuli as assessed by subjective liking of pictures (p &lt; 0.001), appeal ratings of videos (p &lt; 0.001), as well as in ventral striatum responses to pictures (p = 0.003) and non-significantly for ventral striatum responses to videos (p = 0.073). Note that these results are presented descriptively. Inferences about differences among the three groups depend on the tests presented in Table  . 


### Whole brain tests comparing responses to male and female erotic stimuli 
  
Note: Activation patterns are described in greater detail in the discussion, with interpretations of possible functional significances. 

#### Picture stimuli 
  
Comparing activation to female versus male erotic pictures, heterosexual women exhibited relatively greater activity for female pictures in occipital (i.e., visual) and occipitotemporal cortices, with no brain areas showing significantly greater activation for male pictures (Fig.  ; Table  ). Bisexual women also showed greater activity in visual cortices for female relative to male pictures, but they showed greater activity for male pictures in other areas including supramarginal and angular gyri, as well as the posterior cingulate. Homosexual women exhibited significant activations for female compared with male pictures in visual cortex, parietal lobes, and parahippocampal cortex, but with no brain areas showing significantly greater activation for male pictures.   
Differential brain activations towards male and female pictures in heterosexual, bisexual, and homosexual women. Whole brain activations are shown for the male picture minus female picture contrasts (with brain activation evoked by viewing neutral stimuli subtracted from activations toward the erotic pictures). Height threshold is set at p &lt; 0.05 FWE with a cluster threshold of k = 5. Axial slice 31, sagittal slice 50, and coronal slice 38 are shown for all groups. 
    
Differential whole-brain activations in response to male vs. female pictures. 
  


#### Video stimuli 
  
When viewing female compared with male erotic videos (Fig.  ; Table  ), all groups showed activity in bilateral superior temporal cortices, likely indicating an auditory confound in which more extensive and substantial vocalizations were present in female erotic videos . However, this effect appeared to vary by sexual orientation, with homosexual women showing the most extensive and robust evoked activity (peak T = 14.22) compared with heterosexual (peak T = 11.71) and bisexual women (peak T = 8.83). In the opposite direction of greater responses to male compared with female erotic videos, heterosexual and bisexual (but not homosexual) women exhibited activations in occipital cortices. While all groups had greater activity towards male videos in (anterior) superior parietal cortices, these activations appeared to be more extensive and robust in bisexual women (peak T = 11.55) compared with heterosexual (peak T = 7.64) and homosexual women (peak T = 7.99).   
Differential brain activations between male and female videos in heterosexual, bisexual, and homosexual women. Whole brain activations are shown for the male video minus female video contrasts (with brain activation evoked by viewing neutral stimuli subtracted from activation toward the erotic videos). Height threshold is set at p &lt; 0.05 FWE with a cluster threshold of k = 5. Axial slice 37, sagittal slice 61, and coronal slice 38 are shown for all groups. 
    
Differential whole-brain activations in response to male vs. female videos. 
  




## Discussion 
  
In this fMRI study of female sexual orientation—the first to include bisexual women—we extended several key findings from the sexual psychophysiology literature . Using the ventral striatum as a neural measure of incentive motivation, we demonstrated that homosexual women have greater gender bias in their responses to male and female erotic stimuli. 

### Main findings: subjective and ventral striatum responses to male and female erotic stimuli 
  
Direct comparisons of bisexual and heterosexual women revealed no significant differences, with the exception of bisexual women having more gynephilic subjective responses to erotic videos. However, bisexual and heterosexual women did not differ with respect to their ventral striatum responses toward these stimuli. When contrasted to bisexual and heterosexual women, homosexual women showed distinctly greater bias toward female stimuli in both their subjective responses to videos and pictures, and also in their ventral striatum responses to pictures. In sum, our planned contrast findings are consistent with the genital arousal literature in which more category-specific responses were observed in homosexual women . 

Another set of tests, comparing male vs. female stimuli within each group, revealed that neither bisexual nor heterosexual women were significantly biased toward stimuli depicting males or stimuli depicting females. This was true both in ventral striatum response and in subjective arousal, for both picture and video stimuli. Homosexual women, however, were uniquely gynephilic (i.e., female-preferring), with significantly greater responses to female stimuli for subjective responses to pictures, subjective responses to videos, and ventral striatum responses to pictures. This gynephilic bias in homosexual women was consistent with our direct comparisons and previous literature. 

Our findings are only partially consistent with observations from the genital arousal literature in which homosexual and bisexual women both had gynephilic responses, but where heterosexual women had non-specific responses . We found significant biases in ventral striatum responses toward female stimuli among homosexual women, but with more indifferent patterns among heterosexual and bisexual women. However, with one exception—bisexual women showing more gynephilic subjective responses to erotic videos than did heterosexual women, in a direct comparison—heterosexual and bisexual women’s patterns of results did not differ significantly. 

Our a priori tests in the ventral striatum allowed us to explore whether women of different sexual orientations also exhibited different degrees of incentive motivation toward male and female erotic stimuli. But fMRI also provides the ability to look at activation patterns across the entire brain, potentially allowing for a more detailed characterization of the neural systems involved. Below we review activation patterns for each group in viewing male compared with female erotic stimuli, along with some reverse inferences as to their functional significance. 


### Whole brain responses to erotic pictures 
  
For heterosexual women viewing erotic pictures, activity was greater for female relative to male stimuli bilaterally in lateral occipital cortices, likely indicative of visual attention , as well as in right-lateralized fusiform cortex, potentially suggesting face or body processing .   In no brain areas did heterosexual women have significantly greater activation for male relative to female erotic pictures  . Rather, they seemed to have a somewhat gynephilic pattern of visual attention, consistent with results from eye-tracking and looking-time studies in which heterosexual women attended to erotic characteristics of female pictures . 

Bisexual women showed more activity in response to female (relative to male) erotic pictures throughout the visual system, including fusiform cortex, which (as described above) is often associated with perception of faces and bodies . Patterns were similar to those observed in heterosexual women (and presumably with similar functional significances), but with larger spatial extents of activation. Although bisexual and heterosexual women were not directly contrasted, this more extensive visual activation could be taken as support for somewhat greater gynephilic interest on the part of bisexual women, consistent with ventral striatum activation patterns. 

For bisexual women viewing erotic pictures, activity was greater for male relative to female stimuli in posterior midcingulate and right retrosplenial cingulate cortices, potentially suggesting greater perceptual salience and emotional memory for male erotic stimuli . Additional male-biased activations were identified bilaterally in supramarginal and angular gyri, indicating processes relating to mental imagery, or possibly mentalizing . 

Thus, in contrast to heterosexual participants, bisexual women showed greater activity towards male (relative to female) erotic pictures in affect-related brain areas. In this way, it seems that it would be overly simplistic to say that bisexual women are similar to heterosexual women, but with the addition of gynephilic interest. Rather,   bisexual women seem to have greater responses to both male and female erotic stimuli, depending on the brain area being considered  . Patterns of greater overall responsiveness are consistent with suggestions that bisexual women may be distinguished by having overall greater degrees of sexual motivation relative to heterosexual women . 

It is also notable that bisexual women uniquely showed greater activations to male stimuli in areas of the brain implicated in higher-order cognition, including mentalizing. Speculatively, these activations could be related to more complex processing of sexual motivation in bisexual women . To the degree that these activation patterns in bisexual women actually specifically reflect social cognition, the question remains open as to why this may be more likely to be observed in bisexual but not heterosexual or homosexual women. 

For homosexual women viewing erotic pictures, greater activations for female (relative to male) stimuli extended throughout the visual system, with additional clusters in occipitotemporal cortices. Clusters in the right inferior precuneus may have indicated mental imagery , and clusters in posterior parahippocampal cortex may have indicated either memory encoding or retrieval .   For homosexual women, no brain areas had significantly greater activation for male relative to female erotic pictures. Thus, homosexual women were the only group that exhibited an overall pattern of differential brain activity (between male and female sexual stimuli) greater only for pictures depicting their preferred gender  . 


### Whole brain responses to erotic videos 
  
For heterosexual women viewing erotic videos, activity was greater for female relative to male stimuli in bilateral superior temporal cortices likely indicating an auditory confound deriving from more extensive and substantial vocalizations being present in female erotic videos . Activity was greater for male relative to female videos in posterior occipital cortex, likely indicating enhanced visual attention . Further clusters (greater for male compared with female videos) in the inferolateral postcentral gyrus and parietal somatosensory association areas may have indicated awareness of bodily sensations, possibly related to sexual imagery . 

For bisexual women viewing erotic videos, activations were greater for female (relative to male) stimuli in superior temporal cortices, likely indicating the same auditory-related activity present in heterosexual women. Bisexual women’s brain activity was greater for male (relative to female) erotic videos in occipital cortex, likely indicating visual attention . Male-biased activations in somatosensory cortices may have indicated processing of bodily sensations , and further activations in bilateral superior parietal lobules, premotor and supplementary motor cortices, and right supramarginal gyrus may have indicated mental imagery or possibly mirroring with the actors shown in the videos . 

Similar to the findings for erotic pictures,   bisexual women were unique in the degree to which male videos produced activations in brain areas associated with more abstract (and possibly complex) processing  . Again, the significance of this pattern remains unclear. 

For homosexual women viewing female relative to male erotic videos, activity in superior temporal cortices likely indicated the same auditory-related processing observed in heterosexual and bisexual women, albeit more robustly and extensively, consistent with enhanced attention to emotionally salient stimulus features. When viewing male relative to female erotic videos, activations in the right somatosensory cortex may have indicated processing of bodily sensations , which may have been either positive or negative in valence. Thus, while emotionally associated brain areas did not exhibit differential activations for videos,   homosexual women’s particularly strong engagement of auditory cortices for female stimuli provided yet further evidence of uniquely gender-biased responding, relative to heterosexual and bisexual women  . 


### Comparisons with previous findings 
  
Few studies have investigated the category-specificity of brain activity in non-heterosexual women. Ponseti   et al  .  found that both heterosexual and homosexual women showed gender-specific patterns of brain activity in multiple areas, including the ventral striatum. Sylva   et al  .  also found some evidence for category-specific responding in women, although not in the ventral striatum, and without specifically testing whether or not heterosexual or homosexual women differed in their responses. 

The patterns observed Ponseti   et al  .  stand in contrast to the present investigation in which homosexual women tended to be the only group showing strongly category-specific responses to erotic stimuli. One possible interpretations for their findings of category-specific responses in all women was the unusual nature of the stimuli (i.e., close-up images of male and female genitalia, isolated from interpersonal contextual factors) . As suggested by Chivers (2017) , it may be the case that sex and gender cues can produce specific responses in heterosexual women, but that these are usually trumped by contextual factors in driving arousal responses in women. The stimuli utilized in the present study contained contextual factors (e.g. body posture, facial expression) that are more typical of those found in the genital arousal literature. 

However, it should be noted that the present study did not find support for greater category-specificity in homosexual women across all stimulus conditions. Rather, planned contrasts in the ventral striatum only revealed significant group differences between homosexual and non-homosexual women for erotic pictures. There were no significant differences in ventral striatum response between homosexual and non-homosexual women for erotic videos (even though subjective evaluations of those stimuli did significantly differ across the groups). 

This pattern of differing results for pictures versus videos may be related to differences in how individuals respond to these stimuli, limitations of our video paradigm, or both. While erotic videos may theoretically allow for the assessment of qualitatively different states of sexual response, it may be the case that incentive motivation is greatest when stimuli are first presented, but then diminishes with longer stimulus presentations . Additionally, erotic pictures may have been more effective at driving ventral striatum responses due to factors such as unpredictably varied presentation times of preferred stimuli contributing to larger magnitude reward-prediction errors . 


### Limitations 
  
One limitation of nearly all studies of erotic responses in women—including this one—is a failure to control for hormonal conditions or contraceptive usage. By default, it can generally be assumed that most women were not measured within the ovulatory window, when responses to erotic stimuli might be greatest . Additionally, a number of women may have been using hormonal contraceptives. Measuring women’s responses outside of the fertile phase of their cycles—or while they were using hormonal contraceptives —may have yielded a restricted range of arousal responses. However, the specificity of erotic responding has not been shown to be influenced by menstrual cycle in previous studies of genital arousal . 

Another source of potential limitations may have been the nature of the stimuli used. Though our stimuli were pilot-tested and rated by individuals of different sexual orientations in order to confirm that they would appeal to a broad participant sample, it is never possible to ensure that common stimuli will evoke the responses intended. This may be especially true for something as emotionally salient and individual as sexual arousal. Thus, it is possible that category specificity patterns could appear to be different if stimuli better reflected participants’ subjective preferences. This is a limitation of many studies of sexual responding, although data gleaned from more individualized stimulus sets are difficult to interpret. 

One more aspect of the stimuli that is difficult to control for is sensory details that are inherently different between male and female stimuli. Differences in actors’ vocalizations (for videos), actors’ body positions (for both videos and pictures), and actors’ body motions (in the videos) were present (on average) between male and female stimuli. These features are difficult to control for and could conceivably lead to differences in both subjective and neural responding when viewing male vs. female stimuli, especially in more primary sensory areas of the brain such as visual and auditory cortices. However, such differences may also serve to reinforce the gendered nature of the stimuli and improve their correspondence with real-world experiences and real-world arousal. 



## Conclusions 
  
Though the neural data presented here align with previously-observed patterns in women’s genital and subjective arousal, much remains unknown about the relationship between arousal patterns, orientation, and the development of sexual motivation towards particular sexes in women. Our study supports past findings indicating that women tend not to have strongly category-specific responses to erotic stimuli, with homosexual women showing somewhat greater specificity than heterosexual and bisexual women. Future research should explore the extent to which women’s non-specific sexual response contributes to erotic plasticity (i.e., change with context) and sexual fluidity (i.e., change over time) .</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (2)</summary><details class="inner-accordion"><summary>Table 2 (Tab2) - Differential whole-brain activations in response to male vs. female pictures.</summary><div class="table-html"><table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Differential whole-brain activations in response to male vs. female pictures.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">R/L</th><th colspan="1" rowspan="1">Region</th><th colspan="1" rowspan="1">BA</th><th colspan="1" rowspan="1">MNI</th><th colspan="1" rowspan="1">voxels</th><th colspan="1" rowspan="1">peak T</th></tr></thead><tbody><tr><td colspan="6" rowspan="1">
<italic toggle="yes">Heterosexual Women</italic>
</td></tr><tr><td colspan="6" rowspan="1">    Female &gt; Male Pictures</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">lingual gyrus, inferolateral occipital</td><td colspan="1" rowspan="1">18, 19</td><td colspan="1" rowspan="1">(−33 −76 −10)</td><td colspan="1" rowspan="1">21</td><td colspan="1" rowspan="1">8.01</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">fusiform gyrus</td><td colspan="1" rowspan="1">37</td><td colspan="1" rowspan="1">(30 −67 −7)</td><td colspan="1" rowspan="1">26</td><td colspan="1" rowspan="1">7.81</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">inferolateral occipital, fusiform gyrus</td><td colspan="1" rowspan="1">19, 37</td><td colspan="1" rowspan="1">(42 −70 −16)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.78</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">fusiform gyrus</td><td colspan="1" rowspan="1">37</td><td colspan="1" rowspan="1">(30 −58 −10)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.59</td></tr><tr><td colspan="6" rowspan="1">    Male &gt; Female Pictures: no differential activations</td></tr><tr><td colspan="6" rowspan="1">
<italic toggle="yes">Bisexual Women</italic>
</td></tr><tr><td colspan="6" rowspan="1">    Female &gt; Male Pictures</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">inferior occipital cortex, middle occipital gyrus, lingual gyrus</td><td colspan="1" rowspan="1">18, 19</td><td colspan="1" rowspan="1">(36 −70 −10)</td><td colspan="1" rowspan="1">86</td><td colspan="1" rowspan="1">10.68</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(48 −64 −10)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.96</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">fusiform gyrus</td><td colspan="1" rowspan="1">37</td><td colspan="1" rowspan="1">(33 −55 −13)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.15</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">inferolateral occipital, middle occipital gyrus</td><td colspan="1" rowspan="1">19, 18</td><td colspan="1" rowspan="1">(−39 −85 −7)</td><td colspan="1" rowspan="1">138</td><td colspan="1" rowspan="1">9.07</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">18, 19</td><td colspan="1" rowspan="1">(−30 −88 5)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.47</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">inferior occipital gyrus, primary visual cortex, lingual gyrus</td><td colspan="1" rowspan="1">18, 17</td><td colspan="1" rowspan="1">(−24 −94 −4)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.06</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">occipital cortex, middle occipital gyrus</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(30 −79 17)</td><td colspan="1" rowspan="1">43</td><td colspan="1" rowspan="1">9.03</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">occipital cortex, middle occipital gyrus</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(30 −82 26)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.38</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">inferior occipital cortex, lingual gyrus</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">(−24 −85 −13)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">8.22</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">primary visual cortex, cuneus</td><td colspan="1" rowspan="1">17</td><td colspan="1" rowspan="1">(15 −100 −4)</td><td colspan="1" rowspan="1">12</td><td colspan="1" rowspan="1">8.09</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">(42 −82 −1)</td><td colspan="1" rowspan="1">27</td><td colspan="1" rowspan="1">8.05</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">(27 −85 2)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.62</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">(33 −91 8)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">6.86</td></tr><tr><td colspan="2" rowspan="1">    Male &gt; Female Pictures</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1" /></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">angular gyrus, supramarginal gyrus</td><td colspan="1" rowspan="1">39, 40</td><td colspan="1" rowspan="1">(−51 −64 41)</td><td colspan="1" rowspan="1">21</td><td colspan="1" rowspan="1">9.37</td></tr><tr><td colspan="1" rowspan="1">      R/L</td><td colspan="1" rowspan="1">posterior cingulate</td><td colspan="1" rowspan="1">23</td><td colspan="1" rowspan="1">(0 −22 35)</td><td colspan="1" rowspan="1">10</td><td colspan="1" rowspan="1">8.23</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">angular gyrus</td><td colspan="1" rowspan="1">39</td><td colspan="1" rowspan="1">(48 −61 35)</td><td colspan="1" rowspan="1">27</td><td colspan="1" rowspan="1">7.94</td></tr><tr><td colspan="1" rowspan="1">       R</td><td colspan="1" rowspan="1">supramarginal gyrus</td><td colspan="1" rowspan="1">40</td><td colspan="1" rowspan="1">(51 −61 44)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.75</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">retrosplenial cingulate</td><td colspan="1" rowspan="1">30</td><td colspan="1" rowspan="1">(15 −52 29)</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">7.68</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">supramarginal gyrus</td><td colspan="1" rowspan="1">40</td><td colspan="1" rowspan="1">(57 −49 35)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">7.38</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">supramarginal gyrus</td><td colspan="1" rowspan="1">40</td><td colspan="1" rowspan="1">(−36 −61 44)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">7.08</td></tr><tr><td colspan="6" rowspan="1">
<italic toggle="yes">Homosexual Women</italic>
</td></tr><tr><td colspan="6" rowspan="1">    Female &gt; Male Pictures</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(−27 −82 14)</td><td colspan="1" rowspan="1">139</td><td colspan="1" rowspan="1">10.01</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">18, 19</td><td colspan="1" rowspan="1">(−33 −88 −1)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.99</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">middle occipital gyrus, primary visual cortex</td><td colspan="1" rowspan="1">18, 17</td><td colspan="1" rowspan="1">(−18 −97 2)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.57</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(33 −79 14)</td><td colspan="1" rowspan="1">89</td><td colspan="1" rowspan="1">9.83</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">inferior lateral occipital cortex</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(42 −85 2)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.19</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(33 −70 8)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.78</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">fusiform gyrus, posterior paraphippocampal gyrus</td><td colspan="1" rowspan="1">37</td><td colspan="1" rowspan="1">(33 −52 −10)</td><td colspan="1" rowspan="1">36</td><td colspan="1" rowspan="1">9.56</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">fusiform gyrus, middle occipital gyrus</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">(33 −70 −13)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.03</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">primary visual cortex</td><td colspan="1" rowspan="1">17</td><td colspan="1" rowspan="1">(18 −94 −1)</td><td colspan="1" rowspan="1">11</td><td colspan="1" rowspan="1">8.56</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">fusiform gyrus, lingual gyrus, posterior parahippocampal gyrus</td><td colspan="1" rowspan="1">37</td><td colspan="1" rowspan="1">(−33 −61 −7)</td><td colspan="1" rowspan="1">26</td><td colspan="1" rowspan="1">8.45</td></tr><tr><td colspan="1" rowspan="1">       L</td><td colspan="1" rowspan="1">fusiform gyrus</td><td colspan="1" rowspan="1">37</td><td colspan="1" rowspan="1">(−39 −61 −13)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.62</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">precuneus, occipitoparietal</td><td colspan="1" rowspan="1">7</td><td colspan="1" rowspan="1">(24 −73 41)</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">7.18</td></tr><tr><td colspan="6" rowspan="1">    Male &gt; Female Pictures: no differential activations</td></tr></tbody></table></table-wrap></div></details><details class="inner-accordion"><summary>Table 3 (Tab3) - Differential whole-brain activations in response to male vs. female videos.</summary><div class="table-html"><table-wrap id="Tab3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Differential whole-brain activations in response to male vs. female videos.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">R/L</th><th colspan="1" rowspan="1">Region</th><th colspan="1" rowspan="1">BA</th><th colspan="1" rowspan="1">MNI</th><th colspan="1" rowspan="1">voxels</th><th colspan="1" rowspan="1">peak T</th></tr></thead><tbody><tr><td colspan="6" rowspan="1">
<italic toggle="yes">Heterosexual Women</italic>
</td></tr><tr><td colspan="6" rowspan="1">    Female &gt; Male Videos</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior temporal gyrus, primary and secondary auditory cortex</td><td colspan="1" rowspan="1">22, 41, 42</td><td colspan="1" rowspan="1">(−57 −25 8)</td><td colspan="1" rowspan="1">124</td><td colspan="1" rowspan="1">11.71</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior temporal gyrus, primary and secondary auditory cortex</td><td colspan="1" rowspan="1">22, 41, 42</td><td colspan="1" rowspan="1">(−42 −31 11)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">10.15</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior temporal gyrus</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">(−66 −28 8)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.99</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">superior temporal gyrus, primary and secondary auditory cortex</td><td colspan="1" rowspan="1">22, 41, 42</td><td colspan="1" rowspan="1">(54 −16 −1)</td><td colspan="1" rowspan="1">48</td><td colspan="1" rowspan="1">10.42</td></tr><tr><td colspan="6" rowspan="1">    Male &gt; Female Videos</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">primary visual cortex, lingual gyrus</td><td colspan="1" rowspan="1">17, 18</td><td colspan="1" rowspan="1">(−9 −82 2)</td><td colspan="1" rowspan="1">30</td><td colspan="1" rowspan="1">10.14</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">inferolateral postcentral gyrus</td><td colspan="1" rowspan="1">3</td><td colspan="1" rowspan="1">(60 −16 29)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">7.94</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">superior parietal lobule</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">(33 −49 62)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">7.64</td></tr><tr><td colspan="6" rowspan="1">
<italic toggle="yes">Bisexual Women</italic>
</td></tr><tr><td colspan="6" rowspan="1">    Female &gt; Male Videos</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior temporal, primary and secondary auditory cortex</td><td colspan="1" rowspan="1">22, 41, 42</td><td colspan="1" rowspan="1">(−54 −10 5)</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">8.83</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">superior temporal gyrus</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">(63 −25 2)</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">7.26</td></tr><tr><td colspan="6" rowspan="1">    Male &gt; Female Videos</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">superior parietal lobule, extending into supramarginal gyrus</td><td colspan="1" rowspan="1">7, 5, 40</td><td colspan="1" rowspan="1">(36 −46 59)</td><td colspan="1" rowspan="1">211</td><td colspan="1" rowspan="1">11.55</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">postcentral gyrus</td><td colspan="1" rowspan="1">3, 1, 2</td><td colspan="1" rowspan="1">(36 −37 53)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">11.27</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">postcentral sulcus</td><td colspan="1" rowspan="1">1, 2</td><td colspan="1" rowspan="1">(33 −31 41)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">9.51</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">postcentral gyrus, postcentral sulcus</td><td colspan="1" rowspan="1">3, 1, 2</td><td colspan="1" rowspan="1">(−36 −37 56)</td><td colspan="1" rowspan="1">25</td><td colspan="1" rowspan="1">8.45</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior parietal lobule</td><td colspan="1" rowspan="1">5, 7</td><td colspan="1" rowspan="1">(−33 −49 59)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.55</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">supplementary motor area</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">(30 −10 59)</td><td colspan="1" rowspan="1">12</td><td colspan="1" rowspan="1">8.14</td></tr><tr><td colspan="1" rowspan="1">       L</td><td colspan="1" rowspan="1">supplementary motor area</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">(−24 −13 62)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">7.73</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">cuneus</td><td colspan="1" rowspan="1">17</td><td colspan="1" rowspan="1">(−9 −82 2)</td><td colspan="1" rowspan="1">9</td><td colspan="1" rowspan="1">7.6</td></tr><tr><td colspan="6" rowspan="1">
<italic toggle="yes">Homosexual Women</italic>
</td></tr><tr><td colspan="6" rowspan="1">    Female &gt; Male Videos</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">superior temporal gyrus</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">(51 −10 2)</td><td colspan="1" rowspan="1">578</td><td colspan="1" rowspan="1">14.22</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">primary and secondary auditory cortex, superior temporal gyrus</td><td colspan="1" rowspan="1">41 42, 22</td><td colspan="1" rowspan="1">(54 −19 5)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">13.91</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">superior temporal gyrus</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">(60 −1 −4)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">13.26</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior temporal gyrus</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">(−63 −19 2)</td><td colspan="1" rowspan="1">247</td><td colspan="1" rowspan="1">12.7</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">primary and secondary auditory cortex</td><td colspan="1" rowspan="1">41, 42</td><td colspan="1" rowspan="1">(−45 −25 8)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">11.97</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior temporal gyrus</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">(−66 −28 5)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">10.99</td></tr><tr><td colspan="6" rowspan="1">     Male &gt; Female Videos</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">postcentral gyrus</td><td colspan="1" rowspan="1">3</td><td colspan="1" rowspan="1">(33 −34 47)</td><td colspan="1" rowspan="1">7</td><td colspan="1" rowspan="1">7.99</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">postcentral sulcus</td><td colspan="1" rowspan="1">2</td><td colspan="1" rowspan="1">(42 −28 41)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">7.76</td></tr></tbody></table></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 29890323</strong> | Pred included: 6 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29890323/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29890323_analysis_0</td><td>emotional &gt; neutral video clips</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants decode communicative cues (spoken autobiographical narratives, facial/vocal affect) to infer others&#x27; emotions, meeting the social communication (reception) construct.</td></tr>
<tr><td>29890323_analysis_1</td><td>neutral &gt; emotional video clips</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Both emotional and neutral videos convey communicative information (narratives, facial/vocal cues); the contrast isolates communicative/social signal processing differences, aligning with social communication (reception).</td></tr>
<tr><td>29890323_analysis_2</td><td>Positively related to Z-EA scores</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>EA depends on decoding communicative cues (speech, facial expression, prosody) to infer others&#x27; states; modulation by EA maps onto social communication (reception) mechanisms.</td></tr>
<tr><td>29890323_analysis_3</td><td>Negatively related to Z-EA scores</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Regions negatively modulated by EA likely reflect aspects of processing communicative/social signals or alternate strategies; the analysis pertains to social communication mechanisms.</td></tr>
<tr><td>29890323_analysis_4</td><td>Positive correlation with participants&#x27; emotional intensity ratings</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Tracking emotional intensity over time requires decoding communicative information (speech, expression, prosody), fitting the social communication (reception) construct.</td></tr>
<tr><td>29890323_analysis_5</td><td>Negative correlation with participants&#x27; emotional intensity ratings</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Regions negatively tracking rated intensity likely reflect aspects of communicative/social signal processing (e.g., regions downregulated during intense social engagement), so social communication applies.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 6057276)</summary><p><strong>Title:</strong> Tracking emotions in the brain – Revisiting the Empathic Accuracy Task</p><details><summary>Abstract</summary><pre class="paper-text">Many empathy tasks lack ecological validity due to their use of simplistic stimuli and static analytical approaches. Empathic accuracy tasks overcome these limitations by using autobiographical emotional video clips. Usually, a single measure of empathic accuracy is computed by correlating the participants&#x27; continuous ratings of the narrator&#x27;s emotional state with the narrator&#x27;s own ratings. 

In this study, we validated a modified empathic accuracy task. A valence-independent rating of the narrator&#x27;s emotional intensity was added to provide comparability between videos portraying different primary emotions and to explore changes in neural activity related to variations in emotional intensity over time. We also added a new neutral control condition to investigate general emotional processing. In the scanner, 34 healthy participants watched 6 video clips of people talking about an autobiographical event (2 sad, 2 happy and 2 neutral clips) while continuously rating the narrator&#x27;s emotional intensity. 

Fluctuation in perceived emotional intensity correlated with activity in brain regions previously implicated in cognitive empathy (bilateral superior temporal sulcus, temporoparietal junction, and temporal pole) and affective empathy (right anterior insula and inferior frontal gyrus). When emotional video clips were compared to neutral video clips, we observed higher activity in similar brain regions. Empathic accuracy, on the other hand, was only positively related to activation in regions that have been implicated in cognitive empathy. 

Our modified empathic accuracy task provides a new method for studying the underlying components and dynamic processes involved in empathy. While the task elicited both cognitive and affective empathy, successful tracking of others&#x27; emotions relied predominantly on the cognitive components of empathy. The fMRI data analysis techniques developed here may prove valuable in characterising the neural basis of empathic difficulties observed across a range of psychiatric conditions. 
   Highlights  
  
Activity in affective and cognitive empathy related regions during emotional videos. 
  
Activity in similar regions related to changes in perceived emotional intensity. 
  
Only regions implicated in cognitive empathy were associated with empathic accuracy. 
  
No difference between video clips that did and did not elicit affect sharing. 
  
Empathic accuracy related to self-rated perspective-taking but not empathic concern.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Empathy, has been defined as “an emotional response [… which] is similar to one&#x27;s perception […] and understanding […] of the stimulus emotion, with recognition that the source of the emotion is not one&#x27;s own.” ( , page 150). Empathy is crucial for successful social interaction as it allows the individual to predict others&#x27; actions, emotions and intentions ( ). Deficits in empathic processing have been reported in psychiatric disorders such as autism spectrum disorder (ASD), schizophrenia, borderline personality disorder and bipolar disorder ( ). Identifying the neural substrates of empathy in healthy populations is important for understanding conditions that are characterised by empathic difficulties. In neuroscience, the concept of empathy is considered to include separate affective (sharing others&#x27; emotion) and cognitive (understanding others&#x27; emotion) components (for example,  ;  ). Previous research has identified distinct clusters of brain regions involved in affective empathy: medial/anterior cingulate cortex (MCC, ACC), anterior insula (AI) ( ;  ), and supplementary motor area (SMA) ( ). Within the broader domain of social cognition, cognitive empathy overlaps with the affective component of Theory of Mind (ToM) or mentalising, namely the capacity to infer other people&#x27;s thoughts, emotions and intentions without necessarily sharing them ( ). A recent meta-analysis of 144 fMRI studies using ToM tasks ( ) identified the medial prefrontal cortex (mPFC), medial orbitofrontal cortex (mOFC), ACC, precuneus, temporal pole (TP), posterior superior temporal gyrus (pSTS) and temporoparietal junction (TPJ) and inferior frontal gyrus (IFG) as key regions for mentalising. 

However, prior research on the neural mechanisms of empathy has often lacked ecological validity. Studies have often used simplistic stimuli that differ greatly from the complex cues that individuals have to process in real-life situations ( ,  ). Moreover, most studies focus on empathy for pain, while only a few studies have evaluated other emotions (e.g. disgust, happiness, sadness;  ;  ). In addition, empathy has mostly been operationalised as a static trait ( ). However, in the real world empathy fluctuates dynamically ( ). These fluctuations can happen spontaneously because of changes in internal state or in response to shifts in external circumstances, such the emotional intensity and expressivity of others. 

In the current study, we addressed these limitations of previous research by modifying an existing paradigm, the Empathic Accuracy Task (EAT;  ), that incorporates more naturalistic stimuli and reflects the dynamic nature of empathy. Participants (perceivers) watch video clips in which another person (target) describes an emotional autobiographical event. Perceivers continuously rate the target&#x27;s emotion while watching the clips (via button pressing). The EAT measures how   accurately   the perceiver infers changes in the target&#x27;s emotional states by correlating the perceiver&#x27;s ratings with the target&#x27;s ratings of their own emotions (see   for a detailed description).   found that empathic accuracy was associated with higher activation in both affective (i.e. inferior parietal lobule (IPL)) and cognitive (i.e. mPFC) empathy networks. In a recent study of adolescents, empathic accuracy related positively to activation in cognitive empathy or mentalising regions (mPFC, TPJ, STS) and negatively to activation in regions implicated in affective empathy (IPL, ACC, AI;  ). 

In the current study, new video clips were created and the EAT was modified in the following important ways: First, video clips depicted discrete primary emotions (happy, sad, angry, frightened) and participants rated changes in the targets&#x27; emotional intensity (instead of valence) to ensure comparability across different emotions and higher construct validity. Second, we introduced well-matched neutral video clips that acted as a control condition. In this condition, targets described their bedroom. This control condition allowed us to examine the neural correlates of emotion processing irrespective of empathic accuracy. Third, as empathy is a dynamic process, perceivers need to be able to continuously identify changes in the intensity of the target&#x27;s emotional state. We therefore utilised an analysis approach that tracked changes in the target&#x27;s emotional intensity throughout each video clip, in addition to deriving a single index of empathic accuracy (averaged across the clip). Fourth, we included ratings from participants regarding how they felt after watching each video to gain a better understanding of how the neural correlates of EA are influenced by cognitive and affective empathy. Finally, to validate the EAT, we related task performance to self-reported trait empathy and IQ as well as acquiring a normative data set with participants who completed the EAT outside of the scanner. 

The aim of this study was to validate a modified version of the Empathic Accuracy Task, using a staged analysis approach which replicates analyses presented previously in the literature, but which also included additional comparisons. First, we contrasted the blood-oxygen-level dependent (BOLD) responses to emotional and neutral clips to explore correlates of complex and multi-sensory emotional processing during extended clips rather than single emotional images. Second, we validated our emotional intensity rating scale by analysing the neural correlates of intra-individual variations in empathic accuracy. Third, we explored neural correlations with variations in perceived emotional intensity over time, thus capitalising on the availability of continuous ratings throughout each video clip. 

Given the results of prior neuroimaging studies of empathy and mentalising, we had the following hypotheses:   
At the group level, increased BOLD responses would be observed in brain regions previously linked to empathy and mentalising when participants watched targets describe emotional versus neutral events. 
  
There would be positive correlations between intra-individual variations in empathic accuracy and BOLD responses in these regions. 
  
We predicted positive correlations between fluctuations in perceived emotional intensity and BOLD responses in these regions during emotional video clips. 
  


## Methods 
  
### Participants 
  
#### fMRI study 
  
Forty-seven healthy participants aged between 20 and 30 years, fluent in English and with no history of neurological illness, took part in the study. Six participants were excluded from the analysis due to current or recurrent episodes of mental illness as assessed by the Mini International Neuropsychiatric Interview ( ). Five further participants were excluded because of excessive head movement or poor task performance (&lt;2 SD in empathic accuracy (EA) scores) and two participants had incomplete questionnaire data. The final dataset included 34 subjects (19 females, mean age: 24.0 years, SD: 2.7 years). The study received ethical approval from the Camberwell - St. Giles NHS Research Ethics Committee (14/LO/0477) and the University of Southampton Ethics Committee. 


#### Normative data collection 
  
To create a normative data set for the EAT and to validate the stimuli used in the fMRI task, an additional 73 healthy participants completed the EAT outside the MRI scanner. The same inclusion criteria as described above were applied. After excluding 13 participants due to current or recurrent episodes of mental illness, the final dataset included 60 healthy participants (36 females, mean age: 25.2 years, SD: 2.9 years). This aspect of the study was approved by the University of Southampton Ethics Committee. 



### Tasks and stimuli 
  
#### Video acquisition 
  
Eleven native English-speaking students from the University of Southampton acted as targets (8 females, mean age: 20.1 years, SD: 1.64 years). Before filming they were asked to recall a specific autobiographical event (happy, sad, angry or frightened), in which they remembered feeling a strong emotion. Each target wrote a short summary of each event and rated its overall emotional intensity on a 9-point scale (from 1, ‘no emotion’ to 9, ‘very strong emotion’). For the emotional stimuli, only events with a rating of 5 or above were filmed. Each target provided one video clip for each emotion and one clip in which they described their bedroom (neutral condition). An adapted emotion elicitation strategy, which involved imagining being in the situation, was used before filming to reinstate the affective states the targets had felt during the events ( ). They were advised to refrain from making specific reference to their affective state (e.g. happy) but were allowed to use generic descriptions (e.g. upset) or descriptions of bodily symptoms (e.g. shaking). All targets were filmed from the shoulders upwards, in front of a black background, for standardisation purposes. Each clip lasted between 83 and 140 s (mean = 100.3, SD = 15.2). After filming each clip, targets watched the video and continuously rated their emotional intensity using the same 9-point scale as above. Ratings were made by using arrow keys on the keyboard to move a coloured square on the scale (this shifted by one point per button press). Starting point for all ratings was “1”. 

For the fMRI study, the 6 video clips that were selected (one happy, one sad and one neutral video, featuring one female and one male target) were those which received high EA and target expressiveness scores in a pilot study with 13 participants (7 male, mean age: 21.54 years, SD: 2.37 years). A description of the target&#x27;s gender, the emotional condition, the clip length and the target&#x27;s rating of emotional intensity experienced during each clip is presented in  . For pre-training and volume adjustment, one additional sad, one neutral and two happy clips were added (depicting different targets from the main experiment). For the data collection outside the MRI scanner, 27 expressive video clips were selected (7 happy clips, 7 sad clips, 3 angry clips, 3 frightened clips and 7 neutral clips) as well as two happy clips and one sad clip for pre-training purposes. The task and instructions for filming stimuli are available on request.   
Video clips displayed in order of presentation during the Empathic Accuracy Task with target&#x27;s gender, emotional condition and length of the video clip and targets&#x27; average ratings of their own emotional intensity. 
  Table 1   


#### Empathic accuracy task (EAT) 
  
Participants were instructed to continuously rate the perceived emotional intensity of the target ( , top) using the same 9-point scale as above (from 1, ‘no emotion’ to 9, ‘very strong emotion’). In the fMRI study, participants used a button box to provide ratings. In the non-imaging study, participants used the computer&#x27;s arrow keys. The default rating at the start of each video clip was no emotion (i.e. rating of 1). Following each clip, participants were asked: (1) which emotion the target felt most strongly (cognitive empathy: options of “happy”, “angry”, “surprised”, “sad”, “frightened” and “no emotion”); and (2) which emotion they themselves felt most strongly (i.e., affective empathy: same response options as above).   
 Schematic representation of the Empathic Accuracy task and continuous rating scale data.   Top: example of a video clip and rating scale in the Empathic Accuracy Task. The target&#x27;s identity has been disguised in this image. Bottom: Illustration of fluctuations in the   target&#x27;s   emotional intensity, as rated by the target (blue) and an example participant&#x27;s ratings (green). An Empathic Accuracy (EA) score was computed by correlating the participant&#x27;s ratings and the target&#x27;s ratings for each video clip. 
  Fig. 1   


#### Interpersonal reactivity index 
  
The Interpersonal Reactivity Index (IRI) is a widely-used self-report questionnaire that measures dispositional empathy using four subscales: fantasy (FS), empathic concern (EC), perspective taking (PT) and personal distress (PD;  ). 


#### Wechsler abbreviated scale of intelligence, Second Edition 
  
The Wechsler Abbreviated Scale of Intelligence, Second Edition (WASI-II;  ) is a widely-used and reliable test of general intelligence. 



### Procedure 
  
#### fMRI study 
  
The EAT was part of the testing protocol of the English and Romanian Adoptees&#x27; Brain Imaging Study (for further details, see  ). Participants gave written informed consent to participate in the study. All participants completed the MINI and WASI-II, and an online survey, which included the IRI. Participants received pre-training on the fMRI tasks prior to the scan, during which they were familiarised with the EAT and the scanning environment. After observing the experimenter demonstrating how to rate one happy clip, participants rated two clips (one sad, one happy) themselves, while lying in a mock scanner. In the actual EAT experiment, participants watched and rated the 6 video clips in a fixed order ( ). The task took approximately 12 min. Participants were reimbursed for around 6 h of their time with a £100 Amazon voucher. 


#### Normative data collection 
  
For the non-scanning study, participants gave written consent to participate. For pre-training, participants first watched the experimenter rate one happy clip before rating two practice video clips themselves. They then watched and rated 27 video clips in randomised presentation order, in a quiet testing room. This lasted approximately 40 min. Participants also completed an online survey, which included the IRI. Participants were reimbursed for their time with a £15 Amazon voucher. 



### Behavioural data analysis 
  
Participants&#x27; and targets&#x27; ratings were analysed using Matlab 8.2.0 (The MathWorks Inc., Natick, Massachusetts, United States) and SPSS (Version 22, IBM Corp., Armonk, New York, United States). All ratings were separated into 2 s bins and one time-weighted average rating was calculated for each bin. We then tested for correlations between the participants&#x27; and targets&#x27; ratings ( , bottom). The resulting Pearson&#x27;s correlation coefficient for each video clip and each participant is referred to as the EA score. As expected, the variance of the ratings was low for neutral clips. EA scores were therefore only calculated for emotional video clips. EA scores were then r-to-Z transformed to allow comparison between correlation coefficients ( ,  ). 

#### Behavioural analysis of fMRI sample 
  
Paired t-tests examined whether Z-transformed EA scores, affective and cognitive empathy scores differed between happy and sad video clips. Moreover, paired t-tests were performed to test for differences in the average ratings of the target&#x27;s emotional intensity between emotional and neutral as well as happy and sad video clips. A paired   t  -test was also used to test whether Z-EA scores differed between video clips that elicited “affect sharing” (participants reported feeling the same emotion as the target) compared to those that did not (participants reported a different emotion or no emotion). In addition, Pearson correlations were conducted to test for relationships between mean Z-EA scores, the IRI subscales and IQ. 


#### Behavioural analysis of normative data sample 
  
To examine whether the video clips presented in the fMRI study induced Z-EA scores comparable to those in the non-scanning sessions, two Pearson correlations were performed within the normative data sample. Considering happy and sad video clips separately, we examined the correlation between Z-EA scores based on the two video clips presented in the scanner and Z-EA scores based on all seven video clips from the respective emotional category. Moreover, intra-individual standard deviations were calculated based on (1) the four emotional video clips presented in the scanner and (2) all 20 emotional video clips. These were then compared with a paired   t  -test. 



### fMRI data acquisition 
  
Functional images were acquired on a General Electric MR750 3.0 T MR scanner with a 12-channel head coil. A T2*-weighted gradient echo, echo-planar imaging sequence was used, which covered 41 axial slices and recorded 347 vol acquired sequentially, descending (TR/TE 2000/30 ms, flip angle 75°, 64 × 64 matrix, 3 mm thick, field of view (FoV) = 247 mm). To facilitate fMRI data registration and normalisation, we also acquired a T1-weighted Magnetization Prepared Rapid Gradient Echo MPRAGE image (TR/TE 7312/3.02 ms, flip angle 11°, 256 × 256 matrix, 1.2 mm thick, 196 sagittal slices, FoV = 270 mm). 


### fMRI data analysis 
  
We used SPM12 for pre-processing and subject-level (first level) analyses (Wellcome Department of Cognitive Neurology, Institute for Neurology, London, UK). FSL was utilised for cerebrospinal fluid (CSF) regression and statistical nonparametric permutation inference at the group level (second level) with “randomise” ( ; FMRIB Analysis Research, Oxford Centre for Functional MRI of the Brain, Oxford, UK). 

#### Preprocessing 
  
After reorientation, the EPI files were first slice-time corrected (middle slice as reference). Images were then realigned to the first image and subsequently to the time series mean. The mean EPI image was co-registered to the T1-weighted image to allow for normalisation. The structural files were segmented and the resulting grey matter, white matter and CSF files were used to create a common group-specific template using group-wise DARTEL registration ( ). This template was then employed to normalise the functional EPI files to MNI space. This step simultaneously resampled volumes (1.5 mm isotropic) and applied spatial smoothing (Gaussian FWHM kernel of 8 mm). Finally, for each participant, the time course signal of a CSF mask (top 5% from DARTEL CSF component) was extracted in native space. 


#### Emotional vs neutral video clips 
  
At the first level of analysis, each participant&#x27;s pre-processed data were modelled as a block design using a general linear model framework. We included 3 separate regressors (happy, sad, neutral) encoding the predicted BOLD response associated with video presentation, formed by convolution of the canonical haemodynamic response function (HRF) with boxcars delimiting the video presentation. 

We identified regional estimates of BOLD response associated with watching and rating the video clips. Separate parameter estimates for mean response during the emotional (happy and sad) and neutral category compared to the implicit baseline were produced. At the group level, in a random-effects model, paired t-tests were performed to identify clusters that were differentially activated when watching emotional video clips compared to neutral clips. Moreover, happy and sad clips were compared using paired t-tests. 


#### Intra-individual variation in empathic accuracy 
  
In accordance with  , Z-EA scores for each participant and each video clip were added as parametric modulators at the first level of analysis. On the group level, one sample t-tests were performed, to test whether the BOLD response during emotional video clips was modulated by intra-individual variations in Z-EA scores. 


#### Correlation with emotional intensity ratings 
  
We examined how the BOLD time series correlated with the participant&#x27;s ratings of the target&#x27;s emotional intensity. Scans were split and a model was fitted to each emotional video clip in turn. The continuous ratings of the target&#x27;s emotional intensity for each 2 s bin as rated by the participant were entered as regressors of interest. At the group level, one-sample t-tests assessed whether the relationship between BOLD response and changes in the emotional intensity ratings was significantly observed in any brain region across the group. 


#### Exploratory analysis: impact of affect sharing 
  
To examine differences in BOLD response for video clips that induced affect sharing compared to those that did not, we conducted an exploratory post-hoc analysis. We included the 20 participants who showed affect sharing in response to some, but not all video clips in order to be able to create 3 separate conditions in the first level in a block design (shared, non-shared, neutral). For each participant, emotional videos that induced affect sharing (participants reported to have the same emotion as the target) were included in the shared condition, while emotional videos that did not elicit affect sharing (participants reported to have a different emotion than the target or no emotion) were modelled in the non-shared condition. Separate parameter estimates for mean response during affect shared, non-shared and neutral video clip presentation compared to the implicit baseline were calculated. At the group level, paired t-tests were performed to identify clusters that were differentially activated when watching video clips that induced affect sharing compared to non-shared clips. 


#### Movement, scanner drifts and multiple comparisons correction 
  
As well as the regressors described above, all analyses included seven movement parameters (six standard parameters as well as volume-to-volume movement) as nuisance regressors. For each volume-to-volume movement exceeding 1 mm, an additional regressor was included marking the location of that volume and those immediately adjacent (for a summary of volume-to-volume movement see  ). The CSF regressor was also included as a nuisance regressor. To control for task-related hand movement artefacts, button presses were included as condition of no interest. To investigate the effect of controlling for button presses, we additionally repeated all analyses without including this condition. Moreover, we compared button presses during emotional video clips with button presses during neutral video clips as separate conditions to ensure that activity relating to emotion processing was not partialled out. 

Data were high pass filtered with a threshold of 209 s, which corresponds to twice the length of the longest video clip, to control for scanner drifts. 

Results reported are based on Family-Wise Error (FWE) corrected threshold-free cluster enhancement (TFCE:   p   &lt; 0.05 ( )). For each significant cluster, the peak activations with a minimum inter-peak distance of 20 voxels are reported to account for the wide-spanning clusters found in our analyses. 




## Results 
  
### Behavioural data 
  
#### Behavioural analysis of the fMRI sample 
  
On average, participants had high EA scores (mean   r   = .75, mean intra-individual standard deviation (iSD) = .35, range = .13 to .97). Fisher&#x27;s Z-transformed (Z-)EA scores were slightly, but significantly, lower for sad video clips (mean Z-EA = 0.97, SD = 0.21) than happy ones (mean Z-EA = 1.16, SD = 0.19;   t   (33) = 5.17,   p   &lt; .001). As expected, participants&#x27; average ratings of the target&#x27;s emotional intensity were higher for emotional than for neutral video clips (mean emotional = 5.18, mean neutral = 1.75,   t   (33) = 15.29,   p   &lt; .001), with higher ratings for sad compared to happy ones (mean sad = 5.49, mean happy = 4.87,   t   (33) = 3.02,   p   &lt; .01). 

On average, participants correctly inferred the target&#x27;s emotion in 90.4% of clips (emotion identification, SD = 15.1%), with no difference between happy and sad clips (  t   (33) = −0.33,   p   = .74). They also reported experiencing the same emotion as the target for the majority of the emotional video clips (affect sharing, mean = 72.8%, SD = 28.5%), with a higher degree concordance for sad (mean = 79.4%, SD = 32.8%) compared to happy clips (mean = 66.2%, SD = 31.9%;   t   (33) = 2.5,   p   &lt; .05). 13 participants shared the target&#x27;s emotion in every emotional video clip while one participant did not show affect sharing in any of the clips. For the remaining 20 participants who showed a mix of affect sharing and non-sharing, Z-EA scores did not differ for videos that elicited affect sharing (mean Z-EA = 1.09, SD = .24) compared to those that did not (mean Z-EA = 1.06, SD = .33,   t   (19) = .36,   p   = .72). 

Additionally, we found a positive correlation between participants&#x27; mean Z-EA scores and IRI perspective-taking (  r   = .48,   p   &lt; .01). No significant correlations were found between mean Z-EA scores and the other IRI subscales or estimated IQ (all   ps   &gt; .09). 

Note that while we used Pearson&#x27;s product-moment correlation, alternative methods for assessing agreement are available such as the intraclass correlation coefficient. EA scores derived using this measure were highly correlated (r = 0.89) with Pearson&#x27;s correlations. We chose the latter for two reasons. First, we were able to confirm our findings after partialling out dependency over time of the ratings (data not shown) and second, we wished to maintain compatibility with previous studies using similar tasks that also based estimates of inter-rater agreement on Pearson&#x27;s correlations. 


#### Behavioural analysis of the normative data sample 
  
The analysis showed that the mean Z-EA scores for the video clips presented in the fMRI study were strongly positively correlated with Z-EA scores for the seven clips presented in the normative data study (happy:   r   = .82,   p   &lt; .001; sad:   r   = .77,   p   &lt; .001). Furthermore, the intra-individual standard deviation of the four emotional video clips presented in the scanner (mean iSD = .36) did not differ from the individual standard deviation across all 20 emotional video clips presented outside the scanner (mean iSD = .39,   t   (59) = −1.64,   p   = .11). 



### fMRI data 
  
#### Emotional vs. neutral video clips 
  
Group-level analysis revealed a higher BOLD response during emotional compared to neutral clips in a large cluster spanning multiple regions, with peak activations in bilateral occipital poles and inferior lateral occipital cortex ( a,  ). The cluster included bilateral posterior and anterior superior temporal cortex (STC), as well as bilateral temporal pole (TP), bilateral planum temporale and bilateral posterior temporoparietal junction (pTPJ). Higher activation was also seen in right inferior frontal gyrus (IFG; including pars triangularis and opercularis), with the cluster extending into right anterior insular cortex (AI) and right putamen. A second cluster showed higher activation in supplementary motor area (SMA). While participants were watching neutral compared to emotional video clips, activation was higher in left superior lateral occipital cortex, left posterior cingulate cortex (PCC) and left precuneus. Significant activation was similar, albeit more widespread, when not controlling for button presses (see  ). Moreover, when analysing the button press condition separately for emotional and neutral video clips, no brain regions showed significant differences between both button press conditions.   
 Neural substrates of changes in empathy.   a) Significant brain activations when viewing emotional video clips compared to neutral ones. b) Regions significantly positively (red) and negatively (blue) modulated by variations in empathic accuracy (Z-EA scores). c) top: Brain areas significantly positively correlated over time with the participants&#x27; ratings of the target&#x27;s emotional intensity. bottom: BOLD response (after first level regression) of significant clusters (blue) and participant&#x27;s ratings of the target&#x27;s emotional intensity (green) of one exemplary participant. Key: STC - superior temporal cortex, TP - temporal pole, TPJ - temporoparietal junction, IFG - inferior frontal gyrus, SMA - supplementary motor area, aMCC - anterior midcingulate cortex. 
  Fig. 2     
Significant clusters and their peak activations for the contrasts emotional &gt; neutral video clips and neutral &gt; emotional video clips (threshold-free cluster enhancement   p   &lt; 0.05). 
  Table 2   

To explore differences between the different emotion conditions, we also directly compared happy and sad video clips. Activation in the bilateral STC was higher during happy compared to sad clips, while the right paracingulate gyrus and right precuneus showed higher activation during sad video clips (see   and  ). 


#### Intra-individual variation in empathic accuracy 
  
Participants&#x27; intra-individual variations in Z-EA scores were positively related to activation in clusters spanning the bilateral STC, planum temporale, TP and pTPJ, left hippocampus and left amygdala. Activity in the bilateral inferior lateral occipital cortex and fusiform cortex was also positively related to Z-EA scores ( b,  ). Activation in the bilateral paracingulate gyrus and right frontal pole as well as the right middle frontal gyrus was significantly negatively modulated by Z-EA scores.   
Significant clusters and their peak activations for the modulation of BOLD-response by intra-individual variation of Z-EA scores (threshold-free cluster enhancement   p   &lt; 0.05). 
  Table 3   


#### Correlation with emotional intensity ratings 
  
While watching emotional video clips, participants&#x27; fluctuations in ratings of the targets&#x27; emotional intensity were positively correlated over time with changes in BOLD response in multiple brain regions ( c,  ). Associations were found in multiple clusters including bilateral posterior STC, bilateral TP, bilateral IFG (including pars triangularis and opercularis), bilateral SMA, bilateral middle and superior frontal cortices, right anterior midcingulate cortex (aMCC), right AI, bilateral amygdala, bilateral putamen as well as pTPJ and right temporal occipital and anterior temporal fusiform cortex. Emotional intensity ratings and BOLD-response were negatively correlated in the in the bilateral superior lateral occipital cortex, PCC, and precuneus.   shows a binarised overlay of significant clusters in the different analyses.   
Binarised overlay of activations related to a) emotional compared to neutral video clips, b) variation positively related to empathic accuracy and c) positive correlation with emotional intensity. 
  Fig. 3     
Significant clusters and their peak activations for the correlation between BOLD-response and the participants&#x27; ratings of the target&#x27;s emotional intensity (threshold-free cluster enhancement   p   &lt; 0.05). 
  Table 4   


#### Exploratory analysis: impact of affect sharing 
  
For emotional video clips, there were no significant differences in BOLD response between clips that elicited, versus those that did not elicit, affect sharing (i.e. participants reported experiencing the same emotion as the target after providing their continuous ratings). 




## Discussion 
  
We used a modified version of the EAT to study neural substrates of empathic accuracy and to gain a better understanding of its underlying components. We demonstrated that fluctuations in participants&#x27; perceived emotional intensity ratings are correlated with activation in a network of brain regions previously implicated in empathy and broader aspects of social cognition (i.e., mentalising). More specifically, consistent with our first hypothesis, we observed increased activation in brain regions associated with empathy and mentalising when participants watched emotional compared to neutral clips. Supporting our second hypothesis, we found a positive correlation between intra-individual variations in empathic accuracy and the temporal lobe, “mentalising” regions of the same network. Confirming our third hypothesis, we found a correlation between fluctuations in ratings of the targets&#x27; perceived emotional intensity over time and activity in these same regions. This network of brain regions appears not only to have a general role in emotion and empathic processing but is also sensitive to   variations   in the intensity of others&#x27; emotions. 

The superior temporal sulcus (STS), temporoparietal junction (TPJ), and temporal pole (TP) have consistently been associated with mentalising ( ). In our study, these areas were more active with higher EA, i.e. when participants were more accurate at tracking the target&#x27;s emotion. Beyond this, we could also show these regions are sensitive to fluctuations in perceived emotional intensity of others. The STS is thought to facilitate mentalising by interpreting social aspects of observed biological motion ( ,  ) and the region has been implicated in EA ( ,  ). The TPJ is involved in inferring other people&#x27;s temporary mental states ( ) while the TP&#x27;s role in mentalising is thought to involve the integration of multimodal information and recollection of social scripts ( ;  ). Combined, these brain regions are involved in distinct emotional and cognitive processes that are required to perform our modified EAT: they are integral for the successful tracking of others&#x27; emotional intensity and correlate positively with intra-individual variations in EA. 

The anterior insula (AI), anterior midcingulate cortex (aMCC), inferior frontal gyrus (IFG) and supplementary motor area (SMA) have previously been implicated in empathy tasks and are associated with the affect sharing component of empathy (or affective empathy) ( ). Together these regions are implicated in the emotional processing of the modified EAT stimuli. Most importantly, we could show for the first time that their activity tracks the perceived emotional intensity of others. However, activity in these brain regions was not sensitive to changes in EA and thus seems more tied to the subjective perception of other&#x27;s feelings. 

This suggests it is the time-series variation in activation in the temporal lobe regions (STS, TPJ, TP) that might be informative for accurately tracking other people&#x27;s emotions, while activation in the frontal regions (AI, ACC, IFG, SMA) represents a different emotion processing component that does not vary with changes in EA ( ). This is consistent with previous studies on EA, which showed either no correlation between EA and activity in the above frontal regions ( ) or, in the case of adolescents, a negative correlation between EA and ACC and AI activation ( ). Furthermore, we could not replicate an association between EA and activity in the inferior parietal lobe, a region implicated in motor imitation and previously interpreted as an affective processing component of EA ( ). Taken together, these findings provide evidence that EA is more closely related to the concept of cognitive empathy and mentalising than affective empathy and emotion sharing. The role of EA in cognitive but not affective empathy is further supported by the positive correlation between EA scores and the perspective-taking scale of a well-established self-report measure of empathy (the IRI) but not with other more affective subscales such as empathic concern. Moreover, participants&#x27; average EA scores did not differ between videos where they shared the same emotion as the target compared to those were they did not, which again suggests that emotion sharing is neither necessary for, nor facilitates, EA. 

Even if EA does only relate to cognitive but not affective empathy, the EAT as a task successfully elicited affective empathy in most of our participants – they reported sharing the target&#x27;s emotion in 73% of the emotional video clips. However, there were no significant differences in brain activity when rating videos where participants shared the same emotion compared to videos where they did not. This further supports our hypothesis that the higher activation in aMCC, AI, SMA and IFG during emotional clips is associated with more basal, empathy-independent aspects of emotion processing. 

Higher activation of the bilateral STS could also be seen during happy compared to sad video clips, while the right paracingulate gyrus was more highly activated during sad video clips. This is in line with our behavioural findings of, on average, higher EA scores during happy video clips, which suggest more successful tracking and mentalising of the target&#x27;s emotion, while sad video clips induced higher rates of affect sharing among participants. The paracingulate gyrus has previously been implicated in affective empathy ( ). 

During the modified EAT, participants rated fluctuations in emotional intensity rather than valence as this allowed a comparable rating scale across different distinct emotions. Furthermore, previous literature suggests distinct neural correlates for processing emotional intensity and valence ( ), with the amygdala being associated with intensity and the orbitofrontal cortex with valence. In agreement with this, we found that bilateral activation of amygdala but not the orbitofrontal cortex covaried with the emotional intensity of the targets. Unexpectedly activation in the precuneus – a region implicated in self-referential processing ( ) – was stronger during neutral versus emotional clips and correlated negatively with emotional intensity ratings. The precuneus is associated with visual-spatial imagery ( ,  ) and is a component of the default mode network ( ). Higher activation during the neutral videos in which participants described their bedroom, might be explained by higher visual-spatial imagery and an increased tendency for mind-wandering during these less engaging parts of the task ( ). 

Empathy is a complex and dynamic process, which requires multiple higher order functions ( ) such as emotion recognition, multimodal sensory integration, self-other distinction and continuous processing of valence and intensity information. Compared to other commonly used empathy tasks, the modified EAT used a more naturalistic setting to examine which brain regions track fluctuations over time in perceived emotional intensity of others and intra-individual variations in empathic accuracy. Previous studies in the empathy and mentalising literature have largely focused on simplistic stimuli (e.g. static images of hands in painful situations). Compared to these earlier studies, we found that regions that have been separately implicated in mentalising and empathy were all involved in performing the modified EAT. However, only brain regions previously associated with mentalising were found to covary with EA, while regions previously implicated in classic affective empathy paradigms were positively correlated with the emotional intensity of others but were not sensitive to changes in EA. In this more naturalistic and complex task, it seems that an interplay between brain networks associated with mentalising and empathy enables the accurate tracking of other&#x27;s emotions. Furthermore, these regions were sensitive to fluctuations in perceived emotional intensity of others, which serves as a potential mechanism for successful communication between these networks to achieve empathic accuracy. 

A possible limitation of our study was the lower number of emotional video clips in comparison to previous studies on EA ( ,  ). This study was conducted within the framework of a larger project, and thus the scanning time was limited. However, we showed that our chosen video clips led to very similar EA scores relative to those obtained with the larger dataset of 27 video clips in the norm sample. More importantly, the intra-individual variation across videos was also comparable to that seen for the full set of video clips. 

The study had a number of strengths. The original EAT ( ) represented an important advance in empathy research, as it was the first task to utilise naturalistic stimuli and assess EA in an fMRI context. In this modified EAT, the stimuli used for fMRI purposes had been validated in a separate behavioural study. Moreover, we added a neutral control condition, which allowed us to identify brain regions that are generally more active during emotional video clips irrespective of empathic accuracy. Future studies could employ this paradigm to study psychiatric populations with empathy deficits (e.g., adolescents with Conduct Disorder;  ). By additionally taking the neutral control condition into account, one could examine whether emotional clips were ‘neutral-like’ in those with low EA scores. For future studies, it would be worth considering incorporating neutral videos with varying topics other than bedroom descriptions to ensure continued engagement throughout the task (see   for possible examples). Furthermore, we introduced the measurement of emotional intensity rather than valence, which is more closely related to the concept of empathy. This also made the video clips of different emotions comparable and allowed a more fine-grained analysis of changes over time in activation related to the emotional intensity of others. Together, we propose that the three analysis techniques used in this study, should be employed in conjunction to allow a comprehensive study of empathic accuracy and its different components. 

In conclusion, we provide the first evidence that the modified EAT is a suitable paradigm for studying empathy and its underlying components. We show that, while the modified EAT successfully induces both affective and cognitive empathy, EA relies more on cognitive empathy than affect sharing. The neutral control condition and the valence-independent rating scale represent valuable additions to the task. The fMRI data analysis techniques developed and described here may prove valuable in characterising differences between healthy participants and participants with psychiatric conditions associated with empathy deficits. 


## Funding 
  
This work was funded by a project grant from the   to ESB, MM and GF (MR/K022474/1). 


## Declaration of interest 
  
We do not have any financial, institutional or other relationships that might lead to a conflict of interest.</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (3)</summary><details class="inner-accordion"><summary>Table 2 (tbl2) - Significant clusters and their peak activations for the contrasts emotional &gt; neutral video clips and neutral &gt; emotional video clips (threshold-free cluster enhancement pFWE &lt; 0.05).</summary><div class="table-html"><table-wrap id="tbl2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Significant clusters and their peak activations for the contrasts emotional &gt; neutral video clips and neutral &gt; emotional video clips (threshold-free cluster enhancement <italic toggle="yes">p</italic><sub>FWE</sub> &lt; 0.05).</p></caption><alt-text id="alttext0035">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="1">Cluster</th><th rowspan="2" colspan="1">Anatomical region</th><th rowspan="2" colspan="1">Hemisphere</th><th rowspan="2" colspan="1">Cluster size</th><th colspan="3" rowspan="1">MNI coordinates [mm]<hr /></th><th rowspan="2" colspan="1">Peak-level <italic toggle="yes">t</italic></th></tr><tr><th colspan="1" rowspan="1">x</th><th colspan="1" rowspan="1">y</th><th colspan="1" rowspan="1">z</th></tr></thead><tbody><tr><td colspan="8" align="left" rowspan="1"><bold>emotional &gt; neutral video clips</bold><hr /></td></tr><tr><td rowspan="11" align="left" colspan="1">1</td><td align="left" colspan="1" rowspan="1">Occipital Pole</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">9901</td><td align="left" colspan="1" rowspan="1">20</td><td align="left" colspan="1" rowspan="1">−94</td><td align="left" colspan="1" rowspan="1">−2</td><td align="left" colspan="1" rowspan="1">7.33</td></tr><tr><td align="left" colspan="1" rowspan="1">Inferior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−32</td><td align="left" colspan="1" rowspan="1">−90</td><td align="left" colspan="1" rowspan="1">−6</td><td align="left" colspan="1" rowspan="1">6.86</td></tr><tr><td align="left" colspan="1" rowspan="1">Anterior Superior Temporal Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">52</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">−18</td><td align="left" colspan="1" rowspan="1">6.21</td></tr><tr><td align="left" colspan="1" rowspan="1">Occipital Pole</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">−94</td><td align="left" colspan="1" rowspan="1">12</td><td align="left" colspan="1" rowspan="1">5.94</td></tr><tr><td align="left" colspan="1" rowspan="1">Inferior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">−70</td><td align="left" colspan="1" rowspan="1">−4</td><td align="left" colspan="1" rowspan="1">5.1</td></tr><tr><td align="left" colspan="1" rowspan="1">Posterior Superior Temporal Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">56</td><td align="left" colspan="1" rowspan="1">−28</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">4.57</td></tr><tr><td align="left" colspan="1" rowspan="1">Insular Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">28</td><td align="left" colspan="1" rowspan="1">16</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">4.39</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Occipital Fusiform Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">−50</td><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">4.15</td></tr><tr><td align="left" colspan="1" rowspan="1">Frontal Operculum Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">48</td><td align="left" colspan="1" rowspan="1">16</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">4.02</td></tr><tr><td align="left" colspan="1" rowspan="1">Occipital Fusiform Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−36</td><td align="left" colspan="1" rowspan="1">−70</td><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">4.01</td></tr><tr><td align="left" colspan="1" rowspan="1">Occipital Pole</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−10</td><td align="left" colspan="1" rowspan="1">−100</td><td align="left" colspan="1" rowspan="1">−14</td><td align="left" colspan="1" rowspan="1">3.44</td></tr><tr><td rowspan="4" align="left" colspan="1">2</td><td align="left" colspan="1" rowspan="1">Anterior Superior Temporal Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">2243</td><td align="left" colspan="1" rowspan="1">−52</td><td align="left" colspan="1" rowspan="1">−6</td><td align="left" colspan="1" rowspan="1">−14</td><td align="left" colspan="1" rowspan="1">5.32</td></tr><tr><td align="left" colspan="1" rowspan="1">Posterior Supramarginal Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−56</td><td align="left" colspan="1" rowspan="1">−44</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">4.45</td></tr><tr><td align="left" colspan="1" rowspan="1">Middle Temporal Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−44</td><td align="left" colspan="1" rowspan="1">−32</td><td align="left" colspan="1" rowspan="1">−2</td><td align="left" colspan="1" rowspan="1">3.85</td></tr><tr><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−60</td><td align="left" colspan="1" rowspan="1">−20</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">3.48</td></tr><tr><td align="left" colspan="1" rowspan="1">3</td><td align="left" colspan="1" rowspan="1">Supplementary Motor Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">250</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">60</td><td align="left" colspan="1" rowspan="1">5.04</td></tr><tr><td align="left" colspan="1" rowspan="1">4<hr /></td><td align="left" colspan="1" rowspan="1">Temporal Pole<hr /></td><td align="left" colspan="1" rowspan="1">L<hr /></td><td align="left" colspan="1" rowspan="1">6<hr /></td><td align="left" colspan="1" rowspan="1">−46<hr /></td><td align="left" colspan="1" rowspan="1">18<hr /></td><td align="left" colspan="1" rowspan="1">−26<hr /></td><td align="left" colspan="1" rowspan="1">3.53<hr /></td></tr><tr><td colspan="8" align="left" rowspan="1"><bold>neutral &gt; emotional video clips</bold><hr /></td></tr><tr><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">1014</td><td align="left" colspan="1" rowspan="1">−34</td><td align="left" colspan="1" rowspan="1">−80</td><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">6.59</td></tr><tr><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−44</td><td align="left" colspan="1" rowspan="1">−84</td><td align="left" colspan="1" rowspan="1">22</td><td align="left" colspan="1" rowspan="1">5.86</td></tr><tr><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">Posterior Cingulate Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">207</td><td align="left" colspan="1" rowspan="1">−4</td><td align="left" colspan="1" rowspan="1">−38</td><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">8.99</td></tr><tr><td align="left" colspan="1" rowspan="1">3</td><td align="left" colspan="1" rowspan="1">Precuneus Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">86</td><td align="left" colspan="1" rowspan="1">−14</td><td align="left" colspan="1" rowspan="1">−60</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">6.2</td></tr><tr><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">54</td><td align="left" colspan="1" rowspan="1">36</td><td align="left" colspan="1" rowspan="1">−76</td><td align="left" colspan="1" rowspan="1">42</td><td align="left" colspan="1" rowspan="1">5.32</td></tr><tr><td align="left" colspan="1" rowspan="1">5</td><td align="left" colspan="1" rowspan="1">Lingual Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">34</td><td align="left" colspan="1" rowspan="1">−38</td><td align="left" colspan="1" rowspan="1">−10</td><td align="left" colspan="1" rowspan="1">5.49</td></tr><tr><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">30</td><td align="left" colspan="1" rowspan="1">−30</td><td align="left" colspan="1" rowspan="1">−20</td><td align="left" colspan="1" rowspan="1">5.46</td></tr></tbody></table></table-wrap></div></details><details class="inner-accordion"><summary>Table 3 (tbl3) - Significant clusters and their peak activations for the modulation of BOLD-response by intra-individual variation of Z-EA scores (threshold-free cluster enhancement pFWE &lt; 0.05).</summary><div class="table-html"><table-wrap id="tbl3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Significant clusters and their peak activations for the modulation of BOLD-response by intra-individual variation of Z-EA scores (threshold-free cluster enhancement <italic toggle="yes">p</italic><sub>FWE</sub> &lt; 0.05).</p></caption><alt-text id="alttext0040">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="1">Cluster</th><th rowspan="2" colspan="1">Anatomical region</th><th rowspan="2" colspan="1">Hemisphere</th><th rowspan="2" colspan="1">Cluster size</th><th colspan="3" rowspan="1">MNI coordinates [mm]<hr /></th><th rowspan="2" colspan="1">Peak-level <italic toggle="yes">t</italic></th></tr><tr><th colspan="1" rowspan="1">x</th><th colspan="1" rowspan="1">y</th><th colspan="1" rowspan="1">z</th></tr></thead><tbody><tr><td colspan="8" align="left" rowspan="1"><bold>Positively related to Z-EA scores</bold><hr /></td></tr><tr><td rowspan="7" align="left" colspan="1">1</td><td align="left" colspan="1" rowspan="1">Posterior Superior Temporal Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">9036</td><td align="left" colspan="1" rowspan="1">−62</td><td align="left" colspan="1" rowspan="1">−26</td><td align="left" colspan="1" rowspan="1">10</td><td align="left" colspan="1" rowspan="1">9.88</td></tr><tr><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−38</td><td align="left" colspan="1" rowspan="1">−34</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">9.17</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Pole</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−54</td><td align="left" colspan="1" rowspan="1">−2</td><td align="left" colspan="1" rowspan="1">−2</td><td align="left" colspan="1" rowspan="1">7.02</td></tr><tr><td align="left" colspan="1" rowspan="1">Hippocampus</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−20</td><td align="left" colspan="1" rowspan="1">−14</td><td align="left" colspan="1" rowspan="1">−20</td><td align="left" colspan="1" rowspan="1">6.50</td></tr><tr><td align="left" colspan="1" rowspan="1">Inferior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−44</td><td align="left" colspan="1" rowspan="1">−72</td><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">6.03</td></tr><tr><td align="left" colspan="1" rowspan="1">Posterior Temporal Fusiform Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−38</td><td align="left" colspan="1" rowspan="1">−42</td><td align="left" colspan="1" rowspan="1">−26</td><td align="left" colspan="1" rowspan="1">4.67</td></tr><tr><td align="left" colspan="1" rowspan="1">Occipital Fusiform Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−20</td><td align="left" colspan="1" rowspan="1">−90</td><td align="left" colspan="1" rowspan="1">−18</td><td align="left" colspan="1" rowspan="1">4.61</td></tr><tr><td rowspan="2" align="left" colspan="1">2</td><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">2421</td><td align="left" colspan="1" rowspan="1">64</td><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">7.37</td></tr><tr><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">34</td><td align="left" colspan="1" rowspan="1">−28</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">4.93</td></tr><tr><td rowspan="2" align="left" colspan="1">3<hr /></td><td align="left" colspan="1" rowspan="1">Inferior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">2315</td><td align="left" colspan="1" rowspan="1">46</td><td align="left" colspan="1" rowspan="1">−66</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">7.80</td></tr><tr><td align="left" colspan="1" rowspan="1">Occipital Fusiform Cortex<hr /></td><td align="left" colspan="1" rowspan="1">R<hr /></td><td colspan="1" rowspan="1"><hr /></td><td align="left" colspan="1" rowspan="1">22<hr /></td><td align="left" colspan="1" rowspan="1">−88<hr /></td><td align="left" colspan="1" rowspan="1">−8<hr /></td><td align="left" colspan="1" rowspan="1">5.37<hr /></td></tr><tr><td colspan="8" align="left" rowspan="1"><bold>Negatively related to Z-EA scores</bold><hr /></td></tr><tr><td rowspan="3" align="left" colspan="1">1</td><td align="left" colspan="1" rowspan="1">Paracingulate Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">275</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">22</td><td align="left" colspan="1" rowspan="1">48</td><td align="left" colspan="1" rowspan="1">4.11</td></tr><tr><td align="left" colspan="1" rowspan="1">Frontal Pole</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">10</td><td align="left" colspan="1" rowspan="1">62</td><td align="left" colspan="1" rowspan="1">36</td><td align="left" colspan="1" rowspan="1">4.01</td></tr><tr><td align="left" colspan="1" rowspan="1">Paracingulate Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−6</td><td align="left" colspan="1" rowspan="1">44</td><td align="left" colspan="1" rowspan="1">30</td><td align="left" colspan="1" rowspan="1">3.81</td></tr><tr><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">Middle Frontal Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">31</td><td align="left" colspan="1" rowspan="1">36</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">32</td><td align="left" colspan="1" rowspan="1">4.42</td></tr></tbody></table></table-wrap></div></details><details class="inner-accordion"><summary>Table 4 (tbl4) - Significant clusters and their peak activations for the correlation between BOLD-response and the participants&#x27; ratings of the target&#x27;s emotional intensity (threshold-free cluster enhancement pFWE &lt; 0.05).</summary><div class="table-html"><table-wrap id="tbl4" position="float" orientation="portrait"><label>Table 4</label><caption><p>Significant clusters and their peak activations for the correlation between BOLD-response and the participants' ratings of the target's emotional intensity (threshold-free cluster enhancement <italic toggle="yes">p</italic><sub>FWE</sub> &lt; 0.05).</p></caption><alt-text id="alttext0045">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="1">Cluster</th><th rowspan="2" colspan="1">Anatomical region</th><th rowspan="2" colspan="1">Hemisphere</th><th rowspan="2" colspan="1">Cluster size</th><th colspan="3" rowspan="1">MNI coordinates [mm]<hr /></th><th rowspan="2" colspan="1">Peak-level <italic toggle="yes">t</italic></th></tr><tr><th colspan="1" rowspan="1">x</th><th colspan="1" rowspan="1">y</th><th colspan="1" rowspan="1">z</th></tr></thead><tbody><tr><td colspan="8" align="left" rowspan="1"><bold>Positive correlation with participants' emotional intensity ratings</bold><hr /></td></tr><tr><td rowspan="17" align="left" colspan="1">1</td><td align="left" colspan="1" rowspan="1">Posterior Superior Temporal Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">24492</td><td align="left" colspan="1" rowspan="1">58</td><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">9.56</td></tr><tr><td align="left" colspan="1" rowspan="1">Posterior Middle Frontal Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">62</td><td align="left" colspan="1" rowspan="1">−36</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">8.26</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Pole</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">58</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">8.19</td></tr><tr><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−64</td><td align="left" colspan="1" rowspan="1">−14</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">8.05</td></tr><tr><td align="left" colspan="1" rowspan="1">Putamen</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">26</td><td align="left" colspan="1" rowspan="1">−92</td><td align="left" colspan="1" rowspan="1">−6</td><td align="left" colspan="1" rowspan="1">7.4</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Pole</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−56</td><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">−10</td><td align="left" colspan="1" rowspan="1">7.04</td></tr><tr><td align="left" colspan="1" rowspan="1">Middle Frontal Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">48</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">38</td><td align="left" colspan="1" rowspan="1">6.92</td></tr><tr><td align="left" colspan="1" rowspan="1">Middle Temporal Gyrus, temporooccipital part</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">46</td><td align="left" colspan="1" rowspan="1">−56</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">6.49</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Occipital Fusiform Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">−46</td><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">6.06</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Occipital Fusiform Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−46</td><td align="left" colspan="1" rowspan="1">−64</td><td align="left" colspan="1" rowspan="1">−28</td><td align="left" colspan="1" rowspan="1">5.7</td></tr><tr><td align="left" colspan="1" rowspan="1">Insular Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">38</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">−20</td><td align="left" colspan="1" rowspan="1">5.49</td></tr><tr><td align="left" colspan="1" rowspan="1">Middle Temporal Gyrus, temporooccipital part</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−64</td><td align="left" colspan="1" rowspan="1">−44</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">5.31</td></tr><tr><td align="left" colspan="1" rowspan="1">Inferior Frontal Gyrus, pars triangularis</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">56</td><td align="left" colspan="1" rowspan="1">28</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">5.04</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Pole</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−44</td><td align="left" colspan="1" rowspan="1">20</td><td align="left" colspan="1" rowspan="1">−26</td><td align="left" colspan="1" rowspan="1">4.93</td></tr><tr><td align="left" colspan="1" rowspan="1">Occipital Fusiform Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−30</td><td align="left" colspan="1" rowspan="1">−82</td><td align="left" colspan="1" rowspan="1">−18</td><td align="left" colspan="1" rowspan="1">4.87</td></tr><tr><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−40</td><td align="left" colspan="1" rowspan="1">−36</td><td align="left" colspan="1" rowspan="1">10</td><td align="left" colspan="1" rowspan="1">4.84</td></tr><tr><td align="left" colspan="1" rowspan="1">Amygdala</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−18</td><td align="left" colspan="1" rowspan="1">−6</td><td align="left" colspan="1" rowspan="1">−14</td><td align="left" colspan="1" rowspan="1">4.78</td></tr><tr><td rowspan="2" align="left" colspan="1">2</td><td align="left" colspan="1" rowspan="1">Supplementary Motor Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">1735</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">66</td><td align="left" colspan="1" rowspan="1">6.85</td></tr><tr><td align="left" colspan="1" rowspan="1">Anterior Midcingulate Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">38</td><td align="left" colspan="1" rowspan="1">3.74</td></tr><tr><td rowspan="2" align="left" colspan="1">3</td><td align="left" colspan="1" rowspan="1">Precentral Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">1714</td><td align="left" colspan="1" rowspan="1">−40</td><td align="left" colspan="1" rowspan="1">−8</td><td align="left" colspan="1" rowspan="1">56</td><td align="left" colspan="1" rowspan="1">5.64</td></tr><tr><td align="left" colspan="1" rowspan="1">Superior Frontal Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−24</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">72</td><td align="left" colspan="1" rowspan="1">4.96</td></tr><tr><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">Postcentral Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">206</td><td align="left" colspan="1" rowspan="1">−48</td><td align="left" colspan="1" rowspan="1">−26</td><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">4.05</td></tr><tr><td align="left" colspan="1" rowspan="1">5<hr /></td><td align="left" colspan="1" rowspan="1">Putamen<hr /></td><td align="left" colspan="1" rowspan="1">R<hr /></td><td align="left" colspan="1" rowspan="1">173<hr /></td><td align="left" colspan="1" rowspan="1">18<hr /></td><td align="left" colspan="1" rowspan="1">10<hr /></td><td align="left" colspan="1" rowspan="1">6<hr /></td><td align="left" colspan="1" rowspan="1">4.67<hr /></td></tr><tr><td colspan="8" align="left" rowspan="1"><bold>Negative correlation with participants' emotional intensity ratings</bold><hr /></td></tr><tr><td rowspan="7" align="left" colspan="1">1</td><td align="left" colspan="1" rowspan="1">Cuneus Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">10193</td><td align="left" colspan="1" rowspan="1">10</td><td align="left" colspan="1" rowspan="1">−86</td><td align="left" colspan="1" rowspan="1">24</td><td align="left" colspan="1" rowspan="1">6.42</td></tr><tr><td align="left" colspan="1" rowspan="1">Posterior Cingulate Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">−34</td><td align="left" colspan="1" rowspan="1">38</td><td align="left" colspan="1" rowspan="1">5.48</td></tr><tr><td align="left" colspan="1" rowspan="1">Precuneus Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−4</td><td align="left" colspan="1" rowspan="1">−66</td><td align="left" colspan="1" rowspan="1">16</td><td align="left" colspan="1" rowspan="1">5.3</td></tr><tr><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">38</td><td align="left" colspan="1" rowspan="1">−74</td><td align="left" colspan="1" rowspan="1">22</td><td align="left" colspan="1" rowspan="1">5.25</td></tr><tr><td align="left" colspan="1" rowspan="1">Precuneus Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−12</td><td align="left" colspan="1" rowspan="1">−58</td><td align="left" colspan="1" rowspan="1">34</td><td align="left" colspan="1" rowspan="1">4.82</td></tr><tr><td align="left" colspan="1" rowspan="1">Lingual Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">26</td><td align="left" colspan="1" rowspan="1">−52</td><td align="left" colspan="1" rowspan="1">−4</td><td align="left" colspan="1" rowspan="1">4.66</td></tr><tr><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">44</td><td align="left" colspan="1" rowspan="1">−64</td><td align="left" colspan="1" rowspan="1">46</td><td align="left" colspan="1" rowspan="1">3.99</td></tr><tr><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">466</td><td align="left" colspan="1" rowspan="1">−38</td><td align="left" colspan="1" rowspan="1">−70</td><td align="left" colspan="1" rowspan="1">32</td><td align="left" colspan="1" rowspan="1">4.69</td></tr><tr><td align="left" colspan="1" rowspan="1">3</td><td align="left" colspan="1" rowspan="1">Temporal Occipital Fusiform Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">121</td><td align="left" colspan="1" rowspan="1">−24</td><td align="left" colspan="1" rowspan="1">−56</td><td align="left" colspan="1" rowspan="1">−12</td><td align="left" colspan="1" rowspan="1">3.23</td></tr><tr><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">32</td><td align="left" colspan="1" rowspan="1">−50</td><td align="left" colspan="1" rowspan="1">−74</td><td align="left" colspan="1" rowspan="1">26</td><td align="left" colspan="1" rowspan="1">3.77</td></tr></tbody></table></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 30349467</strong> | Pred included: 8 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/30349467/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>30349467_analysis_0</td><td>Leaders only (n = 11)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Leading and following require exchange of socially relevant information (haptic/kinesthetic signaling between partners). The analysis measures communication aspects of interaction (transmission/reception of partner cues), meeting social_communication I1.</td></tr>
<tr><td>30349467_analysis_1</td><td>Followers only (n = 10)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Following depends on reception and interpretation of partner signals (haptic/kinesthetic), which is an instance of social communication (receptive non-facial communication). Meets social_communication I1.</td></tr>
<tr><td>30349467_analysis_2</td><td>Whole-group (n = 18)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast captures bidirectional exchange of movement cues and signaling between partners (leading vs following), reflecting social communication (receptive and productive non-verbal signaling). Meets social_communication I1.</td></tr>
<tr><td>30349467_analysis_3</td><td>Leaders only (n = 11)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Leading involves transmitting signals to the partner and coordinating actions—this is social communication (productive non-facial communication). The leaders-only analysis measures that, meeting social_communication I1.</td></tr>
<tr><td>30349467_analysis_4</td><td>Followers only (n = 10)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Following requires reception and interpretation of partner signals—an instance of social communication (receptive non-facial communication). The followers-only analysis directly assesses this, meeting social_communication I1.</td></tr>
<tr><td>30349467_analysis_5</td><td>Activation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Leader-skill regressions identify areas (insula, STG, dPMC) involved in transmitting/planning signals to a partner—this reflects social communicative aspects of coordinating with another person, meeting social_communication I1.</td></tr>
<tr><td>30349467_analysis_6</td><td>Deactivation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Even deactivations with leader skill relate to aspects of coordinating and signaling in interpersonal interaction, and thus are relevant to social communication (exchange of partner-relevant information). Meets social_communication I1.</td></tr>
<tr><td>30349467_analysis_7</td><td>Leading versus following correlated with skill as a follower.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Follower-skill regressions implicate regions (TPJ, pSTS) involved in perceiving partner signals—this is receptive social communication and meets social_communication I1.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 6186800)</summary><p><strong>Title:</strong> Role-Specific Brain Activations in Leaders and Followers During Joint Action</p><details><summary>Abstract</summary><pre class="paper-text">Much of social interaction in human life requires that individuals perform different roles during joint actions, the most basic distinction being that between a leader and a follower. A number of neuroimaging studies have examined the brain networks for leading and following, but none have examined what effect prior expertise at these roles has on brain activations during joint motor tasks. Couple dancers (e.g., dancers of Tango, Salsa, and swing) are an ideal population in which examine such effects, since leaders and followers of partnered dances have similar overall levels of motor expertise at dancing, but can differ strikingly in their role-specific skill sets. To explore role-specific expertise effects on brain activations for the first time, we recruited nine skilled leaders and nine skilled followers of couple dances for a functional magnetic resonance imaging study. We employed a two-person scanning arrangement that allowed a more naturalistic interaction between two individuals. The dancers interacted physically with an experimenter standing next to the bore of the magnet so as to permit bimanual partnered movements. Together, they alternated between leading and following the joint movements. The results demonstrated that the brain activations during the acts of leading and following were enhanced by prior expertise at being a leader or follower, and that activity in task-specific brain areas tended to be positively correlated with the level of expertise at the corresponding role. These findings provide preliminary evidence that training at one role of a joint motor task can selectively enhance role-related brain activations.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Much joint action between two people involves the contrastive roles of leader and follower ( ). For example, when two people move a sofa, the front person is often the one who navigates the joint movement as well as the one who determines the speed at which the two people move, while the back person responds to these movement-cues and attempts to coordinate his/her actions with the front person. However, the experimental literature that examines joint action in the lab does not give consideration to individual differences, for example the fact that people may be predisposed toward being a leader or follower based on their personality traits or life experiences ( ). In typical studies of joint action, people are randomly assigned to being a leader or follower (or both) of a joint task without assessing individual differences in task expertise that may exist between them. This applies to studies of both experts ( ;  ) and non-experts ( ;  ;  ;  ). This may be problematic since many studies demonstrate that expertise has an effect on behavioral performance and brain activations across many domains ( ;  ;  ). 

An interesting solution to this problem is to examine couple dancers, such as Tango dancers, since such people engage in extensive training to develop expertise at one specific role in the dance, thereby making the assessment of leading/following experience on joint action quite feasible. Leaders and followers of a couple dance have similar overall levels of motor expertise at dancing, but they can differ strikingly in their role-specific skill sets, such that dancers of one role are often unable to dance the opposite role. This applies not merely to the movement patterns themselves, but to the   coordination   skills required for leading (e.g., force conveyance) and following (e.g., responsiveness to force cues). While previous neuroimaging studies have looked separately at the topics of leading/following and expertise, the current study–which is a follow-up analysis to a previously published study from our lab ( )–represents a first attempt at examining   role-specific expertise   at leading and following, doing so using trained leaders and followers of couple dances. The principal aim of the study is to identify role-specific brain activations, namely leading-related activations in trained leaders compared to non-leaders, and following-related activations in trained followers compared to non-followers. 

Previous studies of leading and following have tended to emphasize the networks for leading, more so than those for following. For example, studies of interactive imitation have compared the initiation and imitation of visual actions within the same group of participants, and have highlighted an initiation network involved in self-monitoring, willed action, and decision making ( ;  ;  ;  ). Studies of auditory-entrainment tasks, such as finger tapping, have studied expert leaders or individuals who spontaneously emerge as leaders with in the context of the study, and have similarly identified a network involved in decision making, movement initiation, and self-processing ( ;  ;  ). These studies have provided either no results or inconsistent findings regarding following or expert followers. In a previous publication from our lab ( ), we characterized the networks for leading and following during a joint-action task with physical interaction, using the same dancer participants as those employed in the present study. In accordance with the previous literature, we found that leading showed a motor- and self-oriented profile, engaging areas associated with motor planning, spatial navigation, sequencing, action monitoring, and error correction. In contrast, following showed a far more sensory- and externally oriented profile, revealing areas involved in somatosensation, proprioception, motion tracking, social cognition, and outcome monitoring. However, while that study compared the act of leading with the act of following, it did not assess the influence of prior expertise at being a leader or follower on the brain activations. That was the major objective of the current follow-up analysis, namely to examine role-specific expertise. 

It is well-established that expertise can influence both the structure and function of the brain. There is now a vast literature devoted to various forms of motor, perceptual, and cognitive expertise (reviewed in  ). A general finding of such studies is that brain activations and gray matter volume are enhanced in experts, as compared to non-experts, in areas that process the skills that underlie a person’s domain of expertise ( ;  ;  ). For example, with regard to perceptual tasks, trained musicians and other auditory experts show enhanced effects in auditory cortex ( ,  ;  ;  ), while visual experts show effects in visual cortex ( ,  ;  ). In the motor domain, effects are found in cortical and subcortical motor and premotor areas involved in motor execution, control, planning, and representation ( ;  ). Motor experts, such as athletes, dancers, and musicians, additionally demonstrate changes in perceptual and cognitive areas associated with their trained skills ( ). For example, sensorimotor coupling is enhanced in musicians and athletes ( ;  ). In addition, activations in the action-observation network [including premotor cortex (PMC), superior parietal lobule (SPL), and inferior parietal lobule (IPL)] are enhanced when dancers view specific dance patterns that they are expert in ( ), or when athletes view sports actions that they are expert in ( ), as compared to when the same people view dances or sports movements that they are not trained in.   suggested that this effect was due to motor training, rather than the associated perceptual training. Expertise, in addition to producing enhancements in processing, has also been linked to decreases in the overall number of activated foci in neuroimaging studies, especially in attentional and cognitive-control networks, suggesting an enhancement in automaticity of processing for the trained skill ( ;  ). The “two stage expertise hypothesis” ( ;  ) suggests that short-term training leads to enhancements of brain activations for the trained skill, while long-term training and skill mastering lead instead to decreases or reorganizations in brain activations. 

While previous neuroimaging studies have looked at leading/following and expertise in isolation, no study thus far has combined the two issues, which is the principal objective of the present study. As mentioned above, couple dancers are an ideal cohort for exploring role-specific expertise in leading and following, since they spend many years developing expertise at typically just one of the two roles of the dance. As a result, expert leaders are usually unskilled followers, and vice versa, while both groups have comparable levels of overall motor expertise at the dance. More specifically, leader expertise during couple dancing requires the generation of a motor plan for both the self and the partner, and the efficient conveyance of signals to the partner, while follower expertise requires the tracking of information coming from the leader and its interpretation to construct either an identical or complementary movement pattern in real time. 

In order to assess the effect of role expertise on brain activations during an ecologically valid joint-action task, we carried out an exploratory follow-up analysis to our previous publication that looked at leading and following ( ) in order to examine the effects of role-specific expertise on brain activations. In the previous study, skilled leaders and followers of couple dances performed both a leading and following task in a magnetic resonance imaging (MRI) scanner in interaction with an experimenter standing next to the bore of the magnet. The participant and experimenter were in physical contact at their hands, and alternated between being the leader and follower of joint improvised bimanual movements. The principal aim of the study was to compare brain activity during the acts of leading and following. The current study follows up on those results using the same dataset in order to examine the effects of individual differences on the brain activations, in particular an individual’s expertise at a given role of the dance. The aim was to look for role-specific brain activations, in other words leading-related activations in trained leaders compared to non-leaders (i.e., followers), and following-related activations in trained followers compared to non-followers (i.e., leaders). Based on the literature cited above demonstrating that experts show enhancements in task-specific brain areas compared to non-experts when performing the same tasks, we predicted that leaders, as compared with non-leaders, would show an enhancement of leading-related activations when leading (only), and likewise that followers, as compared with non-followers, would show an enhancement of following-related activations when following (only). Given that we were not able to effectively rule out the influence of gender on dance role in our design, the results need to be viewed as exploratory. 


## Materials and Methods 
  
### Participants 
  
Eighteen participants (nine of each gender) took part in this study after giving their written informed consent in accordance with the Hamilton Integrated Research Ethics Board, who approved the study (St. Joseph’s Healthcare, R. P. #12-3777). They received monetary compensation for their participation. None of them had a past history of neurological or psychiatric disease. An inclusion criterion for the study was that participants have at least 2 years of experience at one or more kinds of couple dances involving leading and following (e.g., Argentine Tango, Salsa, Swing, and Ballroom). Male participants (40.7 ± 14.9 years old) had a mean dance experience of 8.7 ± 7.2 years, principally as leaders, although one male had significant experience as a follower as well. Female participants (40.2 ± 12.3 years old) had a mean dance experience of 5.6 ± 2.9 years, principally as followers, although two females had significant experience as leaders as well. 

On the day of the experiment, participants reported their ability to lead or follow a couple dance using a scale from 0 to 100, where 0 corresponds to no expertise at leading or following, and 100 corresponds to a very high level of expertise at leading or following. Each person did separate ratings for leading and following skill, with results shown in   Figure   . We explained to participants that these scales emphasized the ability to transmit/receive information while dancing with a partner, rather than the ability to perform complex or stylistic movements. Males reported a mean leading ability of 69.8 ± 17.7 (one male was at 35 and the rest ranged from 60 to 90). Likewise, females reported a mean following ability of 77.2 ± 8.3 (ranging from 70 to 90). With regards to the complementary skill, males reported a mean following ability of 33.7 ± 21.6; the male with significant following experience reported his following ability at 78, while all the others males rated it at between 8 and 50. Females reported a mean leading ability of 28.9 ± 25.2; both females with significant leading experience reported their leading ability at 70, while all other females rated themselves at between 5 and 40. Correlations between leading ability, following ability, years of experience at dancing, and age showed that leading ability, but not following ability, correlated with the number of years of experience (  Table   ). Anecdotal evidence suggests that leading skill requires a greater amount of time and effort to achieve than does following skill, which may explain the exclusive correlation of leading skill with years of experience. Since leading and following ability were not anti-correlated in the analysis, participants designated as “leaders” in this study were comprised of all the participants who were primarily trained as leaders for at least 2 years (i.e., all the of males) plus the two participants who, although primarily trained as followers, had significant leading experience and a strong leading ability (two females). Those designated as “followers” were comprised of all the participants who were primarily trained as followers for at least 2 years (i.e., all of the females) plus one participant who, although primarily trained as leader, had significant following experience and a strong following ability (one male). Thus, three participants belonged in both groups. This division was used in only the first set of analyses (see below). 
  
Self-report scales for skills as a leader and follower of couple dances. The   x  -axis shows the self-rating scale for leader skill (left panel) and follower skill (right panel) for couple dancing, where 100 is the highest rating. The   y  -axis of each graph shows the number of participants, from the pool of 18, who rated themselves at the various levels of skill for each role. Female participants are color-coded red and males are color-coded blue, both here and in   Figures  ,   . Participants designated as “leaders” in this study were comprised of all the males plus the two females with strong leading ability, while those designated as “followers” were comprised of all the females plus the male with strong following ability. Leaders are color coded as purple here and in   Figures  –  , whereas followers are color coded as pink here and in   Figures  –   (not to be confused with the color coding of gender). 
    
Correlation between age, years of couple-dance experience, and self-reported leading and following skill. 
    

### Procedure 
  
While the participant was lying supine in the MRI scanner, an experimenter (LASC) stood next to the bore of the scanner in order to have physical contact with the participant’s two hands. The participant’s forearms were fastened to the side of their body such that only their wrists, hands and fingers were able to move. Participants’ hands (palms up) were always below the experimenter’s hands (palms down), so that the participants’ hands could not be passively moved. The experimenter had significant experience both as a follower and a leader of couple dances. Together, the participant and experimenter performed highly controlled joint hand movements in all three planes of motion, alternating between leading and following the joint movement during different task-epochs of the scan. The movement patterns were improvised, rather than pre-learned, in order to maintain an ongoing requirement for motor planning during leading and a comparably heightened sense of responsiveness during following. No external cuing of tempo or rhythm was done with a metronome or with music. Participants performed all conditions with their eyes closed, and were instructed about which task to perform by means of pre-recorded verbal cues delivered through MRI-compatible headphones. Each condition was performed in a random order six times in blocks of 28 s. 

Complete methods and details concerning fMRI acquisition and image analysis, including participant training, are described in  . Briefly, the functional MRI imaging parameters were 2000 ms TR, 35 ms TE, 90° flip angle, 39 axial slices, 4 mm slice thickness, 0 mm gap, 3.75 × 3.75 mm in-plane resolution, 64 × 64 matrix, and 240 mm field of view. An automatic shimming procedure was performed before each scan to minimize inhomogeneities in the static magnetic field. For each of the three functional scans, 216 volumes–corresponding to 12 epochs of 28 s task + 8 s rest–were collected over 7’12”, resulting in a total of 648 volumes. Two magnetic field maps (5 ms then 8 ms TE) with the same imaging parameters as the fMRI were acquired in order to unwarp the data. Unwarping was performed with the relaxation method of “anatabacus”, a plugin in BrainVoyager, in order to correct for non-rigid deformations. In addition, the head-motion parameters were included as nuisance regressors in the analysis. Functional and structural images were processed using BrainVoyager QX 2.8. Coordinate tables were computed using NeuroElf. 


### Analysis 
  
We first performed qualitative analyses on three groups to assess if there were any differences between being a leader and being a follower. Specifically, we carried out three random-effects analyses for the bidirectional contrast “Leading versus Following” (1) for the whole group of 18 participants, (2) for the 11 leaders only, and (3) for the 10 followers only. These were performed at a two-tailed statistical threshold of   p   &lt; 0.005 uncorrected with a cluster-level correction of   k   = 28 voxels determined with Alphasim (family-wise error   p   &lt; 0.05) in NeuroElf. The conjunction of [Leading &gt; Rest] ∩ [Following &gt; Rest] was also performed on these three groups in order to serve as a reference for the general network of brain areas activated by the movement tasks, irrespective of role. It was performed at a two-tailed statistical threshold of   p   &lt; 0.005 uncorrected with a cluster-level correction of   k   = 49 voxels determined with Alphasim. 

Since qualitative differences were found (see Results section), we tested further for the effect of role by performing whole-brain regression analyses on the full group of participants (  n   = 18). We chose to perform statistical regression analyses instead of a direct statistical comparison between leaders and followers for two reasons. First, we consider role expertise to be a continuous trait, rather than a dichotomous one. Dancers can belong to both groups if they are trained at both leading and following. Thus a binary distinction would have led to a “male versus female” contrast, rather than a “leader versus follower” contrast. Second, the number of participants in each group was small (  n   = 10 and 11 for leaders and followers, respectively), whereas the regression involved the full group of 18 participants. Because of the small number of participants in the analysis and because of the small number of female leaders and male followers in the cohort, we consider this an exploratory study. Future studies will need to examine larger numbers of participants who have both leading and following skills, although such dual training tends to be limited to professional teachers of a dance. 

For the whole-brain regression analyses, the self-reported values of leading and following skill were used as covariates in two separate analyses to regress the betas values of the contrast “Leading versus Following”. These regressions were also performed at a two-tailed statistical threshold of   p   &lt; 0.005 uncorrected with a cluster-level correction of   k   = 25 voxels, determined with Alphasim. However, this threshold led to null results, and so we reported the activation at a less stringent threshold of   p   &lt; 0.025 uncorrected with a cluster-level correction of k = 46 voxels, determined with Alphasim. We note that these results should be interpreted with caution and need to be replicated in future analyses. In order to examine the influence of gender, the mean beta value of each activated cluster was extracted for each participant and regressed against his/her corresponding leading or following skill. 



## Results 
  
In order to identify the basic sensorimotor network involved in performing our joint bimanual tasks, we carried out the conjunction of [Leading &gt; Rest] ∩ [Following &gt; Rest], with results shown in   Figure    and Talairach coordinates reported in   Table   . This shared network between leading and following consisted of a widespread sensorimotor cortical (primary motor and somatosensory cortex) and subcortical (thalamus and cerebellum) network, as well as the supplementary motor area (SMA), midcingulate cortex (MCC), SPL, inferior frontal gyrus (IFG), IPL (including the secondary somatosensory cortex [SII] and extending to the insula), and inferior temporal gyrus (ITG), extending to the middle temporal gyrus (MTG). Except for the ITG, which was present in leaders only, this network was found in both leaders and followers. 
  
Shared network for leading and following. The figure shows the results of the conjunction [Leading &gt; Rest] ∩ [Following &gt; Rest] in leaders only (left panel) and followers only (right panel),   p   &lt; 0.005 uncorrected (  k   = 49 voxels). With the exception of the inferior temporal gyrus, the activated network is similar in both followers and leaders. CB, cerebellum; IFG, inferior frontal gyrus; IPL, inferior parietal lobule; ITG, inferior temporal gyrus; MCC, middle cingulate cortex; SMA, supplementary motor area; SMC, sensorimotor cortex; SPL, superior parietal lobule; and Th., thalamus. 
    
The shared network for leading and following. 
    
We next wanted to explore our question of interest, namely whether there was evidence for role-specific activations, in other words activations found only in skilled individuals while performing the role they are trained in. This would reveal whether leaders and followers engage different brain resources during leading and following. As shown in   Figure    and   Table   , we first qualitatively compared three types of analyses of the “Leading &gt; Following” contrast (cyan clusters) and “Following &gt; Leading” contrast (yellow clusters): the whole group of 18 participants; only the leaders (a subset of 11 participants); and only the followers (a subset of 10 participants). Overall, the leaders-only analysis showed basically the same network for leading as the whole group, but no brain areas for following. Likewise, the followers-only analysis showed basically the same network for following as the whole group, but only the dorsolateral prefrontal cortex (DLPFC) for leading (Note that only role-specific activations are labeled in the   Figure   ). 
  
Role-specific brain activations. The figure shows an analysis of the bidirectional “Leading versus Following” contrast in three groupings: the whole group of 18 participants; only the leaders (a subset of 11 participants); and only the followers (a subset of 10 participants). Contrasts are performed at   p   &lt; 0.005 uncorrected (  k   = 28 voxels). The top panel is the midsagittal view, the lower left panel is the left hemisphere, and the lower right panel is the right hemisphere. Each panel is set up as a triad, with the whole group at the top and the restricted analyses of leaders-only and followers-only below that. Cyan clusters and outlines reflect the contrast of “Leading &gt; Following”, whereas yellow clusters and outlines reflect the reverse contrast of “Following &gt; Leading”. In order to facilitate the visualization of role-specific activations, we use colored outlines to represent whole-group activations that are missing in either the leaders-only or the followers-only analyses. More specifically, cyan outlines are regions of whole-group activation that are present in the leaders-only analysis, but not the followers-only analysis, while yellow outlines are regions of whole-group activation that present in the followers-only analysis, but not the leaders-only analysis. The leaders-only analysis shows the same network for leading as the whole group, but no brain areas for following. The followers-only analysis shows the same network for following as the whole group, but only the cerebellum and dorsolateral prefrontal cortex for leading. Only role-specific activations are labeled in this figure. Leading network: CMA: cingulate motor area; PMC, premotor cortex; SMA, supplementary motor area; and SPL, superior parietal lobule. Following network: PCC, posterior cingulate cortex and TPJ, temporo-parietal junction. 
    
Leading versus following in the whole group, the leader-only group, and the follower-only group. 
    
Regarding the leading task, role-specific activations that were found exclusively in skilled leaders (cyan activations in   Figure    in both the leaders-only and whole-group brains that correspond with the cyan outlines in the followers-only brain) were observed in the SMA and cingulate motor area (CMA; top panel), SPL (right and left hemispheres in the lower panels), and PMC(left hemisphere). In addition, while leading, leaders showed a more extended premotor activation than the whole-group, especially in the right hemisphere (  Table   ). 

Regarding the following task, role-specific activations that were found exclusively in skilled followers (yellow activations in   Figure    in both the followers-only and whole-group brains that correspond with the yellow outlines in the leaders-only brain) were observed in the posterior cingulate cortex (PCC; top panel), temporo-parietal junction (TPJ; right and left hemispheres in the lower panels), and parahippocampal cortex (PHC, not shown). In addition, while following, followers showed activity in the posterior superior temporal sulcus (pSTS) that was not present in the whole group (  Table   ). To summarize, the networks associated with leading and following seemed to be more strongly engaged by experts at the corresponding role than non-experts at that role. 

We followed up on these qualitative analyses with whole-brain regressions in which the self-reported expertise at being a leader or follower (see   Figure    above) was used as the covariate for the contrast of leading versus following. Activations for these analyses were only found at a more lenient threshold, but are still reported since they are consistent with both our hypotheses and the qualitative analyses reported above. However, the results should be interpreted with caution.   Figure    shows the regressions with leader skill, and   Figure    shows the regressions with follower skill. The regions where activations during the leading task correlated with leader skill included the SMA, pre-SMA, dorsal PMC (dPMC), superior temporal gyrus (STG), and insula (  Figure    top panel,   Table   ). The regions where activations during the following task correlated with follower skill include the PCC, TPJ, pSTS, and mPFC (  Figure    top panel,   Table   ). For each cluster, the coefficient of determination (  R  ) of the regression of the mean beta value against leader and follower skill is shown in   Tables  ,   , respectively. 
  
Regression of brain activation with leader skill in the whole group of participants. The top panel of this figure shows brain activity that correlates with the contrasts “Leading &gt; Following” (cyan activations) and “Following &gt; Leading” (yellow activations). Contrasts are performed at   p   &lt; 0.02 uncorrected (  k   = 25 voxels). Brain areas for “Leading &gt; Following” that correlate with leader skill include the dorsal premotor cortex (dPMC), insula (Ins.), superior temporal gyrus (STG), and supplementary motor area (SMA). Almost no areas for the contrast “Following &gt; Leading” correlate with leader skill (see   Table   ). The lower plots show mean beta values extracted from the SMA, dPMC, posterior insula and STG against leader skill, where female participants are shown with red dots and male participants with blue dots. Activity for leading increased with increasing leader skill, and this seems to be independent of gender. 
    
Regression of brain activation with the follower skill in the whole group of participants. The top panel of this figure shows brain activity that correlates with the contrasts “Leading &gt; Following” (cyan activations) and “Following &gt; Leading” (yellow activations). Contrasts are performed at   p   &lt; 0.02 uncorrected (  k   = 25 voxels). Brain areas for “Following &gt; Leading” that correlate with follower skill include the medial prefrontal cortex (mPFC), posterior parietal cortex (PCC), posterior superior temporal sulcus (pSTS), and temporo-parietal junction (TPJ). No areas appeared for the contrast “Leading &gt; Following” correlate with follower skill (see   Table   ). The lower plots show mean beta values extracted from the TPJ, PCC, mPFC, and pSTS against follower skill, where female participants are shown with red dots and male participants with blue dots. Activity for following increased with increasing follower skill, and this seems to be independent of gender. 
    
Leading versus following correlated with skill as a leader. 
      
Leading versus following correlated with skill as a follower. 
    
Examples of how the mean beta value in these regions covaries with leader and follower skill are shown in the bottom panels of   Figures  ,   , respectively. The results provide some evidence that activity in these regions might depend on the level of expertise. However, they in no way rule out a gender effect, either alone or in interaction with expertise, and so the results have to be seen as preliminary. In the dPMC and STG (  Figure   , bottom panels), activity for the contrast of “Leading &gt; Following” increased with leader skill, but a male with low leader skill had a low activity, whereas females with high leader skill had a high activity. Other areas that correlated with leader skill had the same trend (not shown). Similarly, in the mPFC and TPJ (  Figure   , bottom panels), activity for the contrast “Leading &gt; Following” decreased with follower skill (that is, “Following &gt; Leading” activity increased with follower skill), but a male with high follower skill had a low activity, similar to females with high follower skill. Other areas that correlated with follower skill had the same trend (not shown). Future studies will be needed to fully exclude the influence of gender on the expertise effects observed here. Hence, the current study must be seen as a pilot study that gives a first glimpse at role-specific expertise effects without being able to effectively factor out the influence of gender. 


## Discussion 
  
This current exploratory study examined for the first time the effect of expertise at the coordinative skills involved in leading and following on brain activations during a joint-action task in a realistic setting. Its results provide support for the existence of role-specific brain activations during joint actions. In particular, we observed that leading-related activations were enhanced in leaders compared to followers when both groups performed the leading task, and that following-related activations were enhanced in followers compared to leaders when both groups performed the following task. Additionally, we showed that leading-related brain regions in the whole group of participants tended to correlate with expertise at being a leader, whereas following-related brain regions tended to correlate with expertise at being a follower. Another way of conceptualizing these results is that the skilled leaders hardly engaged any areas during following that were not already engaged during leading; likewise, the skilled followers hardly engaged any areas during leading that were not already engaged during following. This might explain the null results found in some previous studies when comparing following with leading ( ). These results suggest that expertise at one role of a joint-action task can enhance brain activations for the trained role compared to the untrained role. Hence, not only do the results support the existing literature on expertise effects for motor tasks, but they extend it for the first time to the contrastive roles of leader and follower in joint actions. 

The major finding of the initial qualitative analysis (  Figure   ) was that the brain networks that we observed for leading and following in the whole group seemed to be mainly supported by prior experience at being a leader or follower. In particular, skilled followers strongly engaged the mentalizing and social networks (PCC, TPJ, and STS) while following, which is consistent with a view of following as a process of adapting to one’s partner or as inferring knowledge from one’s partner ( ;  ;  ;  ;  ;  ). In contrast, skilled leaders strongly engaged networks for motor control and planning (SMA, CMA, PMC, and cerebellum) and for spatial navigation and exploration (SPL) while leading, which is consistent with the requirements of the leading role ( ;  ;  ;  ;  ). Interestingly, both skilled leaders and skilled followers activated the DLPFC during leading, which implies that self-initiation and action selection ( ;  ;  ) are probably the most important characteristics of leading, regardless of expertise. 

By performing whole-brain regressions with leading or following skill, we treated being a leader or follower as a continuous trait, rather than a dichotomous one. Although we did not find any activity using our   a priori   threshold, the activations observed at a more lenient threshold were consistent with both our hypotheses and the qualitative results, and are thus reported as exploratory findings. We observed that distinct brain areas tended to correlate with the level of self-reported expertise at being a leader or a follower, respectively. The areas that correlated with follower skill were principally components of the following network, such as the mPFC, PCC, TPJ, and pSTS. Thus, the more that someone is trained at following, the more that s/he will recruit brain regions of the mentalizing and social networks, which might indicate more attention to, or more efficient processing of, social stimuli (i.e., cues coming from the leader) and the mental states of others (i.e., their intentions and action plans). Another characteristic of followers is their ability to track their partner’s movements or other signaling cues so as to produce either imitative or complementary movements. Along these lines, the pSTS has been specifically implicated in the multisensory perception of biological motion ( ;  ), indicating that a trained follower might be specialized in analyzing information coming from the partner’s movement, not least haptic information emanating from body contact ( ). 

In contrast to this profile for following, the areas that tended to correlate with leader skill were mainly part of the leading network, including premotor areas (pre-SMA, SMA, and PMC). Other areas that tended to correlate with leader skill were the insula and STG. This network is quite similar to the one shown to be activated by motor experts in the meta-analysis of  . In addition, all of the areas associated with leader skill in the present study have been previously shown to be involved in improvisation ( ). Since leading requires the ability to improvise movements, we can assume that the better a person is at leading, the better s/he can improvise a motor plan for both the self and the partner, and thus the more s/he recruits premotor areas and the STG. However, it has also been shown that improvisational expertise (in musicians, for example) is related to a deactivation in the DLPFC, TPJ, IFG, and insula ( ;  ), which has been interpreted as indicating an automation of cognitive processing and a greater focus on internal processes during improvisation ( ). The absence of deactivations in these regions in our study can potentially be explained by the fact that our use of a joint task may have precluded the adoption of an internal focus by the participants when leading. Indeed, a study of joint improvisation also found an activation increase in the DLPFC, pre-SMA, and STG ( ), which is quite similar to a situation of improvising with a dance partner when leading. 

Overall, the study integrates two issues in the cognitive neuroscience of motor performance, first the contrast between leading and following, and second the influence of individual differences in motor expertise on brain activations. As mentioned in the Introduction, many experimental studies of joint action randomly assign people to being a leader or follower of a joint task ( ;  ;  ;  ;  ). However, in Western dance culture, people are generally assigned these roles based on their gender, with men tending to be assigned the role of leader in couple dances. Thus, in contrast to a study of piano duetting ( ), for example, people come to a dance study like ours with years of experience at just one role of the joint task. This provides us with the unique ability to examine individual differences in joint action based not on random factors but on role-specific training. Previous studies of expertise processing have demonstrated enhanced brain activations in experts compared to non-experts ( ,  ;  ,  ;  ;  ;  ;  ;  ;  ;  ). However, this has often has been investigated using non-motor tasks, even in motor experts like professional ballet dancers ( ). We have instead probed this using a motor task, with the added benefit of doing this using a joint-action task. The integration of these two issues is that we were able to examine the contrast between leading and following–as per studies of joint action–but to incorporate the factor of prior motor experience, as per studies of expertise processing. The results revealed a clear overlap between these two issues, such that the brain activations during the acts of leading and following were enhanced by prior expertise at being a leader or follower, and that activity in task-specific brain areas tended to be positively correlated with the level of expertise at the corresponding role. In other words, we were able to demonstrate   role-specific enhancements   in brain activation. 

### Limitations 
  
Given that this study was a first attempt to examine the effect of role expertise on brain activations during joint action, we are aware that it has a number of significant limitations. First, we were limited in our ability to measure behavioral performance during task production in the scanner due to an absence of MRI-compatible technologies such as motion capture at our imaging center. Thus, we cannot determine if the differences between leaders and followers seen in the study are due to trait-related differences in activation or behavioral differences as well. The joint-action task performed in this study was quite simple and involved very small hand movements. Hence, it did not require any type of specialized skill, which would foster similar performance in the two groups. In addition, the experimenter was the sole interaction partner for all of the participants in the study and was thus a controlled factor in the interaction. However, the absence of a technology like motion capture means that we are unable to rule out behavioral differences between participants as a source of the results. Further research taking advantage of MRI-compatible technologies will be required to explore this issue. 

Second, the qualitative analyses showed an interesting pattern that was confirmed by the whole-group regression at a more lenient threshold, but not at a standard threshold. Hence, the effects seem to be small. Although the observed activations at the less stringent threshold were consistent with our expectations based on previous studies, the results of this study should be taken with caution and need to be replicated, preferentially with a larger cohort and a wider spread of skill levels. In addition, the skill levels that were used to regress the brain data were self-report data. They might thus have been subject to self-report biases and inaccuracies. However, no objective measure of leadership and/or followership skills exists in the literature. Given the preliminary results of this study, it would be worthwhile to develop such measures in future. Such measures could be used to see if the results of the present study could be replicated based on people’s role expertise in some other motor skill outside of dancing, or even on people’s natural predispositions to be a leader or follower, as related to personality traits and life experiences, rather than the specialized skill of dance training. 

Finally, and importantly, we are unable to rule out gender as a factor in determining the role-specific effects in our study, and hence the results need to be seen as quite preliminary. While the leader and follower groups were not exclusively of one gender, they did have a majority of one gender. Given the evidence for gender effects on a diversity of perceptual, cognitive, and motor tasks ( ;  ;  ;  ;  ), further studies will be required to assess a gender contribution to our results with trained couple dancers. Given the paucity of female leaders and male followers in the world of couple dancing, perhaps the only approach that will be able to address the limitations of the current study is a training study. A study that crosses gender with role during a several-month training program of leading or following for some joint-action task could permit a disentangling of the relative effects of gender and expertise. If female leaders and male followers showed the same role-specific effects as in the current study, this would argue against a gender interpretation in favor of expertise   per se  . Such a study could also reveal potential gender effects as well. 



## Conclusion 
  
This study is the first to look at the influence of prior individual training at being a leader or follower on the brain activations occurring during the acts of leading and following, thereby assessing the effect of role expertise during naturalistic joint action. Our major finding was that leaders and followers do not seem approach leading and following in the same way at the neural level, with leaders engaging more brain resources during leading, and followers during following, thus reflecting role-specific activations. Additionally, we showed that activity in leading-related brain regions tended to correlate with expertise at being a leader, and likewise that activity in following-related brain regions tended to correlate with expertise at being a follower. These findings highlight the fact that the acts of leading and following might be skill-specific, and thus that prior experience at these roles should be assessed when studying leading and following during joint action. However, given our inability to disentangle gender from dance role, the current results must be seen as preliminary. A training study that crosses gender with role will probably be required to truly distinguish dance role from gender. 


## Author Contributions 
  
LC ran the experiment and analyzed the data. LC and SB conceived the experiment, analyzed the results, and wrote the manuscript. 


## Conflict of Interest Statement 
  
The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (4)</summary><details class="inner-accordion"><summary>Table 2 (T2) - The shared network for leading and following.</summary><div class="table-html"><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><p>The shared network for leading and following.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="center" colspan="5" rowspan="1">Leaders only (<italic toggle="yes">n</italic> = 11)</th><th valign="top" align="center" colspan="5" rowspan="1">Followers only (<italic toggle="yes">n</italic> = 10)</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">Area</th><th valign="top" align="left" rowspan="1" colspan="1">BA</th><th valign="top" align="left" rowspan="1" colspan="1">Hemisphere</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">SMC</td><td valign="top" align="left" rowspan="1" colspan="1">1,3,4,5,6,40</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">33</td><td valign="top" align="center" rowspan="1" colspan="1">-40</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">1356</td><td valign="top" align="center" rowspan="1" colspan="1">17.01</td><td valign="top" align="center" rowspan="1" colspan="1">30</td><td valign="top" align="center" rowspan="1" colspan="1">-37</td><td valign="top" align="center" rowspan="1" colspan="1">61</td><td valign="top" align="center" rowspan="1" colspan="1">1149</td><td valign="top" align="center" rowspan="1" colspan="1">16.54</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SMC</td><td valign="top" align="left" rowspan="1" colspan="1">2,3,4,5,6,40</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-39</td><td valign="top" align="center" rowspan="1" colspan="1">-43</td><td valign="top" align="center" rowspan="1" colspan="1">52</td><td valign="top" align="center" rowspan="1" colspan="1">1419</td><td valign="top" align="center" rowspan="1" colspan="1">17.67</td><td valign="top" align="center" rowspan="1" colspan="1">-39</td><td valign="top" align="center" rowspan="1" colspan="1">-37</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">1135</td><td valign="top" align="center" rowspan="1" colspan="1">18.19</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">IFG</td><td valign="top" align="left" rowspan="1" colspan="1">6,13,44</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">54</td><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">19</td><td valign="top" align="center" rowspan="1" colspan="1">174</td><td valign="top" align="center" rowspan="1" colspan="1">5.86</td><td valign="top" align="center" rowspan="1" colspan="1">54</td><td valign="top" align="center" rowspan="1" colspan="1">8</td><td valign="top" align="center" rowspan="1" colspan="1">7</td><td valign="top" align="center" rowspan="1" colspan="1">108</td><td valign="top" align="center" rowspan="1" colspan="1">8.05</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SMA/MCC</td><td valign="top" align="left" rowspan="1" colspan="1">6, 24, 31</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">-13</td><td valign="top" align="center" rowspan="1" colspan="1">52</td><td valign="top" align="center" rowspan="1" colspan="1">610</td><td valign="top" align="center" rowspan="1" colspan="1">14.68</td><td valign="top" align="center" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">-22</td><td valign="top" align="center" rowspan="1" colspan="1">49</td><td valign="top" align="center" rowspan="1" colspan="1">683</td><td valign="top" align="center" rowspan="1" colspan="1">12.14</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">IPL/SII</td><td valign="top" align="left" rowspan="1" colspan="1">13,40,41</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">-28</td><td valign="top" align="center" rowspan="1" colspan="1">25</td><td valign="top" align="center" rowspan="1" colspan="1">294</td><td valign="top" align="center" rowspan="1" colspan="1">7.26</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="center" rowspan="1" colspan="1">-34</td><td valign="top" align="center" rowspan="1" colspan="1">31</td><td valign="top" align="center" rowspan="1" colspan="1">108</td><td valign="top" align="center" rowspan="1" colspan="1">6.82</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">IPL/SII</td><td valign="top" align="left" rowspan="1" colspan="1">13, 22,40</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-51</td><td valign="top" align="center" rowspan="1" colspan="1">-28</td><td valign="top" align="center" rowspan="1" colspan="1">19</td><td valign="top" align="center" rowspan="1" colspan="1">240</td><td valign="top" align="center" rowspan="1" colspan="1">9.52</td><td valign="top" align="center" rowspan="1" colspan="1">-48</td><td valign="top" align="center" rowspan="1" colspan="1">-34</td><td valign="top" align="center" rowspan="1" colspan="1">19</td><td valign="top" align="center" rowspan="1" colspan="1">314</td><td valign="top" align="center" rowspan="1" colspan="1">9.36</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SPL</td><td valign="top" align="left" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">24</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">197</td><td valign="top" align="center" rowspan="1" colspan="1">7.06</td><td valign="top" align="center" rowspan="1" colspan="1">24</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">166</td><td valign="top" align="center" rowspan="1" colspan="1">6.52</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SPL</td><td valign="top" align="left" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-24</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">453</td><td valign="top" align="center" rowspan="1" colspan="1">10.52</td><td valign="top" align="center" rowspan="1" colspan="1">-27</td><td valign="top" align="center" rowspan="1" colspan="1">-55</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">268</td><td valign="top" align="center" rowspan="1" colspan="1">9.76</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ITG</td><td valign="top" align="left" rowspan="1" colspan="1">37</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">51</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-8</td><td valign="top" align="center" rowspan="1" colspan="1">117</td><td valign="top" align="center" rowspan="1" colspan="1">6.45</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ITG</td><td valign="top" align="left" rowspan="1" colspan="1">19, 37</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-48</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">93</td><td valign="top" align="center" rowspan="1" colspan="1">6.30</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Thalamus</td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">15</td><td valign="top" align="center" rowspan="1" colspan="1">-25</td><td valign="top" align="center" rowspan="1" colspan="1">10</td><td valign="top" align="center" rowspan="1" colspan="1">105</td><td valign="top" align="center" rowspan="1" colspan="1">7.03</td><td valign="top" align="center" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">16</td><td valign="top" align="center" rowspan="1" colspan="1">49</td><td valign="top" align="center" rowspan="1" colspan="1">6.73</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Thalamus</td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-15</td><td valign="top" align="center" rowspan="1" colspan="1">-19</td><td valign="top" align="center" rowspan="1" colspan="1">10</td><td valign="top" align="center" rowspan="1" colspan="1">139</td><td valign="top" align="center" rowspan="1" colspan="1">7.41</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cerebellum</td><td valign="top" align="left" rowspan="1" colspan="1">Vermis</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">-3</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">308</td><td valign="top" align="center" rowspan="1" colspan="1">10.74</td><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-14</td><td valign="top" align="center" rowspan="1" colspan="1">163</td><td valign="top" align="center" rowspan="1" colspan="1">7.72</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cerebellum</td><td valign="top" align="left" rowspan="1" colspan="1">Culmen/Declive</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">15</td><td valign="top" align="center" rowspan="1" colspan="1">-49</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">164</td><td valign="top" align="center" rowspan="1" colspan="1">9.53</td><td valign="top" align="center" rowspan="1" colspan="1">12</td><td valign="top" align="center" rowspan="1" colspan="1">-46</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">104</td><td valign="top" align="center" rowspan="1" colspan="1">6.19</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cerebellum</td><td valign="top" align="left" rowspan="1" colspan="1">Culmen/Declive</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-21</td><td valign="top" align="center" rowspan="1" colspan="1">-49</td><td valign="top" align="center" rowspan="1" colspan="1">-20</td><td valign="top" align="center" rowspan="1" colspan="1">140</td><td valign="top" align="center" rowspan="1" colspan="1">10.20</td><td valign="top" align="center" rowspan="1" colspan="1">-15</td><td valign="top" align="center" rowspan="1" colspan="1">-52</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">144</td><td valign="top" align="center" rowspan="1" colspan="1">8.87</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cerebellum</td><td valign="top" align="left" rowspan="1" colspan="1">Tuber/Declive</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">42</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-23</td><td valign="top" align="center" rowspan="1" colspan="1">94</td><td valign="top" align="center" rowspan="1" colspan="1">7.03</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-20</td><td valign="top" align="center" rowspan="1" colspan="1">156</td><td valign="top" align="center" rowspan="1" colspan="1">7.86</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /></tr></tbody></table><table-wrap-foot><attrib><italic toggle="yes">Talairach coordinates for the conjunction of Leading and Following compared to rest (<italic toggle="yes">p</italic> &gt; 0.005 uncorrected, <italic toggle="yes">k</italic> = 49 voxels). IFG, inferior frontal gyrus; IPL, inferior parietal lobule; ITG, inferior temporal gyrus; SII, secondary somatosensory cortex; SMA, supplementary motor area; SMC, sensorimotor cortex; and SPL, superior parietal lobule.</italic></attrib></table-wrap-foot></table-wrap></div></details><details class="inner-accordion"><summary>Table 3 (T3) - Leading versus following in the whole group, the leader-only group, and the follower-only group.</summary><div class="table-html"><table-wrap id="T3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Leading versus following in the whole group, the leader-only group, and the follower-only group.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="center" colspan="5" rowspan="1">Whole-group (<italic toggle="yes">n</italic> = 18)</th><th valign="top" align="center" colspan="5" rowspan="1">Leaders only (<italic toggle="yes">n</italic> = 11)</th><th valign="top" align="center" colspan="5" rowspan="1">Followers only (<italic toggle="yes">n</italic> = 10)</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">Area</th><th valign="top" align="center" rowspan="1" colspan="1">BA</th><th valign="top" align="left" rowspan="1" colspan="1">Hemisphere</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th></tr></thead><tbody><tr><td valign="top" align="left" colspan="2" rowspan="1"><bold>Activations: Leading &gt; Following</bold></td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   pre-SMA</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">5</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">29</td><td valign="top" align="center" rowspan="1" colspan="1">5.24</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   SMA</td><td valign="top" align="center" rowspan="1" colspan="1">4,6</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">-4</td><td valign="top" align="center" rowspan="1" colspan="1">-4</td><td valign="top" align="center" rowspan="1" colspan="1">59</td><td valign="top" align="center" rowspan="1" colspan="1">81</td><td valign="top" align="center" rowspan="1" colspan="1">5.97</td><td valign="top" align="center" rowspan="1" colspan="1">-3</td><td valign="top" align="center" rowspan="1" colspan="1">-13</td><td valign="top" align="center" rowspan="1" colspan="1">64</td><td valign="top" align="center" rowspan="1" colspan="1">109</td><td valign="top" align="center" rowspan="1" colspan="1">6.28</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   CMA</td><td valign="top" align="center" rowspan="1" colspan="1">24</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">-6</td><td valign="top" align="center" rowspan="1" colspan="1">8</td><td valign="top" align="center" rowspan="1" colspan="1">37</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">4.84</td><td valign="top" align="center" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">40</td><td valign="top" align="center" rowspan="1" colspan="1">85</td><td valign="top" align="center" rowspan="1" colspan="1">8.41</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PMC</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">21</td><td valign="top" align="center" rowspan="1" colspan="1">-4</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">145</td><td valign="top" align="center" rowspan="1" colspan="1">8.01</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PMC</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-26</td><td valign="top" align="center" rowspan="1" colspan="1">-13</td><td valign="top" align="center" rowspan="1" colspan="1">54</td><td valign="top" align="center" rowspan="1" colspan="1">51</td><td valign="top" align="center" rowspan="1" colspan="1">4.36</td><td valign="top" align="center" rowspan="1" colspan="1">-30</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">64</td><td valign="top" align="center" rowspan="1" colspan="1">32</td><td valign="top" align="center" rowspan="1" colspan="1">5.71</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   DLPFC</td><td valign="top" align="center" rowspan="1" colspan="1">8,9</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-40</td><td valign="top" align="center" rowspan="1" colspan="1">29</td><td valign="top" align="center" rowspan="1" colspan="1">38</td><td valign="top" align="center" rowspan="1" colspan="1">98</td><td valign="top" align="center" rowspan="1" colspan="1">7.35</td><td valign="top" align="center" rowspan="1" colspan="1">-48</td><td valign="top" align="center" rowspan="1" colspan="1">32</td><td valign="top" align="center" rowspan="1" colspan="1">31</td><td valign="top" align="center" rowspan="1" colspan="1">46</td><td valign="top" align="center" rowspan="1" colspan="1">5.84</td><td valign="top" align="center" rowspan="1" colspan="1">-39</td><td valign="top" align="center" rowspan="1" colspan="1">44</td><td valign="top" align="center" rowspan="1" colspan="1">34</td><td valign="top" align="center" rowspan="1" colspan="1">76</td><td valign="top" align="center" rowspan="1" colspan="1">7.08</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   DLPFC</td><td valign="top" align="center" rowspan="1" colspan="1">9</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">-36</td><td valign="top" align="center" rowspan="1" colspan="1">23</td><td valign="top" align="center" rowspan="1" colspan="1">25</td><td valign="top" align="center" rowspan="1" colspan="1">30</td><td valign="top" align="center" rowspan="1" colspan="1">6.91</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   SPL</td><td valign="top" align="center" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">-73</td><td valign="top" align="center" rowspan="1" colspan="1">42</td><td valign="top" align="center" rowspan="1" colspan="1">70</td><td valign="top" align="center" rowspan="1" colspan="1">6.39</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">-73</td><td valign="top" align="center" rowspan="1" colspan="1">49</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">7.56</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   SPL</td><td valign="top" align="center" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">-73</td><td valign="top" align="center" rowspan="1" colspan="1">36</td><td valign="top" align="center" rowspan="1" colspan="1">84</td><td valign="top" align="center" rowspan="1" colspan="1">5.44</td><td valign="top" align="center" rowspan="1" colspan="1">-18</td><td valign="top" align="center" rowspan="1" colspan="1">-79</td><td valign="top" align="center" rowspan="1" colspan="1">43</td><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">6.34</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   Cerebellum</td><td valign="top" align="center" rowspan="1" colspan="1">Tuber</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">-65</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">37</td><td valign="top" align="center" rowspan="1" colspan="1">5.00</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" colspan="2" rowspan="1"><bold>Deactivations: Following &gt; Leading</bold></td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PCC</td><td valign="top" align="center" rowspan="1" colspan="1">7,31</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">-54</td><td valign="top" align="center" rowspan="1" colspan="1">23</td><td valign="top" align="center" rowspan="1" colspan="1">129</td><td valign="top" align="center" rowspan="1" colspan="1">-5.36</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">31</td><td valign="top" align="center" rowspan="1" colspan="1">92</td><td valign="top" align="center" rowspan="1" colspan="1">-11.12</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   TPJ</td><td valign="top" align="center" rowspan="1" colspan="1">39,40</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="center" rowspan="1" colspan="1">-59</td><td valign="top" align="center" rowspan="1" colspan="1">25</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="center" rowspan="1" colspan="1">-5.33</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">31</td><td valign="top" align="center" rowspan="1" colspan="1">89</td><td valign="top" align="center" rowspan="1" colspan="1">-8.86</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   TPJ</td><td valign="top" align="center" rowspan="1" colspan="1">39,40</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-53</td><td valign="top" align="center" rowspan="1" colspan="1">-63</td><td valign="top" align="center" rowspan="1" colspan="1">23</td><td valign="top" align="center" rowspan="1" colspan="1">105</td><td valign="top" align="center" rowspan="1" colspan="1">-5.70</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   STS</td><td valign="top" align="center" rowspan="1" colspan="1">19,39</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">16</td><td valign="top" align="center" rowspan="1" colspan="1">92</td><td valign="top" align="center" rowspan="1" colspan="1">-10.18</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   STS</td><td valign="top" align="center" rowspan="1" colspan="1">37,39</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">-51</td><td valign="top" align="center" rowspan="1" colspan="1">-52</td><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">66</td><td valign="top" align="center" rowspan="1" colspan="1">-5.94</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   aSTG</td><td valign="top" align="center" rowspan="1" colspan="1">13,22</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">57</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">-2</td><td valign="top" align="center" rowspan="1" colspan="1">72</td><td valign="top" align="center" rowspan="1" colspan="1">-9.70</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   Temporal pole</td><td valign="top" align="center" rowspan="1" colspan="1">38</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">-48</td><td valign="top" align="center" rowspan="1" colspan="1">5</td><td valign="top" align="center" rowspan="1" colspan="1">-23</td><td valign="top" align="center" rowspan="1" colspan="1">30</td><td valign="top" align="center" rowspan="1" colspan="1">-8.09</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PHC</td><td valign="top" align="center" rowspan="1" colspan="1">30,36</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">17</td><td valign="top" align="center" rowspan="1" colspan="1">-33</td><td valign="top" align="center" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">39</td><td valign="top" align="center" rowspan="1" colspan="1">-4.72</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">42</td><td valign="top" align="center" rowspan="1" colspan="1">-40</td><td valign="top" align="center" rowspan="1" colspan="1">1</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="center" rowspan="1" colspan="1">-6.13</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PHC</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-21</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">-6</td><td valign="top" align="center" rowspan="1" colspan="1">103</td><td valign="top" align="center" rowspan="1" colspan="1">-5.76</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">-30</td><td valign="top" align="center" rowspan="1" colspan="1">-22</td><td valign="top" align="center" rowspan="1" colspan="1">-11</td><td valign="top" align="center" rowspan="1" colspan="1">67</td><td valign="top" align="center" rowspan="1" colspan="1">-7.52</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   Thalamus</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">21</td><td valign="top" align="center" rowspan="1" colspan="1">-25</td><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">59</td><td valign="top" align="center" rowspan="1" colspan="1">-6.72</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /></tr></tbody></table><table-wrap-foot><attrib><italic toggle="yes">Talairach coordinates for the contrast “Leading versus Following” in the whole group, leaders only, and followers only (<italic toggle="yes">p</italic> &gt; 0.005 uncorrected, <italic toggle="yes">k</italic> = 28 voxels). aSTG, anterior superior temporal gyrus; CMA, cingulate motor area; DLPFC, dorsolateral prefrontal cortex; PCC, posterior cingulate cortex; PHC, parahippocampal cortex; PMC, premotor cortex; SMA, supplementary motor area; SPL, superior parietal lobule; STS, superior temporal sulcus; and TPJ, temporo-parietal junction.</italic></attrib></table-wrap-foot></table-wrap></div></details><details class="inner-accordion"><summary>Table 4 (T4) - Leading versus following correlated with skill as a leader.</summary><div class="table-html"><table-wrap id="T4" position="float" orientation="portrait"><label>Table 4</label><caption><p>Leading versus following correlated with skill as a leader.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1">Area</th><th valign="top" align="center" rowspan="1" colspan="1">BA</th><th valign="top" align="left" rowspan="1" colspan="1">Hemisphere</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sup>2</sup></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1"><bold>Activation</bold></td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">pre-SMA</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">8</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">0.73</td><td valign="top" align="center" rowspan="1" colspan="1">0.37</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">SMA</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">12</td><td valign="top" align="center" rowspan="1" colspan="1">-7</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">184</td><td valign="top" align="center" rowspan="1" colspan="1">0.81</td><td valign="top" align="center" rowspan="1" colspan="1">0.46</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">dPMC</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">36</td><td valign="top" align="center" rowspan="1" colspan="1">-1</td><td valign="top" align="center" rowspan="1" colspan="1">37</td><td valign="top" align="center" rowspan="1" colspan="1">46</td><td valign="top" align="center" rowspan="1" colspan="1">0.66</td><td valign="top" align="center" rowspan="1" colspan="1">0.49</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">Insula</td><td valign="top" align="center" rowspan="1" colspan="1">13</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">33</td><td valign="top" align="center" rowspan="1" colspan="1">-25</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="center" rowspan="1" colspan="1">101</td><td valign="top" align="center" rowspan="1" colspan="1">0.71</td><td valign="top" align="center" rowspan="1" colspan="1">0.50</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">STG</td><td valign="top" align="center" rowspan="1" colspan="1">41,22</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">54</td><td valign="top" align="center" rowspan="1" colspan="1">-28</td><td valign="top" align="center" rowspan="1" colspan="1">10</td><td valign="top" align="center" rowspan="1" colspan="1">62</td><td valign="top" align="center" rowspan="1" colspan="1">0.66</td><td valign="top" align="center" rowspan="1" colspan="1">0.48</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"><bold>Deactivation</bold></td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">Cingulate</td><td valign="top" align="center" rowspan="1" colspan="1">13,13</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-21</td><td valign="top" align="center" rowspan="1" colspan="1">-34</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="center" rowspan="1" colspan="1">87</td><td valign="top" align="center" rowspan="1" colspan="1">-0.74</td><td valign="top" align="center" rowspan="1" colspan="1">0.37</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">Lingual</td><td valign="top" align="center" rowspan="1" colspan="1">19</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-33</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-2</td><td valign="top" align="center" rowspan="1" colspan="1">46</td><td valign="top" align="center" rowspan="1" colspan="1">-0.72</td><td valign="top" align="center" rowspan="1" colspan="1">0.30</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /></tr></tbody></table><table-wrap-foot><attrib><italic toggle="yes">Talairach coordinates for the contrast “Leading versus Following” in the whole group correlated with the covariate “leader skill” (<italic toggle="yes">p</italic> &gt; 0.025 uncorrected, <italic toggle="yes">k</italic> = 46 voxels). <italic toggle="yes">R</italic><sup>2</sup> is the coefficient of determination of the regression of the cluster’s mean beta value against leader skill. dPMC, dorsal premotor cortex; SMA, supplementary motor area; and STG, superior temporal cortex.</italic></attrib></table-wrap-foot></table-wrap></div></details><details class="inner-accordion"><summary>Table 5 (T5) - Leading versus following correlated with skill as a follower.</summary><div class="table-html"><table-wrap id="T5" position="float" orientation="portrait"><label>Table 5</label><caption><p>Leading versus following correlated with skill as a follower.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1">Area</th><th valign="top" align="center" rowspan="1" colspan="1">BA</th><th valign="top" align="left" rowspan="1" colspan="1">Hemisphere</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sup>2</sup></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1"><bold>Activation</bold></td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"><bold>Deactivation</bold></td><td valign="top" align="left" rowspan="1" colspan="1">PCC</td><td valign="top" align="center" rowspan="1" colspan="1">7,31</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">-55</td><td valign="top" align="center" rowspan="1" colspan="1">37</td><td valign="top" align="center" rowspan="1" colspan="1">78</td><td valign="top" align="center" rowspan="1" colspan="1">-0.75</td><td valign="top" align="center" rowspan="1" colspan="1">0.30</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">mPFC</td><td valign="top" align="center" rowspan="1" colspan="1">9,10</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">22</td><td valign="top" align="center" rowspan="1" colspan="1">64</td><td valign="top" align="center" rowspan="1" colspan="1">-0.73</td><td valign="top" align="center" rowspan="1" colspan="1">0.27</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">pSTS</td><td valign="top" align="center" rowspan="1" colspan="1">19,39</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">42</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">13</td><td valign="top" align="center" rowspan="1" colspan="1">90</td><td valign="top" align="center" rowspan="1" colspan="1">-0.75</td><td valign="top" align="center" rowspan="1" colspan="1">0.61</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">TPJ</td><td valign="top" align="center" rowspan="1" colspan="1">39,40</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">-52</td><td valign="top" align="center" rowspan="1" colspan="1">43</td><td valign="top" align="center" rowspan="1" colspan="1">72</td><td valign="top" align="center" rowspan="1" colspan="1">-0.72</td><td valign="top" align="center" rowspan="1" colspan="1">0.48</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /></tr></tbody></table><table-wrap-foot><attrib><italic toggle="yes">Talairach coordinates for the contrast “Leading versus Following” in the whole group correlated with the covariate “follower skill” (<italic toggle="yes">p</italic> &gt; 0.025 uncorrected, <italic toggle="yes">k</italic> = 46 voxels). <italic toggle="yes">R</italic><sup>2</sup> is the coefficient of determination of the regression of the cluster’s mean beta value against follower skill. mPFC, medial prefrontal cortex; PCC, posterior cingulate cortex; pSTS, superior temporal sulcus; and TPJ, temporo-parietal junction.</italic></attrib></table-wrap-foot></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 31056647</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/31056647/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>31056647_analysis_0</td><td>Emotional support group &gt; no support group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task involves interpersonal exchange (ball-tossing) and nonverbal communicative touch as support; the contrast examines how this social communication modulates neural responses.</td></tr>
<tr><td>31056647_analysis_1</td><td>Appraisal support group &gt; no support group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Appraisal support was delivered as text messages (communicative content) and the task involves interpersonal exchange; the contrast captures effects of this social communication on neural activity (TPJ).</td></tr>
<tr><td>31056647_analysis_2</td><td>Appraisal support group &gt; no support group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The manipulation uses communicative text messages from a friend; the contrast captures how that social communication modulates subsequent neural responses to exclusion.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 6688450)</summary><p><strong>Title:</strong> Social support modulates the neural correlates underlying social exclusion</p><details><summary>Abstract</summary><pre class="paper-text">Ostracism threatens the human need for social interactions, with negative consequences on cognition, affect and behavior. Understanding the mechanisms that can alleviate these consequences has therefore become an important research agenda. In this study, we used behavioral and fMRI measures to advance our understanding how social support can buffer the negative effects of social exclusion. We focused on two different types of support from a friend: emotional support, conveyed by gentle touch and appraisal support, implemented as informative text messages. Seventy-one female participants underwent fMRI scanning while playing a virtual ball-tossing game in the course of which they were excluded. Two consecutive runs of the game were separated according to the participant’s experimental condition (appraisal support, emotional support and no support). Results showed that the experience of social exclusion is modulated by the type of support received. Specifically, emotional support decreased negative emotions and anterior insula activity, while appraisal support increased negative emotions, with concomitant increase of subgenual anterior cingulate cortex and decrease of temporal-parietal junction activity. These divergent effects of social support point to the necessity to characterize whether and under which conditions it represents an effective and positive resource to alleviate the negative consequences of social exclusion.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
The aim of the present study was to investigate how different types of social support reduce negative feelings associated with social exclusion and its activation at the neural level. Human beings have a fundamental need to interact with each other. Ostracism (social exclusion) threatens this need and has various effects on cognition, affect and behavior ( ). It is often associated with experiences of pain, often called social pain, defined as ‘the distressing experience arising from the perception of actual or potential psychological distance from close others or a social group’ ( ;  ).   refers to it as one of the most painful and emotionally unpleasant conditions that the individual can live with, as it bears the risk of damaging his ability to relate to other individuals. Experimental neuroscientific research over the past decades has extensively focused on the understanding of ostracism’s neurophysiological underpinnings. Mainly investigated via computer-controlled ball-tossing games (the cyberball game, see   for review), the experience of exclusion from the game (social exclusion) usually results in feelings of unpleasantness and discomfort, with concomitant recruitment of a network of brain areas associated with the processing of negative affect, such as the dorsal anterior cingulate cortex (dACC), the subgenual anterior cingulate cortex (subACC) ( ;  ;  ;  ) and the anterior insula (AI) ( ). It is currently a matter of debate if the fingerprint of social exclusion resembles the negative experience associated with pain of physical nature ( ;  ). For example, the experiences of social exclusion and physical pain reflect many common psychological and biological characteristics: from the use of similar words (‘I feel hurt’) ( ), the involvement of overlapping neurochemical ( ;  ) and neural systems ( ), to comparable inflammatory responses and genetic regulation ( ;  ;  ). These commonalities may stem from similar adaptive evolutionary functions ( ). As physical damage to an organism threatens its survival, and the presence of pain lead to protective responses via unpleasant and distressing psychological states, feelings of pain and discomfort after separation from the individual’s social group may serve as protective factors preventing such separation. Consequently, social pain may have promoted safety in a similar manner as physical pain; when a ‘socially painful’ event has occurred, it may drive the individual to repair the social relationship or to seek new ones ( ). However, behaviors that are adaptive when an individual experiences acute pain, e.g. avoiding activities that increase pain, when pain becomes chronic may develop into patterns of behavior that are maladaptive and impair long-term health ( ). Similarly, social pain responses that are situationally appropriate, e.g. feeling angry or avoiding a group after being rejected, may lead to less-effective coping and long-term social isolation, when they become a chronic issue ( ). Given the negative and serious long-term consequences of pain exposure, it is mandatory therefore to understand and promote factors that facilitate the remission or prevent the initiation of such psychological and behavioral effects. In that regard, positive aspects of one’s social world (social support) may improve coping responses and overall well-being. For example, according to  , p. 11), social support is configured as an ‘exchange of resources between two individuals, perceived by the one who provides it - or by those who receive it - as something aimed at increasing the well-being of the recipient’.   describes it in terms of perceived and real, useful and/or significant supplies provided by the community, social networks and trustworthy partners associated to the well-being of the subject.   identifies different types of social support:   `  emotional support’ is associated with sharing life experiences and involves the provision of empathy, love, trust and care; `instrumental support’ involves behaviors that directly help people in need using tangible help (like tangible services and economic benefits); `informational support’ involves the provision of advice, suggestions and information that a person can use to address problems; and finally `appraisal support’ involves the provision of information that is useful for evaluation purposes: constructive feedback, affirmation and social comparison. Several empirical studies ( ;  ;  ) have examined the function of social support on the perception of physical pain, demonstrating a remarkable correlation between social support and the reduction of physical pain experience. Meaningful social connections have also been shown to serve a protective role in reducing neural, physiological and neuroendocrine responses to pain and stress including heart rate, blood pressure and cardiovascular and neuroendocrine responses ( ;  ). Given the strong commonalities between physical and social pain, it is not surprising that the interest on the effects of social support on physical pain has been extended to stressors of social nature, with similar results reported. In particular, psychosocial stress caused by social evaluation ( ) has been observed to be reduced by social support ( ). Interestingly, different types of social supports (verbal support, physical contact) have been associated to different reactions in women ( ), suggesting that not all types of social support are effective in reducing the physiological responses to social stress. In spite of the rich scientific literature on social support and psychological stress upon social evaluation, only few studies have directly examined the effects of social support on the feeling of social pain caused by, for example, social exclusion or ostracism. Similarly, to psychological stress, these studies suggest that the presence of a friend ( ), supportive emotional texts ( ) or gentle slow touch ( ) are able to reduce the negative feelings caused by social exclusion. On the neural level, self-reported supportive daily life interactions have been shown to diminish neuroendocrine stress responses and to correlate with decreased activity in the dACC following ostracism ( ). Similarly,   observed that supportive emotional text leads to reduced AI and enhanced theory of mind (ToM) network activity ( ;  ;  ;  ) during social exclusion. 

To date, however, a single study examining how different types of social support modulates feelings of social pain and how this is represented at the neural level has not been performed yet. Our study aimed, for the first time, at disclosing the role of different support strategies in modulating the behavioral and neural correlates involved in social exclusion. Specifically, we used two different types of support: emotional physical support (emotional support), which we implemented as gentle touch, and informational/appraisal support (appraisal support), which we implemented as informative text messages allowing to better understand the situation. In line with the previous literature, we hypothesized feelings of social pain, induced via exclusion from a virtual ball-tossing game, to be reduced after experiencing social support. Furthermore, we hypothesized such behavior to be associated with reduced activity of the neural network involved during the experience of social exclusion. Finally, we expect different neurophysiological effects depending on the type of social support experienced. In particular, we expected the emotional support group to show modulatory effect in the affective network (e.g. AI, ACC) while the appraisal support group to additionally modulate the ToM network ( ) 


## METHODS 
  
### Participants 
  
In total, 81 Italian female volunteers (age, 21.67 ± 2.29 years) with no history of neurological or psychiatric disorders (assessed with semi-structured interviews conducted by a psychologist) were recruited among undergraduate students at the University of Turin. We chose to include females only, as gender differences on social exclusion are well-documented (see  ;  ). All participants were right-handed according to the Edinburgh Handedness Inventory ( ). Female friends of a similar age as the participants were invited to participate in the experiment, and instructed to act as confederates. Subjects were randomly assigned to one of the three groups: appraisal support group (N = 26), emotional support group (N = 26) and no support group (N = 29). Ten participants were excluded from the study because of excessive movement or lack of compliance during the functional Magnetic Resonance Imaging (fMRI) session, leaving the final sample for the three groups as follows: appraisal support group (N = 23), emotional support group (N = 23) and no support group (N = 25). All participants signed the information
consent after the experimental procedures have been described to them. The study was approved by the Bioethics Committee of the University of Turin. 


### Social pain task 
  
In order to create in the fMRI environment the uncomfortable situation in which participants could experience social exclusion, we used a modified version of the well-known `cyberball game’ ( ), which has been widely used in the literature ( ;  ;  ;  ;  ). Our version was developed by  , who replaced the animated cartoons of the cyberball game by videos showing schematic virtual representations of real people tossing the ball to each other. The task was composed of 10 blocks with two experimental conditions: `social inclusion’ and `social exclusion’. In each block the ball-tossing game included a total of 12 passes, distributed between three players (including the participant). In the five blocks inducing the experience of social inclusion, the participant received at least one third of the total passes, while in the five blocks inducing social exclusion, the participant received less than one third of the total passages (see   for a detailed description of the stimuli preparation and procedure). Once the participant received the ball, she had to decide to whom to throw it back by pressing with her index (left player) or middle (right player) finger on an Magnetic Resonance Imaging (MRI) compatible button box. The presentation of the blocks was equal for all the participants with a pseudorandomized order: the first three and the last two blocks belonged to the inclusion condition, while the five blocks placed in the central position of the task belonged to the exclusion condition. Each ball-tossing game had an average duration of 33.5 s (range, 30–40 s). At the end of each game, the participant was asked to answer the question ‘How are your emotions?’ in order to report the valence and intensity of the emotions experienced during the game on a Likert scale with nine discrete values (from −4 = very negative on 0 to +4 = very positive) displayed for 4 s. The answer was given by using the same button box used to throw the ball (see  ). Of crucial relevance, this sequence of 10 blocks was performed twice, in two separate fMRI runs. In between the runs, emotional or appraisal support was provided by the participant’s friend in the two experimental groups, while no support was provided in the control group. 
  
 Exemplar trial for the social pain task.   In each trial, participants played the game with other two virtual players. During the game, once they receive the ball, they have to decide to whom to throw it back (as illustrated by the two arrows) by pressing the left or right key on the pad. In the inclusion condition, participants received the ball at least one third of the total tosses. In the exclusion condition, participants received the ball less than one third of the total tosses. Immediately after the game, they were asked to answer the question ‘How are your emotions?’ on a 9-point Likert scale, displayed for 4 s. Interstimulus interval was randomly jittered between 1 and 3 s. Arrows in the inclusion condition are inserted only for descriptive purposes and not displayed during the game. 
  

### Social support manipulation 
  
Two experimental groups of social support have been defined: emotional and appraisal. In the emotional support group  ,   each confederate (the female friend) was instructed to gently touch the hand of the participant, with the aim of comforting her. The characteristic of this group was the administration of support only through physical contact, without the use of verbal or expressive linguistic expressions. No specific constraints on how to deliver the touch was given to the confederates. Rather, they should hold, caress and tenderly squeeze her friend’s hand as she would normally do when trying to comfort her. 

In the appraisal support group, social support was given by the participant’s friend through text messages delivered and displayed on the back-projection screen in the scanner. In particular, the participants were told that the phrases they read on the monitor were written and sent directly by their friend from a PC situated in another room, where she could follow the game. Each participant saw 10 pre-prepared phrases meant to give additional information in order to help the understanding of the experience of social exclusion (for example: ‘I think that these two players are actually friends’ or ‘I think that when the experiment will end, we’ll see them go away together’). Importantly, the content of the text was never aimed to comfort the subject but rather to give information allowing the reappraisal of the situation, and it was always emotionally neutral. 

Finally, in order to tease apart the effect of the repetition of the task (adaptation, fatigue, etc.), the control group did the social exclusion task twice but without receiving any kind of support in between. We chose such control condition because the mere presence of a friend, even without delivering any social support, could have affected the following experience of social exclusion ( ). All conditions of social support lasted for 3 min and were delivered between runs 1 and 2 of the cyberball game, while the subject was resting inside the scanner. For the no support group, the same interval was kept between runs 1 and 2, and the subject asked to wait still for the next run to start. 


### Procedure 
  
Each participant, previously randomly assigned to one of three groups, and her friend (except for the no support group in which participants came alone) were received in the fMRI room of the hospital and informed about the study. Specifically, participants were told that they would be connected via Internet to two other players, located in another room of the hospital. After the general information, each confederate was accommodated in the adjacent room for observing through a monitor what happens to her friend during the game. Here she was instructed on what she had to do for the different support conditions. For all participants, after the verbal instruction about the cyberball game, a training session was performed outside of the scanner to ensure that participants understood the game. A second short practice session was administered in the scanner to familiarize the participants with the response recording system. The cyberball game was programmed using Cogent toolbox (2000), running on Matlab 2007 (Mathworks, Cherborn, MA, USA). Inside the scanner, the stimuli were presented via a head coil-mounted display system (Resonance Technology, Inc.). The fMRI session was composed of three phases performed on the same day (see  ): (i) social pain task run 1: each participant was scanned while engaging in the virtual cyberball game, as described above; (ii) social support: each experimental group received social support (e.g. emotional or appraisal), while the control group did not receive any kind of support. During this section, no fMRI scanning was performed. (iii) Social pain task run 2: each participant was scanned for the second time while engaging in the virtual cyberball task, as described above. After the fMRI session, each participant answered a brief interview aimed at investigating the believability of the manipulation. In particular, we asked indirect questions such as: ‘What do you think about the players? How was the game for you? Do you have any comments?’ None of the participants expressed doubts about the veracity of the situation. 
  
 Timeline of the fMRI session.   Each fMRI session was divided into three phases performed on the same day: (i) social pain task run 1, (ii) social support (emotional, appraisal, no support), (iii) social pain task run 2. Social support was either emotional or appraisal for a duration of 3 min. In the case of the no support group, a 3 min break between the two runs was carried out. 
  

### MRI data acquisition 
  
The MRI data were acquired using a 3.0 T MRI Scanner (Philips Ingenia) with a 32-channel array head coil. The study was performed at the Center of Brain Imaging 3 T-NIT, at the Hospital Città della Salute e della Scienza in Turin, Italy. Echo-Planar Image (EPI) sequence [TR/TE, 2000/30 ms; 33 slices, matrix size, 64 × 64; interslice gap, 0.5 mm; field of view (FOV), 230 × 230 mm ; flip angle, 90 degrees; slices aligned to the AC-PC line, 230 volumes/run] for functional images was applied. A total of 226 volumes per subject per run were collected. The first four volumes of each run were discarded to allow the equilibration of T1 saturation effects. T1-weighted sequence MP-RAGE (TR, 8.1 ms; TI, 900 ms; TE, 3.7 ms; voxel size, 1 × 1 × 1 mm ) for structural images of the whole brain was used. 
  
Contrasts of interest 
  
Significant voxels are reported threshold of   P   ≤ 0.05 FWE corrected for small volumes.Peak activity coordinates are given in MNI space. Significant value for   P   &lt; 0.001 uncorrected. 
  

### Data analysis 
  
#### Behavioral analysis 
  
Emotional ratings given by the participants after each round of the cyberball game were analysed in order to investigate differences in the emotional experience between exclusion and inclusion trials and between the first and second run, i.e. before and after receiving social support. We conducted a repeated-measures ANOVA with two within-subjects factors, condition (inclusion, exclusion), time (runs 1 and 2), and one between-subject factor, group (emotional support, appraisal support, no support). Ratings of the exclusion condition were multiplied by −1 in order to carry the same direction as the inclusion ratings, allowing to test the three-way interaction. Statistical analyses were performed with IBM SPSS Statistics version 24. 


#### fMRI data analysis 
  
The MRI data were analysed using Statistical Parametric Mapping 12 (SPM12, Wellcome Department of Cognitive Neurology, London, UK) run on Matlab 2007 (Mathworks, Cherborn, MA, USA). All functional images have been pre-processed following this order: spatially realigned to the first volume, co-registered to the mean image, segmented in cerebrospinal fluid tissues, gray matter and white matter, then normalized to the Montreal Neurological Institute (MNI) space and finally smoothed at the first level with an 8 mm full-width half-maximum Gaussian Kernel, with an additional 6 mm at the second level. Low-frequency drifts, high-pass temporal filtering with a cut-off of 128 s was used. After preprocessing, a General Linear Model ( ) for statistical analysis was used for both functional runs. Regressors of interest were convolved with a canonical hemodynamic response function. For each participant’s first level analysis, six regressors were computed: social inclusion (I), social exclusion (I), emotion rating (I), social inclusion (II), social exclusion (II) and emotion rating (II). In addition, six parametric regressors of no interest were added to the design matrix to correct residual effects of head motion. At the second level, four contrasts of interest from the first-level analyses were fed into a flexible factorial design aiming at investigating the effect of social support on social exclusion, using a random effects analysis ( ). Linear contrast of the repeated-measures ANOVA with the within-subject factors, condition (exclusion, inclusion), time (runs 1 and 2), and the between-subject factor, group (emotional support, appraisal support, no support), were used to assess the interaction between the factors group and time. Given the main research question of our paper, only results for the exclusion condition are reported. We performed whole brain analyses with an initial threshold of   P   &lt; 0.001 uncorrected and reports clusters that survived Family-Wise Error (FWE) correction for small volumes (SVC) at   P   &lt; 0.05. For the SVC, we created two binary masks encompassing, first, the affective network specifically detected in social exclusion paradigms, and second, a network associated to representing other minds and intentions (ToM). Both masks are based on the most recent published meta-analyses on social exclusion and ToM, respectively. More specifically, the first mask included coordinates derived from two meta-analyses on social exclusion published by   and  . In spite of repeated attempts, it was, however, not possible to receive the original maps from both authors. Therefore, spheres of 10 mm radius centered on the reported main activation loci were generated and combined into one mask with the toolbox MarsBaR ( ). The second mask was provided as an image-based mask by ( ), based on their meta-analysis on ToM tasks (see   for more details). Given we did not expect the involvement of the ToM network for the emotional support group, only the first (affective) mask was used to investigate differences in activations between this group and the no support group. To investigate differences in activations between the appraisal support and no support groups and the emotional support and appraisal support groups, both the affective and the ToM masks were used. The MRIcron software package ( ;  ) was used for anatomical and cytoarchitectonic display and interpretation. 


#### Brain–behavior correlation analyses 
  
Pearson correlation analyses between brain activity and behavioral ratings were performed with IBM SPSS Statistics version 24. In particular, the difference in activity (∆) between the first and second run of social exclusion in the regions showing significant statistical difference (see  ) was correlated with the difference in emotional ratings between the two runs (run 1 minus run 2). Activity in these regions was extracted with REX (  http://web.mit.edu/swg/rex/rex.pdf  ). Correlations were performed for each group separately and corrected for the number of ROIs used in each group. 




## RESULTS 
  
### Behavioral results 
  
The ANOVA revealed a significant interaction effect time*condition*group [  F   = 3.39,   P   = 0.040, partial Eta squared = 0.091]. All the other effects and interactions were not significant (  F   &lt; .103) . Post hoc pairwise comparisons were used in order to characterize the effect of the triple interaction. In particular, in the emotional support group, a significant difference between exclusion run 1   vs   run 2 was observed, defined by a reduction of unpleasantness ratings during the second run (  M   = 5.57, SE = 1.91,   P   = 0.005). In the appraisal support group  ,   a significant difference between exclusion run 1   vs   run 2 was also observed, but with an opposite pattern, namely an increase of unpleasant emotions in the second run (  M   = −3.93, SE = 1.91,   P   = 0.044). The no support group did not show any significant difference between runs 1 and 2 for both conditions (see  ). Finally, the differences between the inclusion and exclusion runs (Δ inclusion, Δ exclusion) were entered in a one-way ANOVA to assess whether the groups significantly differed. The analysis revealed a significant difference between the groups in the Δ exclusion only (  F   = 6.22,   P   = 0.003). Post hoc multiple comparisons were used in order to characterize the effect. In particular, we observed a significant difference both between the emotional support group and the no support group (  M   = 0.564, SE = 0.265,   P   = 0.037) and the emotional support group and the appraisal support group (  M   = 0.949, SE = 0.270,   P   = 0.001). 
  
 Behavioral results.   Mean and confidence intervals (95%) divided by group, condition and run. Significant differences are marked with an asterisk (  P   &lt; 0.05, based on post hoc pairwise comparisons) 
  

### fMRI results 
  
  Emotional support group vs. No support group.   

 Emotional support (social exclusion run 1 &gt; social exclusion run 2) &gt; no support (social exclusion run 1 &gt; social exclusion run 2).  

The analysis revealed significantly reduced activation in the right AI (rAI, x = 33, y = 27, z = −8) for the emotional support group compared to the no support group ( ;  ) for the second run compared to the first run of social exclusion . 
  
 FMRI results.   Differences in the neural activation between the emotional support group vs. the no support group for the contrast: social exclusion run 1   &gt;   social exclusion run 2. The bar plots represent contrast estimates and 90% confidence intervals in the right AI. For illustrative purposes, statistical maps are displayed with a threshold of   P   &lt; 0.001 uncorrected and superimposed on a standard T1 template. 
  
 Emotional support (social exclusion run 2 &gt; social exclusion run 1) &gt; No support (social exclusion run 2 &gt; social exclusion run 1).  

No suprathreshold voxels were observed for the reverse contrast. 

  Appraisal support group vs. no support group.   

 Appraisal support (social exclusion run 1 &gt; social exclusion run 2) &gt; No support (social exclusion run 1 &gt; social exclusion run 2).  

The analysis revealed significantly reduced activation in the right temporal parietal junction (rTPJ, x = 46, y = −47, z = 27) for the appraisal support group compared to the no support group ( ;  ) for the second compared to the first run of social exclusion. A more liberal threshold of   P   &lt; 0.001 revealed reduced activation also in the left temporal parietal junction (lTPJ, x = −48, y = −53, z = 34). 
  
 FMRI results.   Differences in the neural activation between the appraisal support group   vs   the no support group for the contrast: social exclusion run 1 &gt; social exclusion run 2. The bar plots represent contrast estimates and 90% confidence intervals in the right TPJ. For illustrative purposes, statistical maps are displayed with a threshold of   P   &lt; 0.001 uncorrected and superimposed on a standard T1 template. 
  
 Appraisal support (social exclusion run 2 &gt; social exclusion run 1) &gt; No support (social exclusion run 2 &gt; social exclusion run 1).  

The analysis revealed significantly increased activation in the subACC (x = −5; y = 32, z = −5) for the appraisal support group compared to the no support group ( .  ) for the second compared to the first run of social exclusion. A more liberal threshold of   P   &lt; 0.001 revealed reduced activation also in the ventromedial prefrontal cortex (vmPFC) (2, y = 37, z = −8). 
  
 FMRI results.   Differences in the neural activation between the appraisal support group   vs   the no support group for the contrast: social exclusion run 2 &gt; social exclusion run 1. The bar plots represent contrast estimates and 90% confidence intervals in the subACC. For illustrative purposes, statistical maps are displayed with a threshold of   P   &lt; 0.001 uncorrected and superimposed on a standard T1 template. 
    
 Correlation results.   Scatterplot of the correlation between the difference in subACC activity between exclusion runs 1 and 2 (∆ subACC) and the difference in unpleasantness ratings between exclusion runs 1 and 2. 
  
  Emotional support group vs. Appraisal support group.   

No suprathreshold voxels were observed in any of the possible combinations. 


### Brain-behavior correlation analyses 
  
The following correlations were performed: (i) for the emotional support group, correlation between ∆ activity in rAI and ∆ unpleasantness ratings and (ii) for the appraisal support group, correlation between ∆ activity in rTPJ, subACC and ∆ unpleasantness ratings. The correlation analyses revealed a significant positive relationship between ∆ subACC and ∆ unpleasantness ratings in the appraisal support group (r(23) = 0.443,   P   &lt; 0.017 one tailed, corrected for the number of correlations performed). This means that in the appraisal support group, the increase of subACC activity observed in the second run of exclusion was associated to increased unpleasantness feelings in the second run ( ). All the other correlations were not significant. 



## DISCUSSION 
  
In the present study, we investigated the effects of different types of social support (emotional and appraisal) on the behavioral and neural correlates of the experience of social exclusion. Seventy-one female participants were scanned twice while playing the cyberball game. Between the two runs of the game, different types of support were delivered by a female friend. At the behavioral level, we observed that, compared to the control group (no support), the sample that received emotional support in the form of gentle touch, reported reduced feeling of unpleasantness during exclusion trials between the first and second run of the game, i.e. after they had received the emotional support. Our results are in line with the findings of  , which showed reduced reported distress associated to ostracism, after being touched with optimal speed (3 cm/s) to induce positive feelings and thereby promoting interpersonal touch and affiliative behavior ( ). By adding these results, our study was able to show for the first time that the experience of emotional support is associated, at the neural level, to a reduction of activity in right AI, a brain area involved in the processing of negative affect during social exclusion and self- and other-directed aversive experiences ( ;  ;  ). The effects of emotional social support on the experience of social pain resemble the findings reported on pain of physical nature ( ;  ). In particular, during the administration of painful stimuli, married women who held the hand of their partners indicated a lower value of perceived pain. The subjective experience was correlated with reduced activation of the brain areas involved in pain processing, including the AI ( ). Moreover, imagined social support, provided through the visualization of images portraying of loved ones, was also able to modify the neural activation of the insula ( ;  ) and reduce the feeling of distress upon physical pain. The similar effect of emotional support on social and physical pain suggests overlapping regulatory mechanisms, possibly associated to the activity of the μ-opioid system and its analgesic properties ( ). 

The more informative type of support yielded instead different results. At the behavioral level, participants reported increased feelings of unpleasantness after receiving information about the other two participants. The subjective experience was accompanied by a reduced activation in the right TPJ, an area included in the ToM network ( ;  ;  ) and involved in incongruency detection and self-other distinction ( ;  ;  ). TPJ is considered a central structure implicated in the representation of mental states of others ( ). A recent study has associated the function of this brain region to the update of the internal models of the situation in order to generate appropriate actions to the social contexts ( ). This function is particularly important when faced with unexpected stimuli that demand attention reorienting and model updates. The findings of the present study suggest that the information received during the support possibly allowed the participants to interpret what was happening during the first run of the game. Indeed, the participants that received information (e.g. ‘the two players are friends’ or ‘there is understanding between them’) leading to a better understating of the social situation, showed an increase of unpleasant emotions (possibly anger) and possibly a reduced need to understand what was happening, indicated by reduced activity in TPJ. To corroborate this hypothesis, we observed increased recruitment of the subACC after receiving the appraisal support. Furthermore, the increased activity in subACC was positively correlated with the increased negative feelings reported during the second run. Interestingly, the subACC is a region involved in affective processes but not in physical pain ( ). Several social pain studies have indicated an increase in activity in the subACC during the negative experience of social exclusion ( ;  ;  ).   indicated the possibility that greater responsivity in the subACC during peer rejection could reflect an inability to properly regulate emotions evocated by negative events. In line with this literature, some studies have shown that this area is more responsive to negative emotional stimuli among depressed patients and correlates to the severity of depressive symptoms ( ;  ). Notably and differently from subACC, the increased negative affect did not result in a concomitant increase of AI activity, suggesting that the effects of ostracism on affective pain-related brain areas were not modulated by this type of support received. These findings point to a different role of these two areas in emotional processing during social exclusion, possible link to affective saliency and the need of emotion regulation. Our results are partially in line with the findings by  , who reported different reactions depending on the type of social support (verbal or physical contact) received. In particular, they observed that only physical contact was effective in reducing the symptoms of distress associated to negative social evaluation, while verbal support did not show any different from the no support condition. In our case, though, the appraisal support group showed increased negative feelings and concomitant neural response. It is possible to speculate that the negative reaction observed after appraisal support could have adaptive functions for the person experiencing it, in that it may drive the individual to seek for new relationship, when the actual ones are dysfunctional ( ). 

In conclusion, our study provides the first neuroimaging evidences that experiences of social support can modulate regions of the brain recruited during social pain and possibly responsible for coding the negative valence and intensity of emotion experience. Furthermore, for the first time, we showed that this effect may be different depending on the type of support received. Social support is a very complex phenomenon in which various factors can influence how it is effective for the receiver (e.g. who is providing it, in which form, etc.). It has been shown that it does not always result in a reduction of the negative experiences associated to social stress ( ) and social pain. Instead, as observed in our study, it can also increase the negative emotional experience, which can still be functional for the individual in the short term. Therefore, it is very important to understand under which conditions (contextual, personal, modality, etc.) social support can represent an effective and positive resource to alleviate the negative consequences of social exclusion. Importantly, our findings are restricted to a female sample; therefore not generalizable to the entire population. Future studied are needed to extend these findings to samples representative of the general population such as male participants and different age groups ( ) and to explore alternative types of social support (e.g. instrumental, informational). 


## Supplementary Material</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (1)</summary><details class="inner-accordion"><summary>Table 1 (TB1) - Contrasts of interest</summary><div class="table-html"><table-wrap id="TB1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Contrasts of interest</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /></colgroup><thead><tr><th rowspan="1" colspan="1" /><th colspan="3" align="left" rowspan="1">
<bold>MNI coordinates</bold>
</th><th align="left" rowspan="1" colspan="1">
<bold><italic toggle="yes">Z</italic>-score</bold>
</th><th align="left" rowspan="1" colspan="1">
<bold><italic toggle="yes">T</italic>-value</bold>
</th><th align="center" rowspan="1" colspan="1">
<bold><italic toggle="yes">P</italic>-value</bold>
</th></tr><tr><th align="left" rowspan="1" colspan="1">
<bold>Anatomical region</bold>
</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">X</italic>
</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">Y</italic>
</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">Z</italic>
</th><th rowspan="1" colspan="1" /><th rowspan="1" colspan="1" /><th align="center" rowspan="1" colspan="1">
<italic toggle="yes">FWE corrected</italic>
</th></tr></thead><tbody><tr><td align="left" colspan="7" rowspan="1">
<bold><italic toggle="yes">Emotional support group &gt; no support group</italic></bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Exclusion run 1 &gt; exclusion run 2</italic>
</td><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /></tr><tr><td align="left" rowspan="1" colspan="1">Right AI</td><td align="left" rowspan="1" colspan="1">33</td><td align="left" rowspan="1" colspan="1">27</td><td align="left" rowspan="1" colspan="1">−8</td><td align="left" rowspan="1" colspan="1">3.29</td><td align="left" rowspan="1" colspan="1">3.35</td><td align="left" rowspan="1" colspan="1">.052</td></tr><tr><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /></tr><tr><td align="left" colspan="7" rowspan="1">
<bold><italic toggle="yes">Appraisal support group &gt; no support group</italic></bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Exclusion run 1 &gt; exclusion run 2</italic>
</td><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /></tr><tr><td align="left" rowspan="1" colspan="1">rTPJ</td><td align="left" rowspan="1" colspan="1">46</td><td align="left" rowspan="1" colspan="1">−47</td><td align="left" rowspan="1" colspan="1">27</td><td align="left" rowspan="1" colspan="1">3.42</td><td align="left" rowspan="1" colspan="1">3.48</td><td align="left" rowspan="1" colspan="1">.046</td></tr><tr><td align="left" rowspan="1" colspan="1">lTPJ</td><td align="left" rowspan="1" colspan="1">−48</td><td align="left" rowspan="1" colspan="1">−53</td><td align="left" rowspan="1" colspan="1">34</td><td align="left" rowspan="1" colspan="1">3.22</td><td align="left" rowspan="1" colspan="1">3.27</td><td align="left" rowspan="1" colspan="1">.001<sup>*</sup></td></tr><tr><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /></tr><tr><td align="left" rowspan="1" colspan="1">
<bold><italic toggle="yes">Appraisal support group &gt; no support group</italic></bold>
</td><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Exclusion run 2 &gt; exclusion run 1</italic>
</td><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /></tr><tr><td align="left" rowspan="1" colspan="1">Left subACC</td><td align="left" rowspan="1" colspan="1">−5</td><td align="left" rowspan="1" colspan="1">32</td><td align="left" rowspan="1" colspan="1">−5</td><td align="left" rowspan="1" colspan="1">3.34</td><td align="left" rowspan="1" colspan="1">3.39</td><td align="left" rowspan="1" colspan="1">.046</td></tr><tr><td align="left" rowspan="1" colspan="1">Right vmPFC</td><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">37</td><td align="left" rowspan="1" colspan="1">−8</td><td align="left" rowspan="1" colspan="1">2.99</td><td align="left" rowspan="1" colspan="1">3.03</td><td align="left" rowspan="1" colspan="1">.001<sup>*</sup></td></tr></tbody></table><table-wrap-foot><p>Significant voxels are reported threshold of <italic toggle="yes">P</italic> ≤ 0.05 FWE corrected for small volumes.Peak activity coordinates are given in MNI space.<sup>*</sup>Significant value for <italic toggle="yes">P</italic> &lt; 0.001 uncorrected.</p></table-wrap-foot></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 31142792</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/31142792/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>31142792_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants viewed faces (reception of facial communication). The contrast examines responses to facial features (disfigurement) and face-specific visual areas (fusiform), matching the Social Communication inclusion examples (Viewing pictures of faces).</td></tr>
<tr><td>31142792_analysis_1</td><td>Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although this contrast highlights decreased activation (e.g., ACC) rather than face-selective cortex, the stimuli were faces and the question concerns reception of social/facial information; thus it meets Social Communication criteria (viewing faces).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 6541618)</summary><p><strong>Title:</strong> Behavioural and Neural Responses to Facial Disfigurement</p><details><summary>Abstract</summary><pre class="paper-text">Faces are among the most salient and relevant visual and social stimuli that humans encounter. Attractive faces are associated with positive character traits and social skills and automatically evoke larger neural responses than faces of average attractiveness in ventral occipito-temporal cortical areas. Little is known about the behavioral and neural responses to disfigured faces. In two experiments, we tested the hypotheses that people harbor a disfigured is bad bias and that ventral visual neural responses, known to be amplified to attractive faces, represent an attentional effect to facial salience rather than to their rewarding properties. In our behavioral study (N = 79), we confirmed the existence of an implicit ‘  disfigured is bad  ’ bias. In our functional MRI experiment (N = 31), neural responses to photographs of disfigured faces before treatment evoked greater neural responses within ventral occipito-temporal cortex and diminished responses within anterior cingulate cortex. The occipito-temporal activity supports the hypothesis that these areas are sensitive to attentional, rather than reward properties of faces. The relative deactivation in anterior cingulate cortex, informed by our behavioral study, may reflect suppressed empathy and social cognition and indicate evidence of a possible neural mechanism underlying dehumanization.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Physical appearance has a profound impact on a person’s life. Beautiful people are preferred and enjoy many advantages compared to average-looking people . While conceptually orthogonal, the correlation of attractiveness and positive character traits indicates the prevalence of a ‘beautiful is good’ stereotype . This stereotype might be innate . Attractive people are seen as more trustworthy, socially competent, dominant, better adjusted, more capable in school and work, and also receive greater rewards and lesser punishments than their average looking peers . Adults and children ascribe desirable personality traits to attractive faces of adults and children and discriminate against unattractive faces even if they are friends and family members . Attractiveness and trustworthiness judgments are consistent across cultures  and are made extremely quickly . Longer exposure to a face does not attenuate these biases and instead only consolidates people’s confidence in a judgement already made . Attractiveness also highly influences visual exploration of faces . 

In this study we examine a corollary to the ‘beautiful is good’ stereotype, that an automatic ‘disfigured is bad’ stereotype also exists. People with facial disfigurement are stigmatized and are often targets of discrimination. Looking at disfigured faces makes observers feel less happy, less in control, less dominant, and more aroused . People with facial disfigurements are not only perceived as less attractive and less likely to be selected as romantic partners, they are also thought of as having unfavourable personality traits (e.g., lack of emotional stability, conscientiousness), internal attributes (e.g., unhappiness, lower intelligence), social qualities (e.g., untrustworthiness, unpopularity)  and are treated poorly in social interactions . In popular culture, facial disfigurement is often used to distinguish good and evil characters . Well known examples of disfigured villains are Scar in the   Lion King   (large facial scar over left eye), Freddy Krueger in   Nightmare on Elm Street   (3rd degree burns and exposed tissue), the   James Bond   villains Le Chiffre (facial scar over left eye), Emilio Largo (missing eye), Ernst Stavro Blofeld (large scar over right eye covering most of his right side of the face), and Alec Trevelyan (facial burn scars), Elle Driver in   Kill Bill   (missing eye), Two Face in the   Batman   Universe (acid scars covering the left side of his head), Hopper in   A Bug’s Life   (scar covering right eye), and the Duchess from   Alice in a Wonderland   (Macrocephaly). This ‘disfigurement is bad’ stereotype is only partially explained by lower attractiveness of disfigured faces . 

Attractiveness of faces –and therefore attribution of a’beauty is good’ stereotype- is highly correlated with typicality or statistical averageness of faces . In addition to being statistical averages of groups, attractive faces are also symmetric . Both facial symmetry and averageness are considered markers of physical health and influence peoples’ choices of partners . Disfigured faces are neither typical nor average, and are usually not symmetric. They often deviate substantially from the norm. If proximity to the norm predicts positive social attributions, being ‘different’ could lead to negative evaluations. Disfigured faces might be linked to unfavourable personality traits, internal attributes, and social qualities because they are less typical and deviate from the population average. The association of disfigurement with negative attributes probably drives stigmatization and discrimination of disfigured people in social, academic, and professional contexts . The stigmatization and discrimination of disfigured people likely contributes to low self-esteem  and long term mental health concerns similar to other stigmatized groups that are subject to dehumanization . Dehumanization deprives a person or a group of people of positive human qualities and has been shown for several stigmatized groups such as homeless people and drug addicts . Dehumanization is used as a propaganda tool in political conflicts . The strongest predictors of dehumanization are hypothesized to be perceived competence and warmth . Faces rated lowest on both competence and warmth most robustly evoke dehumanization - including feelings of disgust and lack of empathy . 

Neuroimaging studies show that seeing attractive faces evokes brain responses in reward, emotion, and visual areas compared to seeing faces of average attractiveness . Attractive faces produce activations in areas associated with reward, like the nucleus accumbens , and orbitofrontal cortex . Moreover, attractiveness correlates with increased activations in areas associated with emotion, empathy, and social cognition like the anterior cingulate cortex and medio-prefrontal cortex  the latter being particularly active in tasks in which people are not making explicit attractiveness judgements . Different regions of the prefrontal cortex are selectively responsive to either attractive or unattractive faces  which is consistent with findings that ventral medio-prefrontal cortex processes stimulus value attributes in coordination with higher order visual areas like fusiform gyri and semantic processing areas (posterior superior temporal sulcus) . Orbital frontal  and medial prefrontal cortices  seem to process both aesthetic and moral values and may represent the biological link between these two kinds of evaluation . 

Left and right amygdala seem to be sensitive to both attractive  and unattractive faces . These non-linear effects for extremes at either end of the attractiveness spectrum suggest that amygdala activation reflects sensitivity to valence intensity rather than positive or negative valence per se . In line with the valence processing hypothesis for the functional role of amygdala, increased activation in the amygdala (bilaterally) is linked to untrustworthiness of faces . A meta-analysis of brain activations to attractiveness and trustworthiness suggests that activation of amygdala and adjacent nucleus accumbens is driven by extremes and atypicality . There is some tentative evidence that face typicality can also account for the activations in medio-prefrontal and anterior cingulate cortex . The authors note that the brain networks activated in response to extremes of attractiveness and trustworthiness are remarkably similar to brain networks that process positive and negative emotions . 

In addition to increased brain activations in reward and emotion areas, attractive faces also evoke larger neural responses in selective visual processing areas within ventral occipito-temporal cortex (such as the fusiform face area) as compared to faces of average attractiveness . These areas remain sensitive to facial attractiveness even when subjects are engaged in tasks in which attractiveness judgements are not queried explicitly. These observations have previously been interpreted as evidence that these areas also process rewards. While a reward response is one possible explanation for this amplified neural response to attractive faces, it is also possible that this reflects sensitivity to the saliency of attractive faces . If this alternate hypothesis is true, other salient features, such as disfigurement, should lead to similarly amplified neural responses in visual processing areas. 

Viewing faces of stigmatized groups fails to activate brain regions associated with empathy and social cognition . Krendl and colleagues reported increased activation in anterior insula and amygdala which correlated with self-reported disgust in response to viewing faces of stigmatized groups . The lack of activation in empathy and social cognition regions of the brain is postulated to be a neural correlate of dehumanization . 

Appearance clearly affects how people are viewed and treated by others. The same mechanisms that benefit attractive people in social interaction, put unattractive people at an unfair disadvantage. The effects of discriminating against people with facial disfigurement seem to extend beyond the specific effects of lower overall attractiveness and may tie in more with the pattern of results that have been shown with stigmatized groups. 

The goal of the present study was to test the behavioural and brain responses to facial disfigurement and investigate whether surgical treatment mitigates these responses. In two experiments, we used a set of photograph pairs of patients with different types of facial disfigurements before and after surgical treatment of the disfigurement. In experiment one we tested if people harbour implicit biases against disfigured faces and if such implicit biases were different from consciously aware self-reported explicit biases. In a follow up functional MRI (fMRI) study, we tested differential automatic brain responses to the same picture pairs when naïve participants were engaged in an unrelated cover task. We hypothesized that people have negative biases against faces with disfigurement. For the neural responses to facial disfigurement we tested competing hypotheses: visual cortices respond to rewards per se, or visual cortices respond to salience. In addition, we expected disfigured faces to show selective responses in emotion and valence areas such as anterior insulae and amygdalae and anterior cingulate and lateral or medial prefrontal areas in line with the research reviewed above. 


## Results and Discussion 
  
The behavioural experiment (N = 79, see method section for details) consisted of an implicit association test  (IAT) and an explicit bias questionnaire (EBQ) to test the hypothesis that people have a negative bias for disfigured faces. For the IAT, we used a stimulus set of photographs of real patients taken before and after treatment for disfigurement. The EBQ consisted of 11 questions which query conscious biases against people with facial disfigurements (see   https://osf.io/ca2u9/   for all items and data). We found no indication of an explicit bias. However, we did find that non-disfigured faces were preferred in the IAT (see Fig.  ). This bias was particularly robust for men, consistent with previous findings . Prior exposure to disfigured faces did not modulate the implicit bias of individuals.   
Female respondents demonstrate significantly less, although still strong, implicit preference for non-disfigured faces than male respondents. Male respondents show a moderate explicit preference for non-disfigured faces while women show no explicit preference. Error bars indicate 95% confidence intervals. 
  

We used the same set of photographs of people before and after surgical treatment that we used in the IAT in the fMRI study (N = 31). Participants viewed these photographs and engaged in a gender judgement task. We measured neural responses to facial disfigurement to test competing hypotheses of reward versus salience in visual areas like fusiform face area. If these visual areas respond to rewards, then non-disfigured faces compared to disfigured faces would show increased activity in visual areas linked to face processing. If these visual areas respond to salience, then we should find the opposite results; disfigured faces compared to non-disfigured faces should show increased activity in these areas. Because people with facial disfigurement are likely treated as an outgroup , neural patterns in response to disfigurement should be similar to previous findings investigating other stigmatized groups . We predicted decreased activation in areas linked to social cognition such as medio-prefrontal cortex and anterior cingulate cortex, as well as increased activations in areas linked to disgust and negative emotion like anterior insula and amygdala. 

We found that images of people with facial disfigurement, as compared to images of the same faces after surgical treatment, evoked greater neural responses within ventral occipito-temporal cortex, particularly bilateral fusiform gyri (see Fig.  ), and right inferior frontal cortex. This observation confirms the hypothesis that face processing and adjacent areas respond automatically to the salience of faces, rather than their attractiveness or rewarding properties per se.   
Increased activations (red yellow) and deactivations (blue-green) in response to faces before treatment. Results were corrected for multiple comparisons by familywise error correction at p &lt; 0.05 with Monte Carlo permutation testing in SnPM with a combined cluster-voxel threshold (cluster defining threshold p &lt; 0.001, T &gt; 3.3852). 
  

In addition to increased responses in visual areas, we found decreases in neural response amplitude to disfigured faces in the medial anterior cingulate gyrus extending towards medial prefrontal cortex (see Figs   and  ), as well as in a region stretching from right cuneus to the right calcarine gyrus and right lingual gyrus. This finding is similar to previous observations of neural responses to other stigmatized outgroups such as drug addicts and homeless people  and could reflect suppression empathy and mentalizing or increased demands in cognitive control, e.g. inhibition of staring at the area of lesion or inhibition of inappropriate social behaviour like obvious avoidance. Both possible hypotheses are not mutually exclusive and could be linked to the increased activation in the left inferior frontal gyrus - a region linked to cognitive control.   
Increased activations (red yellow) and deactivations (blue-green) in response to faces before treatment. Results were corrected for multiple comparisons by familywise error correction at p &lt; 0.05 with Monte Carlo permutation testing in SnPM with a combined cluster-voxel threshold (cluster defining threshold p &lt; 0.001, T &gt; 3.3852). 
  

In previous studies, increased amygdala activation has been reported to both positive and negative valence of faces . Moreover, studies investigating the brain responses to extreme outgroups like homeless people and drug addicts find activations in anterior insula where it is typically interpreted as a disgust response . We did not find statistically significant activations in amygdala and anterior insula. It is possible that this lack of effect is because of our smaller stimulus sample or that the difference between before and after stimulus pairs is not large enough to produce statistically significant results in this before-after contrast of the same face. 

In sum, we found that people have implicit negative biases against faces that are disfigured, without being aware of harbouring such biases. Disfigured faces evoke greater neural responses in ventral occipito-temporal and right inferior frontal regions as compared to non-disfigured faces. This finding refutes the hypothesis that attractiveness and reward per se drives automatic ventral cortical responses and instead confirms the idea that ventral occipito-temporal regions are sensitive to the salience of faces. 

Moreover, disfigured faces evoke lower neural responses in the anterior cingulate and medio-prefrontal cortex, as well as some visual areas. This result is similar to previously reported neural responses to stigmatized outgroups like homeless people and drug addicts . In agreement with this research, we speculate that the de-activation of these brain areas upon seeing disfigured faces as opposed to the same faces after surgical treatment possibly reflects an inhibition of empathy and mentalizing or inhibition of socially inappropriate behaviour. The medial anterior cingulate gyrus and the adjacent medial prefrontal cortex are core areas of the theory of mind and empathy networks  and are crucial for inferring other’s beliefs, feelings, and mental states. Together with previous behavioural research showing a clear association of negative personality traits and our findings of an implicit bias against disfigured faces, we take these response patterns as neural evidence for stigmatization. Future research should investigate if the de-activation of anterior cingulate cortex represents a consistent neural marker for dehumanization of people with disfigured faces or if it reflects social adaptive behaviour to people who deviate from the norm. 

The emphasis of attractiveness, its association with positive attributes and robustness of these associations across cultures  highlights the pervasive effect of attractiveness in social interaction. People who fall towards the lower end of the attractiveness spectrum are disadvantaged or even subject to discrimination and social isolation as in the case of facial disfigurement. Encouragingly, our findings suggest that surgical treatment of disfigurement mitigates the negative effects of disfigurement. Our findings highlight the importance of recognizing that we implicitly and automatically regard flawed faces as flawed people and that corrective surgery confers social and psychological benefits to people with facial disfigurement. Alternative prevention strategies against discrimination of disfigured people and effective support for people with facial conditions should be explored. 


## Methods 
  
### Implicit association test (IAT) and explicit bias questionnaire (EBQ) 
  
#### Participants 
  
80 participants were recruited via an online recruiting system for psychology experiments at the University of Pennsylvania (55 female, 25 male, mean age = 23 years, SD = 6.4, range 18–56). The sample size was determined based on estimates suggested by a meta-analysis on attitudes towards individuals with disabilities as measured by an IAT . Prior to participation, participants were informed that the task was about categorising faces and words but were naïve to the fact that some of those faces might be disfigured. Participation was voluntary, and participants received money as compensation. Study procedures were approved by the Institutional Review Board (IRB) at the University of Pennsylvania (Protocol #806447). IRB approval was in accordance with the International Conference on Harmonization and the Belmont report. All participants gave written informed consent. 

One participant was excluded from the data analysis for the IAT because more than 10% of the total test trials were unreasonably fast (&lt;300 ms). After data exclusion, the data of 79 participants went into the final analysis (55 female, mean age = 23 years, SD = 6.4, range 18–56). 


#### Procedure 
  
Task order between the IAT and the EBQ was counterbalanced so that half of the participants completed the IAT first, and half of the participants completed the EBQ first. Participants were seated in a testing room, in front of a testing laptop. After having been briefed on the order of the tasks, participants gave written informed consent. The entire experiment took about 30 minutes. 

The IAT  was designed using E-Prime software and was modelled after the IATs from   Project Implicit   (  https://implicit.harvard.edu  ). A total of 16 words were used for the IAT: 8 were positive words (attractive, happy, approachable, friendly, adore, lovely, spectacular, excellent), and 8 were negative words (ugly, evil, sickening, rotten, disaster, disgust, pain, despise). 

Participants completed the EBQ as a survey on Qualtrics. Questions were modelled after the Project Implicit and Changing Faces explicit questionnaires . The questionnaire included 11 questions asking about participants’ prior exposure to and conscious biases against people with facial disfigurement. Participants responded on a scale ranging from 1 to 7 (see   https://osf.io/ca2u9/   for details). 


#### Pictures 
  
Images consisted of photographs of patients with facial disfigurements before and after corrective surgery. These photos were collected from craniofacial and dental surgery atlases and compilations of plastic surgery results. The disfigured faces were photos of the individuals before treatment that were affected by one of the following disfigurements: carcinoma, hyperpigmentation, birthmark, scar or small wound, facial paralysis, isolated weight loss, bone disfigurement, or facial trauma. The non-disfigured faces were photographs of the same individuals after treatment (see   https://osf.io/ca2u9/   for all stimulus pairs). Pre-treatment and post-treatment photographs were cropped (to show only faces, with some hair and neck) and colour-corrected to match in size and coloring . The stimulus set consisted of 28 faces, of which 22 were female and 6 were male. 16 of the faces were oriented frontally, 10 were oriented in a three-quarters portrait view, and 2 were profiles (see   https://osf.io/ca2u9/  ). 


#### Implicit association test and explicit bias measure results 
  
Explicit scores range from −3 to +3, with zero indicating no relative preference for non-disfigured vs. disfigured faces. Positive scores indicate a preference for non-disfigured faces, and negative scores indicate a preference for disfigured faces. We found a significant implicit preference for non-disfigured faces (mean difference score = 0.90; SD = 0.58; min = −0.26; max = 2.00;   t   = 13.80; 95% CI = 0.77 to 1.03; p &lt; 0.001; Cohen’s   d   = 1.55). This effect was particularly strong for male respondents (see Table   for details, see Fig.  ). Participants showed no significant explicit preference for non-disfigured vs. disfigured faces (mean explicit score = 0.01; SD = 0.51; min = −1.50; max = 1.08.168;   t   = 0.17; 95% CI = −0.11 to 0.12; p = 0.866; Cohen’s   d =   0.02). Prior exposure had no effect on bias for either the IAT or the EBQ. There was a small to moderate correlation between implicit and explicit scores that was, however, not statistically significant (Pearson’s correlation coefficient, r = 0.22; p = 0.052) making it difficult to draw conclusions as to whether people are aware of their biases.   
Implicit preferences for non-disfigured vs. disfigured faces for all participants by gender. 
  
IAT D scores range from −2 to +2, with zero indicating no relative preference for non-disfigured vs. disfigured faces. Positive scores indicate an implicit preference for non-disfigured faces while negative scores indicate an implicit preference for disfigured faces. D scores were interpreted according to specific, conservative break points based on Cohen’s   d  : ±0.15 (‘slight’ bias), 0.35 (‘moderate’ bias), 0.65 (‘strong’ bias). 

Cohen’s   d   is a standardized effect size, interpreted as   d   of 0.2 = small effect,   d   of 0.5 = medium effect, and   d   ≥ 0.8 = large effect. 
  



### FMRI experiment 
  
#### Participants 
  
We recruited 34 healthy right-handed college students from University of Pennsylvania (24 females, 10 males). Age of participants ranged from 18–35 years. Participants had normal or corrected to normal vision and no prior history of psychiatric or neurological disease. Before participation in the study, each individual gave informed consent approved by the IRB at the University of Pennsylvania (Protocol #806447) in accordance with the International Conference on Harmonization and the Belmont report. 

The data of three participants was excluded from the final analysis. One dataset was excluded because of technical failure which stopped the stimulus presentation halfway through the experiment. Two other datasets were excluded because of synchronization problems between experimenter laptop and the scanner triggers. The data of 31 participants entered the final analysis (22 females, 9 males). 

The EBQ for the participants in the fMRI experiment showed that about half of the participants have a close friend or family member with either a facial disfigurement or a disability. Exposure to people with facial disfigurement was normally distributed in the sample, and most participants reported no to slight preference for non-disfigured over disfigured people (22/28 data entries, 5 missing values). 


#### Procedure and stimulus presentation 
  
The experiment consisted of one session. Participants were presented with 28 pictures of faces in randomized order and were asked to decide whether the displayed face was male or female. Half of the presented pictures were photographs of patients before treatment, and half after treatment. The pictures were identical to the ones used in the behavioural experiment (IAT, see above). Stimuli were presented using E-prime software by projecting them onto a screen using a projector outside the MR scanner room, which could be seen by participants through a mirror mounted over the head coil. Each picture was presented for 6 seconds. Responses were recorded with a 2-button response device. After the experiment, a high-resolution anatomical scan (~7 min) was conducted. After the scanning session, participants were taken out of the scanner and completed the EBQ for disfigurement on a testing computer outside the scanner room. This test was identical to the EBQ in the online sample reported above. 


#### fMRI data acquisition and pre-processing 
  
Images of blood-oxygen level dependent (BOLD) changes were acquired with a 3 T Siemens Magnetom Prisma scanner (Erlangen, Germany) with a 64-channel head coil. We used cushions to minimize participants’ head movement. We used two localizing scans and auto-alignment. Functional images were acquired using a standard BOLD sequence (TR: 2000 ms, TE: 30 ms, flip angle: 60 degrees, voxel size: 2.0 × 2.0 × 2.0 mm, 81 slices). High resolution (0.8 × 0.8 × 0.8 mm) structural (anatomical) images were acquired using an SPC T1 GRAPPA sequence . Data were pre-processed using the Matlab toolbox SPM12 (  http://www.fil.ion.ucl.ac.uk/spm  ). Images were motion corrected and registered to the first image of the scanning block. The mean of the motion-corrected images was co-registered with the individual participants’ anatomical scan. The anatomical and functional scans were spatially normalized to the standard MNI template. Finally, all data were spatially smoothed using an isotropic 8 mm full width at half maximum (FWHM) Gaussian kernel. 


#### fMRI data analysis 
  
At the single-subject level, statistical analysis was performed using a general linear model. The motion estimates of the motion correction algorithm were modelled as regressors of no interest to account for head motion. We performed a whole-brain group analysis by directly contrasting the mean activations per condition in a non-parametric design with SnPM (  https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/nichols/software/snpm  ). Results were corrected for multiple comparisons with a combined voxel-cluster level threshold by familywise error correction at p &lt; 0.05 with Monte Carlo permutation testing. 

In addition to the whole brain group analysis, we performed an item-wise region of interest control analysis to test if the effects in the group analysis are driven by specific items. The two clusters were defined by the group contrasts in the whole brain analysis and consisted of one area comprising of the two large occipital activation clusters, and one comprising the (de-)activation cluster in the anterior cingulate cortex. Mean values for these two regions were extracted for each subject and item. The mean values were normalised with the individual subject’s mean activation in this area to create relative difference scores per subject and item. The data for the item-wise analysis were analysed with linear mixed effect models in RStudio. We built one base model for each dependent variable (occipital cluster activation, anterior cingulate cluster activation) that included condition (pre or post surgery picture) as a predictor and subject and item as random factors with random intercepts. We tested for both random factors whether including random slopes for the condition would improve the model fit and tested interactions with gender and EBQ responses with the best base model (see   https://osf.io/ca2u9/   for details). 


#### FMRI sample results 
  
Participants performed at ceiling for the gender judgment task. 

An ANOVA analysis of the reaction time data in the gender judgement task in the scanner revealed no differences in reaction times between before and after treatment pictures (F  = 0.56, p = 0.45, see Fig.  ) and no differences for item (F  = 1.26, p = 0.17) and no interaction between item and face type (F  = 1.06, p = 0.38).   
Reaction times for gender judgement task per item split by face type. Error bars display 95% confidence intervals. 
  

We found increased activations in temporo-occipital regions encompassing bilateral middle occipital and fusiform gyrus, left inferior occipital gyrus, as well as right inferior temporal and right inferior frontal gyrus (Fig.  ; see Table   for details). An area in the medial anterior cingulate cortex and an area in the right calcarine gyrus showed significant decrease in activation in response to faces before surgery (Fig.  ; see Table   for details). All clusters statistically significant at p &lt; 0.05 FWE at the cluster level corrected by Monte Carlo permutation testing (cluster forming threshold p &lt; 0.001 per voxel).   
Increased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing. 
    
Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing. 
  

The ROI analysis controlling for random effects of items and subjects confirmed the results of the whole brain analysis (see Figs   and  , see   https://osf.io/ca2u9/   for analysis code and full statistical details). Whether the picture of a person was presented from before or after surgery had a significant effect on the BOLD activation level in the anterior cingulate cluster (β = −0.15, s.e. = 0.05, t = −2.95), as well as the occipital cortex (β = 0.17, s.e. = 0.03, t = 5.31). Neither gender of the participant, any of the EBQ measures (see Tables   and   for descriptive statistics), or the gender of the depicted person was found to be related to BOLD activation level differences.   
Itemwise mean activation in the occipital cortex. Stimulus items that do not follow the general activation pattern are Item 2, 7, 12, 25, and 28. 
    
Itemwise mean activation in the anterior cingulate cortex. Stimulus items that do not follow the general activation pattern are Item 1, 25, and 28. 
    
Summary of the EBQ responses I. 
    
Summary of the EBQ responses II. 
  



 ## Data Availability

The datasets generated and analysed during the current study will be made available without restriction on Open Science Framework (DOI 10.17605/OSF.IO/CA2U9) upon acceptance of the article for publication, https://osf.io/ca2u9/. https://osf.io/ca2u9/</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (2)</summary><details class="inner-accordion"><summary>Table 2 (Tab2) - Increased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</summary><div class="table-html"><table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Increased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Location</th><th colspan="1" rowspan="1">k</th><th colspan="1" rowspan="1">T-max</th><th colspan="1" rowspan="1">x</th><th colspan="1" rowspan="1">y</th><th colspan="1" rowspan="1">z</th></tr></thead><tbody><tr><td colspan="1" rowspan="1">left lateral occipital gyrus/BA 18</td><td colspan="1" rowspan="1">3442</td><td colspan="1" rowspan="1">8.08</td><td colspan="1" rowspan="1">−28</td><td colspan="1" rowspan="1">−98</td><td colspan="1" rowspan="1">8</td></tr><tr><td colspan="1" rowspan="1">right lateral occipital gyrus/BA 18</td><td colspan="1" rowspan="1">2377</td><td colspan="1" rowspan="1">6.98</td><td colspan="1" rowspan="1">34</td><td colspan="1" rowspan="1">−90</td><td colspan="1" rowspan="1">2</td></tr><tr><td colspan="1" rowspan="1">right inferior frontal gyrus/BA 44</td><td colspan="1" rowspan="1">230</td><td colspan="1" rowspan="1">5.02</td><td colspan="1" rowspan="1">42</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">26</td></tr></tbody></table></table-wrap></div></details><details class="inner-accordion"><summary>Table 3 (Tab3) - Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</summary><div class="table-html"><table-wrap id="Tab3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Location</th><th colspan="1" rowspan="1">k</th><th colspan="1" rowspan="1">T-max</th><th colspan="1" rowspan="1">x</th><th colspan="1" rowspan="1">y</th><th colspan="1" rowspan="1">z</th></tr></thead><tbody><tr><td colspan="1" rowspan="1">left and right anterior cingulate cortex/BA 24</td><td colspan="1" rowspan="1">765</td><td colspan="1" rowspan="1">4.92</td><td colspan="1" rowspan="1">−2</td><td colspan="1" rowspan="1">36</td><td colspan="1" rowspan="1">10</td></tr><tr><td colspan="1" rowspan="1">right calcarine gyrus/BA 18</td><td colspan="1" rowspan="1">247</td><td colspan="1" rowspan="1">4.10</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">−88</td><td colspan="1" rowspan="1">12</td></tr></tbody></table></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 31598216</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=2</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/31598216/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> decision-making &gt; results; socialcommunication, results &gt; decision-making; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>31598216_analysis_0</td><td>decision-making</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>The paradigm includes face stimuli and trustworthiness judgments and requires processing of social signals (facial information, mentalizing), meeting social communication criteria.</td></tr>
<tr><td>31598216_analysis_1</td><td>results</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Result perception engages processing of social feedback and facial trustworthiness updates (faces presented before/after), implicating social communication mechanisms.</td></tr>
<tr><td>31598216_analysis_2</td><td>PATIENTS</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Patient analyses involve face stimuli, trust ratings and mentalizing measures (Reading the Mind in the Eyes), all aspects of social communication.</td></tr>
<tr><td>31598216_analysis_3</td><td>CONTROLS</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>Control analyses include face-based trust ratings and mentalizing, which are core social communication processes.</td></tr>
<tr><td>31598216_analysis_4</td><td>main effect of group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Group-level contrasts involve regions implicated in face processing, trust and mentalizing, consistent with social communication.</td></tr>
<tr><td>31598216_analysis_5</td><td>main effect of condition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Condition contrasts engage face/trust and feedback processing relevant to social communication mechanisms.</td></tr>
<tr><td>31598216_analysis_6</td><td>ROI-main effect of group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>ROIs include regions supporting face and mentalizing processes; analyses relate ROI BOLD to trust and face ratings—core social communication functions.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>31598216_1</td><td>decision-making &gt; results; socialcommunication</td><td>31598216_analysis_0</td><td>decision-making</td><td>0.750</td><td>0.093</td><td>0.290</td><td>unmatched</td><td>coord_count_mismatch, low_coord_high_name, low_total_score</td></tr><tr><td>31598216_2</td><td>results &gt; decision-making; socialcommunication</td><td>31598216_analysis_3</td><td>CONTROLS</td><td>0.182</td><td>0.556</td><td>0.443</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr></tbody></table></div>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 6731699)</summary><p><strong>Title:</strong> Social cognition, behaviour and therapy adherence in frontal lobe epilepsy: a study combining neuroeconomic and neuropsychological methods</p><details><summary>Abstract</summary><pre class="paper-text">Social behaviour of healthy humans and its neural correlates have been extensively studied in social neuroscience and neuroeconomics. Whereas it is well established that several types of epilepsies, such as frontal lobe epilepsy, lead to social cognitive impairments, experimental evidence on how these translate into behavioural symptoms is scarce. Furthermore, it is unclear whether social cognitive or behavioural disturbances have an impact on therapy adherence, which is critical for effective disease management, but generally low in these patients. In order to investigate the relationship between social cognition, social behaviour, and therapy adherence in patients with frontal lobe epilepsies (FLE), we designed a study combining conventional neuropsychological with behavioural economic and functional magnetic resonance imaging (fMRI) methodology. Fifteen patients and 15 healthy controls played a prisoners&#x27; dilemma game (an established game to operationalize social behaviour) while undergoing fMRI. Additionally, social cognitive, basic neuropsychological variables, and therapy adherence were assessed. Our results implicate that social behaviour is indeed affected and can be quantified using neuroeconomic methods in patients with FLE. Impaired social behaviour in these patients might be a consequence of altered brain activation in the medial prefrontal cortex and play a role in low therapy adherence. Finally, this study serves as an example of how to integrate neuroeconomic methods in neurology.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Social cognition is a term used for several high-level cognitive functions that determine human behaviour in a social context. Several studies showed impairments of social cognition in frontal lobe epilepsies (FLE), such as Theory-of-Mind/mentalizing and facial emotion recognition [ ], as well as neuropsychiatric co-morbidities [ ], but studies quantifying social behaviour abnormalities of FLE patients in an experimental sense are scarce. Even though behaviour relies on cognitive functions, the relationship between cognition, behaviour and epilepsy variables is complex and not fully understood [ ], making it difficult to predict the impact of altered social cognition on social behaviour, and more specifically, therapy adherence in FLE. 

In neuroeconomics—the science of studying research questions in economics through the application of neuroscientific methods and theories—pro-social behaviours, such as trust or cooperation, have been extensively studied for decades, mainly through game paradigms, such as the trust game or the prisoners&#x27; dilemma game [ ]. Using this approach, several brain areas were found to play a role in pro-social behaviour of healthy humans. Crucial parts of this brain network reside in the frontal cortex such as the ventromedial frontal/orbitofrontal cortex and the anterior cingulate cortex [ ]. These discoveries have led to the adoption of economic methodology to study behaviour in neurology [ ], but to the best of our knowledge, no such study has focused on the behaviour of patients with epilepsy. 

Adherence to anti-epileptic drug therapy is critical for effective disease management. Although its measurement is difficult without a single method that has yet proved to be the gold standard [ ], therapy adherence of patients suffering from epilepsy is low (at about 30–50%) [ ]. This is unfortunate, as low therapy adherence not only leads to poorer seizure control, but also increases the risk of sudden unexpected death in epilepsy [ ]. The reasons for low therapy adherence in general are still a matter of research [ ]. Low therapy adherence seems to have a multifactorial origin with some factors being associated with neuropsychological impairments and psychiatric co-morbidities of chronic diseases [ ], and others reflecting pathophysiological changes of neural networks specifically affected in epilepsy [ ]. 

We thus designed a study combining conventional neuropsychological with neuroeconomic methods to address the following open research questions:
   
Is there a difference in social behaviour between FLE patients and healthy controls that can be experimentally operationalized? 
  
If such a difference exists, is it associated with altered social cognitive functions and their underlying frontal lobe network? 
  
Does social behaviour affect therapy adherence in FLE? 
  


## Methods 
  
### Subjects 
  
We included 15 FLE patients and 15 healthy controls. We established the following inclusion and exclusion criteria to ensure the feasibility of the study and avoid known confounders in behavioural research. 

Inclusion criteria for patients were: age between 18 and 50, right-handedness (Edinburgh handedness inventory; [ ]), no significant anxiety, depression, or obsessive-compulsive symptoms as assessed by a hospital anxiety and depression scale score (HADS) less than 10 [ ] and an obsessive compulsive inventory score (OCIS) less than 40 [ ], sufficient language skills, and a diagnosis of FLE. The latter was based either upon association between typical frontal lobe seizure semiology and existence of an epileptogenic MRI lesion within the frontal lobe or upon direct recording of seizures of frontal lobe origin during a long-term video-EEG monitoring performed in the Department of Functional Neurology and Epileptology at Hospices Civils de Lyon, France, in patients with normal MRI. Exclusion criteria included pregnancy, non-MRI suitable transplants, major perceptive impairments, non-epileptic seizures, a history of intellectual disability, other known neurological diseases, MRI-lesion outside of the frontal lobe or frontal cortical lesions larger than 1 cm in diameter, and past epilepsy or other brain surgery in order to achieve a patient sample of non-resected participants. 

Inclusion criteria for controls were: age between 18 and 50, right-handedness (Edinburgh handedness inventory; [ ]), no significant anxiety, depression, or obsessive-compulsive symptoms assessed by a HADS Score less than 10 [ ] and an OCIS less than 40 [ ] as well as sufficient language skills. Exclusion criteria were pregnancy, non-MRI suitable transplants, major perceptive impairments, medication other than contraceptives, or a history of neurological or psychiatric diseases. 


### Magnetic resonance data acquisition 
  
All images were acquired using the same MRI machine (Siemens Magneton Prisma 3 Tesla) in one session per participant. Structural MRI and shimming (to minimize field inhomogeneities) were performed on all subjects prior to gradient-echo echoplanar imaging that provided blood oxygen level dependent (BOLD) contrast. Each volume comprised 35 AC-PC aligned slices (order of acquisition: interleaved) with a thickness of 2.5 cm, field of view (FOV) 23 cm, parallel imaging parameters GRAPPA/acceleration factor 2, echo time (TE) 26 ms, repetition time (TR) 2260 ms. 


### Paradigm 
  
We designed our paradigm on the basis of similar experiments in the literature [ ] to ensure comparability. When subjects entered the laboratory for the experiment, they received written instructions explaining the prisoners&#x27; dilemma game (PDG) [ ]. In this game, two players decide at the same time whether or not to cooperate with the other player. Depending on the decisions of both players, there is a monetary pay-off, which is equal for both players in the case of mutual cooperation or defection, whereas in the case of divergent strategies, it is not existent for the cooperating player and highest for the defecting player. The pay-off structure of our version of the game is shown in  .
   
Pay-off structure of the prisoners&#x27; dilemma game. 
  

Participants were told that they would play with real money and be paid out their gain. Payment took place at the end of the experiment, but in fact, all participants received the same amount of money (40€) independent of their actual gain due to ethical considerations. The participants did not know the number of rounds played in the game, but in fact all subjects played 32 rounds. Moreover, participants (player 1) were told that they would be playing with four different human beings (two males and two females—here termed player 2) located in another room, but actually played against a computer-generated, randomized, strategy that simulated the other player&#x27;s behaviour. This means that each participant played 32 rounds with each of the four different counterparts (equals 128 decisions to cooperate or to defect in total). Player 2 was illustrated with face images from an established database (Glasgow Unfamiliar Face Database). Once installed in the MRI machine, participants first had to rate the trustworthiness of the presented player 2 on a Likert-scale between 1 and 7 (1 no trustworthiness, 7 highest possible trustworthiness) being shown a face picture of player 2. Next, the actual game began. Each round of the game consisted of 3 screens: first, the pay-off matrix of the game was shown for 2 s. Then, participants were asked to choose to either cooperate or to defect using two input buttons. After a random time interval between 4 and 6 s simulating variable decision times of player 2, the result of the round was shown for 2 s using the pay-off matrix with the result highlighted in yellow colour. After 32 rounds of the game, the face of player 2 was again shown to the participants, who had to rate the trustworthiness of the face another time in analogy to the start. Finally, the overall gain of the game was shown. See   for a visualization.
   
Visualization of the experimental paradigm. Notes: top left—visualization of the beginning and end of the paradigm, bottom right—visualization of the time period in between (corresponds to dashed line in the top left part of the figure); PDG, prisoners&#x27; dilemma game. 
  


### Demographic data, neuropsychological testing, questionnaires and pill counts 
  
We recorded the following variables in all participants via a self-report questionnaire: age, gender, profession and education. For patients, we included the number of seizures (generalized tonic-clonic seizures and other seizure types separately) in the last three months according to a seizure calendar commonly used in clinical routine and current anticonvulsive treatment (number and names of drugs, number of intakes per day, preparation of drugs for intake by the participant or by a carer) as recorded in the patient&#x27;s medical chart. 

Neuropsychological variables captured in all participants included psychomotor speed and mental flexibility (Trail Making test A and B [ ]), memory for faces and working memory (Faces subsets and numbers of the Wechsler Memory Scale, fourth edition [ ]). 

Social cognition was tested through the Reading the Mind in the Eyes test [ ] for Theory-of-Mind/mentalizing (in this test, subjects have to choose the correct word out of a list describing emotions to corresponding photos of a person&#x27;s expression of eyes) and a trust questionnaire ([ ]; French version), additionally to the trustworthiness ratings of faces during the fMRI paradigm. 

Furthermore, therapy adherence was measured in the patient group through pill counts at two consecutive visits [ ] by reviewing pill bottles of a six months period and several questionnaires comprising the Morisky adherence scale ([ , ]; validated French version), the Beliefs about Medicines Questionnaire (BMQ; [ , ]; validated French version) and the SATMED-Q (Treatment Satisfaction with Medicines Questionnaire, [ ]; French version). These questionnaires are commonly used instruments in research on therapy adherence. 


### Data analysis 
  
#### Imaging data 
  
fMRI data were analysed using SPM 12 ( ). Pre-processing involved slice time correction, realignment, normalization and smoothing with an 8 mm full width at half maximum Gaussian kernel. A general linear model with two conditions (decision phase and result phase) was estimated. No subject had to be excluded. The contrast images calculated for individual subjects were entered into a second level or random effects analysis [ ]. We first calculated   t  -tests for the two conditions (decision/result) for all participants as one group and in a second step analysed each group (patients/controls) separately in the same way. In a third step, we calculated two-sample   t  -tests between the two groups for both conditions. We then implemented a 2 × 2 repeated measures ANOVA using a flexible factorial design with the factors ‘group&#x27; (patients/controls) and ‘condition&#x27; (decision/result) to calculate main effects of group and condition as well as the interaction effect of group and condition. The resultant statistical parametric maps were thresholded using an FWE-corrected   p  -value threshold less than 0.001, reporting clusters greater than 20 voxels (  k   = 20) only. Anatomical structures of cluster maxima were labelled in Talairach space using the Talairach Client ( ). 

Based on the previous research, we   a priori   selected the following regions of interest (ROIs) for further analyses: superior, middle and inferior frontal gyri, medial and lateral orbital gyri, posterior orbital gyrus, straight gyrus, anterior cingulate gyrus, amygdala, thalamus and caudate nucleus. We used adult brain maximum probability maps (© Copyright Imperial College of Science, Technology and Medicine 2007. All rights reserved) to obtain the ROIs [ ]. 

In a subsequent step, we computed the % of BOLD signal change extracted from beta images from significant voxels from the second-level analysis within an 8 mm sphere surrounding the activation peak and calculated correlations with all other variables collected during the study. 


#### Neuropsychological and questionnaire data 
  
The statistical analysis was performed with SPSS, v. 20 and involved two-tailed, non-parametric testing (Wilcoxon test,   χ  ²-test), as well as Spearman&#x27;s rho correlations. We corrected for multiple testing using the Bonferroni–Holm procedure and chose a significance level of less than 0.05. 




## Results 
  
### Behavioural data, neuropsychological variables and questionnaires 
  
We included 15 FLE patients and 15 healthy controls. Four patients (26.67%) were seizure free, while the remaining eleven (73.33%) had a median monthly seizure frequency of 4 during a three-month period preceding the experiment. MRI was normal in nine patients (60%) and showed frontal lobe lesions—cavernoma (one patient), focal cortical dysplasia (one patient), diffuse axonal trauma (one patient), and post-haematoma scars (two patients)—in the remaining five patients (33.3%). The remaining patient had undergone skull (but not brain) surgery in childhood for craniostenosis and the current structural MRI showed no obvious pathological changes. Three patients were on monotherapy and 12 patients were under polytherapy (mean number ± s.d. of anti-epileptic drugs: 2.33 ± 0.98). Mean ± s.d. subjective (Morisky adherence scale) and objective (pill counts) measures of treatment adherence during a six-month period were 1.73 ± 0.88 (range: 0–8, lower number indicates higher adherence) and 7.00 ± 12.45 (number of prescribed pills not taken), indicating moderate level of adherence. Patients also showed high belief of treatment necessity (BMQ mean ± s.d.—necessity 21.13 ± 6.30, range: 5–25), average level of concerns and negative views regarding therapy (BMQ: concerns and harms 24.87 ± 9.36, range: 9–45; overuse 7.47 ± 3.27, range: 4–20) and an average treatment satisfaction (SATMED-Q mean ± s.d.: 65.36 ± 7.88, range: 17–85). Questionnaires capturing beliefs about medicines (  p   = 0.256), treatment satisfaction (  p   = 0.776), or adherence (  p   = 0.056) did not correlate significantly with therapy adherence measured through pill counts, although the Morisky adherence scale was close to being significant (  p   = 0.056). 

For all demographic, neuropsychological and neuroeconomic data collected in this study on both patient and control groups, please refer to  . In summary, there was no significant difference between the patient and control groups regarding age, gender, handedness, education, professional status, trust in doctors as captured by the respective questionnaire [ ], trustworthiness of game opponents, or working memory for numbers. By contrast, patients demonstrated statistically significant worse performance than controls in mentalizing measured through the Reading eyes in the mind test, memory for faces, psychomotor speed during Trail making test A, and mental flexibility performance during Trail making test B.
   
Summary of data collected on both participant groups of our study. 
    

Furthermore, there was also a significant difference in the total number of cooperative choices during the PDG, indicating higher cooperation in patients than in controls. The numbers of cooperative choices between games 1 and 4 differed significantly in both groups, indicating that cooperation decreased significantly between the first and the last game of the experiment in both groups ( ).
   
Mean numbers of cooperative choices of the patient and control groups in the four prisoners&#x27; dilemma games. 
  

Mentalizing correlated strongly with memory for faces (  r   = 0.715;   p   = 0.000) and with trust in doctors and the health system [ ] (  r   = 0.437;   p   = 0.026), showing that participants with high mentalizing abilities also better memorized faces and had higher trust in healthcare. Taken together these data supported a positive correlation between several pro-social cognition and behaviour variables. 

There was a significant positive correlation between cooperative choices in the PDG and missed medication intakes with high values representing low therapy adherence (  r   = 0.686;   p   = 0.010), indicating higher cooperative behaviour in patients with low therapy adherence. 


### Imaging results (see tables  ,   and  ) 
  
#### Voxel-based whole brain analysis 
  
 Brain activation during decision-making.   During the decision-making whether to cooperate or defect when playing the PDG, combined event-related fMRI analysis of data from all subjects (one sample   t  -test) showed significantly activated clusters in right inferior parietal lobule, left precuneus, left lingual gyrus, left insula, the middle and inferior frontal gyri bilaterally, left superior frontal gyrus, as well as the right anterior lobe of the cerebellum ( ). In controls, significant activation was observed in the right fusiform gyrus, right superior frontal gyrus, left insula and left superior temporal gyrus, while in patients, significant activation was observed in the left superior frontal gyrus, left precuneus, and left insula.
   
Visualization of brain activation of all participants (according to one sample   t  -test results reported in  ) during decision-making (yellow) and result perception (red)—thresholded at   p   &lt; 0.001, FWE-corrected,   k   = 20. 
    
Summary of analysis of all participants. FWE,   p   = 0.001. 
    
Summary of per group analyses. FWE,   p   = 0.001. 
    
Analysis of group differences and ROI. 
  

 Brain activation during result phase.   During the perception of the results of the game, combined analysis of data from all subjects (one sample   t  -test) showed activation in the inferior parietal lobules bilaterally and the right middle temporal gyrus—these results are visualized in   as well. In controls, significant activation was observed in the left middle frontal gyrus, inferior parietal lobules bilaterally, right inferior temporal gyrus, and left cingulate gyrus. In patients, significant activation was observed in both inferior parietal lobules, left superior temporal gyrus, right middle frontal gyrus, right thalamus, left insula and right precuneus. 

 Individual group comparisons for both conditions.   Two-sample   t  -tests between the patient and control groups did not yield any significant results for both conditions. 

 Integrated comparison across groups and conditions.   In the full factorial analysis, the main group effect (  F  -test) showed significant clusters in the right and left medial prefrontal cortex—Brodmann area 10 ( ). In the main effect of condition (decision-making versus result phase of the game), the following clusters were significant: right anterior lobe of the cerebellum, left precentral gyrus, right inferior frontal gyrus, right insula, left middle temporal gyrus and left superior frontal gyrus. There was no significant interaction between group and condition.
   
Difference in activation between the patient and control groups (main effect of groups,   F  -test) in the whole-brain analysis—thresholded at   p   &lt; 0.001, FWE-corrected,   k   = 20. 
  


#### Region of interest analysis 
  
ROI analysis (  F  -test) showed differences in activation between the patient and control groups in the right and left superior frontal gyrus, while the   t  -test analyses showed no significant differences. The mean % of BOLD signal changes in these ROIs in the result condition showed a significant negative correlation with initial trust (  r   = −0.448;   p   = 0.017), indicating that participants with high initial trust showed lower signal change in these brain areas during result perception. The mean % of BOLD signal changes in the choice condition also correlated significantly with differences between the initial and final trust (  r   = −0.618;   p   = 0.000; first ROI and   r   = −0.545;   p   = 0.003 for the second ROI), meaning that participants with a high decrease of trust from the beginning to the end of the games showed lower signal changes while making their choices in the game. 

Another significant correlation was detected between therapy adherence and the % BOLD signal change in the choice condition (  r   = 0.565;   p   = 0.044 for the first ROI) showing that participants with low therapy adherence had higher signal changes in this brain area. 




## Discussion 
  
In this study, we first confirmed that mentalizing is impaired in patients with FLE, as previously established [ ]. These abnormalities correlated with an altered memory for faces, a previously unreported finding in FLE, which may shed light on the mechanisms underlying dysfunction in social behaviour. Interestingly, there is evidence of frontal lobe contributions to memory for faces [ ]. Furthermore, mentalizing abilities correlated with the results of the trust in healthcare questionnaire implying an overall good correlation of pro-social cognition variables in our study. 

One of the goals of this study was to find out whether deficits in social cognition in FLE patients could lead to differences in social behaviour that can be operationalized. In fact, we showed that FLE patients behave differently from healthy controls in the PDG, but in a counterintuitive way, since they cooperated more than controls. While this finding was unexpected, we observed a frequent pattern of evolving cooperation during the game in both the patients&#x27; and controls&#x27; groups, with higher rates of cooperation in early game phases and less cooperative behaviour in the later ones [ ]. This strategy is consistent with the objective of maximizing profit. Indeed, future possible interactions encourage people to cooperate in the early phases of the game, with the hope to initiate a mutually cooperative relationship. If not reciprocated, strategy will later shift to defection in players who want to maximize their own profit [ ]. 

In this context, higher cooperation in patients might result from impaired negative feedback when cooperative behaviour is not reciprocated, especially because the ability to shift behaviour from cooperation to defection in such a case is dependent on frontal lobe functions [ ]. Alternatively, patients&#x27; behaviour could be interpreted as a preference for higher delayed rewards since higher cooperation leads to higher future profits [ ]. Although evidence on time preference for rewards (also referred to as ‘delay discounting&#x27;) in neurological patients is scarce, altered delay discounting as another behavioural symptom of neurological disorders was reported before [ ]. 

Prior functional brain imaging studies investigating cooperative behaviour through the PDG found, in essence, three brain networks to be active during the game: several frontal brain areas, especially the medial prefrontal cortex, as well as reward and limbic brain regions [ ] reflecting the cognitive functions necessary to successfully play the game, such as decision-making and reward-based learning. Our analyses of brain activation of all participants showed a similar pattern of brain areas to be active during the game. However, when looking at the patient and control group separately, controls, but not patients showed significant activation in several temporal regions during decision-making whether to cooperate or not. Interestingly, those regions have been implicated in both Theory-of-Mind and facial processing tasks (e.g. [ ]) and thus further substantiate our behavioural results of higher mentalizing and face memory abilities in the control group. During the perception of the results during the game, the cingulate gyrus, a region thought to be active in situations of cognitive conflict [ ], was significantly activated in controls, but not in patients. When opponents in an economic game do not reciprocate benevolent behaviour, this is perceived as a conflict between economic self-interest (i.e. rationality) and fairness considerations [ ]. Hypothetically, patients might have not perceived cognitive conflicts to the same extent as controls in such game results, which might also have led to higher cooperative behaviour. 

When comparing our functional imaging results between the patient and control groups on both whole-brain and ROI level, it becomes apparent that the activation of the medial prefrontal cortex (MPFC) differed significantly between the two groups during the game. The MPFC plays a pivotal role in social behaviour and decision-making [ , ], which is reflected in our results of correlating MPFC activation changes and face trustworthiness evaluations, although those have been based upon a low number of data points only. Therefore, a difference in MPFC activation between two groups showing such divergent cooperative behaviour seems plausible. Our fMRI findings do not allow us to infer the directionality of the observed difference in activation, potentially because of either the difference being based on both conditions rather than differences in activation during the individual conditions, the   t  -test comparisons between groups being underpowered, and/or the difference in activation between the groups not being sufficiently strong to pass the set threshold in the   t  -test analyses. Although a pathology-driven lower activation might seem more plausible, a compensatory higher activation is also possible [ ]. Further imaging studies are needed to address this question. 

Another goal of our study was to investigate possible links between cooperative behaviour and therapy adherence in FLE. Our results show that cooperative behaviour in the PDG correlated negatively with therapy adherence in patients (high cooperative behaviour correlating with low therapy adherence). Even though we could not find any previous literature on the relationship between cooperative behaviour and therapy adherence, there is some evidence that decision-making, in general, has effects on therapy adherence with lower decision-making skills leading to lower adherence [ ]. As we have outlined before, higher cooperation can be interpreted as lower social decision-making abilities in the context of our experiment. Thus, the inverse relation between therapy adherence and cooperative behaviour actually fits into the current scientific framework on the interaction of behaviour, decision-making and therapy adherence. Importantly, there was no correlation between questionnaire data about therapy adherence and pill counts in our study, reflecting the known difficulties in measuring adherence in epilepsy (e.g. [ ]). Moreover, the MPFC activation during the game showed a correlation with therapy adherence reflecting the close link between social cognition and therapy adherence. 

We acknowledge that our study design of comparing people with FLE to healthy controls does not allow distinguishing whether the study findings apply solely to FLE or to epilepsy in general. Looking at the published literature on other epilepsy syndromes with frontal lobe dysfunction, such as juvenile myoclonic epilepsy or genetic generalized epilepsies, it becomes apparent that patients with these types of epilepsies also show abnormalities in social functions such as Theory-of-Mind [ , ]. In our view, neuroeconomic methodology could help us to determine the behavioural consequences of these social cognitive impairments. Further studies comparing cooperative behaviour and therapy adherence between FLE and other epileptic disorders are therefore warranted. 


## Conclusion 
  
To conclude, our results implicate that (i) social behaviour is affected by FLE and (ii) can be measured using neuroeconomic methods. Impaired social behaviour in FLE might (iii) be a consequence of differing MPFC activation and (iv) might play a role in low therapy adherence. This is important as therapy adherence is difficult to measure, especially in patients with epilepsy where classical tools of measurement (e.g. questionnaires or electronic devices) have been shown to be imprecise [ ]. Integrating neuroeconomic testing of social behaviour into the neuropsychological testing routine could help to better understand therapy adherence of patients with epilepsies and consequently help to improve patient care through an identification of patients at risk. 


## Supplementary Material</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (3)</summary><details class="inner-accordion"><summary>Table 3. (RSOS180850TB3) - Summary of analysis of all participants. FWE, p = 0.001.</summary><div class="table-html"><table-wrap id="RSOS180850TB3" orientation="portrait" position="float"><label>Table 3.</label><caption><p>Summary of analysis of all participants. FWE, <italic toggle="yes">p</italic> = 0.001.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1">hemisphere</th><th align="left" rowspan="1" colspan="1">lobe</th><th align="left" rowspan="1" colspan="1">cluster covering</th><th align="left" rowspan="1" colspan="1">Brodmann area</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">p</italic>-value</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">K</italic> size</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">T</italic> score</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th></tr></thead><tbody><tr><td colspan="10" rowspan="1"><italic toggle="yes">decision-making</italic></td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">1720</td><td rowspan="1" colspan="1">13.10</td><td rowspan="1" colspan="1">48</td><td rowspan="1" colspan="1">−38</td><td rowspan="1" colspan="1">46</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">precuneus</td><td rowspan="1" colspan="1">BA 7</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">1748</td><td rowspan="1" colspan="1">12.95</td><td rowspan="1" colspan="1">−20</td><td rowspan="1" colspan="1">−66</td><td rowspan="1" colspan="1">52</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">occipital</td><td rowspan="1" colspan="1">lingual gyrus</td><td rowspan="1" colspan="1">BA 18</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">817</td><td rowspan="1" colspan="1">10.63</td><td rowspan="1" colspan="1">−4</td><td rowspan="1" colspan="1">−82</td><td rowspan="1" colspan="1">−6</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">middle frontal gyrus</td><td rowspan="1" colspan="1">BA 6</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">194</td><td rowspan="1" colspan="1">10.58</td><td rowspan="1" colspan="1">−28</td><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">52</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">BA 13</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">130</td><td rowspan="1" colspan="1">10.55</td><td rowspan="1" colspan="1">−30</td><td rowspan="1" colspan="1">22</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">middle frontal gyrus</td><td rowspan="1" colspan="1">BA 9</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">733</td><td rowspan="1" colspan="1">10.43</td><td rowspan="1" colspan="1">46</td><td rowspan="1" colspan="1">34</td><td rowspan="1" colspan="1">30</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">cerebellum anterior lobe</td><td rowspan="1" colspan="1">culmen</td><td rowspan="1" colspan="1">—</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">368</td><td rowspan="1" colspan="1">10.32</td><td rowspan="1" colspan="1">26</td><td rowspan="1" colspan="1">−48</td><td rowspan="1" colspan="1">−14</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">inferior frontal gyrus</td><td rowspan="1" colspan="1">BA 47</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">300</td><td rowspan="1" colspan="1">10.27</td><td rowspan="1" colspan="1">42</td><td rowspan="1" colspan="1">20</td><td rowspan="1" colspan="1">−2</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">inferior frontal gyrus</td><td rowspan="1" colspan="1">BA 6</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">105</td><td rowspan="1" colspan="1">9.90</td><td rowspan="1" colspan="1">28</td><td rowspan="1" colspan="1">16</td><td rowspan="1" colspan="1">56</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">superior frontal gyrus</td><td rowspan="1" colspan="1">BA 6</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">166</td><td rowspan="1" colspan="1">9.62</td><td rowspan="1" colspan="1">−6</td><td rowspan="1" colspan="1">12</td><td rowspan="1" colspan="1">52</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">temporal</td><td rowspan="1" colspan="1">middle temporal gyrus</td><td rowspan="1" colspan="1">BA 37</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">40</td><td rowspan="1" colspan="1">9.36</td><td rowspan="1" colspan="1">56</td><td rowspan="1" colspan="1">−52</td><td rowspan="1" colspan="1">−12</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">results</italic></td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">343</td><td rowspan="1" colspan="1">10.57</td><td rowspan="1" colspan="1">52</td><td rowspan="1" colspan="1">−50</td><td rowspan="1" colspan="1">46</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">temporal</td><td rowspan="1" colspan="1">middle temporal gyrus</td><td rowspan="1" colspan="1">BA 21</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">74</td><td rowspan="1" colspan="1">8.72</td><td rowspan="1" colspan="1">58</td><td rowspan="1" colspan="1">−34</td><td rowspan="1" colspan="1">−8</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">36</td><td rowspan="1" colspan="1">8.14</td><td rowspan="1" colspan="1">−58</td><td rowspan="1" colspan="1">−42</td><td rowspan="1" colspan="1">40</td></tr></tbody></table></table-wrap>
</div></details><details class="inner-accordion"><summary>Table 4. (RSOS180850TB4) - Summary of per group analyses. FWE, p = 0.001.</summary><div class="table-html"><table-wrap id="RSOS180850TB4" orientation="portrait" position="float"><label>Table 4.</label><caption><p>Summary of per group analyses. FWE, <italic toggle="yes">p</italic> = 0.001.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1">hemisphere</th><th align="left" rowspan="1" colspan="1">lobe</th><th align="left" rowspan="1" colspan="1">cluster covering</th><th align="left" rowspan="1" colspan="1">Brodmann area</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">p</italic>-value</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">K</italic> size</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">T</italic> score</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th></tr></thead><tbody><tr><td colspan="10" rowspan="1">PATIENTS</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">decision-making</italic></td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">occipital</td><td rowspan="1" colspan="1">lingual gyrus</td><td rowspan="1" colspan="1">BA 18</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">11 406</td><td rowspan="1" colspan="1">10.64</td><td rowspan="1" colspan="1">24</td><td rowspan="1" colspan="1">−74</td><td rowspan="1" colspan="1">−10</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">superior frontal gyrus</td><td rowspan="1" colspan="1">BA 6</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">8491</td><td rowspan="1" colspan="1">10.28</td><td rowspan="1" colspan="1">−16</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">64</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">subcortical insula</td><td rowspan="1" colspan="1">claustrum insula</td><td rowspan="1" colspan="1">—</td><td rowspan="1" colspan="1">0.006</td><td rowspan="1" colspan="1">456</td><td rowspan="1" colspan="1">7.63</td><td rowspan="1" colspan="1">−28</td><td rowspan="1" colspan="1">22</td><td rowspan="1" colspan="1">−2</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">results</italic></td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">4211</td><td rowspan="1" colspan="1">12.48</td><td rowspan="1" colspan="1">−60</td><td rowspan="1" colspan="1">−42</td><td rowspan="1" colspan="1">38</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">5094</td><td rowspan="1" colspan="1">11.53</td><td rowspan="1" colspan="1">58</td><td rowspan="1" colspan="1">−44</td><td rowspan="1" colspan="1">44</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">middle frontal gyrus</td><td rowspan="1" colspan="1">BA 46</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">2994</td><td rowspan="1" colspan="1">10.12</td><td rowspan="1" colspan="1">42</td><td rowspan="1" colspan="1">48</td><td rowspan="1" colspan="1">14</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">cerebellum</td><td rowspan="1" colspan="1">anterior lobe</td><td rowspan="1" colspan="1">—</td><td rowspan="1" colspan="1">0.003</td><td rowspan="1" colspan="1">439</td><td rowspan="1" colspan="1">6.91</td><td rowspan="1" colspan="1">−2</td><td rowspan="1" colspan="1">−54</td><td rowspan="1" colspan="1">−4</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">precuneus</td><td rowspan="1" colspan="1">BA 7</td><td rowspan="1" colspan="1">0.016</td><td rowspan="1" colspan="1">304</td><td rowspan="1" colspan="1">6.35</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">−78</td><td rowspan="1" colspan="1">46</td></tr><tr><td colspan="10" rowspan="1">CONTROLS</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">decision-making</italic></td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">temporal occipital</td><td rowspan="1" colspan="1">fusiform gyrus</td><td rowspan="1" colspan="1">BA 37</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">17309</td><td rowspan="1" colspan="1">13.94</td><td rowspan="1" colspan="1">54</td><td rowspan="1" colspan="1">−54</td><td rowspan="1" colspan="1">−16</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">superior frontal gyrus</td><td rowspan="1" colspan="1">BA 9</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">9140</td><td rowspan="1" colspan="1">11.15</td><td rowspan="1" colspan="1">42</td><td rowspan="1" colspan="1">36</td><td rowspan="1" colspan="1">28</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">BA 13</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">756</td><td rowspan="1" colspan="1">8.1</td><td rowspan="1" colspan="1">−34</td><td rowspan="1" colspan="1">18</td><td rowspan="1" colspan="1">8</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">temporal</td><td rowspan="1" colspan="1">superior temporal gyrus</td><td rowspan="1" colspan="1">BA 22</td><td rowspan="1" colspan="1">0.020</td><td rowspan="1" colspan="1">238</td><td rowspan="1" colspan="1">7.69</td><td rowspan="1" colspan="1">−50</td><td rowspan="1" colspan="1">−48</td><td rowspan="1" colspan="1">14</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">results</italic></td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">middle frontal gyrus</td><td rowspan="1" colspan="1">BA 10</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">67860</td><td rowspan="1" colspan="1">9.12</td><td rowspan="1" colspan="1">−34</td><td rowspan="1" colspan="1">48</td><td rowspan="1" colspan="1">−2</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">3234</td><td rowspan="1" colspan="1">7.88</td><td rowspan="1" colspan="1">−46</td><td rowspan="1" colspan="1">−46</td><td rowspan="1" colspan="1">56</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">1086</td><td rowspan="1" colspan="1">7.11</td><td rowspan="1" colspan="1">52</td><td rowspan="1" colspan="1">−50</td><td rowspan="1" colspan="1">46</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">temporal</td><td rowspan="1" colspan="1">inferior temporal gyrus</td><td rowspan="1" colspan="1">BA 20</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">596</td><td rowspan="1" colspan="1">7.08</td><td rowspan="1" colspan="1">58</td><td rowspan="1" colspan="1">−40</td><td rowspan="1" colspan="1">16</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">cingulate gyrus</td><td rowspan="1" colspan="1">BA 24</td><td rowspan="1" colspan="1">0.046</td><td rowspan="1" colspan="1">219</td><td rowspan="1" colspan="1">5.41</td><td rowspan="1" colspan="1">−2</td><td rowspan="1" colspan="1">−14</td><td rowspan="1" colspan="1">38</td></tr></tbody></table></table-wrap>
</div></details><details class="inner-accordion"><summary>Table 5. (RSOS180850TB5) - Analysis of group differences and ROI.</summary><div class="table-html"><table-wrap id="RSOS180850TB5" orientation="portrait" position="float"><label>Table 5.</label><caption><p>Analysis of group differences and ROI.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1">hemisphere</th><th align="left" rowspan="1" colspan="1">lobe</th><th align="left" rowspan="1" colspan="1">cluster covering</th><th align="left" rowspan="1" colspan="1">Brodmann area</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">p</italic>-value</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">K</italic> size</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">F</italic> score</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th></tr></thead><tbody><tr><td colspan="10" rowspan="1"><italic toggle="yes">main effect of group</italic></td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">medial frontal gyrus</td><td rowspan="1" colspan="1">BA 10</td><td rowspan="1" colspan="1">0.034</td><td rowspan="1" colspan="1">214</td><td rowspan="1" colspan="1">22.35</td><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">62</td><td rowspan="1" colspan="1">14</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">main effect of condition</italic></td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">cerebellum</td><td rowspan="1" colspan="1">anterior lobe</td><td rowspan="1" colspan="1">—</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">11343</td><td rowspan="1" colspan="1">108.49</td><td rowspan="1" colspan="1">26</td><td rowspan="1" colspan="1">−50</td><td rowspan="1" colspan="1">−12</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">precentral gyrus</td><td rowspan="1" colspan="1">BA 9</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">1439</td><td rowspan="1" colspan="1">81.83</td><td rowspan="1" colspan="1">−36</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">34</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">inferior frontal gyrus</td><td rowspan="1" colspan="1">BA 9</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">1770</td><td rowspan="1" colspan="1">64.81</td><td rowspan="1" colspan="1">42</td><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">26</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">BA 13</td><td rowspan="1" colspan="1">0.002</td><td rowspan="1" colspan="1">414</td><td rowspan="1" colspan="1">56.86</td><td rowspan="1" colspan="1">32</td><td rowspan="1" colspan="1">22</td><td rowspan="1" colspan="1">−2</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">temporal</td><td rowspan="1" colspan="1">middle temporal gyrus</td><td rowspan="1" colspan="1">BA 21</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">6458</td><td rowspan="1" colspan="1">52.55</td><td rowspan="1" colspan="1">−56</td><td rowspan="1" colspan="1">−22</td><td rowspan="1" colspan="1">−14</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">BA 13</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">4254</td><td rowspan="1" colspan="1">50.94</td><td rowspan="1" colspan="1">42</td><td rowspan="1" colspan="1">−12</td><td rowspan="1" colspan="1">6</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">cingulate gyrus</td><td rowspan="1" colspan="1">BA 24</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">1827</td><td rowspan="1" colspan="1">40.86</td><td rowspan="1" colspan="1">−4</td><td rowspan="1" colspan="1">−14</td><td rowspan="1" colspan="1">40</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">ROI-main effect of group</italic></td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">superior frontal gyrus</td><td rowspan="1" colspan="1">BA 10</td><td rowspan="1" colspan="1">0.007</td><td rowspan="1" colspan="1">219</td><td rowspan="1" colspan="1">31.57</td><td rowspan="1" colspan="1">14</td><td rowspan="1" colspan="1">60</td><td rowspan="1" colspan="1">14</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">middle frontal gyrus</td><td rowspan="1" colspan="1">BA 10</td><td rowspan="1" colspan="1">0.046</td><td rowspan="1" colspan="1">124</td><td rowspan="1" colspan="1">19.57</td><td rowspan="1" colspan="1">−32</td><td rowspan="1" colspan="1">54</td><td rowspan="1" colspan="1">8</td></tr></tbody></table></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 15006683</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/15006683/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>15006683_analysis_0</td><td>Relational vs. alone segment</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Stimuli include socially communicative cues (faces, speech in some clips, actions) and the contrast isolates interactive segments that require processing of communicative signals; reported activation in STS and fusiform supports inclusion.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 15488424</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/15488424/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>15488424_analysis_0</td><td>Cooperation versus independent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although not a face/gaze task, the cooperation condition requires exchange of socially relevant information (action-based communication, coordinated moves), so it measures social communication between players.</td></tr>
<tr><td>15488424_analysis_1</td><td>Competition versus independent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Competition involves exchange of socially relevant signals through actions (blocking/supporting moves) and requires interpreting them, fitting the social communication construct.</td></tr>
<tr><td>15488424_analysis_2</td><td>Cooperation versus competition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast isolates neural differences in how social information is exchanged/coordinated in cooperative vs competitive contexts, thus addressing social communication processes.</td></tr>
<tr><td>15488424_analysis_3</td><td>Competition versus cooperation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast isolates how social communicative exchanges differ when competing versus cooperating (e.g., blocking vs supporting moves), so it meets the social communication criterion.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16122944</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16122944/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16122944_analysis_0</td><td>‘ToM’ - ‘Physical 1’</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The ToM task requires decoding socially relevant information (intentions/emotional cues) from visual narratives, which maps onto receptive social communication (interpretation of social signals).</td></tr>
<tr><td>16122944_analysis_1</td><td>‘Empathy’ - ‘Physical 2’</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The empathy contrast requires decoding emotional states and socially relevant cues from visual stimuli, aligning with receptive social communication measures (affect recognition).</td></tr>
<tr><td>16122944_analysis_2</td><td>Conjuction analysis</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Overlapping activations for ToM and empathy stem from interpreting social cues and emotions in narratives, consistent with social communication (receptive processing of social information).</td></tr>
<tr><td>16122944_analysis_3</td><td>(‘ToM’ - ‘Physical 1’) - (‘Empathy’ - ‘Physical 2’)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The interaction highlights neural differences in processing social information (intentionality, agency) versus empathy, which pertains to aspects of social communication (interpretation of social signals).</td></tr>
<tr><td>16122944_analysis_4</td><td>(‘Empathy’ - ‘Physical 2’) - (‘ToM’ - ‘Physical 1’)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Empathy-specific activations reflect decoding others’ emotional states from stimuli, consistent with receptive social communication (emotion recognition).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 17964185</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=1, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/17964185/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>17964185_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Participants viewed faces (personalized peers and celebrities) and made social approach/avoid decisions; the analysis involves receptive facial/social cues and interpersonal communication-related processing, so it qualifies as social communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>17964185_1</td><td>(peer vs. celebrity) x (positive vs. non-positive); socialcommunication</td><td>17964185_analysis_0</td><td>analysis_0</td><td>0.167</td><td>0.903</td><td>0.682</td><td>uncertain</td><td>high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18486491</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18486491/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18486491_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The iterated PD is an interactive exchange of socially relevant information between agents (reciprocity, signaling), fitting social communication constructs.</td></tr>
<tr><td>18486491_analysis_1</td><td>analysis_1</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The PD involves exchange of socially relevant choices and outcomes (communication/reciprocity), fitting social communication.</td></tr>
<tr><td>18486491_analysis_2</td><td>analysis_2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis links behavior in an interactive exchange (cooperation/defection signaling) with neural responses, reflecting social communication processes.</td></tr>
<tr><td>18486491_analysis_3</td><td>DMPFC connectivity with in-group - DMPFC connectivity with out-group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Changes in functional connectivity during interactive sessions reflect exchange-related neural dynamics and communication-related processing between agents.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18537114</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18537114/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18537114_analysis_0</td><td>P</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task does not involve exchange of social information, faces, gaze, or communicative gestures; it is non-social reward learning.</td></tr>
<tr><td>18537114_analysis_1</td><td>S</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The paradigm involves exchange between agents (ball passing) and reception/production of social actions; it is an interaction paradigm measuring socially relevant information exchange, fitting social communication criteria.</td></tr>
<tr><td>18537114_analysis_2</td><td>Brain activation in Pavlovian compared with social task</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>By contrasting Social and Pavlovian tasks, this analysis isolates regions engaged by social interaction/communication (ball-passing exchanges), meeting social communication criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18633788</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18633788/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18633788_analysis_0</td><td>Stories &gt; Rest</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Story presentation involves interpreting rules and speakers’ communicative intent (comprehension module); this contrast measures the interpretive/communicative processing of social rule stories.</td></tr>
<tr><td>18633788_analysis_1</td><td>Rest &gt; Stories</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast reflects greater activation at rest relative to stories and does not directly measure social-communication processes (which are engaged during story interpretation).</td></tr>
<tr><td>18633788_analysis_2</td><td>Cards&gt;Rest</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The card phase is a decision/violation-detection stage rather than a communicative-comprehension stage; it does not directly index social-communication (receptive/expressive) processing.</td></tr>
<tr><td>18633788_analysis_3</td><td>Rest&gt;Cards</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Rest &gt; Cards indicates deactivation relative to the decision phase and does not directly index social communication.</td></tr>
<tr><td>18633788_analysis_4</td><td>Stories: Social Contracts &gt; Precautions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Interpreting social-contract stories requires inferring communicative intent and the social meaning of conditional offers (comprehension of social exchanges), thus indexing social-communication processes.</td></tr>
<tr><td>18633788_analysis_5</td><td>Stories: Social Contracts &gt; Descriptives</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Interpreting social contract stories involves reconstructing speakers’ social intent and the communicative meaning of offers; thus this contrast indexes social-communication processes.</td></tr>
<tr><td>18633788_analysis_6</td><td>Cards: Social Contracts &gt; Precautions</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The card phase is about violation detection (categorization/information search) rather than communicative reception/production; it does not directly measure social-communication processes.</td></tr>
<tr><td>18633788_analysis_7</td><td>Cards: Social Contracts &gt; Descriptives</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast focuses on violation-detection responses (decision-making) rather than communicative signal reception/production.</td></tr>
<tr><td>18633788_analysis_8</td><td>Stories: Precautions &gt; Social Contracts</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Interpreting precautionary stories requires comprehending rule content and the communicative context (instruction/warning), so the story-interpretation contrast taps social-communication-related comprehension processes.</td></tr>
<tr><td>18633788_analysis_9</td><td>Stories: Precautions &gt; Descriptives</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Interpreting precautionary rules involves processing communicated safety constraints and intentions behind warnings/instructions; thus the contrast engages social-communication/comprehension processes.</td></tr>
<tr><td>18633788_analysis_10</td><td>Cards: Precautions &gt; Social Contracts</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The card phase is a decision/search stage, not a measure of communicative reception/production or the comprehension of communicative intent per se.</td></tr>
<tr><td>18633788_analysis_11</td><td>Cards: Precautions &gt; Descriptives</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Card-phase contrasts index violation detection rather than communicative reception/production.</td></tr>
<tr><td>18633788_analysis_12</td><td>Stories: Descriptives &gt; Social Contracts</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Interpreting descriptive stories involves comprehension of communicative content about individuals’ preferences/traits; as a story-interpretation contrast it indexes social-communication processes to some degree.</td></tr>
<tr><td>18633788_analysis_13</td><td>Stories: Descriptives &gt; Precautions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Both descriptive and precautionary stories require comprehension of communicated information about people’s behavior; this contrast indexes interpretation/comprehension (social-communication) processes during stories.</td></tr>
<tr><td>18633788_analysis_14</td><td>Cards: Descriptives &gt; Social Contracts</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Card-choice contrasts reflect violation-detection decision processes rather than receptive/expressive social-communication measures.</td></tr>
<tr><td>18633788_analysis_15</td><td>Cards: Descriptives &gt; Precautions</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Card-choice phase is about violation detection and does not measure communicative reception/production directly.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18633856</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18633856/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18633856_analysis_0</td><td>Angry&gt;Happy</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The paradigm centers on reception of facial communication and direct eye gaze (dynamic facial expressions and gaze shifts). The contrast measures neural responses to facial/communicative signals, satisfying social communication criteria.</td></tr>
<tr><td>18633856_analysis_1</td><td>Happy&gt;Angry</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Happy&gt;Angry evaluates neural responses to facial communication and gaze (reception of facial expressions), fitting the social communication inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 19439183</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19439183/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19439183_analysis_0</td><td>Pain &gt; no pain</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although stimuli are social scenes, this contrast isolates pain-related processing (pain vs no pain) rather than explicit social-communication exchanges (e.g., facial/expressive communication, gaze, or communicative signals).</td></tr>
<tr><td>19439183_analysis_1</td><td>No pain &gt; pain</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast identifies regions more active for non-painful scenes than painful ones; it does not explicitly probe communicative reception/production (e.g., facial or gaze processing) as defined by the social_communication construct.</td></tr>
<tr><td>19439183_analysis_2</td><td>Self + Other &gt; Self</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast isolates brain responses to the presence of another agent (TPJ, STS, temporal pole, mPFC) which index processing of social cues and interpretation of actions/intentionality—functions consistent with social communication (reception of non-facial communication/biological motion).</td></tr>
<tr><td>19439183_analysis_3</td><td>Self &gt; Self + Other</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Self &gt; Self+Other contrasts emphasize self-related processing rather than reception/production of social communicative signals; it is not primarily a social-communication contrast.</td></tr>
<tr><td>19439183_analysis_4</td><td>PCO &gt; PCS</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>PCO &gt; PCS isolates connectivity when pain is intentionally inflicted by another—this involves processing of social interaction cues and intentionality (biological motion/agentive signals), which map onto social-communication reception (non-facial cues).</td></tr>
<tr><td>19439183_analysis_5</td><td>PCS &gt; PCO</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although PCS &gt; PCO emphasizes self-caused pain, it still probes social-contextual modulation of communication-related networks (connectivity of amygdala with ACC/SMA/insula) and reflects processing of social/affective signals tied to observed actions; thus it is reasonably classified under social_communication (reception of non-facial social cues/biological motion).</td></tr>
<tr><td>19439183_analysis_6</td><td>Emotion contagion score (during PCS trials)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Emotional contagion relates to reception of affective signals from others (a receptive social-communication process). Correlating EC with neural responses implies modulation of social-communicative perception (e.g., affect recognition, vicarious emotional processing).</td></tr>
<tr><td>19439183_analysis_7</td><td>Pain ratings (during PCS trials)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>While ratings index affective evaluation, this analysis does not directly probe communicative exchange (facial/gaze/expressive communication) as defined by the social_communication construct.</td></tr>
<tr><td>19439183_analysis_8</td><td>Emotion contagion score (during PCO trials)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Emotional contagion represents receptive affective processing (recognition/ resonance with others&#x27; emotions). Correlating EC with neural responses during PCO trials implicates social-communication reception mechanisms.</td></tr>
<tr><td>19439183_analysis_9</td><td>Pain ratings (during PCO trials)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>While ratings reflect affective evaluation, this analysis does not directly probe communicative signal reception/production (faces, gaze, explicit communicative acts) as defined by social_communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 19733672</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19733672/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19733672_analysis_0</td><td>Movie scenes theory of mind (ToM) &gt; movie scenes physical inference (PI)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The movie scenes involve audiovisual social cues (faces, expressions, speech) and the contrast targets implicit social message decoding (face/gaze/STS/FFA engagement reported), matching social-communication (receptive) processes.</td></tr>
<tr><td>19733672_analysis_1</td><td>Silent answer theory of mind (ToM) &gt; silent answer physical inference (PI)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Silent-answer mode requires comprehension and internal processing of social communicative content (verbal/visual cues); observed recruitment of STS/IFG supports classification under social communication reception.</td></tr>
<tr><td>19733672_analysis_2</td><td>MC answer theory of mind (ToM) &gt; MC answer physical inference (PI)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>MC-answer mode engages language and face/STS systems (reported IFG, STS activations) and requires comprehension/selection of socially relevant communicative content, matching Social Communication.</td></tr>
<tr><td>19733672_analysis_3</td><td>IC 1 (rank 8 within all 80 components)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>IC1 includes core face-processing and STS regions (FFA, occipito-temporal, STS) linked to reception of facial communication and dynamic social cues, meeting Social Communication inclusion.</td></tr>
<tr><td>19733672_analysis_4</td><td>IC 2 (rank 13 within all 80 components)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>IC2 includes classical language and comprehension regions (STS, IFG) and was differentially engaged during explicit ToM MC answers, matching Social Communication (receptive language/semantic processing of social content).</td></tr>
<tr><td>19733672_analysis_5</td><td>IC 3 (rank 19 within all 80 components)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>IC3 is primarily a self-referential / higher-level mentalizing network (dmPFC, precuneus) rather than a network for reception/production of social communicative signals (faces/voice/IFG language); it therefore does not meet Social Communication inclusion for this component.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 19944083</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19944083/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19944083_analysis_0</td><td>Emotional &gt; neutral stimuli</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants view pictures conveying others&#x27; emotions via body posture and situational context (faces featureless). This maps onto reception of non-facial social communication (I1) and thus meets the social_communication inclusion criterion.</td></tr>
<tr><td>19944083_analysis_1</td><td>Neutral &gt; emotional stimuli</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Even though the contrast emphasizes neutral over emotional scenes, the stimuli require decoding social cues from body/posture/context (reception of non-facial communication). This satisfies I1 for social_communication.</td></tr>
<tr><td>19944083_analysis_2</td><td>Social relation &gt; single person</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Viewing interactions between two people requires processing communicative and social cues (reception of non-facial/bodily social signals), matching I1 for social_communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20119878</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20119878/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20119878_analysis_0</td><td>Neural regions active to warmth expectancy violation social targets</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants view pictures of people (faces/social targets) and make trait-related inferences — this involves reception of facial/social cues and trait-relevant communication; therefore it meets criteria for social communication.</td></tr>
<tr><td>20119878_analysis_1</td><td>Neural regions active to warmth expectancy consistent social targets</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast involves viewing pictures of people and processing facial/social cues in order to evaluate trait consistency, satisfying the social communication inclusion criteria (reception of facial communication).</td></tr>
<tr><td>20119878_analysis_2</td><td>Neural regions active to competence expectancy violation social targets</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants view pictures of people and make trait inferences (competence), involving reception of facial/social cues; thus it meets criteria for social communication.</td></tr>
<tr><td>20119878_analysis_3</td><td>Neural regions active to competence expectancy consistent social targets</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants view pictures of people and evaluate trait-consistency (competence), engaging reception of facial and social cues — consistent with social communication inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21206532</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=4</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21206532/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> faces &gt; watches; socialcommunication, objects &gt; faces; socialcommunication, objects &gt; watches; socialcommunication, watches &gt; faces; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21206532_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>The task involves reception of facial information (viewing faces and judging identity), which falls under Social Communication (receptive facial communication / facial recognition). The contrasts and ROI definitions explicitly involve faces versus non-faces and thus measure social communication-related processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>21206532_1</td><td>faces &gt; watches; socialcommunication</td><td>21206532_analysis_0</td><td>analysis_0</td><td>0.240</td><td>0.000</td><td>0.072</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>21206532_2</td><td>objects &gt; faces; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>21206532_3</td><td>objects &gt; watches; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>21206532_4</td><td>watches &gt; faces; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21249224</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21249224/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21249224_analysis_0</td><td>Brain areas showing increased activity in response to the social interaction condition.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task involves perceiving communicative, gesture-based interactions (point-light displays conveying social intentions and communicative acts). This fits Social Communication (reception of non-facial communication) and matches the inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 22174872</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/22174872/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>22174872_analysis_0</td><td>Subtraction of neutral form emotional trials.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Emotional prosody comprehension is an auditory social-communication task (reception of non-facial communication). The emotional vs neutral contrast directly measures receptive social communication.</td></tr>
<tr><td>22174872_analysis_1</td><td>Subtraction of simple from complex emotion trials.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast isolates processes involved in decoding vocal emotional signals (reception of non-facial communication) with greater recruitment for complex emotions, meeting Social Communication inclusion criteria.</td></tr>
<tr><td>22174872_analysis_2</td><td>Subtraction of simple from complex emotion trials (controlled for pitch).</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Even after controlling for pitch, the contrast indexes enhanced decoding of vocal emotional content for complex emotions (reception of non-facial communication), satisfying Social Communication criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 22841755</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/22841755/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>22841755_analysis_0</td><td>Power motive</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Stimuli are movie clips with conversational/speech and emotionally expressive social interactions (reception of auditory/affective social cues). The contrast (power &gt; control) assesses responses to social-communicative content, meeting the social communication criteria.</td></tr>
<tr><td>22841755_analysis_1</td><td>Affiliation motive</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Affiliation clips contain emotionally expressive social interactions, conversations and bonding cues (auditory/visual communicative content); the analysis measures responses to such social-communicative stimuli.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23813661</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23813661/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23813661_analysis_0</td><td>Regions more responsive to meaningful than scrambled videos</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Stimuli are communicative nonverbal interactions and the contrast isolates processing of socially relevant communicative motion versus scrambled; thus it measures social communication (receptive non-facial communication).</td></tr>
<tr><td>23813661_analysis_1</td><td>1. Contingent &gt; Mirrored</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contingency manipulation directly pertains to the exchange of socially relevant information between agents (interactive dynamics), fitting Social Communication (reception of non-facial communicative cues).</td></tr>
<tr><td>23813661_analysis_2</td><td>2. Mirrored &gt; Contingent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Mirrored vs contingent comparisons probe communicative relevance (loss of interactive dynamics) and the reception of nonverbal communicative cues, thus mapping to Social Communication.</td></tr>
<tr><td>23813661_analysis_3</td><td>1. Rigid &gt; Smooth</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Movement fluency affects perception of communicative intent and animacy; the contrast examines reception of nonverbal communicative cues (non-facial), fitting Social Communication inclusion criteria.</td></tr>
<tr><td>23813661_analysis_4</td><td>2. Interaction: (Contingent &gt; Mirrored) &gt; (Smooth &gt; Rigid)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The interaction tests how communicative dynamics (contingency) and motion fluency jointly affect reception of nonverbal communication, thus indexing Social Communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24294841</strong> | Pred included: 8 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24294841/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24294841_analysis_0</td><td>Main effect (ES + TS + SRS) - (EN + TN + SRN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants view pictures of people and infer emotional/internal states from visual cues; this involves reception of facial/nonverbal emotion cues consistent with social communication measures.</td></tr>
<tr><td>24294841_analysis_1</td><td>Empathy ES-EN</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Empathy trials use visual emotional cues from pictured individuals to infer feelings, which fits reception of facial/nonverbal communication (receptive social communication).</td></tr>
<tr><td>24294841_analysis_2</td><td>ToM TS-TN</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The ToM condition uses pictures and social cues to infer others’ mental states; this involves receptive social communication processes (interpreting facial/situational cues).</td></tr>
<tr><td>24294841_analysis_3</td><td>Self-reference SRS-SRN</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although self-focused, the condition involves interpreting social/affective cues from pictured people to construct an emotional response; this engages receptive social communication processes (viewing faces/emotional context).</td></tr>
<tr><td>24294841_analysis_4</td><td>Empathy &gt; ToM(ES + EN)-(TS + TN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Both Empathy and ToM conditions require interpreting social/emotional cues from pictured persons (receptive social communication), so the contrast involves social communication processes.</td></tr>
<tr><td>24294841_analysis_5</td><td>ToM &gt; Empathy(TS + TN)-(ES + EN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>ToM and Empathy use visual social cues to infer mental states and emotions, engaging receptive social communication processes; thus the contrast involves social communication.</td></tr>
<tr><td>24294841_analysis_6</td><td>Empathy &gt; Self-reference(ES + EN)-(SRS + SRN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Both conditions use visual social/affective cues from pictured people to infer feelings (empathy) or imagine oneself (self-reference); these engage receptive social communication mechanisms.</td></tr>
<tr><td>24294841_analysis_7</td><td>Self-reference &gt; Empathy(SRS + SRN)-(ES + EN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Both self-reference and empathy conditions are performed on social stimuli (people in pictures) and involve interpreting social/affective cues, engaging receptive social communication processes.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24726338</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24726338/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24726338_analysis_0</td><td>Self &gt; other</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task involves trait judgments (self vs other) using words, not social communication (faces, gaze, or communicative signals).</td></tr>
<tr><td>24726338_analysis_1</td><td>Other &gt; self</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task is trait judgment about self/other (text-based), not social communication processing such as facial or communicative signal reception/production.</td></tr>
<tr><td>24726338_analysis_2</td><td>Communication &gt; moral (collapsing across self/other)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast explicitly targets communication-related stimuli (communication &gt; moral), directly indexing social communication processing.</td></tr>
<tr><td>24726338_analysis_3</td><td>Moral &gt; communication (collapsing across self/other)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast emphasizes moral content over communication; it does not specifically measure social communication processes.</td></tr>
<tr><td>24726338_analysis_4</td><td>Self &gt; other (collapsing across moral/communication)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast targets self-referential versus other-referential processing of statements rather than social communication per se (faces, gaze, or communicative signals).</td></tr>
<tr><td>24726338_analysis_5</td><td>Other &gt; self (collapsing across moral/communication)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast focuses on self vs other referential processing of sentences rather than social communicative signals or face/gaze processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24772075</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24772075/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24772075_analysis_0</td><td>MAIN EFFECT: FEEDBACK ONSET: SELF &gt; OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast isolates processing of communicated social feedback (exchange of socially relevant information); therefore measures social communication.</td></tr>
<tr><td>24772075_analysis_1</td><td>MAIN EFFECT: FEEDBACK ONSET: OTHER &gt; SELF</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast isolates processing of communicated social information about another person (receptive social communication).</td></tr>
<tr><td>24772075_analysis_2</td><td>INTERACTION: FEEDBACK ONSET: (SELF &gt; OTHER) × (GERMAN &gt; CHINESE)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis concerns processing of socially communicated feedback and how this differs across cultures, meeting social communication criteria.</td></tr>
<tr><td>24772075_analysis_3</td><td>WHOLE-BRAIN CORRELATION WITH OVERALL RELATIVE ABSOLUTE MEAN UPDATES IN THE CONTRAST FEEDBACK ONSET: SELF &gt; OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Links neural responses to communicated social feedback (self vs other) with updating behavior, fitting social communication constructs.</td></tr>
<tr><td>24772075_analysis_4</td><td>FEEDBACK RATING (TRIAL-BY-TRIAL CORRELATION): SELF &gt; OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis concerns neural response to communicated evaluative feedback (reception of social information), matching social communication criteria.</td></tr>
<tr><td>24772075_analysis_5</td><td>FEEDBACK DISCREPANCIES (POSITIVE TRIAL-BY-TRIAL CORRELATION): SELF AND OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis measures how communicated feedback (discrepancies) is received and processed, a social-communication function (receptive processing).</td></tr>
<tr><td>24772075_analysis_6</td><td>FEEDBACK DISCREPANCIES (NEGATIVE TRIAL-BY-TRIAL CORRELATION): SELF AND OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Focuses on neural responses to communicated feedback discrepancies (receptive social communication); thus included under social communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24824165</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24824165/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24824165_analysis_0</td><td>Clusters of activation for (atypical-upright+atypical-inverted)-(typical-upright+typical-inverted) assessed by using all data</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants view non-verbal, point-light biological motion of two agents and the analysis probes perception of their interactive/non-interactive communication (receptive non-facial social communication), fitting the Social Communication construct.</td></tr>
<tr><td>24824165_analysis_1</td><td>Table 2. Clusters of activation for (atypical-upright+atypical-inverted)-(typical-upright+typical-inverted) assessed by using two separate subsets of data</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast evaluates perception of non-verbal interactive cues in point-light displays (receptive non-facial communication), which fits Social Communication inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25118071</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25118071/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25118071_analysis_0</td><td>CV&gt;NV contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants viewed dynamic social interactions with audio and visual cues; the stimuli require reception of affective/non-verbal cues and processing of communicative social content. The CV&gt;NV contrast therefore captures social communication processes.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25315788</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25315788/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25315788_analysis_0</td><td>A. Univariate analysis: friend minus pc</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task involves receiving and evaluating text messages (socially relevant communicative signals); univariate contrast tests sender-dependent processing of communicative input.</td></tr>
<tr><td>25315788_analysis_1</td><td>B. Searchlight-based MVPA: happy versus sad</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Classification of message content (affective words) is a receptive form of social communication (non-facial, text-based); MVPA assesses representation of communicative content.</td></tr>
<tr><td>25315788_analysis_2</td><td>C. Searchlight-based MVCA: friend minus pc</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>MVCA examines how communicative content (text messages) is transmitted from sensory regions to a social-cognitive hub (dmPFC) depending on perceived sender—directly about social communication processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25534111</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=2, unmatched=2</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25534111/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> (IJA − IJAc) − (RJA − RJAc); socialcommunication, IJA &gt; IJAc; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25534111_analysis_0</td><td>Responding to joint attention (RJA - RJAc)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Contrast involves gaze following, eye contact and communicative exchange (responding to a partner&#x27;s gaze), core social-communication processes (production/reception of facial/gaze cues).</td></tr>
<tr><td>25534111_analysis_1</td><td>Initiating joint attention (IJA - IJAc)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>IJA entails production of communicative gaze cues (initiating, guiding) and evaluating partner responses—core social communication processes (gaze production/reception).</td></tr>
<tr><td>25534111_analysis_2</td><td>Conjunction of initiating and responding to joint attention (IJA - IJAc) with (RJA - RJAc)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Common activation across IJA and RJA reflects shared mechanisms for gaze-based communication (production/reception of gaze cues), fitting social communication.</td></tr>
<tr><td>25534111_analysis_3</td><td>Initiating Joint attention minus responding to joint attention (IJA - IJAc) - (RJA - RJAc)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>IJA&gt;RJA reflects greater engagement of communicative gaze-production and evaluation systems when initiating versus responding—within social communication domain.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25534111_1</td><td>(IJA - IJAc) with (RJA - RJAc); socialcommunication</td><td>25534111_analysis_2</td><td>Conjunction of initiating and responding to joint attention (IJA - IJAc) with (RJA - RJAc)</td><td>0.500</td><td>0.571</td><td>0.550</td><td>uncertain</td><td>low_total_score</td></tr><tr><td>25534111_2</td><td>(IJA − IJAc) − (RJA − RJAc); socialcommunication</td><td>25534111_analysis_3</td><td>Initiating Joint attention minus responding to joint attention (IJA - IJAc) - (RJA - RJAc)</td><td>0.410</td><td>0.435</td><td>0.427</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>25534111_3</td><td>IJA &gt; IJAc; socialcommunication</td><td>25534111_analysis_1</td><td>Initiating joint attention (IJA - IJAc)</td><td>0.367</td><td>0.567</td><td>0.507</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>25534111_4</td><td>RJA &gt; RJAc; socialcommunication</td><td>25534111_analysis_0</td><td>Responding to joint attention (RJA - RJAc)</td><td>0.346</td><td>0.733</td><td>0.617</td><td>uncertain</td><td>coord_count_mismatch</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25697049</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25697049/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25697049_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The localizer explicitly used faces and bodies (faces &gt; cars, bodies &gt; cars) to define face/body selective ROIs, matching the Social Communication construct (reception of facial/body communication).</td></tr>
<tr><td>25697049_analysis_1</td><td>Congruent Interactions &gt; Incongruent Interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast involves viewing dyads with mutual facing, related body posture and facial cues vs incongruent pairings; this taps reception of facial/body communicative signals and thus Social Communication.</td></tr>
<tr><td>25697049_analysis_2</td><td>Incongruent Interactions &gt; Congruent Interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Incongruent vs congruent interactions modulate face/body processing and communicative cues; the contrast assesses reception of facial/body signals in differing social contexts, fitting Social Communication.</td></tr>
<tr><td>25697049_analysis_3</td><td>Congruent Interactions &gt; Non-Interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Comparing congruent interactions to non-interactions involves differences in mutual facing, body posture, and communicative facial cues, and therefore assesses reception of social communication signals.</td></tr>
<tr><td>25697049_analysis_4</td><td>Non-Interactions &gt; Congruent Interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Non-interactions differ from congruent interactions in communicative cues (lack of mutual gaze, facing), so this contrast probes reception of social communicative signals (or their absence).</td></tr>
<tr><td>25697049_analysis_5</td><td>Incongruent Interactions &gt; Non-Interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast contrasts two scene types that differ in communicative and relational cues (incongruent pairings vs non-facing non-interactions), thus tapping the reception of facial/body communicative signals.</td></tr>
<tr><td>25697049_analysis_6</td><td>Non-Interactions &gt; Incongruent Interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Non-interactions vs incongruent interactions differ in communicative cues (mutual facing, related body posture), so the contrast assesses reception of communicative facial/body signals and their contextual integration.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25729358</strong> | Pred included: 6 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25729358/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25729358_analysis_0</td><td>simple deception and sophisticated deception trials vs. truth trials</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task centers on sending messages and influencing a receiver (exchange of socially relevant information), so it measures social communication processes.</td></tr>
<tr><td>25729358_analysis_1</td><td>Simple deception vs. truth</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Simple deception involves producing a false message to influence another—directly a form of social communication.</td></tr>
<tr><td>25729358_analysis_2</td><td>Sophisticated deception vs. truth</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Sophisticated deception entails communicative acts (sending truthful message with deceptive intent), falling squarely under social communication.</td></tr>
<tr><td>25729358_analysis_3</td><td>Sophisticated deception vs. simple deception</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Both conditions involve different communicative strategies (truthful vs. false messaging with different intents), so this contrast probes social communication mechanisms.</td></tr>
<tr><td>25729358_analysis_4</td><td>Truth vs. simple and sophisticated deception</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Truthful vs. deceptive messaging directly tests communicative behavior and its social consequences; it measures social communication.</td></tr>
<tr><td>25729358_analysis_5</td><td>Parametric analysis modeling the incentive to deceive for simple deception trials</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Modeling how incentives modulate deceptive messaging addresses communicative behavior under varying social stakes, fitting social communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25911123</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25911123/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25911123_analysis_0</td><td>Response to &lt;i&gt;why&lt;/i&gt; &gt; &lt;i&gt;how&lt;/i&gt; questions for all stimulus categories</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The social conditions involve viewing faces and intentional actions and asking about emotional/behavioral states (receptive facial/emotional recognition, action understanding), meeting examples for social communication tasks (faces &gt; non-faces, emotion recognition).</td></tr>
<tr><td>25911123_analysis_1</td><td>Stronger response to &lt;i&gt;why&lt;/i&gt; &gt; &lt;i&gt;how&lt;/i&gt; questions for social than for nonsocial stimuli</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The social &gt; nonsocial why&gt;how effect isolates processing of facial emotional expressions and intentional actions and mental-state inferences (receptive social communication), consistent with social communication inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26143208</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=8</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26143208/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> FAC-Angry &gt; Control; socialcommunication, FAC-Angry &gt; FAC-Happy; socialcommunication, FAC-Happy &gt; Control; socialcommunication, [constant ISI &gt; variable ISI] - FAC-Angry; socialcommunication, [constant ISI &gt; variable ISI] - FAC-happy; socialcommunication, [constant ISI &gt; variable ISI] - control task; socialcommunication, [variable ISI &gt; constant ISI] - FAC-angry; socialcommunication, [variable ISI &gt; constant ISI] - control task; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26143208_analysis_0</td><td>Interaction: FAC-Angry × Control</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>Participants viewed faces with emotional expressions (angry, happy), which constitutes reception of facial communication (affective facial cues). The analysis therefore measures Social Communication per the I1 criterion (face viewing/emotion perception).</td></tr>
<tr><td>26143208_analysis_1</td><td>Interaction: FAC-Happy × Control</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>Viewing faces with emotional expressions (happy) entails reception of facial communication (affective cue processing). The analysis therefore satisfies the Social Communication inclusion criterion (I1).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26143208_1</td><td>FAC-Angry &gt; Control; socialcommunication</td><td>26143208_analysis_0</td><td>Interaction: FAC-Angry × Control</td><td>0.706</td><td>0.000</td><td>0.212</td><td>unmatched</td><td>coord_count_mismatch, low_total_score, name_only_signal</td></tr><tr><td>26143208_2</td><td>FAC-Angry &gt; FAC-Happy; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>26143208_3</td><td>FAC-Happy &gt; Control; socialcommunication</td><td>26143208_analysis_1</td><td>Interaction: FAC-Happy × Control</td><td>0.706</td><td>0.064</td><td>0.257</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>26143208_4</td><td>[constant ISI &gt; variable ISI] - FAC-Angry; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>26143208_5</td><td>[constant ISI &gt; variable ISI] - FAC-happy; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>26143208_6</td><td>[constant ISI &gt; variable ISI] - control task; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>26143208_7</td><td>[variable ISI &gt; constant ISI] - FAC-angry; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>26143208_8</td><td>[variable ISI &gt; constant ISI] - control task; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26235682</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26235682/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26235682_analysis_0</td><td>Positive vs. Neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Stimuli combine faces and verbal comments and require reception of socially relevant facial/linguistic communication; this fits Social Communication (reception of facial and verbal social signals).</td></tr>
<tr><td>26235682_analysis_1</td><td>Negative vs. Neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants view faces paired with verbal negative comments (reception of facial/verbal social cues), which fits Social Communication.</td></tr>
<tr><td>26235682_analysis_2</td><td>(Positive &gt; Neutral) inclusively masked with (Negative &gt; Neutral)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The conjunction isolates regions responding to reception of face+comment stimuli (facial and verbal social communication), matching Social Communication criteria.</td></tr>
<tr><td>26235682_analysis_3</td><td>(Positive vs. Neutral) vs. (Negative vs. Neutral)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Isolating regions responsive to positive evaluative face+comment stimuli involves reception of social communication (faces and verbal praise), fitting Social Communication.</td></tr>
<tr><td>26235682_analysis_4</td><td>(Negative vs. Neutral) vs. (Positive vs. Neutral)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Negative evaluative face+comment stimuli require reception of social communication (facial and verbal cues); thus Social Communication is relevant.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26262561</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26262561/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26262561_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task centers on gaze following (reception and production of eye-gaze signals) using faces of political figures; contrasts and behavioural measures index gaze-based social communication and its neural correlates, satisfying social communication criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26323252</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26323252/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26323252_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task requires perception and evaluation of another person&#x27;s affective state (pain) from visual stimuli — receptive social communication (affect recognition).</td></tr>
<tr><td>26323252_analysis_1</td><td>Main effect of condition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Distinguishing ingroup/outgroup involves processing social cues and recognizing others (visual social stimuli); it falls within receptive social communication.</td></tr>
<tr><td>26323252_analysis_2</td><td>Main effect of gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Gender effects on responses to observed pain reflect differences in receptive social communication/affect recognition and were derived from viewing social stimuli, so inclusion is appropriate.</td></tr>
<tr><td>26323252_analysis_3</td><td>Pain×condition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Interaction involves differential perception of others&#x27; affect (pain) depending on social category; this is receptive social communication (affect recognition modulated by group cues).</td></tr>
<tr><td>26323252_analysis_4</td><td>Pain×gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The interaction reflects differences in how observers of different genders perceive others&#x27; pain — receptive social communication (affect recognition) — and is therefore included.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26365506</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26365506/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26365506_analysis_0</td><td>Self-referential &gt; social</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although social stimuli (people/faces) are present, this contrast (self &gt; social) is designed to detect self-referential effects and not to measure social communication (e.g., faces &gt; non-faces or emotion recognition).</td></tr>
<tr><td>26365506_analysis_1</td><td>Social &gt; self-referential</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The social &gt; self contrast involves viewing human stimuli (faces/bodies) and produced activations in face/body and social perception regions (TPJ, FFA, EBA), matching Social Communication (reception of facial/non-facial communication).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26567160</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26567160/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26567160_analysis_0</td><td>Attitude &gt; Age</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Stimuli are faces and participants evaluate/mentalize about others (face-based social judgments), matching Social Communication (reception of facial communication).</td></tr>
<tr><td>26567160_analysis_1</td><td>Trust &gt; Age</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants view faces and form social judgments (trust), aligning with reception of facial communication.</td></tr>
<tr><td>26567160_analysis_2</td><td>Distrust &gt; Age</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Faces are the stimuli and participants infer negative social intent (distrust), fitting Social Communication (face reception) inclusion.</td></tr>
<tr><td>26567160_analysis_3</td><td>Trust</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis concerns responses to faces and social judgments (trust) — falls under reception of facial social communication.</td></tr>
<tr><td>26567160_analysis_4</td><td>Distrust</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Distrust analyses involve face-based judgments and likely inner speech/cognitive labeling of social cues — within Social Communication (reception of facial communication).</td></tr>
<tr><td>26567160_analysis_5</td><td>Precuneus Connectivity-Positive</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Seed connectivity derived from face-based trust manipulation (reception/processing of faces) aligns with Social Communication.</td></tr>
<tr><td>26567160_analysis_6</td><td>IFG Connectivity-Negative</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Seed region derived from face-based distrust task and examines communication/attribution networks tied to face evaluation — fits Social Communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26803059</strong> | Pred included: 10 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26803059/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26803059_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analyses involve perception of facial expressions and affect recognition (receptive facial communication), matching the Social Communication inclusion criteria.</td></tr>
<tr><td>26803059_analysis_1</td><td>EFE &gt; no EFE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast directly contrasts presence vs absence of facial expressions (receptive facial communication), meeting Social Communication criteria.</td></tr>
<tr><td>26803059_analysis_2</td><td>No EFE &gt; EFE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although this contrast emphasizes absence of facial cues, it still pertains to social communication processes (how facial signals affect processing).</td></tr>
<tr><td>26803059_analysis_3</td><td>Correlation of differential activation EFE &gt; no EFE with RT reduction by EFE, P &lt; 0.001 (uncorrected); k = 10</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Correlates neural sensitivity to facial expressions with behavioral facilitation—directly relevant to Reception of Facial Communication (social communication).</td></tr>
<tr><td>26803059_analysis_4</td><td>Correlation of differential activation no EFE &gt; EFE with RT reduction by EFE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Even as a contrast emphasizing lack of facial cues, the analysis addresses social communication mechanisms (how facial information affects communication/inference).</td></tr>
<tr><td>26803059_analysis_5</td><td>Greater EFE effects with explicit demand of affective ToM judgments (EFE ToM - no EFE ToM) - (EFE no ToM - no EFE no ToM)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Examines modulation of face-derived signals (communication) by ToM demand, fitting Social Communication (reception) criteria.</td></tr>
<tr><td>26803059_analysis_6</td><td>Greater EFE effects with explicit demand of affective ToM judgments (EFE no ToM - no EFE no ToM) - (EFE ToM - no EFE ToM)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast evaluates how facial communication interacts with task demand — relevant to reception of facial communication.</td></tr>
<tr><td>26803059_analysis_7</td><td>PPI</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Connectivity changes relate to processing of facial expressions (EFE) and their impact on communication of affect—meets Social Communication criteria (reception and network-level effects).</td></tr>
<tr><td>26803059_analysis_8</td><td>Pos. correlation: affective ToM matching with consensus judgments and PPI effects</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Correlation assesses relation between network responsiveness to facial/affective signals and successful reception/interpretation—meets Social Communication criteria.</td></tr>
<tr><td>26803059_analysis_9</td><td>Neg. correlation: affective ToM matching with consensus judgments and PPI effects</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Negative correlations indicate network aspects of processing facial/affective signals that relate to communication/perception of emotion—meets Social Communication criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26812250</strong> | Pred included: 11 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26812250/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26812250_analysis_0</td><td>Negative vs. positive performance feedback.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The feedback was delivered verbally and constitutes social/auditory communication; the contrast therefore involves processing of communicated social information.</td></tr>
<tr><td>26812250_analysis_1</td><td>(A) Emotional empathic (EE) &gt; emotional unempathic (EN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast involves reception and evaluation of verbal social communication (compassionate vs uncaring utterances), matching the social communication construct.</td></tr>
<tr><td>26812250_analysis_2</td><td>(B) Unempathic (EN + CN) &gt; empathic (EE + CE)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast evaluates reception of socially relevant verbal communications that differ in interpersonal attitude, therefore directly indexing social communication processing.</td></tr>
<tr><td>26812250_analysis_3</td><td>(C) Cognitive unempathic (CN) &gt; cognitive empathic (CE)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast evaluates reception of differing verbal communicative acts (paraphrasing vs denial of understanding), fitting social communication criteria.</td></tr>
<tr><td>26812250_analysis_4</td><td>(D) Empathic (EE + CE) &gt; high level baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast involves reception of verbal empathic communications versus baseline and thus assesses processing of social communicative signals.</td></tr>
<tr><td>26812250_analysis_5</td><td>(E) Emotional empathic (EE) &gt; high level baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast assesses reception of affect-laden verbal social communication (compassionate comments) vs baseline, fitting social communication criteria.</td></tr>
<tr><td>26812250_analysis_6</td><td>(F) Cognitive empathic (CE) &gt; high level baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast tests reception of verbal communicative acts that convey understanding (paraphrase) versus baseline, directly mapping onto social communication.</td></tr>
<tr><td>26812250_analysis_7</td><td>(A) Emotional (EE + EN) &gt; cognitive (CE + CN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast compares two forms of verbal social communication (emotional vs cognitive empathy), directly relevant to reception/processing of social communicative content.</td></tr>
<tr><td>26812250_analysis_8</td><td>(B) Emotional empathic (EE) &gt; cognitive empathic (CE)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast concerns reception of two distinct verbal social communicative styles (compassionate vs paraphrase) and therefore maps onto social communication processes.</td></tr>
<tr><td>26812250_analysis_9</td><td>(C) Emotional unempathic (EN) &gt; cognitive unempathic (CN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>It evaluates reception and differential processing of verbal unempathic communications (affective vs cognitive denial), fitting the social communication construct.</td></tr>
<tr><td>26812250_analysis_10</td><td>(D) Cognitive unempathic (CN) &gt; emotional unempathic (EN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast concerns processing of differing verbal communicative content (denial of understanding vs dismissal) and thus qualifies as social communication processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26892859</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26892859/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26892859_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants viewed audiovisual sentences delivered by posers communicating social-evaluative content; this is a receptive social communication task (affective speech/facial/body cues) so the contrasts measure social communication processing.</td></tr>
<tr><td>26892859_analysis_1</td><td>analysis_1</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analyses derive from viewing and processing of audiovisual social-evaluative sentences (receptive social communication), satisfying criteria for social_communication.</td></tr>
<tr><td>26892859_analysis_2</td><td>analysis_2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The conjunction reflects common neural responses to communicated social-evaluative content across valences, thus indexing social communication processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27039141</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27039141/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27039141_analysis_0</td><td>A Meaning &gt; Physical</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The meaning task elicits regions (IFG, TPJ) associated with semantic/communicative processing and the authors describe reading models as communicative signs; thus it taps aspects of social communication (receptive interpretation of communicative object meaning).</td></tr>
<tr><td>27039141_analysis_1</td><td>B Physical &gt; Meaning</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast focuses on physical/perceptual processing of objects (fusiform, motor areas) rather than receptive or productive social communication, so it does not meet the Social Communication inclusion. </td></tr>
<tr><td>27039141_analysis_2</td><td>C Own &gt; Other (meaning related task)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast targets self- versus other-related responses to one&#x27;s own models (episodic/self-referential processing) rather than explicit social communication (e.g., face/gesture/language communication).</td></tr>
<tr><td>27039141_analysis_3</td><td>D Own &gt; Other × Collective &gt; Individual (meaning related task)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although IFG is engaged, the effect is described as relating to social salience/emotion and affiliation rather than explicit communicative exchange or reception/production of communicative signals; the task does not directly measure social communication. </td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27090501</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27090501/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27090501_analysis_0</td><td>Significant activation for the (toss - button press) high-frequency effect.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the task is minimalistic, ball-tosses function as nonverbal social exchange/attention signals; the contrast isolates responses to increased social exchange. The analysis therefore indexes social communicative exchange (receptive/reciprocal nonverbal communication) and meets I1.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27095057</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27095057/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27095057_analysis_0</td><td>Social-Reg &gt; Social-Look</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Social-Reg trials used a psychotherapist&#x27;s video instructions (spoken communicative guidance) to induce reappraisal. The contrast captures processing of this socially conveyed regulatory communication and its neural effects.</td></tr>
<tr><td>27095057_analysis_1</td><td>(Reg &gt; Look)_Social ∩ (Reg &gt; Look)_Self</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Both tasks present regulatory instructions (video vs written cue) and the conjunction likely captures processing related to instruction reception and regulation implementation; thus it engages social-communication-related processes tied to receiving regulatory cues.</td></tr>
<tr><td>27095057_analysis_2</td><td>Distinct areas recruited by Social-Reg</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Distinct Social-Reg activations reflect processing of the psychotherapist’s social guidance and perspective-taking demands, i.e., socially communicated reappraisal; thus the contrast indexes social communication processes relevant to regulation.</td></tr>
<tr><td>27095057_analysis_3</td><td>Distinct areas recruited by Self-Reg</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Self-Reg used written cues and pre-learned distancing strategies without an interpersonal communicator; the contrast isolates intrapersonal regulation and is not a social-communication task (no face-to-face or communicative exchange).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27096431</strong> | Pred included: 14 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27096431/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27096431_analysis_0</td><td>Aggressive reaction to provoking opponent &gt; non aggressive reaction to non-provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The paradigm involves exchange of socially-relevant information (choices of aversive feedback, outcome feedback) and interactive decisions; the contrast reflects production/reception of socially relevant signals in an interaction.</td></tr>
<tr><td>27096431_analysis_1</td><td>Non aggressive reaction to non-provoking opponent &gt; aggressive reaction to provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Involves interactive exchange (choice of feedback, social outcome), so captures social communication processes in the task.</td></tr>
<tr><td>27096431_analysis_2</td><td>Won &gt; lost</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Outcome phase communicates socially-relevant information (opponent choices, feedback); the won&gt;lost contrast captures processing of that interpersonal exchange.</td></tr>
<tr><td>27096431_analysis_3</td><td>Lost &gt; won</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Outcome phase contains communicated feedback from opponents; lost &gt; won reflects processing of that interpersonal communication (including anticipation of aversive feedback).</td></tr>
<tr><td>27096431_analysis_4</td><td>Won against the provoking opponent &gt; won against the non-provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Outcome information and opponent behavior are exchanged; this opponent‑specific contrast reflects processing of communicated social signals and their effects.</td></tr>
<tr><td>27096431_analysis_5</td><td>Won against the non-provoking opponent &gt; won against the provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Outcome phase conveys social information and opponent-specific differences; this contrast captures processing of that communicated social content.</td></tr>
<tr><td>27096431_analysis_6</td><td>Lost against the provoking opponent &gt; lost against the non-provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Outcome phase contains communicated opponent feedback; opponent-specific loss contrasts capture processing of that social communication.</td></tr>
<tr><td>27096431_analysis_7</td><td>Lost against the non-provoking opponent &gt; lost against the provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Outcome information is communicated by the opponent; opponent-specific contrasts index processing of that interpersonal exchange.</td></tr>
<tr><td>27096431_analysis_8</td><td>Retaliation independent of opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Decision phase entails producing communicative social signals (chosen feedback) and receiving outcome feedback; the contrast captures social communicative behavior.</td></tr>
<tr><td>27096431_analysis_9</td><td>Retaliation interacting with provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Decision to retaliate against a provoking opponent reflects communicative social behavior (sending a punitive signal) and processing of exchanged social information.</td></tr>
<tr><td>27096431_analysis_10</td><td>Retaliation interacting with non-provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Choosing feedback to a non‑provoking opponent remains an act of social communication (producing a signal in an interpersonal exchange), so the contrast indexes social communication processes.</td></tr>
<tr><td>27096431_analysis_11</td><td>Won</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Outcome information (win) is communicated in the task and has interpersonal significance; this contrast reflects processing of that communicated social outcome.</td></tr>
<tr><td>27096431_analysis_12</td><td>Won against provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Outcome and feedback are part of an interpersonal exchange; this opponent-specific win contrast captures processing of socially communicated signals.</td></tr>
<tr><td>27096431_analysis_13</td><td>Won against non-provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Outcome feedback and opponent behavior are components of the interpersonal exchange; this contrast reflects processing of that social communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27494142</strong> | Pred included: 6 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27494142/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27494142_analysis_0</td><td>CC-A</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task involves exchange of socially relevant information (decisions to cooperate/defect) and includes partner face presentation; CC &gt; others reflects communicative social interaction outcomes (fits I1 examples such as cooperation and face stimuli).</td></tr>
<tr><td>27494142_analysis_1</td><td>A-CC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast reflects processing of social interaction outcomes (partners’ cooperative/defective behavior) and comes from a task that involves face stimuli and social exchange—consistent with social communication.</td></tr>
<tr><td>27494142_analysis_2</td><td>Gain-Loss</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contextual modulation (gain vs loss) affects processing of social interaction outcomes and is based on a task that uses partner faces and communicative choices; this falls under social communication measures.</td></tr>
<tr><td>27494142_analysis_3</td><td>Loss-Gain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast concerns contextual modulation of social interaction outcomes (partner behavior and participant responses) within a task that involves face presentation and social exchange; thus it falls under social communication.</td></tr>
<tr><td>27494142_analysis_4</td><td>(CC-A) Gain - (CC-A) Loss</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast examines context-dependent neural responses to cooperative social exchanges (partner behavior and outcome communication), consistent with social communication processes and the task&#x27;s face/social-exchange elements.</td></tr>
<tr><td>27494142_analysis_5</td><td>(CC-A) Loss - (CC-A) Gain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast concerns context-dependent differences in neural responses to social exchange outcomes from the PD task and therefore qualifies as social communication-related.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 28504911</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/28504911/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>28504911_analysis_0</td><td>Select &gt; content</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Social communication focuses on exchange of socially relevant information (e.g., face/voice processing, communicative production/reception). The select &gt; content contrast is about personal selection rather than communicative exchange; although social cognition ROIs are engaged, the task does not measure social-communication subconstructs, so I1 is not met.</td></tr>
<tr><td>28504911_analysis_1</td><td>Share &gt; content</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The share condition operationalizes communicative behavior (deciding to transmit information to others), fitting Social Communication (exchange of socially relevant information). The contrast measures communicative intent and thus meets I1.</td></tr>
<tr><td>28504911_analysis_2</td><td>Share &gt; select</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This contrast compares communicative (sharing) to non-communicative (selecting) judgments, directly probing social-communication processes (information transmission). Thus it meets Social Communication I1.</td></tr>
<tr><td>28504911_analysis_3</td><td>Select</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This analysis concerns preference to select for personal reading and its neural correlates; it does not test communicative exchange per se (not a faces/voice/communication task), so Social Communication I1 is not met.</td></tr>
<tr><td>28504911_analysis_4</td><td>Share</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This analysis examines neural scaling with propensity to communicate/share information (production of social communication), fitting Social Communication inclusion (exchange/production of social information). Meets I1.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 28905269</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/28905269/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>28905269_analysis_0</td><td>Regions with increased activation for monogamous men compared to non-monogamous men for the romantic &gt; neutral contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Viewing images of couples engages perception of social interactions and affective cues (receptive aspects of social communication); the contrast involves processing socially communicative visual information.</td></tr>
<tr><td>28905269_analysis_1</td><td>Regions with increased activation for non-monogamous men compared to monogamous men for the romantic &gt; sexual contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Comparing romantic to sexual images engages perception of social interactions and affective cues (receptive social-communication processes), and the contrast captures differences in processing those cues.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29097704</strong> | Pred included: 14 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29097704/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29097704_analysis_0</td><td>FT&gt;FA</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Judging facing direction from biological motion conveys socially relevant communicative information (non-facial body cues). PLD and HL stimuli serve as social signals; this contrast examines communicative/social cue processing.</td></tr>
<tr><td>29097704_analysis_1</td><td>FA&gt;FT</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Direction of motion is a communicative social cue (approach/avoidance); FA&gt;FT examines processing of non-facial social signals conveyed by body motion.</td></tr>
<tr><td>29097704_analysis_2</td><td>HL&gt;PLD</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Human-like videos provide richer social communicative cues (body kinematics, context) than PLD; the contrast assesses processing of communicative social information.</td></tr>
<tr><td>29097704_analysis_3</td><td>PLD&gt;HL</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>PLD carry non-facial communicative information via body motion; the contrast evaluates processing of such social communicative signals.</td></tr>
<tr><td>29097704_analysis_4</td><td>STIMULUS*DIRECTION</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Interaction probes how communicative cues (body motion type and approach/away) affect neural processing of social signals, fitting Social Communication.</td></tr>
<tr><td>29097704_analysis_5</td><td>POST-HOC ANALYSIS</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Post-hoc contrasts examine communicative content of specific conditions (e.g., how PLD vs HL in FT/FA influence social signal processing), matching Social Communication.</td></tr>
<tr><td>29097704_analysis_6</td><td>FT_PLD&gt;HL</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>FT_PLD&gt;HL examines communicative body cues for approaching agents, assessing non-facial social communication from motion signals.</td></tr>
<tr><td>29097704_analysis_7</td><td>FT_HL&gt;PLD</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>HL facing-toward provide strong social communicative signals (approach); the contrast examines processing of those communicative cues relative to PLD.</td></tr>
<tr><td>29097704_analysis_8</td><td>FA_PLD&gt;HL</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>FA_PLD&gt;HL evaluates communicative information derived from body motion even when agent is facing away; PLD can still convey non-facial social cues relevant to communication processing.</td></tr>
<tr><td>29097704_analysis_9</td><td>FA_HL&gt;PLD</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>FA_HL&gt;PLD assesses processing of communicative cues present in HL even when agent is facing away; examines social communication via body motion cues.</td></tr>
<tr><td>29097704_analysis_10</td><td>PLD_FT&gt;FA</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast inspects communicative information in PLD across directions (approach vs away), reflecting social-communicative processing of non-facial cues.</td></tr>
<tr><td>29097704_analysis_11</td><td>PLD_FA&gt;FT</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The comparison evaluates communicative cues in PLD when facing away versus toward, relevant to social communication via body motion.</td></tr>
<tr><td>29097704_analysis_12</td><td>HL_FT&gt;FA</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>HL directionality conveys communicative approach/avoidance cues; this contrast probes processing of such social-communication signals in rich (HL) stimuli.</td></tr>
<tr><td>29097704_analysis_13</td><td>HL_FA&gt;FT</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>HL facing-away still transmits communicative body cues; contrast assesses social-communication processing differences for away vs toward HL stimuli.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29265483</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29265483/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29265483_analysis_0</td><td>1a. Observation of social interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task involves perception of socially relevant scenes (affective and cooperative interactions) and affect recognition/interpretation, fitting Social Communication (receptive social information).</td></tr>
<tr><td>29265483_analysis_1</td><td>1b. Cooperativity&gt;affectivity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Viewing cooperative interactions involves reception of socially relevant cues (intentions, shared goals), fitting Social Communication (receptive social information).</td></tr>
<tr><td>29265483_analysis_2</td><td>1c. Affectivity&gt;cooperativity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Affective scenes require reception and interpretation of affective social signals (faces, gaze, contact), fitting Social Communication (receptive aspects).</td></tr>
<tr><td>29265483_analysis_3</td><td>analysis_3</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The effective connectivity analyses address how brain regions exchange socially relevant information (intention/affect decoding), consistent with Social Communication (reception/integration of social cues).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29324862</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29324862/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29324862_analysis_0</td><td>Partnering.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The paradigm emphasizes haptic exchange of information (signalling and reception of force cues) and the contrast isolates brain areas for interpersonal communication via touch (pSTS, S1/S2), meeting the social communication definition.</td></tr>
<tr><td>29324862_analysis_1</td><td>Leading &gt; Conjunction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Leading involves sending haptic cues/intention-signals to a partner; this contrast targets brain areas involved in producing the interpersonal signals required for coordination (motor planning and sensorimotor mapping), so it is relevant to social communication.</td></tr>
<tr><td>29324862_analysis_2</td><td>Following &gt; Conjunction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Following specifically indexes the reception and use of haptic cues from a partner (sensory and motion-tracking regions); this is a form of social communication (receptive channel), meeting the annotation criteria.</td></tr>
<tr><td>29324862_analysis_3</td><td>Mutual &gt; Conjunction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Mutual interaction requires reciprocal exchange and coordination (shared timing/agency) and engages social-perceptual systems; thus the contrast indexes interpersonal communication and coordination processes.</td></tr>
<tr><td>29324862_analysis_4</td><td>Improvisation</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although one improvised condition (Leading) is partnered, the contrast combines partnered and solo improvisation and is aimed at generative processes rather than exchange of social signals; it does not specifically measure social communication.</td></tr>
<tr><td>29324862_analysis_5</td><td>Self-initiation</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This analysis targets the distinction between self-initiated and externally-triggered actions (agency) and is not explicitly probing exchange of social signals or communicative reception/production, so it does not meet the social-communication inclusion criterion.</td></tr>
<tr><td>29324862_analysis_6</td><td>Joint improvisation.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Joint improvisation/Leading&gt;Solo aims to isolate brain regions involved in signalling movement intentions to a partner (haptic communication). This directly targets social communication (production of non-facial, haptic signals), so it meets the annotation criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 30056560</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/30056560/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> CON: own&gt;unknown masked with group×(own&gt;unknown); socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>30056560_analysis_0</td><td>Happy-own&gt;neutral-own</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This is a facial affect recognition task (viewing and classifying faces), i.e., reception of facial communication. The contrast isolates processing of an emotional facial signal (happy) versus neutral, meeting the Social Communication inclusion criteria.</td></tr>
<tr><td>30056560_analysis_1</td><td>ELM: happy-own&gt;neutral-own masked with group×(happy-own&gt;neutral-own)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>The analysis concerns reception of facial emotional communication (happy vs neutral) and tests group-specific modulation of that processing, fitting Social Communication inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>30056560_1</td><td>CON: own&gt;unknown masked with group×(own&gt;unknown); socialcommunication</td><td>30056560_analysis_1</td><td>ELM: happy-own&gt;neutral-own masked with group×(happy-own&gt;neutral-own)</td><td>0.694</td><td>0.000</td><td>0.208</td><td>unmatched</td><td>coord_count_mismatch, low_total_score, name_only_signal</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 30649548</strong> | Pred included: 6 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/30649548/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>30649548_analysis_0</td><td>Cooperation &gt; competition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although not an explicit verbal/face communication task, the paradigm involves exchange of socially relevant actions and intent signaling between partners; the contrast probes interpersonal exchange and information transfer (iBBDs), fitting social communication inclusion criteria.</td></tr>
<tr><td>30649548_analysis_1</td><td>Competition &gt; cooperation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Competitive interactions involve exchange of action-based signals and intent inference; the contrast examines interpersonal information exchange and adaptive responding, consistent with social communication processes.</td></tr>
<tr><td>30649548_analysis_2</td><td>CN &gt; TB</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Concurrent exchanges require real-time exchange of action-based signals and mutual adaptation (information transmission), so CN &gt; TB probes social communication processes in interaction.</td></tr>
<tr><td>30649548_analysis_3</td><td>TB (COO &amp;gt; COM) &amp;gt; CN (COO &amp;gt; COM)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>By comparing how cooperative vs competitive effects differ across TB and CN, this contrast probes differences in interpersonal information exchange dynamics and adaptive communicative signaling across interaction types.</td></tr>
<tr><td>30649548_analysis_4</td><td>Builder (COO &gt; COM) &gt; Other (COO &gt; COM)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Role-specific differences in responses to a partner’s moves reflect differences in interpersonal signaling and interpretation; this contrast probes communicative aspects of the interaction (how information is used differently by roles).</td></tr>
<tr><td>30649548_analysis_5</td><td>Other (COO &gt; COM) &gt; Builder (COO &gt; COM)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrasting Others vs Builders in their cooperative&gt;competitive responses indexes differences in how communicative signals are generated and interpreted by different roles, fitting social communication.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>
</details></section><section id="bucket-false-negative"><details class="bucket" open><summary><h2>False Negative (6)</h2></summary><p><strong>Match status totals:</strong> accepted=9 | uncertain=0 | unmatched=6</p>
<details class="doc-card">
  <summary><strong>PMID 15528097</strong> | Pred included: 0 | Manual included (accepted matches only): 1 | Correct overlaps: 0 | Match statuses: accepted=1, uncertain=0, unmatched=4</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/15528097/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Embarrassment &gt; Neutral; socialcommunication, Embarrassment &gt; guilt; socialcommunication, Guilt &gt; Embarrassment; socialcommunication, Guilt &gt; Neutral; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>15528097_analysis_0</td><td>Brain regions commonly activated by guilt and embarrassment conditions</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The task does not target social communication per se (e.g., face/gaze processing, affect recognition, or production/reception of communicative signals). Stimuli were short sentences and the focus was evaluative social cognition/ToM rather than communication-specific processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>15528097_1</td><td>Embarrassment &gt; Neutral; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>15528097_2</td><td>Embarrassment &gt; guilt; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>15528097_3</td><td>G &gt; N masked E &gt; N; socialcommunication</td><td>15528097_analysis_0</td><td>Brain regions commonly activated by guilt and embarrassment conditions</td><td>0.275</td><td>1.000</td><td>0.783</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>15528097_4</td><td>Guilt &gt; Embarrassment; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>15528097_5</td><td>Guilt &gt; Neutral; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21320516</strong> | Pred included: 0 | Manual included (accepted matches only): 1 | Correct overlaps: 0 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21320516/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21320516_analysis_0</td><td>Female and male</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although cartoons convey social content, the task is not an explicit social-communication paradigm (e.g., face processing, gaze, or explicit communicative exchange); authors focus on emotion, appraisal, and ToM rather than message exchange or facial communication.</td></tr>
<tr><td>21320516_analysis_1</td><td>Female and male</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although affect recognition and social cue processing are involved, the parametric modulation is focused on amusement ratings, not explicit social-communication tasks (no face/gaze/expressive production or reception paradigms).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>21320516_1</td><td>cartoons &gt; neutral pictures; socialcommunication</td><td>21320516_analysis_0</td><td>Female and male</td><td>0.238</td><td>1.000</td><td>0.771</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21600991</strong> | Pred included: 1 | Manual included (accepted matches only): 1 | Correct overlaps: 0 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21600991/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21600991_analysis_0</td><td>Activations in the retrieval of social context</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The social condition required interactive story generation and reciprocal communicative behavior (alternating generation and reading aloud), so the contrast probes social communication processes associated with collaboration and exchange of information.</td></tr>
<tr><td>21600991_analysis_1</td><td>Activations in the retrieval of self-generation</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although encoding included communicative interaction, the self-generation contrast isolates self vs other source effects in memory (self-referential generation) rather than communication/perception of communicative signals (faces, gaze, expressive behavior); it therefore does not meet Social Communication inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>21600991_1</td><td>Retrieval of social context &gt; Retrieval of self-generation; socialcommunication</td><td>21600991_analysis_1</td><td>Activations in the retrieval of self-generation</td><td>0.724</td><td>1.000</td><td>0.917</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23221019</strong> | Pred included: 0 | Manual included (accepted matches only): 3 | Correct overlaps: 0 | Match statuses: accepted=3, uncertain=0, unmatched=2</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23221019/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Conjunction of all Conditions &gt; Fixation Baseline; socialcommunication, Load x Goal &gt; Fixation Baseline; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23221019_analysis_0</td><td>Conjunction of all conditions versus fixation baseline</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>Although participants view hand actions, the task probes action understanding and mentalizing rather than communicative exchange (no face/gaze/emotion communication or explicit communicative gestures), so it does not meet Social Communication criteria.</td></tr>
<tr><td>23221019_analysis_1</td><td>Conjunction of low-load/how-goal condition &gt; low-load/observe-goal condition and high-load/how-goal condition &gt; high-load/observe-goal condition</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast focuses on understanding implementation of actions (how) vs observation, not on communicative reception or production (faces, gaze, emotion), so it does not meet Social Communication criteria.</td></tr>
<tr><td>23221019_analysis_2</td><td>Modulation by load for any of the four goals</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>This is a cognitive-load modulation analysis of action/mentalizing systems, not an assessment of communicative reception/production (faces/gaze/emotion), so Social Communication criteria are not met.</td></tr>
<tr><td>23221019_analysis_3</td><td>Low-load/why-goal condition compared with high-load/why-goal condition, low-load/how-goal condition, and high-load/how-goal condition combined</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although mentalizing concerns understanding others, this contrast targets causal attribution/mental-state inference rather than communicative reception/production (faces, gaze, emotion), so Social Communication criteria are not met.</td></tr>
<tr><td>23221019_analysis_4</td><td>Load-by-goal interaction</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">UNMATCHED</span></td><td></td><td>The analysis examines goal × load effects on action understanding and mentalizing rather than communicative behaviors or face/gaze/emotion processing; thus Social Communication criteria are not met.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>23221019_1</td><td>Conjunction of all Conditions &gt; Fixation Baseline; socialcommunication</td><td>23221019_analysis_0</td><td>Conjunction of all conditions versus fixation baseline</td><td>0.932</td><td>0.200</td><td>0.420</td><td>unmatched</td><td>coord_count_mismatch, low_coord_high_name, low_total_score</td></tr><tr><td>23221019_2</td><td>Conjunction of low-load/how-goal condition &gt; low-load/observe-goal condition and high-load/how-goal condition &gt; high-load/observe-goal condition; socialcommunication</td><td>23221019_analysis_1</td><td>Conjunction of low-load/how-goal condition &gt; low-load/observe-goal condition and high-load/how-goal condition &gt; high-load/observe-goal condition</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23221019_3</td><td>Load x Goal &gt; Fixation Baseline; socialcommunication</td><td>23221019_analysis_4</td><td>Load-by-goal interaction</td><td>0.545</td><td>0.429</td><td>0.464</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>23221019_4</td><td>Low-load/why-goal condition compared with high-load/why-goal condition, low-load/how-goal condition, and high-load/how-goal condition combined; socialcommunication</td><td>23221019_analysis_3</td><td>Low-load/why-goal condition compared with high-load/why-goal condition, low-load/how-goal condition, and high-load/how-goal condition combined</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23221019_5</td><td>Modulation by load for any of the four goals; socialcommunication</td><td>23221019_analysis_2</td><td>Modulation by load for any of the four goals</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 28931014</strong> | Pred included: 0 | Manual included (accepted matches only): 2 | Correct overlaps: 0 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/28931014/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>28931014_analysis_0</td><td>Fingerspelling &gt; Letter</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>While the stimuli include fingerspelling (a communicative handgesture), the experimental contrast targets phonological maintenance and modality-specific neural substrates of memory/processing, not social-communicative behavior per se (e.g., emotion recognition, gaze, face/hand communication as social signals). The analysis does not operationalize social communication measures as required by inclusion criterion I1.</td></tr>
<tr><td>28931014_analysis_1</td><td>Letter &gt; Fingerspelling</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The Letter &gt; Fingerspelling contrast targets visual letter-processing and phonological memory rather than social communicative behavior. It does not operationalize receptive/expressive social communication constructs (faces, emotion, gaze, social hand gestures) required by inclusion criterion I1.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>28931014_1</td><td>Fingerspelling &gt; Letter; socialcommunication</td><td>28931014_analysis_0</td><td>Fingerspelling &gt; Letter</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>28931014_2</td><td>Letter &gt; Fingerspelling; socialcommunication</td><td>28931014_analysis_1</td><td>Letter &gt; Fingerspelling</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29039129</strong> | Pred included: 0 | Manual included (accepted matches only): 1 | Correct overlaps: 0 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29039129/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29039129_analysis_0</td><td>Incongruent vs. Congruent</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although the stimuli are social, the IAT contrast indexes implicit attitudes about relationship concepts rather than explicit reception or production of social communicative signals (e.g., facial expression or gaze). The task is not primarily a face/communication perception task, so it does not meet the Social Communication inclusion criterion.</td></tr>
<tr><td>29039129_analysis_1</td><td>Congruent vs. Incongruent</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>As above, the contrast reflects implicit evaluative associations rather than processing of communicative facial or gaze cues; it does not meet the Social Communication inclusion criterion focused on reception/production of social signals.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29039129_1</td><td>Incongruent &gt; Congruent; socialcommunication</td><td>29039129_analysis_0</td><td>Incongruent vs. Congruent</td><td>0.917</td><td>1.000</td><td>0.975</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>
</details></section>
  
</body>
</html>
