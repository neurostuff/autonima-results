<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>perception_others review report</title>
  <style>
    :root {
      --bg: #f7f6f2;
      --panel: #ffffff;
      --ink: #1d2730;
      --line: #d8dde3;
    }
    body { margin: 0; padding: 1.25rem; font-family: "IBM Plex Sans", "Segoe UI", sans-serif; background: var(--bg); color: var(--ink); }
    header { background: var(--panel); border: 1px solid var(--line); border-radius: 10px; padding: 1rem; margin-bottom: 1rem; }
    .top-nav { position: sticky; top: 0; z-index: 10; display: flex; flex-wrap: wrap; gap: 0.5rem; background: #eef3f2; border: 1px solid var(--line); border-radius: 10px; padding: 0.6rem; margin-bottom: 1rem; }
    .top-nav a { display: inline-block; padding: 0.35rem 0.6rem; border: 1px solid var(--line); border-radius: 999px; background: #fff; text-decoration: none; font-size: 0.9rem; color: #0e4f85; }
    section { margin-bottom: 1rem; }
    .bucket > summary, .doc-card > summary, .inner-accordion > summary { cursor: pointer; }
    .doc-card { background: var(--panel); border: 1px solid var(--line); border-radius: 10px; padding: 0.85rem; margin-bottom: 0.85rem; }
    .table-wrap, .table-html { overflow-x: auto; }
    .inner-accordion { margin-top: 0.6rem; border-top: 1px dashed var(--line); padding-top: 0.4rem; }
    .paper-text { white-space: pre-wrap; max-height: 26rem; overflow-y: auto; background: #fbfcfe; border: 1px solid var(--line); border-radius: 8px; padding: 0.6rem; font-size: 0.88rem; line-height: 1.35; }
    table { width: 100%; border-collapse: collapse; font-size: 0.9rem; }
    th, td { border: 1px solid var(--line); padding: 0.45rem; vertical-align: top; text-align: left; }
    th { background: #edf2f5; }
    .decision-cell, .confusion-cell { text-align: center; vertical-align: middle; }
    .decision-pill, .confusion-pill {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      min-width: 1.55rem;
      padding: 0.12rem 0.45rem;
      border-radius: 999px;
      font-weight: 700;
      font-size: 0.82rem;
      border: 1px solid transparent;
    }
    .decision-include { background: #e9f8ef; color: #1f7a3d; border-color: #b7e4c6; }
    .decision-exclude { background: #fdecec; color: #9b1c1c; border-color: #f6caca; }
    .decision-none { background: #f2f4f7; color: #5b6775; border-color: #dde3ea; }
    .confusion-good { background: #e9f8ef; color: #166534; border-color: #b7e4c6; }
    .confusion-bad { background: #fdecec; color: #991b1b; border-color: #f6caca; }
    .confusion-na { background: #f2f4f7; color: #5b6775; border-color: #dde3ea; }
    a { color: #0e4f85; }
  </style>
</head>
<body>
  <header>
    <a id="top"></a>
    <h1>perception_others report</h1>
    <p>Manual benchmark is sliced to the auto PMID universe from <code>outputs/nimads_annotation.json</code>. Analysis-level truth uses accepted fuzzy matches only.</p>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Level</th>
            <th>TP</th>
            <th>FP</th>
            <th>FN</th>
            <th>Precision</th>
            <th>Recall</th>
            <th>F1</th>
            <th>Manual Positives</th>
            <th>Predicted Positives</th>
            <th>Universe</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Document bucket overlap</td>
            <td>63</td>
            <td>61</td>
            <td>5</td>
            <td>0.508</td>
            <td>0.926</td>
            <td>0.656</td>
            <td>68</td>
            <td>124</td>
            <td>129</td>
          </tr>
          <tr>
            <td>Study inclusion</td>
            <td>70</td>
            <td>54</td>
            <td>4</td>
            <td>0.565</td>
            <td>0.946</td>
            <td>0.707</td>
            <td>74</td>
            <td>124</td>
            <td>134</td>
          </tr>
          <tr>
            <td>Analysis inclusion (accepted matches only)</td>
            <td>198</td>
            <td>352</td>
            <td>18</td>
            <td>0.360</td>
            <td>0.917</td>
            <td>0.517</td>
            <td>216</td>
            <td>550</td>
            <td>640</td>
          </tr>
        </tbody>
      </table>
    </div>
  </header>
  <nav class="top-nav">
    <a href="#bucket-correct">Correct (63)</a>
    <a href="#bucket-false-positive">False Positive (61)</a>
    <a href="#bucket-false-negative">False Negative (5)</a>
    <a href="#missing-manual">Missing PMIDs (0)</a>
    <a href="#top">Top</a>
  </nav>
  <section id="bucket-correct"><details class="bucket"><summary><h2>Correct (63)</h2></summary><p><strong>Match status totals:</strong> accepted=210 | uncertain=5 | unmatched=13</p>
<details class="doc-card">
  <summary><strong>PMID 29890323</strong> | Pred included: 6 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29890323/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29890323_analysis_0</td><td>emotional &gt; neutral video clips</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast measures awareness and reasoning about others&#x27; emotional states (empathic accuracy and intensity tracking), directly matching Perception and Understanding of Others.</td></tr>
<tr><td>29890323_analysis_1</td><td>neutral &gt; emotional video clips</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The Empathic Accuracy Task directly measures understanding of others&#x27; emotional states and variations in empathic accuracy; the contrast is relevant to Perception and Understanding of Others.</td></tr>
<tr><td>29890323_analysis_2</td><td>Positively related to Z-EA scores</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Empathic accuracy and continuous ratings directly measure understanding and reasoning about others&#x27; emotional states (mentalising/understanding others).</td></tr>
<tr><td>29890323_analysis_3</td><td>Negatively related to Z-EA scores</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis directly addresses understanding others&#x27; emotional states (empathic accuracy and mentalising), matching the perception and understanding of others construct.</td></tr>
<tr><td>29890323_analysis_4</td><td>Positive correlation with participants&#x27; emotional intensity ratings</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Directly assesses representations about others&#x27; emotional states (continuous ratings of targets&#x27; emotional intensity), matching understanding of others.</td></tr>
<tr><td>29890323_analysis_5</td><td>Negative correlation with participants&#x27; emotional intensity ratings</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis directly measures participants&#x27; perception and tracking of others&#x27; emotional states (empathic accuracy and intensity ratings), satisfying Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29890323_1</td><td>emotional &gt; neutral video clips; others</td><td>29890323_analysis_0</td><td>emotional &gt; neutral video clips</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29890323_2</td><td>neutral &gt; emotional video clips; others</td><td>29890323_analysis_1</td><td>neutral &gt; emotional video clips</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 6057276)</summary><p><strong>Title:</strong> Tracking emotions in the brain – Revisiting the Empathic Accuracy Task</p><details><summary>Abstract</summary><pre class="paper-text">Many empathy tasks lack ecological validity due to their use of simplistic stimuli and static analytical approaches. Empathic accuracy tasks overcome these limitations by using autobiographical emotional video clips. Usually, a single measure of empathic accuracy is computed by correlating the participants&#x27; continuous ratings of the narrator&#x27;s emotional state with the narrator&#x27;s own ratings. 

In this study, we validated a modified empathic accuracy task. A valence-independent rating of the narrator&#x27;s emotional intensity was added to provide comparability between videos portraying different primary emotions and to explore changes in neural activity related to variations in emotional intensity over time. We also added a new neutral control condition to investigate general emotional processing. In the scanner, 34 healthy participants watched 6 video clips of people talking about an autobiographical event (2 sad, 2 happy and 2 neutral clips) while continuously rating the narrator&#x27;s emotional intensity. 

Fluctuation in perceived emotional intensity correlated with activity in brain regions previously implicated in cognitive empathy (bilateral superior temporal sulcus, temporoparietal junction, and temporal pole) and affective empathy (right anterior insula and inferior frontal gyrus). When emotional video clips were compared to neutral video clips, we observed higher activity in similar brain regions. Empathic accuracy, on the other hand, was only positively related to activation in regions that have been implicated in cognitive empathy. 

Our modified empathic accuracy task provides a new method for studying the underlying components and dynamic processes involved in empathy. While the task elicited both cognitive and affective empathy, successful tracking of others&#x27; emotions relied predominantly on the cognitive components of empathy. The fMRI data analysis techniques developed here may prove valuable in characterising the neural basis of empathic difficulties observed across a range of psychiatric conditions. 
   Highlights  
  
Activity in affective and cognitive empathy related regions during emotional videos. 
  
Activity in similar regions related to changes in perceived emotional intensity. 
  
Only regions implicated in cognitive empathy were associated with empathic accuracy. 
  
No difference between video clips that did and did not elicit affect sharing. 
  
Empathic accuracy related to self-rated perspective-taking but not empathic concern.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Empathy, has been defined as “an emotional response [… which] is similar to one&#x27;s perception […] and understanding […] of the stimulus emotion, with recognition that the source of the emotion is not one&#x27;s own.” ( , page 150). Empathy is crucial for successful social interaction as it allows the individual to predict others&#x27; actions, emotions and intentions ( ). Deficits in empathic processing have been reported in psychiatric disorders such as autism spectrum disorder (ASD), schizophrenia, borderline personality disorder and bipolar disorder ( ). Identifying the neural substrates of empathy in healthy populations is important for understanding conditions that are characterised by empathic difficulties. In neuroscience, the concept of empathy is considered to include separate affective (sharing others&#x27; emotion) and cognitive (understanding others&#x27; emotion) components (for example,  ;  ). Previous research has identified distinct clusters of brain regions involved in affective empathy: medial/anterior cingulate cortex (MCC, ACC), anterior insula (AI) ( ;  ), and supplementary motor area (SMA) ( ). Within the broader domain of social cognition, cognitive empathy overlaps with the affective component of Theory of Mind (ToM) or mentalising, namely the capacity to infer other people&#x27;s thoughts, emotions and intentions without necessarily sharing them ( ). A recent meta-analysis of 144 fMRI studies using ToM tasks ( ) identified the medial prefrontal cortex (mPFC), medial orbitofrontal cortex (mOFC), ACC, precuneus, temporal pole (TP), posterior superior temporal gyrus (pSTS) and temporoparietal junction (TPJ) and inferior frontal gyrus (IFG) as key regions for mentalising. 

However, prior research on the neural mechanisms of empathy has often lacked ecological validity. Studies have often used simplistic stimuli that differ greatly from the complex cues that individuals have to process in real-life situations ( ,  ). Moreover, most studies focus on empathy for pain, while only a few studies have evaluated other emotions (e.g. disgust, happiness, sadness;  ;  ). In addition, empathy has mostly been operationalised as a static trait ( ). However, in the real world empathy fluctuates dynamically ( ). These fluctuations can happen spontaneously because of changes in internal state or in response to shifts in external circumstances, such the emotional intensity and expressivity of others. 

In the current study, we addressed these limitations of previous research by modifying an existing paradigm, the Empathic Accuracy Task (EAT;  ), that incorporates more naturalistic stimuli and reflects the dynamic nature of empathy. Participants (perceivers) watch video clips in which another person (target) describes an emotional autobiographical event. Perceivers continuously rate the target&#x27;s emotion while watching the clips (via button pressing). The EAT measures how   accurately   the perceiver infers changes in the target&#x27;s emotional states by correlating the perceiver&#x27;s ratings with the target&#x27;s ratings of their own emotions (see   for a detailed description).   found that empathic accuracy was associated with higher activation in both affective (i.e. inferior parietal lobule (IPL)) and cognitive (i.e. mPFC) empathy networks. In a recent study of adolescents, empathic accuracy related positively to activation in cognitive empathy or mentalising regions (mPFC, TPJ, STS) and negatively to activation in regions implicated in affective empathy (IPL, ACC, AI;  ). 

In the current study, new video clips were created and the EAT was modified in the following important ways: First, video clips depicted discrete primary emotions (happy, sad, angry, frightened) and participants rated changes in the targets&#x27; emotional intensity (instead of valence) to ensure comparability across different emotions and higher construct validity. Second, we introduced well-matched neutral video clips that acted as a control condition. In this condition, targets described their bedroom. This control condition allowed us to examine the neural correlates of emotion processing irrespective of empathic accuracy. Third, as empathy is a dynamic process, perceivers need to be able to continuously identify changes in the intensity of the target&#x27;s emotional state. We therefore utilised an analysis approach that tracked changes in the target&#x27;s emotional intensity throughout each video clip, in addition to deriving a single index of empathic accuracy (averaged across the clip). Fourth, we included ratings from participants regarding how they felt after watching each video to gain a better understanding of how the neural correlates of EA are influenced by cognitive and affective empathy. Finally, to validate the EAT, we related task performance to self-reported trait empathy and IQ as well as acquiring a normative data set with participants who completed the EAT outside of the scanner. 

The aim of this study was to validate a modified version of the Empathic Accuracy Task, using a staged analysis approach which replicates analyses presented previously in the literature, but which also included additional comparisons. First, we contrasted the blood-oxygen-level dependent (BOLD) responses to emotional and neutral clips to explore correlates of complex and multi-sensory emotional processing during extended clips rather than single emotional images. Second, we validated our emotional intensity rating scale by analysing the neural correlates of intra-individual variations in empathic accuracy. Third, we explored neural correlations with variations in perceived emotional intensity over time, thus capitalising on the availability of continuous ratings throughout each video clip. 

Given the results of prior neuroimaging studies of empathy and mentalising, we had the following hypotheses:   
At the group level, increased BOLD responses would be observed in brain regions previously linked to empathy and mentalising when participants watched targets describe emotional versus neutral events. 
  
There would be positive correlations between intra-individual variations in empathic accuracy and BOLD responses in these regions. 
  
We predicted positive correlations between fluctuations in perceived emotional intensity and BOLD responses in these regions during emotional video clips. 
  


## Methods 
  
### Participants 
  
#### fMRI study 
  
Forty-seven healthy participants aged between 20 and 30 years, fluent in English and with no history of neurological illness, took part in the study. Six participants were excluded from the analysis due to current or recurrent episodes of mental illness as assessed by the Mini International Neuropsychiatric Interview ( ). Five further participants were excluded because of excessive head movement or poor task performance (&lt;2 SD in empathic accuracy (EA) scores) and two participants had incomplete questionnaire data. The final dataset included 34 subjects (19 females, mean age: 24.0 years, SD: 2.7 years). The study received ethical approval from the Camberwell - St. Giles NHS Research Ethics Committee (14/LO/0477) and the University of Southampton Ethics Committee. 


#### Normative data collection 
  
To create a normative data set for the EAT and to validate the stimuli used in the fMRI task, an additional 73 healthy participants completed the EAT outside the MRI scanner. The same inclusion criteria as described above were applied. After excluding 13 participants due to current or recurrent episodes of mental illness, the final dataset included 60 healthy participants (36 females, mean age: 25.2 years, SD: 2.9 years). This aspect of the study was approved by the University of Southampton Ethics Committee. 



### Tasks and stimuli 
  
#### Video acquisition 
  
Eleven native English-speaking students from the University of Southampton acted as targets (8 females, mean age: 20.1 years, SD: 1.64 years). Before filming they were asked to recall a specific autobiographical event (happy, sad, angry or frightened), in which they remembered feeling a strong emotion. Each target wrote a short summary of each event and rated its overall emotional intensity on a 9-point scale (from 1, ‘no emotion’ to 9, ‘very strong emotion’). For the emotional stimuli, only events with a rating of 5 or above were filmed. Each target provided one video clip for each emotion and one clip in which they described their bedroom (neutral condition). An adapted emotion elicitation strategy, which involved imagining being in the situation, was used before filming to reinstate the affective states the targets had felt during the events ( ). They were advised to refrain from making specific reference to their affective state (e.g. happy) but were allowed to use generic descriptions (e.g. upset) or descriptions of bodily symptoms (e.g. shaking). All targets were filmed from the shoulders upwards, in front of a black background, for standardisation purposes. Each clip lasted between 83 and 140 s (mean = 100.3, SD = 15.2). After filming each clip, targets watched the video and continuously rated their emotional intensity using the same 9-point scale as above. Ratings were made by using arrow keys on the keyboard to move a coloured square on the scale (this shifted by one point per button press). Starting point for all ratings was “1”. 

For the fMRI study, the 6 video clips that were selected (one happy, one sad and one neutral video, featuring one female and one male target) were those which received high EA and target expressiveness scores in a pilot study with 13 participants (7 male, mean age: 21.54 years, SD: 2.37 years). A description of the target&#x27;s gender, the emotional condition, the clip length and the target&#x27;s rating of emotional intensity experienced during each clip is presented in  . For pre-training and volume adjustment, one additional sad, one neutral and two happy clips were added (depicting different targets from the main experiment). For the data collection outside the MRI scanner, 27 expressive video clips were selected (7 happy clips, 7 sad clips, 3 angry clips, 3 frightened clips and 7 neutral clips) as well as two happy clips and one sad clip for pre-training purposes. The task and instructions for filming stimuli are available on request.   
Video clips displayed in order of presentation during the Empathic Accuracy Task with target&#x27;s gender, emotional condition and length of the video clip and targets&#x27; average ratings of their own emotional intensity. 
  Table 1   


#### Empathic accuracy task (EAT) 
  
Participants were instructed to continuously rate the perceived emotional intensity of the target ( , top) using the same 9-point scale as above (from 1, ‘no emotion’ to 9, ‘very strong emotion’). In the fMRI study, participants used a button box to provide ratings. In the non-imaging study, participants used the computer&#x27;s arrow keys. The default rating at the start of each video clip was no emotion (i.e. rating of 1). Following each clip, participants were asked: (1) which emotion the target felt most strongly (cognitive empathy: options of “happy”, “angry”, “surprised”, “sad”, “frightened” and “no emotion”); and (2) which emotion they themselves felt most strongly (i.e., affective empathy: same response options as above).   
 Schematic representation of the Empathic Accuracy task and continuous rating scale data.   Top: example of a video clip and rating scale in the Empathic Accuracy Task. The target&#x27;s identity has been disguised in this image. Bottom: Illustration of fluctuations in the   target&#x27;s   emotional intensity, as rated by the target (blue) and an example participant&#x27;s ratings (green). An Empathic Accuracy (EA) score was computed by correlating the participant&#x27;s ratings and the target&#x27;s ratings for each video clip. 
  Fig. 1   


#### Interpersonal reactivity index 
  
The Interpersonal Reactivity Index (IRI) is a widely-used self-report questionnaire that measures dispositional empathy using four subscales: fantasy (FS), empathic concern (EC), perspective taking (PT) and personal distress (PD;  ). 


#### Wechsler abbreviated scale of intelligence, Second Edition 
  
The Wechsler Abbreviated Scale of Intelligence, Second Edition (WASI-II;  ) is a widely-used and reliable test of general intelligence. 



### Procedure 
  
#### fMRI study 
  
The EAT was part of the testing protocol of the English and Romanian Adoptees&#x27; Brain Imaging Study (for further details, see  ). Participants gave written informed consent to participate in the study. All participants completed the MINI and WASI-II, and an online survey, which included the IRI. Participants received pre-training on the fMRI tasks prior to the scan, during which they were familiarised with the EAT and the scanning environment. After observing the experimenter demonstrating how to rate one happy clip, participants rated two clips (one sad, one happy) themselves, while lying in a mock scanner. In the actual EAT experiment, participants watched and rated the 6 video clips in a fixed order ( ). The task took approximately 12 min. Participants were reimbursed for around 6 h of their time with a £100 Amazon voucher. 


#### Normative data collection 
  
For the non-scanning study, participants gave written consent to participate. For pre-training, participants first watched the experimenter rate one happy clip before rating two practice video clips themselves. They then watched and rated 27 video clips in randomised presentation order, in a quiet testing room. This lasted approximately 40 min. Participants also completed an online survey, which included the IRI. Participants were reimbursed for their time with a £15 Amazon voucher. 



### Behavioural data analysis 
  
Participants&#x27; and targets&#x27; ratings were analysed using Matlab 8.2.0 (The MathWorks Inc., Natick, Massachusetts, United States) and SPSS (Version 22, IBM Corp., Armonk, New York, United States). All ratings were separated into 2 s bins and one time-weighted average rating was calculated for each bin. We then tested for correlations between the participants&#x27; and targets&#x27; ratings ( , bottom). The resulting Pearson&#x27;s correlation coefficient for each video clip and each participant is referred to as the EA score. As expected, the variance of the ratings was low for neutral clips. EA scores were therefore only calculated for emotional video clips. EA scores were then r-to-Z transformed to allow comparison between correlation coefficients ( ,  ). 

#### Behavioural analysis of fMRI sample 
  
Paired t-tests examined whether Z-transformed EA scores, affective and cognitive empathy scores differed between happy and sad video clips. Moreover, paired t-tests were performed to test for differences in the average ratings of the target&#x27;s emotional intensity between emotional and neutral as well as happy and sad video clips. A paired   t  -test was also used to test whether Z-EA scores differed between video clips that elicited “affect sharing” (participants reported feeling the same emotion as the target) compared to those that did not (participants reported a different emotion or no emotion). In addition, Pearson correlations were conducted to test for relationships between mean Z-EA scores, the IRI subscales and IQ. 


#### Behavioural analysis of normative data sample 
  
To examine whether the video clips presented in the fMRI study induced Z-EA scores comparable to those in the non-scanning sessions, two Pearson correlations were performed within the normative data sample. Considering happy and sad video clips separately, we examined the correlation between Z-EA scores based on the two video clips presented in the scanner and Z-EA scores based on all seven video clips from the respective emotional category. Moreover, intra-individual standard deviations were calculated based on (1) the four emotional video clips presented in the scanner and (2) all 20 emotional video clips. These were then compared with a paired   t  -test. 



### fMRI data acquisition 
  
Functional images were acquired on a General Electric MR750 3.0 T MR scanner with a 12-channel head coil. A T2*-weighted gradient echo, echo-planar imaging sequence was used, which covered 41 axial slices and recorded 347 vol acquired sequentially, descending (TR/TE 2000/30 ms, flip angle 75°, 64 × 64 matrix, 3 mm thick, field of view (FoV) = 247 mm). To facilitate fMRI data registration and normalisation, we also acquired a T1-weighted Magnetization Prepared Rapid Gradient Echo MPRAGE image (TR/TE 7312/3.02 ms, flip angle 11°, 256 × 256 matrix, 1.2 mm thick, 196 sagittal slices, FoV = 270 mm). 


### fMRI data analysis 
  
We used SPM12 for pre-processing and subject-level (first level) analyses (Wellcome Department of Cognitive Neurology, Institute for Neurology, London, UK). FSL was utilised for cerebrospinal fluid (CSF) regression and statistical nonparametric permutation inference at the group level (second level) with “randomise” ( ; FMRIB Analysis Research, Oxford Centre for Functional MRI of the Brain, Oxford, UK). 

#### Preprocessing 
  
After reorientation, the EPI files were first slice-time corrected (middle slice as reference). Images were then realigned to the first image and subsequently to the time series mean. The mean EPI image was co-registered to the T1-weighted image to allow for normalisation. The structural files were segmented and the resulting grey matter, white matter and CSF files were used to create a common group-specific template using group-wise DARTEL registration ( ). This template was then employed to normalise the functional EPI files to MNI space. This step simultaneously resampled volumes (1.5 mm isotropic) and applied spatial smoothing (Gaussian FWHM kernel of 8 mm). Finally, for each participant, the time course signal of a CSF mask (top 5% from DARTEL CSF component) was extracted in native space. 


#### Emotional vs neutral video clips 
  
At the first level of analysis, each participant&#x27;s pre-processed data were modelled as a block design using a general linear model framework. We included 3 separate regressors (happy, sad, neutral) encoding the predicted BOLD response associated with video presentation, formed by convolution of the canonical haemodynamic response function (HRF) with boxcars delimiting the video presentation. 

We identified regional estimates of BOLD response associated with watching and rating the video clips. Separate parameter estimates for mean response during the emotional (happy and sad) and neutral category compared to the implicit baseline were produced. At the group level, in a random-effects model, paired t-tests were performed to identify clusters that were differentially activated when watching emotional video clips compared to neutral clips. Moreover, happy and sad clips were compared using paired t-tests. 


#### Intra-individual variation in empathic accuracy 
  
In accordance with  , Z-EA scores for each participant and each video clip were added as parametric modulators at the first level of analysis. On the group level, one sample t-tests were performed, to test whether the BOLD response during emotional video clips was modulated by intra-individual variations in Z-EA scores. 


#### Correlation with emotional intensity ratings 
  
We examined how the BOLD time series correlated with the participant&#x27;s ratings of the target&#x27;s emotional intensity. Scans were split and a model was fitted to each emotional video clip in turn. The continuous ratings of the target&#x27;s emotional intensity for each 2 s bin as rated by the participant were entered as regressors of interest. At the group level, one-sample t-tests assessed whether the relationship between BOLD response and changes in the emotional intensity ratings was significantly observed in any brain region across the group. 


#### Exploratory analysis: impact of affect sharing 
  
To examine differences in BOLD response for video clips that induced affect sharing compared to those that did not, we conducted an exploratory post-hoc analysis. We included the 20 participants who showed affect sharing in response to some, but not all video clips in order to be able to create 3 separate conditions in the first level in a block design (shared, non-shared, neutral). For each participant, emotional videos that induced affect sharing (participants reported to have the same emotion as the target) were included in the shared condition, while emotional videos that did not elicit affect sharing (participants reported to have a different emotion than the target or no emotion) were modelled in the non-shared condition. Separate parameter estimates for mean response during affect shared, non-shared and neutral video clip presentation compared to the implicit baseline were calculated. At the group level, paired t-tests were performed to identify clusters that were differentially activated when watching video clips that induced affect sharing compared to non-shared clips. 


#### Movement, scanner drifts and multiple comparisons correction 
  
As well as the regressors described above, all analyses included seven movement parameters (six standard parameters as well as volume-to-volume movement) as nuisance regressors. For each volume-to-volume movement exceeding 1 mm, an additional regressor was included marking the location of that volume and those immediately adjacent (for a summary of volume-to-volume movement see  ). The CSF regressor was also included as a nuisance regressor. To control for task-related hand movement artefacts, button presses were included as condition of no interest. To investigate the effect of controlling for button presses, we additionally repeated all analyses without including this condition. Moreover, we compared button presses during emotional video clips with button presses during neutral video clips as separate conditions to ensure that activity relating to emotion processing was not partialled out. 

Data were high pass filtered with a threshold of 209 s, which corresponds to twice the length of the longest video clip, to control for scanner drifts. 

Results reported are based on Family-Wise Error (FWE) corrected threshold-free cluster enhancement (TFCE:   p   &lt; 0.05 ( )). For each significant cluster, the peak activations with a minimum inter-peak distance of 20 voxels are reported to account for the wide-spanning clusters found in our analyses. 




## Results 
  
### Behavioural data 
  
#### Behavioural analysis of the fMRI sample 
  
On average, participants had high EA scores (mean   r   = .75, mean intra-individual standard deviation (iSD) = .35, range = .13 to .97). Fisher&#x27;s Z-transformed (Z-)EA scores were slightly, but significantly, lower for sad video clips (mean Z-EA = 0.97, SD = 0.21) than happy ones (mean Z-EA = 1.16, SD = 0.19;   t   (33) = 5.17,   p   &lt; .001). As expected, participants&#x27; average ratings of the target&#x27;s emotional intensity were higher for emotional than for neutral video clips (mean emotional = 5.18, mean neutral = 1.75,   t   (33) = 15.29,   p   &lt; .001), with higher ratings for sad compared to happy ones (mean sad = 5.49, mean happy = 4.87,   t   (33) = 3.02,   p   &lt; .01). 

On average, participants correctly inferred the target&#x27;s emotion in 90.4% of clips (emotion identification, SD = 15.1%), with no difference between happy and sad clips (  t   (33) = −0.33,   p   = .74). They also reported experiencing the same emotion as the target for the majority of the emotional video clips (affect sharing, mean = 72.8%, SD = 28.5%), with a higher degree concordance for sad (mean = 79.4%, SD = 32.8%) compared to happy clips (mean = 66.2%, SD = 31.9%;   t   (33) = 2.5,   p   &lt; .05). 13 participants shared the target&#x27;s emotion in every emotional video clip while one participant did not show affect sharing in any of the clips. For the remaining 20 participants who showed a mix of affect sharing and non-sharing, Z-EA scores did not differ for videos that elicited affect sharing (mean Z-EA = 1.09, SD = .24) compared to those that did not (mean Z-EA = 1.06, SD = .33,   t   (19) = .36,   p   = .72). 

Additionally, we found a positive correlation between participants&#x27; mean Z-EA scores and IRI perspective-taking (  r   = .48,   p   &lt; .01). No significant correlations were found between mean Z-EA scores and the other IRI subscales or estimated IQ (all   ps   &gt; .09). 

Note that while we used Pearson&#x27;s product-moment correlation, alternative methods for assessing agreement are available such as the intraclass correlation coefficient. EA scores derived using this measure were highly correlated (r = 0.89) with Pearson&#x27;s correlations. We chose the latter for two reasons. First, we were able to confirm our findings after partialling out dependency over time of the ratings (data not shown) and second, we wished to maintain compatibility with previous studies using similar tasks that also based estimates of inter-rater agreement on Pearson&#x27;s correlations. 


#### Behavioural analysis of the normative data sample 
  
The analysis showed that the mean Z-EA scores for the video clips presented in the fMRI study were strongly positively correlated with Z-EA scores for the seven clips presented in the normative data study (happy:   r   = .82,   p   &lt; .001; sad:   r   = .77,   p   &lt; .001). Furthermore, the intra-individual standard deviation of the four emotional video clips presented in the scanner (mean iSD = .36) did not differ from the individual standard deviation across all 20 emotional video clips presented outside the scanner (mean iSD = .39,   t   (59) = −1.64,   p   = .11). 



### fMRI data 
  
#### Emotional vs. neutral video clips 
  
Group-level analysis revealed a higher BOLD response during emotional compared to neutral clips in a large cluster spanning multiple regions, with peak activations in bilateral occipital poles and inferior lateral occipital cortex ( a,  ). The cluster included bilateral posterior and anterior superior temporal cortex (STC), as well as bilateral temporal pole (TP), bilateral planum temporale and bilateral posterior temporoparietal junction (pTPJ). Higher activation was also seen in right inferior frontal gyrus (IFG; including pars triangularis and opercularis), with the cluster extending into right anterior insular cortex (AI) and right putamen. A second cluster showed higher activation in supplementary motor area (SMA). While participants were watching neutral compared to emotional video clips, activation was higher in left superior lateral occipital cortex, left posterior cingulate cortex (PCC) and left precuneus. Significant activation was similar, albeit more widespread, when not controlling for button presses (see  ). Moreover, when analysing the button press condition separately for emotional and neutral video clips, no brain regions showed significant differences between both button press conditions.   
 Neural substrates of changes in empathy.   a) Significant brain activations when viewing emotional video clips compared to neutral ones. b) Regions significantly positively (red) and negatively (blue) modulated by variations in empathic accuracy (Z-EA scores). c) top: Brain areas significantly positively correlated over time with the participants&#x27; ratings of the target&#x27;s emotional intensity. bottom: BOLD response (after first level regression) of significant clusters (blue) and participant&#x27;s ratings of the target&#x27;s emotional intensity (green) of one exemplary participant. Key: STC - superior temporal cortex, TP - temporal pole, TPJ - temporoparietal junction, IFG - inferior frontal gyrus, SMA - supplementary motor area, aMCC - anterior midcingulate cortex. 
  Fig. 2     
Significant clusters and their peak activations for the contrasts emotional &gt; neutral video clips and neutral &gt; emotional video clips (threshold-free cluster enhancement   p   &lt; 0.05). 
  Table 2   

To explore differences between the different emotion conditions, we also directly compared happy and sad video clips. Activation in the bilateral STC was higher during happy compared to sad clips, while the right paracingulate gyrus and right precuneus showed higher activation during sad video clips (see   and  ). 


#### Intra-individual variation in empathic accuracy 
  
Participants&#x27; intra-individual variations in Z-EA scores were positively related to activation in clusters spanning the bilateral STC, planum temporale, TP and pTPJ, left hippocampus and left amygdala. Activity in the bilateral inferior lateral occipital cortex and fusiform cortex was also positively related to Z-EA scores ( b,  ). Activation in the bilateral paracingulate gyrus and right frontal pole as well as the right middle frontal gyrus was significantly negatively modulated by Z-EA scores.   
Significant clusters and their peak activations for the modulation of BOLD-response by intra-individual variation of Z-EA scores (threshold-free cluster enhancement   p   &lt; 0.05). 
  Table 3   


#### Correlation with emotional intensity ratings 
  
While watching emotional video clips, participants&#x27; fluctuations in ratings of the targets&#x27; emotional intensity were positively correlated over time with changes in BOLD response in multiple brain regions ( c,  ). Associations were found in multiple clusters including bilateral posterior STC, bilateral TP, bilateral IFG (including pars triangularis and opercularis), bilateral SMA, bilateral middle and superior frontal cortices, right anterior midcingulate cortex (aMCC), right AI, bilateral amygdala, bilateral putamen as well as pTPJ and right temporal occipital and anterior temporal fusiform cortex. Emotional intensity ratings and BOLD-response were negatively correlated in the in the bilateral superior lateral occipital cortex, PCC, and precuneus.   shows a binarised overlay of significant clusters in the different analyses.   
Binarised overlay of activations related to a) emotional compared to neutral video clips, b) variation positively related to empathic accuracy and c) positive correlation with emotional intensity. 
  Fig. 3     
Significant clusters and their peak activations for the correlation between BOLD-response and the participants&#x27; ratings of the target&#x27;s emotional intensity (threshold-free cluster enhancement   p   &lt; 0.05). 
  Table 4   


#### Exploratory analysis: impact of affect sharing 
  
For emotional video clips, there were no significant differences in BOLD response between clips that elicited, versus those that did not elicit, affect sharing (i.e. participants reported experiencing the same emotion as the target after providing their continuous ratings). 




## Discussion 
  
We used a modified version of the EAT to study neural substrates of empathic accuracy and to gain a better understanding of its underlying components. We demonstrated that fluctuations in participants&#x27; perceived emotional intensity ratings are correlated with activation in a network of brain regions previously implicated in empathy and broader aspects of social cognition (i.e., mentalising). More specifically, consistent with our first hypothesis, we observed increased activation in brain regions associated with empathy and mentalising when participants watched emotional compared to neutral clips. Supporting our second hypothesis, we found a positive correlation between intra-individual variations in empathic accuracy and the temporal lobe, “mentalising” regions of the same network. Confirming our third hypothesis, we found a correlation between fluctuations in ratings of the targets&#x27; perceived emotional intensity over time and activity in these same regions. This network of brain regions appears not only to have a general role in emotion and empathic processing but is also sensitive to   variations   in the intensity of others&#x27; emotions. 

The superior temporal sulcus (STS), temporoparietal junction (TPJ), and temporal pole (TP) have consistently been associated with mentalising ( ). In our study, these areas were more active with higher EA, i.e. when participants were more accurate at tracking the target&#x27;s emotion. Beyond this, we could also show these regions are sensitive to fluctuations in perceived emotional intensity of others. The STS is thought to facilitate mentalising by interpreting social aspects of observed biological motion ( ,  ) and the region has been implicated in EA ( ,  ). The TPJ is involved in inferring other people&#x27;s temporary mental states ( ) while the TP&#x27;s role in mentalising is thought to involve the integration of multimodal information and recollection of social scripts ( ;  ). Combined, these brain regions are involved in distinct emotional and cognitive processes that are required to perform our modified EAT: they are integral for the successful tracking of others&#x27; emotional intensity and correlate positively with intra-individual variations in EA. 

The anterior insula (AI), anterior midcingulate cortex (aMCC), inferior frontal gyrus (IFG) and supplementary motor area (SMA) have previously been implicated in empathy tasks and are associated with the affect sharing component of empathy (or affective empathy) ( ). Together these regions are implicated in the emotional processing of the modified EAT stimuli. Most importantly, we could show for the first time that their activity tracks the perceived emotional intensity of others. However, activity in these brain regions was not sensitive to changes in EA and thus seems more tied to the subjective perception of other&#x27;s feelings. 

This suggests it is the time-series variation in activation in the temporal lobe regions (STS, TPJ, TP) that might be informative for accurately tracking other people&#x27;s emotions, while activation in the frontal regions (AI, ACC, IFG, SMA) represents a different emotion processing component that does not vary with changes in EA ( ). This is consistent with previous studies on EA, which showed either no correlation between EA and activity in the above frontal regions ( ) or, in the case of adolescents, a negative correlation between EA and ACC and AI activation ( ). Furthermore, we could not replicate an association between EA and activity in the inferior parietal lobe, a region implicated in motor imitation and previously interpreted as an affective processing component of EA ( ). Taken together, these findings provide evidence that EA is more closely related to the concept of cognitive empathy and mentalising than affective empathy and emotion sharing. The role of EA in cognitive but not affective empathy is further supported by the positive correlation between EA scores and the perspective-taking scale of a well-established self-report measure of empathy (the IRI) but not with other more affective subscales such as empathic concern. Moreover, participants&#x27; average EA scores did not differ between videos where they shared the same emotion as the target compared to those were they did not, which again suggests that emotion sharing is neither necessary for, nor facilitates, EA. 

Even if EA does only relate to cognitive but not affective empathy, the EAT as a task successfully elicited affective empathy in most of our participants – they reported sharing the target&#x27;s emotion in 73% of the emotional video clips. However, there were no significant differences in brain activity when rating videos where participants shared the same emotion compared to videos where they did not. This further supports our hypothesis that the higher activation in aMCC, AI, SMA and IFG during emotional clips is associated with more basal, empathy-independent aspects of emotion processing. 

Higher activation of the bilateral STS could also be seen during happy compared to sad video clips, while the right paracingulate gyrus was more highly activated during sad video clips. This is in line with our behavioural findings of, on average, higher EA scores during happy video clips, which suggest more successful tracking and mentalising of the target&#x27;s emotion, while sad video clips induced higher rates of affect sharing among participants. The paracingulate gyrus has previously been implicated in affective empathy ( ). 

During the modified EAT, participants rated fluctuations in emotional intensity rather than valence as this allowed a comparable rating scale across different distinct emotions. Furthermore, previous literature suggests distinct neural correlates for processing emotional intensity and valence ( ), with the amygdala being associated with intensity and the orbitofrontal cortex with valence. In agreement with this, we found that bilateral activation of amygdala but not the orbitofrontal cortex covaried with the emotional intensity of the targets. Unexpectedly activation in the precuneus – a region implicated in self-referential processing ( ) – was stronger during neutral versus emotional clips and correlated negatively with emotional intensity ratings. The precuneus is associated with visual-spatial imagery ( ,  ) and is a component of the default mode network ( ). Higher activation during the neutral videos in which participants described their bedroom, might be explained by higher visual-spatial imagery and an increased tendency for mind-wandering during these less engaging parts of the task ( ). 

Empathy is a complex and dynamic process, which requires multiple higher order functions ( ) such as emotion recognition, multimodal sensory integration, self-other distinction and continuous processing of valence and intensity information. Compared to other commonly used empathy tasks, the modified EAT used a more naturalistic setting to examine which brain regions track fluctuations over time in perceived emotional intensity of others and intra-individual variations in empathic accuracy. Previous studies in the empathy and mentalising literature have largely focused on simplistic stimuli (e.g. static images of hands in painful situations). Compared to these earlier studies, we found that regions that have been separately implicated in mentalising and empathy were all involved in performing the modified EAT. However, only brain regions previously associated with mentalising were found to covary with EA, while regions previously implicated in classic affective empathy paradigms were positively correlated with the emotional intensity of others but were not sensitive to changes in EA. In this more naturalistic and complex task, it seems that an interplay between brain networks associated with mentalising and empathy enables the accurate tracking of other&#x27;s emotions. Furthermore, these regions were sensitive to fluctuations in perceived emotional intensity of others, which serves as a potential mechanism for successful communication between these networks to achieve empathic accuracy. 

A possible limitation of our study was the lower number of emotional video clips in comparison to previous studies on EA ( ,  ). This study was conducted within the framework of a larger project, and thus the scanning time was limited. However, we showed that our chosen video clips led to very similar EA scores relative to those obtained with the larger dataset of 27 video clips in the norm sample. More importantly, the intra-individual variation across videos was also comparable to that seen for the full set of video clips. 

The study had a number of strengths. The original EAT ( ) represented an important advance in empathy research, as it was the first task to utilise naturalistic stimuli and assess EA in an fMRI context. In this modified EAT, the stimuli used for fMRI purposes had been validated in a separate behavioural study. Moreover, we added a neutral control condition, which allowed us to identify brain regions that are generally more active during emotional video clips irrespective of empathic accuracy. Future studies could employ this paradigm to study psychiatric populations with empathy deficits (e.g., adolescents with Conduct Disorder;  ). By additionally taking the neutral control condition into account, one could examine whether emotional clips were ‘neutral-like’ in those with low EA scores. For future studies, it would be worth considering incorporating neutral videos with varying topics other than bedroom descriptions to ensure continued engagement throughout the task (see   for possible examples). Furthermore, we introduced the measurement of emotional intensity rather than valence, which is more closely related to the concept of empathy. This also made the video clips of different emotions comparable and allowed a more fine-grained analysis of changes over time in activation related to the emotional intensity of others. Together, we propose that the three analysis techniques used in this study, should be employed in conjunction to allow a comprehensive study of empathic accuracy and its different components. 

In conclusion, we provide the first evidence that the modified EAT is a suitable paradigm for studying empathy and its underlying components. We show that, while the modified EAT successfully induces both affective and cognitive empathy, EA relies more on cognitive empathy than affect sharing. The neutral control condition and the valence-independent rating scale represent valuable additions to the task. The fMRI data analysis techniques developed and described here may prove valuable in characterising differences between healthy participants and participants with psychiatric conditions associated with empathy deficits. 


## Funding 
  
This work was funded by a project grant from the   to ESB, MM and GF (MR/K022474/1). 


## Declaration of interest 
  
We do not have any financial, institutional or other relationships that might lead to a conflict of interest.</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (3)</summary><details class="inner-accordion"><summary>Table 2 (tbl2) - Significant clusters and their peak activations for the contrasts emotional &gt; neutral video clips and neutral &gt; emotional video clips (threshold-free cluster enhancement pFWE &lt; 0.05).</summary><div class="table-html"><table-wrap id="tbl2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Significant clusters and their peak activations for the contrasts emotional &gt; neutral video clips and neutral &gt; emotional video clips (threshold-free cluster enhancement <italic toggle="yes">p</italic><sub>FWE</sub> &lt; 0.05).</p></caption><alt-text id="alttext0035">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="1">Cluster</th><th rowspan="2" colspan="1">Anatomical region</th><th rowspan="2" colspan="1">Hemisphere</th><th rowspan="2" colspan="1">Cluster size</th><th colspan="3" rowspan="1">MNI coordinates [mm]<hr /></th><th rowspan="2" colspan="1">Peak-level <italic toggle="yes">t</italic></th></tr><tr><th colspan="1" rowspan="1">x</th><th colspan="1" rowspan="1">y</th><th colspan="1" rowspan="1">z</th></tr></thead><tbody><tr><td colspan="8" align="left" rowspan="1"><bold>emotional &gt; neutral video clips</bold><hr /></td></tr><tr><td rowspan="11" align="left" colspan="1">1</td><td align="left" colspan="1" rowspan="1">Occipital Pole</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">9901</td><td align="left" colspan="1" rowspan="1">20</td><td align="left" colspan="1" rowspan="1">−94</td><td align="left" colspan="1" rowspan="1">−2</td><td align="left" colspan="1" rowspan="1">7.33</td></tr><tr><td align="left" colspan="1" rowspan="1">Inferior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−32</td><td align="left" colspan="1" rowspan="1">−90</td><td align="left" colspan="1" rowspan="1">−6</td><td align="left" colspan="1" rowspan="1">6.86</td></tr><tr><td align="left" colspan="1" rowspan="1">Anterior Superior Temporal Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">52</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">−18</td><td align="left" colspan="1" rowspan="1">6.21</td></tr><tr><td align="left" colspan="1" rowspan="1">Occipital Pole</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">−94</td><td align="left" colspan="1" rowspan="1">12</td><td align="left" colspan="1" rowspan="1">5.94</td></tr><tr><td align="left" colspan="1" rowspan="1">Inferior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">−70</td><td align="left" colspan="1" rowspan="1">−4</td><td align="left" colspan="1" rowspan="1">5.1</td></tr><tr><td align="left" colspan="1" rowspan="1">Posterior Superior Temporal Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">56</td><td align="left" colspan="1" rowspan="1">−28</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">4.57</td></tr><tr><td align="left" colspan="1" rowspan="1">Insular Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">28</td><td align="left" colspan="1" rowspan="1">16</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">4.39</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Occipital Fusiform Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">−50</td><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">4.15</td></tr><tr><td align="left" colspan="1" rowspan="1">Frontal Operculum Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">48</td><td align="left" colspan="1" rowspan="1">16</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">4.02</td></tr><tr><td align="left" colspan="1" rowspan="1">Occipital Fusiform Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−36</td><td align="left" colspan="1" rowspan="1">−70</td><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">4.01</td></tr><tr><td align="left" colspan="1" rowspan="1">Occipital Pole</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−10</td><td align="left" colspan="1" rowspan="1">−100</td><td align="left" colspan="1" rowspan="1">−14</td><td align="left" colspan="1" rowspan="1">3.44</td></tr><tr><td rowspan="4" align="left" colspan="1">2</td><td align="left" colspan="1" rowspan="1">Anterior Superior Temporal Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">2243</td><td align="left" colspan="1" rowspan="1">−52</td><td align="left" colspan="1" rowspan="1">−6</td><td align="left" colspan="1" rowspan="1">−14</td><td align="left" colspan="1" rowspan="1">5.32</td></tr><tr><td align="left" colspan="1" rowspan="1">Posterior Supramarginal Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−56</td><td align="left" colspan="1" rowspan="1">−44</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">4.45</td></tr><tr><td align="left" colspan="1" rowspan="1">Middle Temporal Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−44</td><td align="left" colspan="1" rowspan="1">−32</td><td align="left" colspan="1" rowspan="1">−2</td><td align="left" colspan="1" rowspan="1">3.85</td></tr><tr><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−60</td><td align="left" colspan="1" rowspan="1">−20</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">3.48</td></tr><tr><td align="left" colspan="1" rowspan="1">3</td><td align="left" colspan="1" rowspan="1">Supplementary Motor Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">250</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">60</td><td align="left" colspan="1" rowspan="1">5.04</td></tr><tr><td align="left" colspan="1" rowspan="1">4<hr /></td><td align="left" colspan="1" rowspan="1">Temporal Pole<hr /></td><td align="left" colspan="1" rowspan="1">L<hr /></td><td align="left" colspan="1" rowspan="1">6<hr /></td><td align="left" colspan="1" rowspan="1">−46<hr /></td><td align="left" colspan="1" rowspan="1">18<hr /></td><td align="left" colspan="1" rowspan="1">−26<hr /></td><td align="left" colspan="1" rowspan="1">3.53<hr /></td></tr><tr><td colspan="8" align="left" rowspan="1"><bold>neutral &gt; emotional video clips</bold><hr /></td></tr><tr><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">1014</td><td align="left" colspan="1" rowspan="1">−34</td><td align="left" colspan="1" rowspan="1">−80</td><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">6.59</td></tr><tr><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−44</td><td align="left" colspan="1" rowspan="1">−84</td><td align="left" colspan="1" rowspan="1">22</td><td align="left" colspan="1" rowspan="1">5.86</td></tr><tr><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">Posterior Cingulate Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">207</td><td align="left" colspan="1" rowspan="1">−4</td><td align="left" colspan="1" rowspan="1">−38</td><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">8.99</td></tr><tr><td align="left" colspan="1" rowspan="1">3</td><td align="left" colspan="1" rowspan="1">Precuneus Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">86</td><td align="left" colspan="1" rowspan="1">−14</td><td align="left" colspan="1" rowspan="1">−60</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">6.2</td></tr><tr><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">54</td><td align="left" colspan="1" rowspan="1">36</td><td align="left" colspan="1" rowspan="1">−76</td><td align="left" colspan="1" rowspan="1">42</td><td align="left" colspan="1" rowspan="1">5.32</td></tr><tr><td align="left" colspan="1" rowspan="1">5</td><td align="left" colspan="1" rowspan="1">Lingual Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">34</td><td align="left" colspan="1" rowspan="1">−38</td><td align="left" colspan="1" rowspan="1">−10</td><td align="left" colspan="1" rowspan="1">5.49</td></tr><tr><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">30</td><td align="left" colspan="1" rowspan="1">−30</td><td align="left" colspan="1" rowspan="1">−20</td><td align="left" colspan="1" rowspan="1">5.46</td></tr></tbody></table></table-wrap></div></details><details class="inner-accordion"><summary>Table 3 (tbl3) - Significant clusters and their peak activations for the modulation of BOLD-response by intra-individual variation of Z-EA scores (threshold-free cluster enhancement pFWE &lt; 0.05).</summary><div class="table-html"><table-wrap id="tbl3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Significant clusters and their peak activations for the modulation of BOLD-response by intra-individual variation of Z-EA scores (threshold-free cluster enhancement <italic toggle="yes">p</italic><sub>FWE</sub> &lt; 0.05).</p></caption><alt-text id="alttext0040">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="1">Cluster</th><th rowspan="2" colspan="1">Anatomical region</th><th rowspan="2" colspan="1">Hemisphere</th><th rowspan="2" colspan="1">Cluster size</th><th colspan="3" rowspan="1">MNI coordinates [mm]<hr /></th><th rowspan="2" colspan="1">Peak-level <italic toggle="yes">t</italic></th></tr><tr><th colspan="1" rowspan="1">x</th><th colspan="1" rowspan="1">y</th><th colspan="1" rowspan="1">z</th></tr></thead><tbody><tr><td colspan="8" align="left" rowspan="1"><bold>Positively related to Z-EA scores</bold><hr /></td></tr><tr><td rowspan="7" align="left" colspan="1">1</td><td align="left" colspan="1" rowspan="1">Posterior Superior Temporal Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">9036</td><td align="left" colspan="1" rowspan="1">−62</td><td align="left" colspan="1" rowspan="1">−26</td><td align="left" colspan="1" rowspan="1">10</td><td align="left" colspan="1" rowspan="1">9.88</td></tr><tr><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−38</td><td align="left" colspan="1" rowspan="1">−34</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">9.17</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Pole</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−54</td><td align="left" colspan="1" rowspan="1">−2</td><td align="left" colspan="1" rowspan="1">−2</td><td align="left" colspan="1" rowspan="1">7.02</td></tr><tr><td align="left" colspan="1" rowspan="1">Hippocampus</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−20</td><td align="left" colspan="1" rowspan="1">−14</td><td align="left" colspan="1" rowspan="1">−20</td><td align="left" colspan="1" rowspan="1">6.50</td></tr><tr><td align="left" colspan="1" rowspan="1">Inferior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−44</td><td align="left" colspan="1" rowspan="1">−72</td><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">6.03</td></tr><tr><td align="left" colspan="1" rowspan="1">Posterior Temporal Fusiform Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−38</td><td align="left" colspan="1" rowspan="1">−42</td><td align="left" colspan="1" rowspan="1">−26</td><td align="left" colspan="1" rowspan="1">4.67</td></tr><tr><td align="left" colspan="1" rowspan="1">Occipital Fusiform Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−20</td><td align="left" colspan="1" rowspan="1">−90</td><td align="left" colspan="1" rowspan="1">−18</td><td align="left" colspan="1" rowspan="1">4.61</td></tr><tr><td rowspan="2" align="left" colspan="1">2</td><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">2421</td><td align="left" colspan="1" rowspan="1">64</td><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">7.37</td></tr><tr><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">34</td><td align="left" colspan="1" rowspan="1">−28</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">4.93</td></tr><tr><td rowspan="2" align="left" colspan="1">3<hr /></td><td align="left" colspan="1" rowspan="1">Inferior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">2315</td><td align="left" colspan="1" rowspan="1">46</td><td align="left" colspan="1" rowspan="1">−66</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">7.80</td></tr><tr><td align="left" colspan="1" rowspan="1">Occipital Fusiform Cortex<hr /></td><td align="left" colspan="1" rowspan="1">R<hr /></td><td colspan="1" rowspan="1"><hr /></td><td align="left" colspan="1" rowspan="1">22<hr /></td><td align="left" colspan="1" rowspan="1">−88<hr /></td><td align="left" colspan="1" rowspan="1">−8<hr /></td><td align="left" colspan="1" rowspan="1">5.37<hr /></td></tr><tr><td colspan="8" align="left" rowspan="1"><bold>Negatively related to Z-EA scores</bold><hr /></td></tr><tr><td rowspan="3" align="left" colspan="1">1</td><td align="left" colspan="1" rowspan="1">Paracingulate Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">275</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">22</td><td align="left" colspan="1" rowspan="1">48</td><td align="left" colspan="1" rowspan="1">4.11</td></tr><tr><td align="left" colspan="1" rowspan="1">Frontal Pole</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">10</td><td align="left" colspan="1" rowspan="1">62</td><td align="left" colspan="1" rowspan="1">36</td><td align="left" colspan="1" rowspan="1">4.01</td></tr><tr><td align="left" colspan="1" rowspan="1">Paracingulate Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−6</td><td align="left" colspan="1" rowspan="1">44</td><td align="left" colspan="1" rowspan="1">30</td><td align="left" colspan="1" rowspan="1">3.81</td></tr><tr><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">Middle Frontal Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">31</td><td align="left" colspan="1" rowspan="1">36</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">32</td><td align="left" colspan="1" rowspan="1">4.42</td></tr></tbody></table></table-wrap></div></details><details class="inner-accordion"><summary>Table 4 (tbl4) - Significant clusters and their peak activations for the correlation between BOLD-response and the participants&#x27; ratings of the target&#x27;s emotional intensity (threshold-free cluster enhancement pFWE &lt; 0.05).</summary><div class="table-html"><table-wrap id="tbl4" position="float" orientation="portrait"><label>Table 4</label><caption><p>Significant clusters and their peak activations for the correlation between BOLD-response and the participants' ratings of the target's emotional intensity (threshold-free cluster enhancement <italic toggle="yes">p</italic><sub>FWE</sub> &lt; 0.05).</p></caption><alt-text id="alttext0045">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="1">Cluster</th><th rowspan="2" colspan="1">Anatomical region</th><th rowspan="2" colspan="1">Hemisphere</th><th rowspan="2" colspan="1">Cluster size</th><th colspan="3" rowspan="1">MNI coordinates [mm]<hr /></th><th rowspan="2" colspan="1">Peak-level <italic toggle="yes">t</italic></th></tr><tr><th colspan="1" rowspan="1">x</th><th colspan="1" rowspan="1">y</th><th colspan="1" rowspan="1">z</th></tr></thead><tbody><tr><td colspan="8" align="left" rowspan="1"><bold>Positive correlation with participants' emotional intensity ratings</bold><hr /></td></tr><tr><td rowspan="17" align="left" colspan="1">1</td><td align="left" colspan="1" rowspan="1">Posterior Superior Temporal Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">24492</td><td align="left" colspan="1" rowspan="1">58</td><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">9.56</td></tr><tr><td align="left" colspan="1" rowspan="1">Posterior Middle Frontal Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">62</td><td align="left" colspan="1" rowspan="1">−36</td><td align="left" colspan="1" rowspan="1">0</td><td align="left" colspan="1" rowspan="1">8.26</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Pole</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">58</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">8.19</td></tr><tr><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−64</td><td align="left" colspan="1" rowspan="1">−14</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">8.05</td></tr><tr><td align="left" colspan="1" rowspan="1">Putamen</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">26</td><td align="left" colspan="1" rowspan="1">−92</td><td align="left" colspan="1" rowspan="1">−6</td><td align="left" colspan="1" rowspan="1">7.4</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Pole</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−56</td><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">−10</td><td align="left" colspan="1" rowspan="1">7.04</td></tr><tr><td align="left" colspan="1" rowspan="1">Middle Frontal Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">48</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">38</td><td align="left" colspan="1" rowspan="1">6.92</td></tr><tr><td align="left" colspan="1" rowspan="1">Middle Temporal Gyrus, temporooccipital part</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">46</td><td align="left" colspan="1" rowspan="1">−56</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">6.49</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Occipital Fusiform Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">−46</td><td align="left" colspan="1" rowspan="1">−16</td><td align="left" colspan="1" rowspan="1">6.06</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Occipital Fusiform Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−46</td><td align="left" colspan="1" rowspan="1">−64</td><td align="left" colspan="1" rowspan="1">−28</td><td align="left" colspan="1" rowspan="1">5.7</td></tr><tr><td align="left" colspan="1" rowspan="1">Insular Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">38</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">−20</td><td align="left" colspan="1" rowspan="1">5.49</td></tr><tr><td align="left" colspan="1" rowspan="1">Middle Temporal Gyrus, temporooccipital part</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−64</td><td align="left" colspan="1" rowspan="1">−44</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">5.31</td></tr><tr><td align="left" colspan="1" rowspan="1">Inferior Frontal Gyrus, pars triangularis</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">56</td><td align="left" colspan="1" rowspan="1">28</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">5.04</td></tr><tr><td align="left" colspan="1" rowspan="1">Temporal Pole</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−44</td><td align="left" colspan="1" rowspan="1">20</td><td align="left" colspan="1" rowspan="1">−26</td><td align="left" colspan="1" rowspan="1">4.93</td></tr><tr><td align="left" colspan="1" rowspan="1">Occipital Fusiform Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−30</td><td align="left" colspan="1" rowspan="1">−82</td><td align="left" colspan="1" rowspan="1">−18</td><td align="left" colspan="1" rowspan="1">4.87</td></tr><tr><td align="left" colspan="1" rowspan="1">Planum Temporale</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−40</td><td align="left" colspan="1" rowspan="1">−36</td><td align="left" colspan="1" rowspan="1">10</td><td align="left" colspan="1" rowspan="1">4.84</td></tr><tr><td align="left" colspan="1" rowspan="1">Amygdala</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−18</td><td align="left" colspan="1" rowspan="1">−6</td><td align="left" colspan="1" rowspan="1">−14</td><td align="left" colspan="1" rowspan="1">4.78</td></tr><tr><td rowspan="2" align="left" colspan="1">2</td><td align="left" colspan="1" rowspan="1">Supplementary Motor Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">1735</td><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">66</td><td align="left" colspan="1" rowspan="1">6.85</td></tr><tr><td align="left" colspan="1" rowspan="1">Anterior Midcingulate Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">14</td><td align="left" colspan="1" rowspan="1">38</td><td align="left" colspan="1" rowspan="1">3.74</td></tr><tr><td rowspan="2" align="left" colspan="1">3</td><td align="left" colspan="1" rowspan="1">Precentral Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">1714</td><td align="left" colspan="1" rowspan="1">−40</td><td align="left" colspan="1" rowspan="1">−8</td><td align="left" colspan="1" rowspan="1">56</td><td align="left" colspan="1" rowspan="1">5.64</td></tr><tr><td align="left" colspan="1" rowspan="1">Superior Frontal Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−24</td><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">72</td><td align="left" colspan="1" rowspan="1">4.96</td></tr><tr><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">Postcentral Gyrus</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">206</td><td align="left" colspan="1" rowspan="1">−48</td><td align="left" colspan="1" rowspan="1">−26</td><td align="left" colspan="1" rowspan="1">40</td><td align="left" colspan="1" rowspan="1">4.05</td></tr><tr><td align="left" colspan="1" rowspan="1">5<hr /></td><td align="left" colspan="1" rowspan="1">Putamen<hr /></td><td align="left" colspan="1" rowspan="1">R<hr /></td><td align="left" colspan="1" rowspan="1">173<hr /></td><td align="left" colspan="1" rowspan="1">18<hr /></td><td align="left" colspan="1" rowspan="1">10<hr /></td><td align="left" colspan="1" rowspan="1">6<hr /></td><td align="left" colspan="1" rowspan="1">4.67<hr /></td></tr><tr><td colspan="8" align="left" rowspan="1"><bold>Negative correlation with participants' emotional intensity ratings</bold><hr /></td></tr><tr><td rowspan="7" align="left" colspan="1">1</td><td align="left" colspan="1" rowspan="1">Cuneus Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td align="left" colspan="1" rowspan="1">10193</td><td align="left" colspan="1" rowspan="1">10</td><td align="left" colspan="1" rowspan="1">−86</td><td align="left" colspan="1" rowspan="1">24</td><td align="left" colspan="1" rowspan="1">6.42</td></tr><tr><td align="left" colspan="1" rowspan="1">Posterior Cingulate Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">−34</td><td align="left" colspan="1" rowspan="1">38</td><td align="left" colspan="1" rowspan="1">5.48</td></tr><tr><td align="left" colspan="1" rowspan="1">Precuneus Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−4</td><td align="left" colspan="1" rowspan="1">−66</td><td align="left" colspan="1" rowspan="1">16</td><td align="left" colspan="1" rowspan="1">5.3</td></tr><tr><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">38</td><td align="left" colspan="1" rowspan="1">−74</td><td align="left" colspan="1" rowspan="1">22</td><td align="left" colspan="1" rowspan="1">5.25</td></tr><tr><td align="left" colspan="1" rowspan="1">Precuneus Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">−12</td><td align="left" colspan="1" rowspan="1">−58</td><td align="left" colspan="1" rowspan="1">34</td><td align="left" colspan="1" rowspan="1">4.82</td></tr><tr><td align="left" colspan="1" rowspan="1">Lingual Gyrus</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">26</td><td align="left" colspan="1" rowspan="1">−52</td><td align="left" colspan="1" rowspan="1">−4</td><td align="left" colspan="1" rowspan="1">4.66</td></tr><tr><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">R</td><td colspan="1" rowspan="1" /><td align="left" colspan="1" rowspan="1">44</td><td align="left" colspan="1" rowspan="1">−64</td><td align="left" colspan="1" rowspan="1">46</td><td align="left" colspan="1" rowspan="1">3.99</td></tr><tr><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">466</td><td align="left" colspan="1" rowspan="1">−38</td><td align="left" colspan="1" rowspan="1">−70</td><td align="left" colspan="1" rowspan="1">32</td><td align="left" colspan="1" rowspan="1">4.69</td></tr><tr><td align="left" colspan="1" rowspan="1">3</td><td align="left" colspan="1" rowspan="1">Temporal Occipital Fusiform Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">121</td><td align="left" colspan="1" rowspan="1">−24</td><td align="left" colspan="1" rowspan="1">−56</td><td align="left" colspan="1" rowspan="1">−12</td><td align="left" colspan="1" rowspan="1">3.23</td></tr><tr><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">Superior Lateral Occipital Cortex</td><td align="left" colspan="1" rowspan="1">L</td><td align="left" colspan="1" rowspan="1">32</td><td align="left" colspan="1" rowspan="1">−50</td><td align="left" colspan="1" rowspan="1">−74</td><td align="left" colspan="1" rowspan="1">26</td><td align="left" colspan="1" rowspan="1">3.77</td></tr></tbody></table></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 30349467</strong> | Pred included: 5 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=2, unmatched=3</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/30349467/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Activations: Leading &gt; Following (Followers only); others, Activations: Leading &gt; Following (Whole-group); others, Deactivations: Following &gt; Leading (Whole-group); others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>30349467_analysis_0</td><td>Leaders only (n = 11)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Following explicitly involves attending to the partner and inferring partner actions/intentions; authors report activations in mentalizing and action-perception areas (TPJ, pSTS, PCC) and interpret these as tracking/understanding the partner, satisfying perception-of-others criteria.</td></tr>
<tr><td>30349467_analysis_1</td><td>Followers only (n = 10)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Following explicitly involves tracking partner cues and inferring actions/intentions; the reported activations (TPJ, pSTS, PCC, mPFC) map onto action perception and understanding others’ mental states. The analysis therefore meets both inclusion criteria for perception and understanding of others.</td></tr>
<tr><td>30349467_analysis_2</td><td>Activations: Leading &gt; Following</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Following requires tracking and inferring partner actions/intentions; activations in mentalizing and action-perception regions (TPJ, pSTS, PCC, mPFC) indicate the analysis measures perception/understanding of others.</td></tr>
<tr><td>30349467_analysis_3</td><td>Deactivations: Following &gt; Leading</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Results and contrasts emphasize perception/understanding of others (mentalizing network, TPJ, pSTS, tracking partner movements), so the analysis measures perception/understanding of others. </td></tr>
<tr><td>30349467_analysis_4</td><td>Activation</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TN</span></td><td></td><td>This specific analysis (leading vs following correlated with leader skill) primarily probes leader-related motor and self-initiation networks rather than perception/understanding of others; follower-related regressions (not this analysis) more directly target mentalizing/action perception.</td></tr>
<tr><td>30349467_analysis_5</td><td>Deactivation</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TN</span></td><td></td><td>This specific analysis tests leader-skill correlations with Leading&gt;Following (motor/premotor network) rather than explicit perception or inference about others’ mental states (the follower-skill regressions probe mentalizing more directly).</td></tr>
<tr><td>30349467_analysis_6</td><td>Leading versus following correlated with skill as a follower.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The following-related activations and their correlation with follower skill engage regions implicated in action perception and understanding others’ mental states (pSTS, TPJ, PCC, mPFC). The contrast assesses perception/understanding of others’ actions and intentions, satisfying I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>30349467_1</td><td>Activations: Leading &gt; Following (Followers only); others</td><td>30349467_analysis_6</td><td>Leading versus following correlated with skill as a follower.</td><td>0.491</td><td>0.000</td><td>0.147</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>30349467_2</td><td>Activations: Leading &gt; Following (Leaders only); others</td><td>30349467_analysis_2</td><td>Activations: Leading &gt; Following</td><td>0.810</td><td>0.529</td><td>0.614</td><td>uncertain</td><td>coord_count_mismatch</td></tr><tr><td>30349467_3</td><td>Activations: Leading &gt; Following (Whole-group); others</td><td>30349467_analysis_4</td><td>Activation</td><td>0.357</td><td>0.000</td><td>0.107</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>30349467_4</td><td>Deactivations: Following &gt; Leading (Followers only); others</td><td>30349467_analysis_3</td><td>Deactivations: Following &gt; Leading</td><td>0.800</td><td>0.571</td><td>0.640</td><td>uncertain</td><td>coord_count_mismatch</td></tr><tr><td>30349467_5</td><td>Deactivations: Following &gt; Leading (Whole-group); others</td><td>30349467_analysis_5</td><td>Deactivation</td><td>0.400</td><td>0.000</td><td>0.120</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>30349467_6</td><td>[Leading &gt; Rest] and [Following &gt; Rest] (followers only); others</td><td>30349467_analysis_1</td><td>Followers only (n = 10)</td><td>0.380</td><td>1.000</td><td>0.814</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>30349467_7</td><td>[Leading &gt; Rest] and [Following &gt; Rest] (leaders only); others</td><td>30349467_analysis_0</td><td>Leaders only (n = 11)</td><td>0.347</td><td>1.000</td><td>0.804</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 6186800)</summary><p><strong>Title:</strong> Role-Specific Brain Activations in Leaders and Followers During Joint Action</p><details><summary>Abstract</summary><pre class="paper-text">Much of social interaction in human life requires that individuals perform different roles during joint actions, the most basic distinction being that between a leader and a follower. A number of neuroimaging studies have examined the brain networks for leading and following, but none have examined what effect prior expertise at these roles has on brain activations during joint motor tasks. Couple dancers (e.g., dancers of Tango, Salsa, and swing) are an ideal population in which examine such effects, since leaders and followers of partnered dances have similar overall levels of motor expertise at dancing, but can differ strikingly in their role-specific skill sets. To explore role-specific expertise effects on brain activations for the first time, we recruited nine skilled leaders and nine skilled followers of couple dances for a functional magnetic resonance imaging study. We employed a two-person scanning arrangement that allowed a more naturalistic interaction between two individuals. The dancers interacted physically with an experimenter standing next to the bore of the magnet so as to permit bimanual partnered movements. Together, they alternated between leading and following the joint movements. The results demonstrated that the brain activations during the acts of leading and following were enhanced by prior expertise at being a leader or follower, and that activity in task-specific brain areas tended to be positively correlated with the level of expertise at the corresponding role. These findings provide preliminary evidence that training at one role of a joint motor task can selectively enhance role-related brain activations.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Much joint action between two people involves the contrastive roles of leader and follower ( ). For example, when two people move a sofa, the front person is often the one who navigates the joint movement as well as the one who determines the speed at which the two people move, while the back person responds to these movement-cues and attempts to coordinate his/her actions with the front person. However, the experimental literature that examines joint action in the lab does not give consideration to individual differences, for example the fact that people may be predisposed toward being a leader or follower based on their personality traits or life experiences ( ). In typical studies of joint action, people are randomly assigned to being a leader or follower (or both) of a joint task without assessing individual differences in task expertise that may exist between them. This applies to studies of both experts ( ;  ) and non-experts ( ;  ;  ;  ). This may be problematic since many studies demonstrate that expertise has an effect on behavioral performance and brain activations across many domains ( ;  ;  ). 

An interesting solution to this problem is to examine couple dancers, such as Tango dancers, since such people engage in extensive training to develop expertise at one specific role in the dance, thereby making the assessment of leading/following experience on joint action quite feasible. Leaders and followers of a couple dance have similar overall levels of motor expertise at dancing, but they can differ strikingly in their role-specific skill sets, such that dancers of one role are often unable to dance the opposite role. This applies not merely to the movement patterns themselves, but to the   coordination   skills required for leading (e.g., force conveyance) and following (e.g., responsiveness to force cues). While previous neuroimaging studies have looked separately at the topics of leading/following and expertise, the current study–which is a follow-up analysis to a previously published study from our lab ( )–represents a first attempt at examining   role-specific expertise   at leading and following, doing so using trained leaders and followers of couple dances. The principal aim of the study is to identify role-specific brain activations, namely leading-related activations in trained leaders compared to non-leaders, and following-related activations in trained followers compared to non-followers. 

Previous studies of leading and following have tended to emphasize the networks for leading, more so than those for following. For example, studies of interactive imitation have compared the initiation and imitation of visual actions within the same group of participants, and have highlighted an initiation network involved in self-monitoring, willed action, and decision making ( ;  ;  ;  ). Studies of auditory-entrainment tasks, such as finger tapping, have studied expert leaders or individuals who spontaneously emerge as leaders with in the context of the study, and have similarly identified a network involved in decision making, movement initiation, and self-processing ( ;  ;  ). These studies have provided either no results or inconsistent findings regarding following or expert followers. In a previous publication from our lab ( ), we characterized the networks for leading and following during a joint-action task with physical interaction, using the same dancer participants as those employed in the present study. In accordance with the previous literature, we found that leading showed a motor- and self-oriented profile, engaging areas associated with motor planning, spatial navigation, sequencing, action monitoring, and error correction. In contrast, following showed a far more sensory- and externally oriented profile, revealing areas involved in somatosensation, proprioception, motion tracking, social cognition, and outcome monitoring. However, while that study compared the act of leading with the act of following, it did not assess the influence of prior expertise at being a leader or follower on the brain activations. That was the major objective of the current follow-up analysis, namely to examine role-specific expertise. 

It is well-established that expertise can influence both the structure and function of the brain. There is now a vast literature devoted to various forms of motor, perceptual, and cognitive expertise (reviewed in  ). A general finding of such studies is that brain activations and gray matter volume are enhanced in experts, as compared to non-experts, in areas that process the skills that underlie a person’s domain of expertise ( ;  ;  ). For example, with regard to perceptual tasks, trained musicians and other auditory experts show enhanced effects in auditory cortex ( ,  ;  ;  ), while visual experts show effects in visual cortex ( ,  ;  ). In the motor domain, effects are found in cortical and subcortical motor and premotor areas involved in motor execution, control, planning, and representation ( ;  ). Motor experts, such as athletes, dancers, and musicians, additionally demonstrate changes in perceptual and cognitive areas associated with their trained skills ( ). For example, sensorimotor coupling is enhanced in musicians and athletes ( ;  ). In addition, activations in the action-observation network [including premotor cortex (PMC), superior parietal lobule (SPL), and inferior parietal lobule (IPL)] are enhanced when dancers view specific dance patterns that they are expert in ( ), or when athletes view sports actions that they are expert in ( ), as compared to when the same people view dances or sports movements that they are not trained in.   suggested that this effect was due to motor training, rather than the associated perceptual training. Expertise, in addition to producing enhancements in processing, has also been linked to decreases in the overall number of activated foci in neuroimaging studies, especially in attentional and cognitive-control networks, suggesting an enhancement in automaticity of processing for the trained skill ( ;  ). The “two stage expertise hypothesis” ( ;  ) suggests that short-term training leads to enhancements of brain activations for the trained skill, while long-term training and skill mastering lead instead to decreases or reorganizations in brain activations. 

While previous neuroimaging studies have looked at leading/following and expertise in isolation, no study thus far has combined the two issues, which is the principal objective of the present study. As mentioned above, couple dancers are an ideal cohort for exploring role-specific expertise in leading and following, since they spend many years developing expertise at typically just one of the two roles of the dance. As a result, expert leaders are usually unskilled followers, and vice versa, while both groups have comparable levels of overall motor expertise at the dance. More specifically, leader expertise during couple dancing requires the generation of a motor plan for both the self and the partner, and the efficient conveyance of signals to the partner, while follower expertise requires the tracking of information coming from the leader and its interpretation to construct either an identical or complementary movement pattern in real time. 

In order to assess the effect of role expertise on brain activations during an ecologically valid joint-action task, we carried out an exploratory follow-up analysis to our previous publication that looked at leading and following ( ) in order to examine the effects of role-specific expertise on brain activations. In the previous study, skilled leaders and followers of couple dances performed both a leading and following task in a magnetic resonance imaging (MRI) scanner in interaction with an experimenter standing next to the bore of the magnet. The participant and experimenter were in physical contact at their hands, and alternated between being the leader and follower of joint improvised bimanual movements. The principal aim of the study was to compare brain activity during the acts of leading and following. The current study follows up on those results using the same dataset in order to examine the effects of individual differences on the brain activations, in particular an individual’s expertise at a given role of the dance. The aim was to look for role-specific brain activations, in other words leading-related activations in trained leaders compared to non-leaders (i.e., followers), and following-related activations in trained followers compared to non-followers (i.e., leaders). Based on the literature cited above demonstrating that experts show enhancements in task-specific brain areas compared to non-experts when performing the same tasks, we predicted that leaders, as compared with non-leaders, would show an enhancement of leading-related activations when leading (only), and likewise that followers, as compared with non-followers, would show an enhancement of following-related activations when following (only). Given that we were not able to effectively rule out the influence of gender on dance role in our design, the results need to be viewed as exploratory. 


## Materials and Methods 
  
### Participants 
  
Eighteen participants (nine of each gender) took part in this study after giving their written informed consent in accordance with the Hamilton Integrated Research Ethics Board, who approved the study (St. Joseph’s Healthcare, R. P. #12-3777). They received monetary compensation for their participation. None of them had a past history of neurological or psychiatric disease. An inclusion criterion for the study was that participants have at least 2 years of experience at one or more kinds of couple dances involving leading and following (e.g., Argentine Tango, Salsa, Swing, and Ballroom). Male participants (40.7 ± 14.9 years old) had a mean dance experience of 8.7 ± 7.2 years, principally as leaders, although one male had significant experience as a follower as well. Female participants (40.2 ± 12.3 years old) had a mean dance experience of 5.6 ± 2.9 years, principally as followers, although two females had significant experience as leaders as well. 

On the day of the experiment, participants reported their ability to lead or follow a couple dance using a scale from 0 to 100, where 0 corresponds to no expertise at leading or following, and 100 corresponds to a very high level of expertise at leading or following. Each person did separate ratings for leading and following skill, with results shown in   Figure   . We explained to participants that these scales emphasized the ability to transmit/receive information while dancing with a partner, rather than the ability to perform complex or stylistic movements. Males reported a mean leading ability of 69.8 ± 17.7 (one male was at 35 and the rest ranged from 60 to 90). Likewise, females reported a mean following ability of 77.2 ± 8.3 (ranging from 70 to 90). With regards to the complementary skill, males reported a mean following ability of 33.7 ± 21.6; the male with significant following experience reported his following ability at 78, while all the others males rated it at between 8 and 50. Females reported a mean leading ability of 28.9 ± 25.2; both females with significant leading experience reported their leading ability at 70, while all other females rated themselves at between 5 and 40. Correlations between leading ability, following ability, years of experience at dancing, and age showed that leading ability, but not following ability, correlated with the number of years of experience (  Table   ). Anecdotal evidence suggests that leading skill requires a greater amount of time and effort to achieve than does following skill, which may explain the exclusive correlation of leading skill with years of experience. Since leading and following ability were not anti-correlated in the analysis, participants designated as “leaders” in this study were comprised of all the participants who were primarily trained as leaders for at least 2 years (i.e., all the of males) plus the two participants who, although primarily trained as followers, had significant leading experience and a strong leading ability (two females). Those designated as “followers” were comprised of all the participants who were primarily trained as followers for at least 2 years (i.e., all of the females) plus one participant who, although primarily trained as leader, had significant following experience and a strong following ability (one male). Thus, three participants belonged in both groups. This division was used in only the first set of analyses (see below). 
  
Self-report scales for skills as a leader and follower of couple dances. The   x  -axis shows the self-rating scale for leader skill (left panel) and follower skill (right panel) for couple dancing, where 100 is the highest rating. The   y  -axis of each graph shows the number of participants, from the pool of 18, who rated themselves at the various levels of skill for each role. Female participants are color-coded red and males are color-coded blue, both here and in   Figures  ,   . Participants designated as “leaders” in this study were comprised of all the males plus the two females with strong leading ability, while those designated as “followers” were comprised of all the females plus the male with strong following ability. Leaders are color coded as purple here and in   Figures  –  , whereas followers are color coded as pink here and in   Figures  –   (not to be confused with the color coding of gender). 
    
Correlation between age, years of couple-dance experience, and self-reported leading and following skill. 
    

### Procedure 
  
While the participant was lying supine in the MRI scanner, an experimenter (LASC) stood next to the bore of the scanner in order to have physical contact with the participant’s two hands. The participant’s forearms were fastened to the side of their body such that only their wrists, hands and fingers were able to move. Participants’ hands (palms up) were always below the experimenter’s hands (palms down), so that the participants’ hands could not be passively moved. The experimenter had significant experience both as a follower and a leader of couple dances. Together, the participant and experimenter performed highly controlled joint hand movements in all three planes of motion, alternating between leading and following the joint movement during different task-epochs of the scan. The movement patterns were improvised, rather than pre-learned, in order to maintain an ongoing requirement for motor planning during leading and a comparably heightened sense of responsiveness during following. No external cuing of tempo or rhythm was done with a metronome or with music. Participants performed all conditions with their eyes closed, and were instructed about which task to perform by means of pre-recorded verbal cues delivered through MRI-compatible headphones. Each condition was performed in a random order six times in blocks of 28 s. 

Complete methods and details concerning fMRI acquisition and image analysis, including participant training, are described in  . Briefly, the functional MRI imaging parameters were 2000 ms TR, 35 ms TE, 90° flip angle, 39 axial slices, 4 mm slice thickness, 0 mm gap, 3.75 × 3.75 mm in-plane resolution, 64 × 64 matrix, and 240 mm field of view. An automatic shimming procedure was performed before each scan to minimize inhomogeneities in the static magnetic field. For each of the three functional scans, 216 volumes–corresponding to 12 epochs of 28 s task + 8 s rest–were collected over 7’12”, resulting in a total of 648 volumes. Two magnetic field maps (5 ms then 8 ms TE) with the same imaging parameters as the fMRI were acquired in order to unwarp the data. Unwarping was performed with the relaxation method of “anatabacus”, a plugin in BrainVoyager, in order to correct for non-rigid deformations. In addition, the head-motion parameters were included as nuisance regressors in the analysis. Functional and structural images were processed using BrainVoyager QX 2.8. Coordinate tables were computed using NeuroElf. 


### Analysis 
  
We first performed qualitative analyses on three groups to assess if there were any differences between being a leader and being a follower. Specifically, we carried out three random-effects analyses for the bidirectional contrast “Leading versus Following” (1) for the whole group of 18 participants, (2) for the 11 leaders only, and (3) for the 10 followers only. These were performed at a two-tailed statistical threshold of   p   &lt; 0.005 uncorrected with a cluster-level correction of   k   = 28 voxels determined with Alphasim (family-wise error   p   &lt; 0.05) in NeuroElf. The conjunction of [Leading &gt; Rest] ∩ [Following &gt; Rest] was also performed on these three groups in order to serve as a reference for the general network of brain areas activated by the movement tasks, irrespective of role. It was performed at a two-tailed statistical threshold of   p   &lt; 0.005 uncorrected with a cluster-level correction of   k   = 49 voxels determined with Alphasim. 

Since qualitative differences were found (see Results section), we tested further for the effect of role by performing whole-brain regression analyses on the full group of participants (  n   = 18). We chose to perform statistical regression analyses instead of a direct statistical comparison between leaders and followers for two reasons. First, we consider role expertise to be a continuous trait, rather than a dichotomous one. Dancers can belong to both groups if they are trained at both leading and following. Thus a binary distinction would have led to a “male versus female” contrast, rather than a “leader versus follower” contrast. Second, the number of participants in each group was small (  n   = 10 and 11 for leaders and followers, respectively), whereas the regression involved the full group of 18 participants. Because of the small number of participants in the analysis and because of the small number of female leaders and male followers in the cohort, we consider this an exploratory study. Future studies will need to examine larger numbers of participants who have both leading and following skills, although such dual training tends to be limited to professional teachers of a dance. 

For the whole-brain regression analyses, the self-reported values of leading and following skill were used as covariates in two separate analyses to regress the betas values of the contrast “Leading versus Following”. These regressions were also performed at a two-tailed statistical threshold of   p   &lt; 0.005 uncorrected with a cluster-level correction of   k   = 25 voxels, determined with Alphasim. However, this threshold led to null results, and so we reported the activation at a less stringent threshold of   p   &lt; 0.025 uncorrected with a cluster-level correction of k = 46 voxels, determined with Alphasim. We note that these results should be interpreted with caution and need to be replicated in future analyses. In order to examine the influence of gender, the mean beta value of each activated cluster was extracted for each participant and regressed against his/her corresponding leading or following skill. 



## Results 
  
In order to identify the basic sensorimotor network involved in performing our joint bimanual tasks, we carried out the conjunction of [Leading &gt; Rest] ∩ [Following &gt; Rest], with results shown in   Figure    and Talairach coordinates reported in   Table   . This shared network between leading and following consisted of a widespread sensorimotor cortical (primary motor and somatosensory cortex) and subcortical (thalamus and cerebellum) network, as well as the supplementary motor area (SMA), midcingulate cortex (MCC), SPL, inferior frontal gyrus (IFG), IPL (including the secondary somatosensory cortex [SII] and extending to the insula), and inferior temporal gyrus (ITG), extending to the middle temporal gyrus (MTG). Except for the ITG, which was present in leaders only, this network was found in both leaders and followers. 
  
Shared network for leading and following. The figure shows the results of the conjunction [Leading &gt; Rest] ∩ [Following &gt; Rest] in leaders only (left panel) and followers only (right panel),   p   &lt; 0.005 uncorrected (  k   = 49 voxels). With the exception of the inferior temporal gyrus, the activated network is similar in both followers and leaders. CB, cerebellum; IFG, inferior frontal gyrus; IPL, inferior parietal lobule; ITG, inferior temporal gyrus; MCC, middle cingulate cortex; SMA, supplementary motor area; SMC, sensorimotor cortex; SPL, superior parietal lobule; and Th., thalamus. 
    
The shared network for leading and following. 
    
We next wanted to explore our question of interest, namely whether there was evidence for role-specific activations, in other words activations found only in skilled individuals while performing the role they are trained in. This would reveal whether leaders and followers engage different brain resources during leading and following. As shown in   Figure    and   Table   , we first qualitatively compared three types of analyses of the “Leading &gt; Following” contrast (cyan clusters) and “Following &gt; Leading” contrast (yellow clusters): the whole group of 18 participants; only the leaders (a subset of 11 participants); and only the followers (a subset of 10 participants). Overall, the leaders-only analysis showed basically the same network for leading as the whole group, but no brain areas for following. Likewise, the followers-only analysis showed basically the same network for following as the whole group, but only the dorsolateral prefrontal cortex (DLPFC) for leading (Note that only role-specific activations are labeled in the   Figure   ). 
  
Role-specific brain activations. The figure shows an analysis of the bidirectional “Leading versus Following” contrast in three groupings: the whole group of 18 participants; only the leaders (a subset of 11 participants); and only the followers (a subset of 10 participants). Contrasts are performed at   p   &lt; 0.005 uncorrected (  k   = 28 voxels). The top panel is the midsagittal view, the lower left panel is the left hemisphere, and the lower right panel is the right hemisphere. Each panel is set up as a triad, with the whole group at the top and the restricted analyses of leaders-only and followers-only below that. Cyan clusters and outlines reflect the contrast of “Leading &gt; Following”, whereas yellow clusters and outlines reflect the reverse contrast of “Following &gt; Leading”. In order to facilitate the visualization of role-specific activations, we use colored outlines to represent whole-group activations that are missing in either the leaders-only or the followers-only analyses. More specifically, cyan outlines are regions of whole-group activation that are present in the leaders-only analysis, but not the followers-only analysis, while yellow outlines are regions of whole-group activation that present in the followers-only analysis, but not the leaders-only analysis. The leaders-only analysis shows the same network for leading as the whole group, but no brain areas for following. The followers-only analysis shows the same network for following as the whole group, but only the cerebellum and dorsolateral prefrontal cortex for leading. Only role-specific activations are labeled in this figure. Leading network: CMA: cingulate motor area; PMC, premotor cortex; SMA, supplementary motor area; and SPL, superior parietal lobule. Following network: PCC, posterior cingulate cortex and TPJ, temporo-parietal junction. 
    
Leading versus following in the whole group, the leader-only group, and the follower-only group. 
    
Regarding the leading task, role-specific activations that were found exclusively in skilled leaders (cyan activations in   Figure    in both the leaders-only and whole-group brains that correspond with the cyan outlines in the followers-only brain) were observed in the SMA and cingulate motor area (CMA; top panel), SPL (right and left hemispheres in the lower panels), and PMC(left hemisphere). In addition, while leading, leaders showed a more extended premotor activation than the whole-group, especially in the right hemisphere (  Table   ). 

Regarding the following task, role-specific activations that were found exclusively in skilled followers (yellow activations in   Figure    in both the followers-only and whole-group brains that correspond with the yellow outlines in the leaders-only brain) were observed in the posterior cingulate cortex (PCC; top panel), temporo-parietal junction (TPJ; right and left hemispheres in the lower panels), and parahippocampal cortex (PHC, not shown). In addition, while following, followers showed activity in the posterior superior temporal sulcus (pSTS) that was not present in the whole group (  Table   ). To summarize, the networks associated with leading and following seemed to be more strongly engaged by experts at the corresponding role than non-experts at that role. 

We followed up on these qualitative analyses with whole-brain regressions in which the self-reported expertise at being a leader or follower (see   Figure    above) was used as the covariate for the contrast of leading versus following. Activations for these analyses were only found at a more lenient threshold, but are still reported since they are consistent with both our hypotheses and the qualitative analyses reported above. However, the results should be interpreted with caution.   Figure    shows the regressions with leader skill, and   Figure    shows the regressions with follower skill. The regions where activations during the leading task correlated with leader skill included the SMA, pre-SMA, dorsal PMC (dPMC), superior temporal gyrus (STG), and insula (  Figure    top panel,   Table   ). The regions where activations during the following task correlated with follower skill include the PCC, TPJ, pSTS, and mPFC (  Figure    top panel,   Table   ). For each cluster, the coefficient of determination (  R  ) of the regression of the mean beta value against leader and follower skill is shown in   Tables  ,   , respectively. 
  
Regression of brain activation with leader skill in the whole group of participants. The top panel of this figure shows brain activity that correlates with the contrasts “Leading &gt; Following” (cyan activations) and “Following &gt; Leading” (yellow activations). Contrasts are performed at   p   &lt; 0.02 uncorrected (  k   = 25 voxels). Brain areas for “Leading &gt; Following” that correlate with leader skill include the dorsal premotor cortex (dPMC), insula (Ins.), superior temporal gyrus (STG), and supplementary motor area (SMA). Almost no areas for the contrast “Following &gt; Leading” correlate with leader skill (see   Table   ). The lower plots show mean beta values extracted from the SMA, dPMC, posterior insula and STG against leader skill, where female participants are shown with red dots and male participants with blue dots. Activity for leading increased with increasing leader skill, and this seems to be independent of gender. 
    
Regression of brain activation with the follower skill in the whole group of participants. The top panel of this figure shows brain activity that correlates with the contrasts “Leading &gt; Following” (cyan activations) and “Following &gt; Leading” (yellow activations). Contrasts are performed at   p   &lt; 0.02 uncorrected (  k   = 25 voxels). Brain areas for “Following &gt; Leading” that correlate with follower skill include the medial prefrontal cortex (mPFC), posterior parietal cortex (PCC), posterior superior temporal sulcus (pSTS), and temporo-parietal junction (TPJ). No areas appeared for the contrast “Leading &gt; Following” correlate with follower skill (see   Table   ). The lower plots show mean beta values extracted from the TPJ, PCC, mPFC, and pSTS against follower skill, where female participants are shown with red dots and male participants with blue dots. Activity for following increased with increasing follower skill, and this seems to be independent of gender. 
    
Leading versus following correlated with skill as a leader. 
      
Leading versus following correlated with skill as a follower. 
    
Examples of how the mean beta value in these regions covaries with leader and follower skill are shown in the bottom panels of   Figures  ,   , respectively. The results provide some evidence that activity in these regions might depend on the level of expertise. However, they in no way rule out a gender effect, either alone or in interaction with expertise, and so the results have to be seen as preliminary. In the dPMC and STG (  Figure   , bottom panels), activity for the contrast of “Leading &gt; Following” increased with leader skill, but a male with low leader skill had a low activity, whereas females with high leader skill had a high activity. Other areas that correlated with leader skill had the same trend (not shown). Similarly, in the mPFC and TPJ (  Figure   , bottom panels), activity for the contrast “Leading &gt; Following” decreased with follower skill (that is, “Following &gt; Leading” activity increased with follower skill), but a male with high follower skill had a low activity, similar to females with high follower skill. Other areas that correlated with follower skill had the same trend (not shown). Future studies will be needed to fully exclude the influence of gender on the expertise effects observed here. Hence, the current study must be seen as a pilot study that gives a first glimpse at role-specific expertise effects without being able to effectively factor out the influence of gender. 


## Discussion 
  
This current exploratory study examined for the first time the effect of expertise at the coordinative skills involved in leading and following on brain activations during a joint-action task in a realistic setting. Its results provide support for the existence of role-specific brain activations during joint actions. In particular, we observed that leading-related activations were enhanced in leaders compared to followers when both groups performed the leading task, and that following-related activations were enhanced in followers compared to leaders when both groups performed the following task. Additionally, we showed that leading-related brain regions in the whole group of participants tended to correlate with expertise at being a leader, whereas following-related brain regions tended to correlate with expertise at being a follower. Another way of conceptualizing these results is that the skilled leaders hardly engaged any areas during following that were not already engaged during leading; likewise, the skilled followers hardly engaged any areas during leading that were not already engaged during following. This might explain the null results found in some previous studies when comparing following with leading ( ). These results suggest that expertise at one role of a joint-action task can enhance brain activations for the trained role compared to the untrained role. Hence, not only do the results support the existing literature on expertise effects for motor tasks, but they extend it for the first time to the contrastive roles of leader and follower in joint actions. 

The major finding of the initial qualitative analysis (  Figure   ) was that the brain networks that we observed for leading and following in the whole group seemed to be mainly supported by prior experience at being a leader or follower. In particular, skilled followers strongly engaged the mentalizing and social networks (PCC, TPJ, and STS) while following, which is consistent with a view of following as a process of adapting to one’s partner or as inferring knowledge from one’s partner ( ;  ;  ;  ;  ;  ). In contrast, skilled leaders strongly engaged networks for motor control and planning (SMA, CMA, PMC, and cerebellum) and for spatial navigation and exploration (SPL) while leading, which is consistent with the requirements of the leading role ( ;  ;  ;  ;  ). Interestingly, both skilled leaders and skilled followers activated the DLPFC during leading, which implies that self-initiation and action selection ( ;  ;  ) are probably the most important characteristics of leading, regardless of expertise. 

By performing whole-brain regressions with leading or following skill, we treated being a leader or follower as a continuous trait, rather than a dichotomous one. Although we did not find any activity using our   a priori   threshold, the activations observed at a more lenient threshold were consistent with both our hypotheses and the qualitative results, and are thus reported as exploratory findings. We observed that distinct brain areas tended to correlate with the level of self-reported expertise at being a leader or a follower, respectively. The areas that correlated with follower skill were principally components of the following network, such as the mPFC, PCC, TPJ, and pSTS. Thus, the more that someone is trained at following, the more that s/he will recruit brain regions of the mentalizing and social networks, which might indicate more attention to, or more efficient processing of, social stimuli (i.e., cues coming from the leader) and the mental states of others (i.e., their intentions and action plans). Another characteristic of followers is their ability to track their partner’s movements or other signaling cues so as to produce either imitative or complementary movements. Along these lines, the pSTS has been specifically implicated in the multisensory perception of biological motion ( ;  ), indicating that a trained follower might be specialized in analyzing information coming from the partner’s movement, not least haptic information emanating from body contact ( ). 

In contrast to this profile for following, the areas that tended to correlate with leader skill were mainly part of the leading network, including premotor areas (pre-SMA, SMA, and PMC). Other areas that tended to correlate with leader skill were the insula and STG. This network is quite similar to the one shown to be activated by motor experts in the meta-analysis of  . In addition, all of the areas associated with leader skill in the present study have been previously shown to be involved in improvisation ( ). Since leading requires the ability to improvise movements, we can assume that the better a person is at leading, the better s/he can improvise a motor plan for both the self and the partner, and thus the more s/he recruits premotor areas and the STG. However, it has also been shown that improvisational expertise (in musicians, for example) is related to a deactivation in the DLPFC, TPJ, IFG, and insula ( ;  ), which has been interpreted as indicating an automation of cognitive processing and a greater focus on internal processes during improvisation ( ). The absence of deactivations in these regions in our study can potentially be explained by the fact that our use of a joint task may have precluded the adoption of an internal focus by the participants when leading. Indeed, a study of joint improvisation also found an activation increase in the DLPFC, pre-SMA, and STG ( ), which is quite similar to a situation of improvising with a dance partner when leading. 

Overall, the study integrates two issues in the cognitive neuroscience of motor performance, first the contrast between leading and following, and second the influence of individual differences in motor expertise on brain activations. As mentioned in the Introduction, many experimental studies of joint action randomly assign people to being a leader or follower of a joint task ( ;  ;  ;  ;  ). However, in Western dance culture, people are generally assigned these roles based on their gender, with men tending to be assigned the role of leader in couple dances. Thus, in contrast to a study of piano duetting ( ), for example, people come to a dance study like ours with years of experience at just one role of the joint task. This provides us with the unique ability to examine individual differences in joint action based not on random factors but on role-specific training. Previous studies of expertise processing have demonstrated enhanced brain activations in experts compared to non-experts ( ,  ;  ,  ;  ;  ;  ;  ;  ;  ;  ). However, this has often has been investigated using non-motor tasks, even in motor experts like professional ballet dancers ( ). We have instead probed this using a motor task, with the added benefit of doing this using a joint-action task. The integration of these two issues is that we were able to examine the contrast between leading and following–as per studies of joint action–but to incorporate the factor of prior motor experience, as per studies of expertise processing. The results revealed a clear overlap between these two issues, such that the brain activations during the acts of leading and following were enhanced by prior expertise at being a leader or follower, and that activity in task-specific brain areas tended to be positively correlated with the level of expertise at the corresponding role. In other words, we were able to demonstrate   role-specific enhancements   in brain activation. 

### Limitations 
  
Given that this study was a first attempt to examine the effect of role expertise on brain activations during joint action, we are aware that it has a number of significant limitations. First, we were limited in our ability to measure behavioral performance during task production in the scanner due to an absence of MRI-compatible technologies such as motion capture at our imaging center. Thus, we cannot determine if the differences between leaders and followers seen in the study are due to trait-related differences in activation or behavioral differences as well. The joint-action task performed in this study was quite simple and involved very small hand movements. Hence, it did not require any type of specialized skill, which would foster similar performance in the two groups. In addition, the experimenter was the sole interaction partner for all of the participants in the study and was thus a controlled factor in the interaction. However, the absence of a technology like motion capture means that we are unable to rule out behavioral differences between participants as a source of the results. Further research taking advantage of MRI-compatible technologies will be required to explore this issue. 

Second, the qualitative analyses showed an interesting pattern that was confirmed by the whole-group regression at a more lenient threshold, but not at a standard threshold. Hence, the effects seem to be small. Although the observed activations at the less stringent threshold were consistent with our expectations based on previous studies, the results of this study should be taken with caution and need to be replicated, preferentially with a larger cohort and a wider spread of skill levels. In addition, the skill levels that were used to regress the brain data were self-report data. They might thus have been subject to self-report biases and inaccuracies. However, no objective measure of leadership and/or followership skills exists in the literature. Given the preliminary results of this study, it would be worthwhile to develop such measures in future. Such measures could be used to see if the results of the present study could be replicated based on people’s role expertise in some other motor skill outside of dancing, or even on people’s natural predispositions to be a leader or follower, as related to personality traits and life experiences, rather than the specialized skill of dance training. 

Finally, and importantly, we are unable to rule out gender as a factor in determining the role-specific effects in our study, and hence the results need to be seen as quite preliminary. While the leader and follower groups were not exclusively of one gender, they did have a majority of one gender. Given the evidence for gender effects on a diversity of perceptual, cognitive, and motor tasks ( ;  ;  ;  ;  ), further studies will be required to assess a gender contribution to our results with trained couple dancers. Given the paucity of female leaders and male followers in the world of couple dancing, perhaps the only approach that will be able to address the limitations of the current study is a training study. A study that crosses gender with role during a several-month training program of leading or following for some joint-action task could permit a disentangling of the relative effects of gender and expertise. If female leaders and male followers showed the same role-specific effects as in the current study, this would argue against a gender interpretation in favor of expertise   per se  . Such a study could also reveal potential gender effects as well. 



## Conclusion 
  
This study is the first to look at the influence of prior individual training at being a leader or follower on the brain activations occurring during the acts of leading and following, thereby assessing the effect of role expertise during naturalistic joint action. Our major finding was that leaders and followers do not seem approach leading and following in the same way at the neural level, with leaders engaging more brain resources during leading, and followers during following, thus reflecting role-specific activations. Additionally, we showed that activity in leading-related brain regions tended to correlate with expertise at being a leader, and likewise that activity in following-related brain regions tended to correlate with expertise at being a follower. These findings highlight the fact that the acts of leading and following might be skill-specific, and thus that prior experience at these roles should be assessed when studying leading and following during joint action. However, given our inability to disentangle gender from dance role, the current results must be seen as preliminary. A training study that crosses gender with role will probably be required to truly distinguish dance role from gender. 


## Author Contributions 
  
LC ran the experiment and analyzed the data. LC and SB conceived the experiment, analyzed the results, and wrote the manuscript. 


## Conflict of Interest Statement 
  
The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (4)</summary><details class="inner-accordion"><summary>Table 2 (T2) - The shared network for leading and following.</summary><div class="table-html"><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><p>The shared network for leading and following.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="center" colspan="5" rowspan="1">Leaders only (<italic toggle="yes">n</italic> = 11)</th><th valign="top" align="center" colspan="5" rowspan="1">Followers only (<italic toggle="yes">n</italic> = 10)</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">Area</th><th valign="top" align="left" rowspan="1" colspan="1">BA</th><th valign="top" align="left" rowspan="1" colspan="1">Hemisphere</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">SMC</td><td valign="top" align="left" rowspan="1" colspan="1">1,3,4,5,6,40</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">33</td><td valign="top" align="center" rowspan="1" colspan="1">-40</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">1356</td><td valign="top" align="center" rowspan="1" colspan="1">17.01</td><td valign="top" align="center" rowspan="1" colspan="1">30</td><td valign="top" align="center" rowspan="1" colspan="1">-37</td><td valign="top" align="center" rowspan="1" colspan="1">61</td><td valign="top" align="center" rowspan="1" colspan="1">1149</td><td valign="top" align="center" rowspan="1" colspan="1">16.54</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SMC</td><td valign="top" align="left" rowspan="1" colspan="1">2,3,4,5,6,40</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-39</td><td valign="top" align="center" rowspan="1" colspan="1">-43</td><td valign="top" align="center" rowspan="1" colspan="1">52</td><td valign="top" align="center" rowspan="1" colspan="1">1419</td><td valign="top" align="center" rowspan="1" colspan="1">17.67</td><td valign="top" align="center" rowspan="1" colspan="1">-39</td><td valign="top" align="center" rowspan="1" colspan="1">-37</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">1135</td><td valign="top" align="center" rowspan="1" colspan="1">18.19</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">IFG</td><td valign="top" align="left" rowspan="1" colspan="1">6,13,44</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">54</td><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">19</td><td valign="top" align="center" rowspan="1" colspan="1">174</td><td valign="top" align="center" rowspan="1" colspan="1">5.86</td><td valign="top" align="center" rowspan="1" colspan="1">54</td><td valign="top" align="center" rowspan="1" colspan="1">8</td><td valign="top" align="center" rowspan="1" colspan="1">7</td><td valign="top" align="center" rowspan="1" colspan="1">108</td><td valign="top" align="center" rowspan="1" colspan="1">8.05</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SMA/MCC</td><td valign="top" align="left" rowspan="1" colspan="1">6, 24, 31</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">-13</td><td valign="top" align="center" rowspan="1" colspan="1">52</td><td valign="top" align="center" rowspan="1" colspan="1">610</td><td valign="top" align="center" rowspan="1" colspan="1">14.68</td><td valign="top" align="center" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">-22</td><td valign="top" align="center" rowspan="1" colspan="1">49</td><td valign="top" align="center" rowspan="1" colspan="1">683</td><td valign="top" align="center" rowspan="1" colspan="1">12.14</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">IPL/SII</td><td valign="top" align="left" rowspan="1" colspan="1">13,40,41</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">-28</td><td valign="top" align="center" rowspan="1" colspan="1">25</td><td valign="top" align="center" rowspan="1" colspan="1">294</td><td valign="top" align="center" rowspan="1" colspan="1">7.26</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="center" rowspan="1" colspan="1">-34</td><td valign="top" align="center" rowspan="1" colspan="1">31</td><td valign="top" align="center" rowspan="1" colspan="1">108</td><td valign="top" align="center" rowspan="1" colspan="1">6.82</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">IPL/SII</td><td valign="top" align="left" rowspan="1" colspan="1">13, 22,40</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-51</td><td valign="top" align="center" rowspan="1" colspan="1">-28</td><td valign="top" align="center" rowspan="1" colspan="1">19</td><td valign="top" align="center" rowspan="1" colspan="1">240</td><td valign="top" align="center" rowspan="1" colspan="1">9.52</td><td valign="top" align="center" rowspan="1" colspan="1">-48</td><td valign="top" align="center" rowspan="1" colspan="1">-34</td><td valign="top" align="center" rowspan="1" colspan="1">19</td><td valign="top" align="center" rowspan="1" colspan="1">314</td><td valign="top" align="center" rowspan="1" colspan="1">9.36</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SPL</td><td valign="top" align="left" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">24</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">197</td><td valign="top" align="center" rowspan="1" colspan="1">7.06</td><td valign="top" align="center" rowspan="1" colspan="1">24</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">166</td><td valign="top" align="center" rowspan="1" colspan="1">6.52</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SPL</td><td valign="top" align="left" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-24</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">453</td><td valign="top" align="center" rowspan="1" colspan="1">10.52</td><td valign="top" align="center" rowspan="1" colspan="1">-27</td><td valign="top" align="center" rowspan="1" colspan="1">-55</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">268</td><td valign="top" align="center" rowspan="1" colspan="1">9.76</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ITG</td><td valign="top" align="left" rowspan="1" colspan="1">37</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">51</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-8</td><td valign="top" align="center" rowspan="1" colspan="1">117</td><td valign="top" align="center" rowspan="1" colspan="1">6.45</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ITG</td><td valign="top" align="left" rowspan="1" colspan="1">19, 37</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-48</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">93</td><td valign="top" align="center" rowspan="1" colspan="1">6.30</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Thalamus</td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">15</td><td valign="top" align="center" rowspan="1" colspan="1">-25</td><td valign="top" align="center" rowspan="1" colspan="1">10</td><td valign="top" align="center" rowspan="1" colspan="1">105</td><td valign="top" align="center" rowspan="1" colspan="1">7.03</td><td valign="top" align="center" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">16</td><td valign="top" align="center" rowspan="1" colspan="1">49</td><td valign="top" align="center" rowspan="1" colspan="1">6.73</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Thalamus</td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-15</td><td valign="top" align="center" rowspan="1" colspan="1">-19</td><td valign="top" align="center" rowspan="1" colspan="1">10</td><td valign="top" align="center" rowspan="1" colspan="1">139</td><td valign="top" align="center" rowspan="1" colspan="1">7.41</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cerebellum</td><td valign="top" align="left" rowspan="1" colspan="1">Vermis</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">-3</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">308</td><td valign="top" align="center" rowspan="1" colspan="1">10.74</td><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-14</td><td valign="top" align="center" rowspan="1" colspan="1">163</td><td valign="top" align="center" rowspan="1" colspan="1">7.72</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cerebellum</td><td valign="top" align="left" rowspan="1" colspan="1">Culmen/Declive</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">15</td><td valign="top" align="center" rowspan="1" colspan="1">-49</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">164</td><td valign="top" align="center" rowspan="1" colspan="1">9.53</td><td valign="top" align="center" rowspan="1" colspan="1">12</td><td valign="top" align="center" rowspan="1" colspan="1">-46</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">104</td><td valign="top" align="center" rowspan="1" colspan="1">6.19</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cerebellum</td><td valign="top" align="left" rowspan="1" colspan="1">Culmen/Declive</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-21</td><td valign="top" align="center" rowspan="1" colspan="1">-49</td><td valign="top" align="center" rowspan="1" colspan="1">-20</td><td valign="top" align="center" rowspan="1" colspan="1">140</td><td valign="top" align="center" rowspan="1" colspan="1">10.20</td><td valign="top" align="center" rowspan="1" colspan="1">-15</td><td valign="top" align="center" rowspan="1" colspan="1">-52</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">144</td><td valign="top" align="center" rowspan="1" colspan="1">8.87</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cerebellum</td><td valign="top" align="left" rowspan="1" colspan="1">Tuber/Declive</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">42</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-23</td><td valign="top" align="center" rowspan="1" colspan="1">94</td><td valign="top" align="center" rowspan="1" colspan="1">7.03</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-20</td><td valign="top" align="center" rowspan="1" colspan="1">156</td><td valign="top" align="center" rowspan="1" colspan="1">7.86</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /></tr></tbody></table><table-wrap-foot><attrib><italic toggle="yes">Talairach coordinates for the conjunction of Leading and Following compared to rest (<italic toggle="yes">p</italic> &gt; 0.005 uncorrected, <italic toggle="yes">k</italic> = 49 voxels). IFG, inferior frontal gyrus; IPL, inferior parietal lobule; ITG, inferior temporal gyrus; SII, secondary somatosensory cortex; SMA, supplementary motor area; SMC, sensorimotor cortex; and SPL, superior parietal lobule.</italic></attrib></table-wrap-foot></table-wrap></div></details><details class="inner-accordion"><summary>Table 3 (T3) - Leading versus following in the whole group, the leader-only group, and the follower-only group.</summary><div class="table-html"><table-wrap id="T3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Leading versus following in the whole group, the leader-only group, and the follower-only group.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="center" colspan="5" rowspan="1">Whole-group (<italic toggle="yes">n</italic> = 18)</th><th valign="top" align="center" colspan="5" rowspan="1">Leaders only (<italic toggle="yes">n</italic> = 11)</th><th valign="top" align="center" colspan="5" rowspan="1">Followers only (<italic toggle="yes">n</italic> = 10)</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">Area</th><th valign="top" align="center" rowspan="1" colspan="1">BA</th><th valign="top" align="left" rowspan="1" colspan="1">Hemisphere</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th></tr></thead><tbody><tr><td valign="top" align="left" colspan="2" rowspan="1"><bold>Activations: Leading &gt; Following</bold></td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   pre-SMA</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">5</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">29</td><td valign="top" align="center" rowspan="1" colspan="1">5.24</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   SMA</td><td valign="top" align="center" rowspan="1" colspan="1">4,6</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">-4</td><td valign="top" align="center" rowspan="1" colspan="1">-4</td><td valign="top" align="center" rowspan="1" colspan="1">59</td><td valign="top" align="center" rowspan="1" colspan="1">81</td><td valign="top" align="center" rowspan="1" colspan="1">5.97</td><td valign="top" align="center" rowspan="1" colspan="1">-3</td><td valign="top" align="center" rowspan="1" colspan="1">-13</td><td valign="top" align="center" rowspan="1" colspan="1">64</td><td valign="top" align="center" rowspan="1" colspan="1">109</td><td valign="top" align="center" rowspan="1" colspan="1">6.28</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   CMA</td><td valign="top" align="center" rowspan="1" colspan="1">24</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">-6</td><td valign="top" align="center" rowspan="1" colspan="1">8</td><td valign="top" align="center" rowspan="1" colspan="1">37</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">4.84</td><td valign="top" align="center" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">40</td><td valign="top" align="center" rowspan="1" colspan="1">85</td><td valign="top" align="center" rowspan="1" colspan="1">8.41</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PMC</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">21</td><td valign="top" align="center" rowspan="1" colspan="1">-4</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">145</td><td valign="top" align="center" rowspan="1" colspan="1">8.01</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PMC</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-26</td><td valign="top" align="center" rowspan="1" colspan="1">-13</td><td valign="top" align="center" rowspan="1" colspan="1">54</td><td valign="top" align="center" rowspan="1" colspan="1">51</td><td valign="top" align="center" rowspan="1" colspan="1">4.36</td><td valign="top" align="center" rowspan="1" colspan="1">-30</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">64</td><td valign="top" align="center" rowspan="1" colspan="1">32</td><td valign="top" align="center" rowspan="1" colspan="1">5.71</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   DLPFC</td><td valign="top" align="center" rowspan="1" colspan="1">8,9</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-40</td><td valign="top" align="center" rowspan="1" colspan="1">29</td><td valign="top" align="center" rowspan="1" colspan="1">38</td><td valign="top" align="center" rowspan="1" colspan="1">98</td><td valign="top" align="center" rowspan="1" colspan="1">7.35</td><td valign="top" align="center" rowspan="1" colspan="1">-48</td><td valign="top" align="center" rowspan="1" colspan="1">32</td><td valign="top" align="center" rowspan="1" colspan="1">31</td><td valign="top" align="center" rowspan="1" colspan="1">46</td><td valign="top" align="center" rowspan="1" colspan="1">5.84</td><td valign="top" align="center" rowspan="1" colspan="1">-39</td><td valign="top" align="center" rowspan="1" colspan="1">44</td><td valign="top" align="center" rowspan="1" colspan="1">34</td><td valign="top" align="center" rowspan="1" colspan="1">76</td><td valign="top" align="center" rowspan="1" colspan="1">7.08</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   DLPFC</td><td valign="top" align="center" rowspan="1" colspan="1">9</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">-36</td><td valign="top" align="center" rowspan="1" colspan="1">23</td><td valign="top" align="center" rowspan="1" colspan="1">25</td><td valign="top" align="center" rowspan="1" colspan="1">30</td><td valign="top" align="center" rowspan="1" colspan="1">6.91</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   SPL</td><td valign="top" align="center" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">-73</td><td valign="top" align="center" rowspan="1" colspan="1">42</td><td valign="top" align="center" rowspan="1" colspan="1">70</td><td valign="top" align="center" rowspan="1" colspan="1">6.39</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">-73</td><td valign="top" align="center" rowspan="1" colspan="1">49</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">7.56</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   SPL</td><td valign="top" align="center" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">-73</td><td valign="top" align="center" rowspan="1" colspan="1">36</td><td valign="top" align="center" rowspan="1" colspan="1">84</td><td valign="top" align="center" rowspan="1" colspan="1">5.44</td><td valign="top" align="center" rowspan="1" colspan="1">-18</td><td valign="top" align="center" rowspan="1" colspan="1">-79</td><td valign="top" align="center" rowspan="1" colspan="1">43</td><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">6.34</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   Cerebellum</td><td valign="top" align="center" rowspan="1" colspan="1">Tuber</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">-65</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">37</td><td valign="top" align="center" rowspan="1" colspan="1">5.00</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" colspan="2" rowspan="1"><bold>Deactivations: Following &gt; Leading</bold></td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PCC</td><td valign="top" align="center" rowspan="1" colspan="1">7,31</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">-54</td><td valign="top" align="center" rowspan="1" colspan="1">23</td><td valign="top" align="center" rowspan="1" colspan="1">129</td><td valign="top" align="center" rowspan="1" colspan="1">-5.36</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">31</td><td valign="top" align="center" rowspan="1" colspan="1">92</td><td valign="top" align="center" rowspan="1" colspan="1">-11.12</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   TPJ</td><td valign="top" align="center" rowspan="1" colspan="1">39,40</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="center" rowspan="1" colspan="1">-59</td><td valign="top" align="center" rowspan="1" colspan="1">25</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="center" rowspan="1" colspan="1">-5.33</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">31</td><td valign="top" align="center" rowspan="1" colspan="1">89</td><td valign="top" align="center" rowspan="1" colspan="1">-8.86</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   TPJ</td><td valign="top" align="center" rowspan="1" colspan="1">39,40</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-53</td><td valign="top" align="center" rowspan="1" colspan="1">-63</td><td valign="top" align="center" rowspan="1" colspan="1">23</td><td valign="top" align="center" rowspan="1" colspan="1">105</td><td valign="top" align="center" rowspan="1" colspan="1">-5.70</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   STS</td><td valign="top" align="center" rowspan="1" colspan="1">19,39</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">16</td><td valign="top" align="center" rowspan="1" colspan="1">92</td><td valign="top" align="center" rowspan="1" colspan="1">-10.18</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   STS</td><td valign="top" align="center" rowspan="1" colspan="1">37,39</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">-51</td><td valign="top" align="center" rowspan="1" colspan="1">-52</td><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">66</td><td valign="top" align="center" rowspan="1" colspan="1">-5.94</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   aSTG</td><td valign="top" align="center" rowspan="1" colspan="1">13,22</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">57</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">-2</td><td valign="top" align="center" rowspan="1" colspan="1">72</td><td valign="top" align="center" rowspan="1" colspan="1">-9.70</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   Temporal pole</td><td valign="top" align="center" rowspan="1" colspan="1">38</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">-48</td><td valign="top" align="center" rowspan="1" colspan="1">5</td><td valign="top" align="center" rowspan="1" colspan="1">-23</td><td valign="top" align="center" rowspan="1" colspan="1">30</td><td valign="top" align="center" rowspan="1" colspan="1">-8.09</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PHC</td><td valign="top" align="center" rowspan="1" colspan="1">30,36</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">17</td><td valign="top" align="center" rowspan="1" colspan="1">-33</td><td valign="top" align="center" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">39</td><td valign="top" align="center" rowspan="1" colspan="1">-4.72</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">42</td><td valign="top" align="center" rowspan="1" colspan="1">-40</td><td valign="top" align="center" rowspan="1" colspan="1">1</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="center" rowspan="1" colspan="1">-6.13</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PHC</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-21</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">-6</td><td valign="top" align="center" rowspan="1" colspan="1">103</td><td valign="top" align="center" rowspan="1" colspan="1">-5.76</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">-30</td><td valign="top" align="center" rowspan="1" colspan="1">-22</td><td valign="top" align="center" rowspan="1" colspan="1">-11</td><td valign="top" align="center" rowspan="1" colspan="1">67</td><td valign="top" align="center" rowspan="1" colspan="1">-7.52</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   Thalamus</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">21</td><td valign="top" align="center" rowspan="1" colspan="1">-25</td><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">59</td><td valign="top" align="center" rowspan="1" colspan="1">-6.72</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /></tr></tbody></table><table-wrap-foot><attrib><italic toggle="yes">Talairach coordinates for the contrast “Leading versus Following” in the whole group, leaders only, and followers only (<italic toggle="yes">p</italic> &gt; 0.005 uncorrected, <italic toggle="yes">k</italic> = 28 voxels). aSTG, anterior superior temporal gyrus; CMA, cingulate motor area; DLPFC, dorsolateral prefrontal cortex; PCC, posterior cingulate cortex; PHC, parahippocampal cortex; PMC, premotor cortex; SMA, supplementary motor area; SPL, superior parietal lobule; STS, superior temporal sulcus; and TPJ, temporo-parietal junction.</italic></attrib></table-wrap-foot></table-wrap></div></details><details class="inner-accordion"><summary>Table 4 (T4) - Leading versus following correlated with skill as a leader.</summary><div class="table-html"><table-wrap id="T4" position="float" orientation="portrait"><label>Table 4</label><caption><p>Leading versus following correlated with skill as a leader.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1">Area</th><th valign="top" align="center" rowspan="1" colspan="1">BA</th><th valign="top" align="left" rowspan="1" colspan="1">Hemisphere</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sup>2</sup></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1"><bold>Activation</bold></td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">pre-SMA</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">8</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">0.73</td><td valign="top" align="center" rowspan="1" colspan="1">0.37</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">SMA</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">12</td><td valign="top" align="center" rowspan="1" colspan="1">-7</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">184</td><td valign="top" align="center" rowspan="1" colspan="1">0.81</td><td valign="top" align="center" rowspan="1" colspan="1">0.46</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">dPMC</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">36</td><td valign="top" align="center" rowspan="1" colspan="1">-1</td><td valign="top" align="center" rowspan="1" colspan="1">37</td><td valign="top" align="center" rowspan="1" colspan="1">46</td><td valign="top" align="center" rowspan="1" colspan="1">0.66</td><td valign="top" align="center" rowspan="1" colspan="1">0.49</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">Insula</td><td valign="top" align="center" rowspan="1" colspan="1">13</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">33</td><td valign="top" align="center" rowspan="1" colspan="1">-25</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="center" rowspan="1" colspan="1">101</td><td valign="top" align="center" rowspan="1" colspan="1">0.71</td><td valign="top" align="center" rowspan="1" colspan="1">0.50</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">STG</td><td valign="top" align="center" rowspan="1" colspan="1">41,22</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">54</td><td valign="top" align="center" rowspan="1" colspan="1">-28</td><td valign="top" align="center" rowspan="1" colspan="1">10</td><td valign="top" align="center" rowspan="1" colspan="1">62</td><td valign="top" align="center" rowspan="1" colspan="1">0.66</td><td valign="top" align="center" rowspan="1" colspan="1">0.48</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"><bold>Deactivation</bold></td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">Cingulate</td><td valign="top" align="center" rowspan="1" colspan="1">13,13</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-21</td><td valign="top" align="center" rowspan="1" colspan="1">-34</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="center" rowspan="1" colspan="1">87</td><td valign="top" align="center" rowspan="1" colspan="1">-0.74</td><td valign="top" align="center" rowspan="1" colspan="1">0.37</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">Lingual</td><td valign="top" align="center" rowspan="1" colspan="1">19</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-33</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-2</td><td valign="top" align="center" rowspan="1" colspan="1">46</td><td valign="top" align="center" rowspan="1" colspan="1">-0.72</td><td valign="top" align="center" rowspan="1" colspan="1">0.30</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /></tr></tbody></table><table-wrap-foot><attrib><italic toggle="yes">Talairach coordinates for the contrast “Leading versus Following” in the whole group correlated with the covariate “leader skill” (<italic toggle="yes">p</italic> &gt; 0.025 uncorrected, <italic toggle="yes">k</italic> = 46 voxels). <italic toggle="yes">R</italic><sup>2</sup> is the coefficient of determination of the regression of the cluster’s mean beta value against leader skill. dPMC, dorsal premotor cortex; SMA, supplementary motor area; and STG, superior temporal cortex.</italic></attrib></table-wrap-foot></table-wrap></div></details><details class="inner-accordion"><summary>Table 5 (T5) - Leading versus following correlated with skill as a follower.</summary><div class="table-html"><table-wrap id="T5" position="float" orientation="portrait"><label>Table 5</label><caption><p>Leading versus following correlated with skill as a follower.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1">Area</th><th valign="top" align="center" rowspan="1" colspan="1">BA</th><th valign="top" align="left" rowspan="1" colspan="1">Hemisphere</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sup>2</sup></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1"><bold>Activation</bold></td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"><bold>Deactivation</bold></td><td valign="top" align="left" rowspan="1" colspan="1">PCC</td><td valign="top" align="center" rowspan="1" colspan="1">7,31</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">-55</td><td valign="top" align="center" rowspan="1" colspan="1">37</td><td valign="top" align="center" rowspan="1" colspan="1">78</td><td valign="top" align="center" rowspan="1" colspan="1">-0.75</td><td valign="top" align="center" rowspan="1" colspan="1">0.30</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">mPFC</td><td valign="top" align="center" rowspan="1" colspan="1">9,10</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">22</td><td valign="top" align="center" rowspan="1" colspan="1">64</td><td valign="top" align="center" rowspan="1" colspan="1">-0.73</td><td valign="top" align="center" rowspan="1" colspan="1">0.27</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">pSTS</td><td valign="top" align="center" rowspan="1" colspan="1">19,39</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">42</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">13</td><td valign="top" align="center" rowspan="1" colspan="1">90</td><td valign="top" align="center" rowspan="1" colspan="1">-0.75</td><td valign="top" align="center" rowspan="1" colspan="1">0.61</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">TPJ</td><td valign="top" align="center" rowspan="1" colspan="1">39,40</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">-52</td><td valign="top" align="center" rowspan="1" colspan="1">43</td><td valign="top" align="center" rowspan="1" colspan="1">72</td><td valign="top" align="center" rowspan="1" colspan="1">-0.72</td><td valign="top" align="center" rowspan="1" colspan="1">0.48</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /></tr></tbody></table><table-wrap-foot><attrib><italic toggle="yes">Talairach coordinates for the contrast “Leading versus Following” in the whole group correlated with the covariate “follower skill” (<italic toggle="yes">p</italic> &gt; 0.025 uncorrected, <italic toggle="yes">k</italic> = 46 voxels). <italic toggle="yes">R</italic><sup>2</sup> is the coefficient of determination of the regression of the cluster’s mean beta value against follower skill. mPFC, medial prefrontal cortex; PCC, posterior cingulate cortex; pSTS, superior temporal sulcus; and TPJ, temporo-parietal junction.</italic></attrib></table-wrap-foot></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 14568477</strong> | Pred included: 4 | Manual included (accepted matches only): 4 | Correct overlaps: 4 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/14568477/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>14568477_analysis_0</td><td>Internal &gt; external</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>External attributions (attributing events to others or situation) probe representation and understanding of others&#x27; responsibility and intentionality, matching perception/understanding of others; satisfies both I1 and I2.</td></tr>
<tr><td>14568477_analysis_1</td><td>External &gt; internal</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>External &gt; Internal contrasts attribution to others/situations vs self and therefore directly probes perception and understanding of others&#x27; responsibility/intentionality.</td></tr>
<tr><td>14568477_analysis_2</td><td>Self-serving bias</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrasts involve attributing causation to others (external-personal) and thus involve perception/understanding of others’ responsibility/intentionality; the analysis measures this construct.</td></tr>
<tr><td>14568477_analysis_3</td><td>Non-self-serving bias</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The non-self-serving contrast includes external attribution of positive events (attributing events to others), which indexes perception and understanding of others&#x27; responsibility/intentionality.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>14568477_1</td><td>External &gt; internal; others</td><td>14568477_analysis_1</td><td>External &gt; internal</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>14568477_2</td><td>Internal &gt; external; others</td><td>14568477_analysis_0</td><td>Internal &gt; external</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>14568477_3</td><td>Non-self-serving bias &gt; Self-serving bias; others</td><td>14568477_analysis_3</td><td>Non-self-serving bias</td><td>0.677</td><td>1.000</td><td>0.903</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>14568477_4</td><td>Self-serving bias &gt; Non-self-serving bias; others</td><td>14568477_analysis_2</td><td>Self-serving bias</td><td>0.586</td><td>1.000</td><td>0.876</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 14980212</strong> | Pred included: 4 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/14980212/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>14980212_analysis_0</td><td>Affective-Neutral Faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast probes representations and evaluations of others’ social/moral status (cooperators, defectors, intentionality), directly measuring perception and understanding of others.</td></tr>
<tr><td>14980212_analysis_1</td><td>Cooperator-Neutral Faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Directly assesses representations and judgments about others&#x27; traits/behavior (cooperators vs neutral), satisfying perception/understanding of others.</td></tr>
<tr><td>14980212_analysis_2</td><td>Defector-Neutral Faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast interrogates representations and judgments about others&#x27; traits and intentions (cooperator/defector), meeting perception-of-others criteria.</td></tr>
<tr><td>14980212_analysis_3</td><td>Cerebral Foci of Activation for Intentional versus Nonintentional Cooperator Faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast probes representations about others’ social behavior and intentionality (understanding others’ moral status), directly addressing Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>14980212_1</td><td>Affective &gt; Neutral faces; socialcommunication</td><td>14980212_analysis_0</td><td>Affective-Neutral Faces</td><td>0.917</td><td>1.000</td><td>0.975</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>14980212_2</td><td>Cooperator &gt; Neutral faces; socialcommunication</td><td>14980212_analysis_1</td><td>Cooperator-Neutral Faces</td><td>0.920</td><td>1.000</td><td>0.976</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>14980212_3</td><td>Defector &gt; Neutral faces; socialcommunication</td><td>14980212_analysis_2</td><td>Defector-Neutral Faces</td><td>0.913</td><td>1.000</td><td>0.974</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 15006683</strong> | Pred included: 1 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/15006683/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>15006683_analysis_0</td><td>Relational vs. alone segment</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast requires perceiving and interpreting others’ actions, intentions, and relations (action perception and understanding mental states), meeting the criteria for perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>15006683_1</td><td>Relational &gt; alone; others</td><td>15006683_analysis_0</td><td>Relational vs. alone segment</td><td>0.739</td><td>1.000</td><td>0.922</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 15488424</strong> | Pred included: 4 | Manual included (accepted matches only): 4 | Correct overlaps: 4 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/15488424/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>15488424_analysis_0</td><td>Cooperation versus independent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The cooperation condition requires representing and predicting the other&#x27;s actions/intentions (mentalizing, understanding others), and the contrast isolates those processes relative to independent play.</td></tr>
<tr><td>15488424_analysis_1</td><td>Competition versus independent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Competition requires anticipating and attributing intentions to others (mentalizing). The contrast probes perception and understanding of others, meeting both inclusion criteria.</td></tr>
<tr><td>15488424_analysis_2</td><td>Cooperation versus competition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Task requires reasoning about others&#x27; intentions and actions (mentalizing, predicting opponent/cooperator behavior), satisfying Perception and Understanding of Others.</td></tr>
<tr><td>15488424_analysis_3</td><td>Competition versus cooperation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Competition in particular engages mentalizing and representation of others&#x27; intentions (medial prefrontal activation); the contrasts assess perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>15488424_1</td><td>Competition versus cooperation; affiliation</td><td>15488424_analysis_3</td><td>Competition versus cooperation</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>15488424_2</td><td>Competition versus independent; affiliation</td><td>15488424_analysis_1</td><td>Competition versus independent</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>15488424_3</td><td>Cooperation versus competition; affiliation</td><td>15488424_analysis_2</td><td>Cooperation versus competition</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>15488424_4</td><td>Cooperation versus independent; affiliation</td><td>15488424_analysis_0</td><td>Cooperation versus independent</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 15808992</strong> | Pred included: 2 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/15808992/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>15808992_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis contrasts self vs a familiar other and examines neural responses to the other condition (social representations), thus addressing perception and understanding of others.</td></tr>
<tr><td>15808992_analysis_1</td><td>other-self contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrasts involve processing of a familiar other and examine neural responses to others versus self, meeting criteria for perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>15808992_1</td><td>Other - Self; socialcommunication</td><td>15808992_analysis_1</td><td>other-self contrast</td><td>0.645</td><td>1.000</td><td>0.894</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16035037</strong> | Pred included: 4 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16035037/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16035037_analysis_0</td><td>CBF change during distracter minus null contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast directly measures perception of others&#x27; faces (unknown/distracter faces) versus baseline, satisfying criteria for Perception and Understanding of Others.</td></tr>
<tr><td>16035037_analysis_1</td><td>familiar minus distracter contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Familiar vs unknown face contrast indexes representations and recognition of others (their identity/familiarity), meeting perception/understanding of others criteria.</td></tr>
<tr><td>16035037_analysis_2</td><td>Local maxima of CBF change during self minus distracter contrast corrected at P = 0.001 and cluster at P = 0.05</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast focuses on self-related processing (self minus distracter) rather than representing or reasoning about others&#x27; states; it does not primarily measure perception/understanding of others.</td></tr>
<tr><td>16035037_analysis_3</td><td>familiar minus self contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast also involves processing a personally familiar other, meeting criteria for perception/understanding of others.</td></tr>
<tr><td>16035037_analysis_4</td><td>Local maxima of CBF change during self minus familiar contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast involves recognition of a familiar other (personally familiar face) and thus taps perception/understanding of others, meeting the inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>16035037_2</td><td>familiar face &gt; distracter; socialcommunication</td><td>16035037_analysis_1</td><td>familiar minus distracter contrast</td><td>0.667</td><td>1.000</td><td>0.900</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>16035037_3</td><td>familiar face &gt; self face; socialcommunication</td><td>16035037_analysis_3</td><td>familiar minus self contrast</td><td>0.604</td><td>1.000</td><td>0.881</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16122944</strong> | Pred included: 5 | Manual included (accepted matches only): 5 | Correct overlaps: 5 | Match statuses: accepted=5, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16122944/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16122944_analysis_0</td><td>‘ToM’ - ‘Physical 1’</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Theory of Mind contrast explicitly measures understanding and reasoning about others&#x27; mental states (mentalizing), directly satisfying both inclusion criteria for Perception and Understanding of Others.</td></tr>
<tr><td>16122944_analysis_1</td><td>‘Empathy’ - ‘Physical 2’</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The empathy contrast directly assesses representation and reasoning about others&#x27; emotional states (Understanding Mental States subconstruct), satisfying the criteria for Perception and Understanding of Others.</td></tr>
<tr><td>16122944_analysis_2</td><td>Conjuction analysis</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrasts explicitly probe perception and understanding of others (Theory of Mind, empathy), matching the Understanding Mental States subconstruct.</td></tr>
<tr><td>16122944_analysis_3</td><td>(‘ToM’ - ‘Physical 1’) - (‘Empathy’ - ‘Physical 2’)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The ToM and Empathy conditions explicitly require representing and reasoning about others’ mental and emotional states, matching the ‘Perception and Understanding of Others’ construct (including Understanding Mental States).</td></tr>
<tr><td>16122944_analysis_4</td><td>(‘Empathy’ - ‘Physical 2’) - (‘ToM’ - ‘Physical 1’)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis explicitly measures understanding of others&#x27; mental states (ToM) and emotional states (empathy), directly matching Perception and Understanding of Others. Satisfies I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>16122944_1</td><td>(Empathy - Physical 2) - (ToM - Physical 1); others</td><td>16122944_analysis_4</td><td>(‘Empathy’ - ‘Physical 2’) - (‘ToM’ - ‘Physical 1’)</td><td>0.915</td><td>1.000</td><td>0.974</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>16122944_2</td><td>(ToM - Physical 1) - (Empathy - Physical 2); others</td><td>16122944_analysis_3</td><td>(‘ToM’ - ‘Physical 1’) - (‘Empathy’ - ‘Physical 2’)</td><td>0.915</td><td>1.000</td><td>0.974</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>16122944_3</td><td>Conjunction Analysis; others</td><td>16122944_analysis_2</td><td>Conjuction analysis</td><td>0.974</td><td>1.000</td><td>0.992</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>16122944_4</td><td>Empathy - Physical 2; others</td><td>16122944_analysis_1</td><td>‘Empathy’ - ‘Physical 2’</td><td>0.909</td><td>1.000</td><td>0.973</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>16122944_5</td><td>ToM - Physical 1; others</td><td>16122944_analysis_0</td><td>‘ToM’ - ‘Physical 1’</td><td>0.889</td><td>1.000</td><td>0.967</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18501639</strong> | Pred included: 13 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18501639/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18501639_analysis_0</td><td>Main effect [(Sf + Sn)–(Ff + Fn) masked by Sf–Ff and Sn–Fn]</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The reported contrast is self &gt; friend (self-specific). It does not primarily measure perception/understanding of others; although other contrasts in the study examine friend&gt;self, this specific analysis targets self-processing and therefore does not meet the inclusion criteria for perception of others.</td></tr>
<tr><td>18501639_analysis_1</td><td>Face (simple effect) [Sf–Ff]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Includes explicit contrasts involving others (friend, unfamiliar person) and probes representation and differentiation of others (friend vs unfamiliar vs self), meeting the criteria for perception/understanding of others.</td></tr>
<tr><td>18501639_analysis_2</td><td>Name (simple effect) [Sn–Fn]</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>While the task includes perception of others, this specific contrast isolates self &gt; friend (self-specific effect) rather than directly measuring perception/understanding of others (e.g., friend vs stranger). It therefore does not meet the intended criteria for an others-focused contrast.</td></tr>
<tr><td>18501639_analysis_3</td><td>Face specific (interaction) [(Sf–Ff)–(Sn–Fn) masked by Sf–Ff]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrasts involve friend and unfamiliar person and examine neural responses to others (friend vs self vs stranger), meeting criteria for perception and understanding of others.</td></tr>
<tr><td>18501639_analysis_4</td><td>Name specific (interaction) [(Sn–Fn)–(Sf–Ff) masked by Sn–Fn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analyses contrast friend and unfamiliar persons and examine cortical responses to others (friend/unfamiliar), directly indexing perception and understanding of others.</td></tr>
<tr><td>18501639_analysis_5</td><td>Main effect [(Ff + Fn)–(Sf + Sn) masked by Ff–Sf and Fn–Sn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast involves friend (a familiar other) versus self and examines neural responses to others across modalities (face and name), measuring perception/understanding of others.</td></tr>
<tr><td>18501639_analysis_6</td><td>Face (simple effect) [Ff–Sf]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast compares friend (other) to self for face stimuli, directly assessing perception/understanding of another person.</td></tr>
<tr><td>18501639_analysis_7</td><td>Name (simple effect) [Fn–Sn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis includes contrasts assessing friend and unfamiliar person processing (friend vs self, friend vs unfamiliar), addressing perception and understanding of others and social-cognitive representations, fulfilling the inclusion criteria.</td></tr>
<tr><td>18501639_analysis_8</td><td>Face specific (interaction) [(Ff–Sf)–(Fn–Sn) masked by Ff–Sf]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrasts include friend and unfamiliar person conditions and test differential processing of others (friend vs self vs unfamiliar), addressing perception and understanding of others.</td></tr>
<tr><td>18501639_analysis_9</td><td>Name specific (interaction) [(Fn–Sn)–(Ff–Sf) masked by Fn–Sn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrasts comparing friend and unfamiliar persons probe representations and understanding of others (friend vs stranger and differences in temporoparietal activation), meeting criteria for perception of others.</td></tr>
<tr><td>18501639_analysis_10</td><td>Main effect [(Sf + Ff + Sn + Fn)–2(Cf + Cn) masked by Sf–Cf, Ff–Cf, Sn–Cn, and Fn–Cn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast includes friend and unfamiliar person conditions and identifies familiarity-dependent activation for others (friend vs unfamiliar), directly addressing perception/understanding of others; thus it meets I1 and I2.</td></tr>
<tr><td>18501639_analysis_11</td><td>Face (simple effect) [Sf + Ff–2Cf masked by Sf–Cf, and Ff–Cf]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrasts probe recognition of others (friend and unfamiliar person) and distinctions between them (friend vs self, friend vs control), addressing perception and understanding of others.</td></tr>
<tr><td>18501639_analysis_12</td><td>Name (simple effect) [Sn + Fn–2Cn masked by Sn–Cn, and Fn–Cn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrasts examine friend and unfamiliar person recognition and friend-vs-self effects, measuring perception and understanding of others.</td></tr>
<tr><td>18501639_analysis_13</td><td>Face specific (interaction) [(Sf + Ff–2Cf)–(Sn + Fn–2Cn) masked by Sf + Ff–2Cf]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrasts include friend and unfamiliar person conditions and assess recognition/familiarity of others, meeting criteria for perception and understanding of others (e.g., familiar vs stranger).</td></tr>
<tr><td>18501639_analysis_14</td><td>Name specific (interaction) [(Sn + Fn–2Cn)–(Sf + Ff–2Cf) masked by Sn + Fn–2Cn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast includes friend versus unfamiliar person components (familiar vs control) and thus indexes perception/understanding of others (familiar vs unfamiliar), meeting the criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>18501639_1</td><td>Face (simple effect) [Ff–Sf]; socialcommunication</td><td>18501639_analysis_6</td><td>Face (simple effect) [Ff–Sf]</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18501639_6</td><td>Main effect [(Ff+Fn)–(Sf+Sn) masked by Ff–Sf and Fn–Sn]; socialcommunication</td><td>18501639_analysis_5</td><td>Main effect [(Ff + Fn)–(Sf + Sn) masked by Ff–Sf and Fn–Sn]</td><td>0.965</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18514546</strong> | Pred included: 4 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18514546/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18514546_analysis_0</td><td>SELF versus high-level baseline</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The specified analysis is SELF versus high-level baseline, which targets self-related emotional responses rather than representations or judgments about others (OTHER-task would map to this construct, but not this specific contrast).</td></tr>
<tr><td>18514546_analysis_1</td><td>OTHER versus high-level baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The OTHER vs high-level baseline contrast explicitly measures understanding of others&#x27; emotional states from faces (Theory of Mind/emotion inference), satisfying perception of others.</td></tr>
<tr><td>18514546_analysis_2</td><td>analysis_2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>OTHER-task requires evaluating the emotional state expressed by faces, i.e., understanding mental states of others.</td></tr>
<tr><td>18514546_analysis_3</td><td>Interaction (SELF_f – B_f) – (SELF_m – B_m)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific analysis contrasts SELF-related activity between females and males (SELF_f vs SELF_m); it does not target perception/understanding of others (OTHER-task) so does not satisfy the &#x27;perception of others&#x27; criterion.</td></tr>
<tr><td>18514546_analysis_4</td><td>Interaction (SELF_m – B_m) – (SELF_f – B_f)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The OTHER-task requires evaluation of the emotional state expressed by faces, directly measuring perception and understanding of others&#x27; mental/emotional states.</td></tr>
<tr><td>18514546_analysis_5</td><td>Interaction (OTHER_f – B_f) – (OTHER_m – B_m)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The OTHER task and its gender interaction target inference about others’ emotional states and ToM-related processing (TPJ, STS), matching perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>18514546_1</td><td>Other &gt; high-level baseline; socialcommunication</td><td>18514546_analysis_1</td><td>OTHER versus high-level baseline</td><td>0.881</td><td>1.000</td><td>0.964</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18633788</strong> | Pred included: 12 | Manual included (accepted matches only): 11 | Correct overlaps: 9 | Match statuses: accepted=11, uncertain=1, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18633788/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Cards: Descriptives &gt; Precautions; others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18633788_analysis_0</td><td>Stories&gt;Rest</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although the study examines theory-of-mind for social-contract stories, the specific Stories&gt;Rest contrast is collapsed across story types and does not specifically isolate representations of others’ mental states (Understanding Mental States). The ToM-specific contrasts (social contract &gt; precaution) would meet this criterion, but Stories&gt;Rest does not.</td></tr>
<tr><td>18633788_analysis_1</td><td>Rest&gt;Stories</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task requires representing and reasoning about others’ desires, intentions, and mental states (ToM); it directly measures perception and understanding of others.</td></tr>
<tr><td>18633788_analysis_2</td><td>Cards &gt; Rest</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis explicitly examines understanding and inference about others’ mental states (ToM) during interpretation of social contracts versus precautions, directly fitting Perception and Understanding of Others (Understanding Mental States).</td></tr>
<tr><td>18633788_analysis_3</td><td>Rest &gt; Cards</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The Wason tasks require inferring others&#x27; desires/intentions (ToM); Rest&gt;Cards contrast implicates ToM-related areas, so it measures Perception and Understanding of Others (Understanding Mental States).</td></tr>
<tr><td>18633788_analysis_4</td><td>Social Contracts
&gt;Precautions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The study directly assesses Understanding Mental States (Theory of Mind) when interpreting social contracts versus precautions, satisfying the Perception and Understanding of Others construct.</td></tr>
<tr><td>18633788_analysis_5</td><td>Social Contracts&gt;Descriptives</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The study explicitly investigates understanding and inferring others&#x27; mental states (ToM: beliefs, desires, intentions) during social-exchange reasoning; this directly satisfies the Perception and Understanding of Others criteria.</td></tr>
<tr><td>18633788_analysis_6</td><td>Social Contracts&gt;Precautions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The analysis explicitly probes understanding others’ mental states (ToM) and representations about others (cheater detection), directly matching Perception and Understanding of Others (Understanding Mental States); satisfies I1 and I2.</td></tr>
<tr><td>18633788_analysis_7</td><td>Social Contracts&gt;Descriptives</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The social contract task explicitly requires representing and reasoning about others’ desires, intentions, and potential intentionality (ToM). This satisfies criteria for Perception and Understanding of Others, including Understanding Mental States.</td></tr>
<tr><td>18633788_analysis_8</td><td>Stories: Precautions &gt; Social Contracts</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The study explicitly targets understanding and reasoning about others’ mental states (ToM) and inferring desires/intentions in social-exchange contexts, matching Understanding Mental States.</td></tr>
<tr><td>18633788_analysis_9</td><td>Stories: Precautions &gt; Descriptives</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although the overall study examines theory-of-mind, this specific contrast (Precautions &gt; Descriptives) focuses on precautionary rule interpretation, which the authors state does not require inferring others’ mental states. Thus it does not meet the ‘Understanding Mental States’ subconstruct for this contrast.</td></tr>
<tr><td>18633788_analysis_10</td><td>Cards: Precautions &gt; Social Contracts</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast explicitly involves inferring and reasoning about others’ mental states (ToM) and detecting cheaters in social-exchange scenarios, directly mapping onto Understanding Mental States; meets I1 and I2.</td></tr>
<tr><td>18633788_analysis_11</td><td>Cards: Precautions &gt; Descriptives</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast involves reasoning about other people’s actions and states (are they in danger from not taking precautions), which falls under perception/understanding of others (action/mental-state-related reasoning).</td></tr>
<tr><td>18633788_analysis_12</td><td>Stories</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis directly probes understanding of others’ mental states (ToM), including anterior/posterior temporal activations tied to inferring desires, intentions, and detecting cheaters—matching Understanding Mental States.</td></tr>
<tr><td>18633788_analysis_13</td><td>Cards</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The task requires representing and reasoning about others’ mental states (beliefs, desires, intentions) and distinguishes ToM-related activations; it therefore measures Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>18633788_1</td><td>Cards &gt; Rest; others</td><td>18633788_analysis_2</td><td>Cards &gt; Rest</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633788_10</td><td>Stories: Precautions &gt; Descriptives; others</td><td>18633788_analysis_9</td><td>Stories: Precautions &gt; Descriptives</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633788_11</td><td>Stories: Precautions &gt; Social Contracts; others</td><td>18633788_analysis_8</td><td>Stories: Precautions &gt; Social Contracts</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633788_12</td><td>Stories: Social Contracts &gt; Descriptives; others</td><td>18633788_analysis_5</td><td>Social Contracts&gt;Descriptives</td><td>0.873</td><td>1.000</td><td>0.962</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633788_13</td><td>Stories: Social Contracts &gt; Precautions; others</td><td>18633788_analysis_4</td><td>Social Contracts
&gt;Precautions</td><td>0.870</td><td>1.000</td><td>0.961</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633788_2</td><td>Cards: Descriptives &gt; Precautions; others</td><td>18633788_analysis_6</td><td>Social Contracts&gt;Precautions</td><td>0.635</td><td>0.000</td><td>0.190</td><td>unmatched</td><td>low_total_score, missing_coords_on_one_side, name_only_signal</td></tr><tr><td>18633788_3</td><td>Cards: Descriptives &gt; Social Contracts; others</td><td>18633788_analysis_13</td><td>Cards</td><td>0.233</td><td>0.727</td><td>0.579</td><td>uncertain</td><td>coord_count_mismatch</td></tr><tr><td>18633788_4</td><td>Cards: Precautions &gt; Descriptives; others</td><td>18633788_analysis_11</td><td>Cards: Precautions &gt; Descriptives</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633788_5</td><td>Cards: Precautions &gt; Social Contracts; others</td><td>18633788_analysis_10</td><td>Cards: Precautions &gt; Social Contracts</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633788_6</td><td>Rest &gt; Cards; others</td><td>18633788_analysis_3</td><td>Rest &gt; Cards</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633788_7</td><td>Rest &gt; Stories; others</td><td>18633788_analysis_1</td><td>Rest&gt;Stories</td><td>1.000</td><td>0.950</td><td>0.965</td><td>accepted</td><td>high_coord_match</td></tr><tr><td>18633788_8</td><td>Stories &gt; Rest; others</td><td>18633788_analysis_0</td><td>Stories&gt;Rest</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633788_9</td><td>Stories: Descriptives &gt; Precautions; others</td><td>18633788_analysis_12</td><td>Stories</td><td>0.333</td><td>1.000</td><td>0.800</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18633856</strong> | Pred included: 2 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18633856/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18633856_analysis_0</td><td>Angry&gt;Happy</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast measures perception and understanding of others’ emotional states, intentions (dominance/affiliation) and biological motion — directly addressing perception/understanding of others.</td></tr>
<tr><td>18633856_analysis_1</td><td>Happy&gt;Angry</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis assesses perception and understanding of others&#x27; emotional states and intentions (affiliation/dominance), matching this construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>18633856_1</td><td>Angry &gt; Happy; affiliation</td><td>18633856_analysis_0</td><td>Angry&gt;Happy</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18633856_2</td><td>Happy &gt; Angry; affiliation</td><td>18633856_analysis_1</td><td>Happy&gt;Angry</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 19439183</strong> | Pred included: 9 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19439183/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19439183_analysis_0</td><td>Pain &gt; no pain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast indexes perception and understanding of others&#x27; pain (representations of others&#x27; emotional states and actions), matching this construct.</td></tr>
<tr><td>19439183_analysis_1</td><td>Self + Other &gt; Self</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast directly probes perception and understanding of others (their pain and intentionality); it satisfies inclusion criteria for Perception and Understanding of Others.</td></tr>
<tr><td>19439183_analysis_2</td><td>Self &gt; Self + Other</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast directly manipulates presence of another person and intentionality, assessing representation and inference about others&#x27; actions and mental states (agency), meeting perception/understanding of others.</td></tr>
<tr><td>19439183_analysis_3</td><td>PCO &gt; PCS</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast directly assesses perception and understanding of others (intentionality/agency—pain caused by another vs by self), satisfying social-other perception criteria.</td></tr>
<tr><td>19439183_analysis_4</td><td>PCS &gt; PCO</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast compares self versus other agency and the paradigm explicitly manipulates perception/understanding of others&#x27; actions and intentions, so it measures perception of others (I1 and I2).</td></tr>
<tr><td>19439183_analysis_5</td><td>Emotion contagion score (during PCS trials)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Emotion Contagion score correlated with brain activity during viewing of pain (PCS) reflects representations and assessment of others&#x27; emotional states—fits Perception and Understanding of Others.</td></tr>
<tr><td>19439183_analysis_6</td><td>Pain ratings (during PCS trials)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrasts and reported results explicitly address perception and understanding of others’ pain and intentionality (PCO vs PCS, social context main effect, TPJ, mPFC, amygdala activations), matching Perception and Understanding of Others.</td></tr>
<tr><td>19439183_analysis_7</td><td>Emotion contagion score (during PCO trials)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>PCO trials involve perceiving and evaluating another person&#x27;s pain and agency; correlating EC with PCO activity directly measures understanding of others&#x27; emotional states.</td></tr>
<tr><td>19439183_analysis_8</td><td>Pain ratings (during PCO trials)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>PCO trials directly assess perception and evaluation of another&#x27;s pain and intentionality (understanding others&#x27; states), satisfying this construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>19439183_1</td><td>Pain &gt; no pain; others</td><td>19439183_analysis_0</td><td>Pain &gt; no pain</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>19439183_2</td><td>Self + Other &gt; Self; others</td><td>19439183_analysis_1</td><td>Self + Other &gt; Self</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 19733672</strong> | Pred included: 6 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19733672/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19733672_analysis_0</td><td>Movie scenes theory of mind (ToM) &gt; movie scenes physical inference (PI)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Theory of Mind explicitly concerns Understanding Mental States of others, which directly matches Perception and Understanding of Others. The contrast measures this construct.</td></tr>
<tr><td>19733672_analysis_1</td><td>Silent answer theory of mind (ToM) &gt; silent answer physical inference (PI)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast explicitly targets understanding others&#x27; mental states (Theory of Mind &gt; physical inference), matching the &#x27;Understanding Mental States&#x27; subconstruct.</td></tr>
<tr><td>19733672_analysis_2</td><td>MC answer theory of mind (ToM) &gt; MC answer physical inference (PI)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This analysis explicitly tests understanding of others&#x27; mental states (Theory of Mind) using movie stimuli and MC answers; the contrast ToM&gt;PI directly measures perception and understanding of others.</td></tr>
<tr><td>19733672_analysis_3</td><td>IC 1 (rank 8 within all 80 components)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The paradigm explicitly probes Perception and Understanding of Others (Theory of Mind) and contrasts ToM vs PI directly measure mental state understanding.</td></tr>
<tr><td>19733672_analysis_4</td><td>IC 2 (rank 13 within all 80 components)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis targets understanding others&#x27; mental states (Theory of Mind); IC2 spatial map includes regions (STS, TPJ, temporal pole) associated with understanding mental states and shows ToM-related modulation.</td></tr>
<tr><td>19733672_analysis_5</td><td>IC 3 (rank 19 within all 80 components)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study explicitly measures understanding of others&#x27; mental states (Theory of Mind) during movie scenes and answers; this directly satisfies Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>19733672_1</td><td>MC answer theory of mind (ToM)NMC answer physical inference (PI); others</td><td>19733672_analysis_2</td><td>MC answer theory of mind (ToM) &gt; MC answer physical inference (PI)</td><td>0.969</td><td>1.000</td><td>0.991</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>19733672_2</td><td>Movie scenes theory of mind (ToM) &gt; movie scenes physical inference (PI); others</td><td>19733672_analysis_0</td><td>Movie scenes theory of mind (ToM) &gt; movie scenes physical inference (PI)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>19733672_3</td><td>Silent answer theory of mind (ToM) &gt; silent answer physical inference (PI); others</td><td>19733672_analysis_1</td><td>Silent answer theory of mind (ToM) &gt; silent answer physical inference (PI)</td><td>1.000</td><td>0.800</td><td>0.860</td><td>accepted</td><td>high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20045478</strong> | Pred included: 3 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20045478/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20045478_analysis_0</td><td>Self &gt; syllables</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The specific contrast is Self &gt; syllables and does not index perception or understanding of others (though other contrasts in the study do), so it does not meet the inclusion criteria.</td></tr>
<tr><td>20045478_analysis_1</td><td>Other &gt; syllables</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis directly measures representations and judgments about another person&#x27;s traits (close friend), matching Perception and Understanding of Others.</td></tr>
<tr><td>20045478_analysis_2</td><td>Self &gt; other</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis includes trait judgments about a close friend and examines representation and inference about another person (including perceived similarity), meeting Perception and Understanding of Others criteria.</td></tr>
<tr><td>20045478_analysis_3</td><td>Other &gt; self</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis directly assesses judgments about another person (friend) and how perceived similarity modulates neural responses—meets perception/understanding of others criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>20045478_1</td><td>Other &gt; self; others</td><td>20045478_analysis_3</td><td>Other &gt; self</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>20045478_2</td><td>Other &gt; syllables; others</td><td>20045478_analysis_1</td><td>Other &gt; syllables</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20056152</strong> | Pred included: 2 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20056152/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20056152_analysis_0</td><td>Places &gt; bodies</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The study explicitly evaluates representation of another&#x27;s mental content (what the avatar is seeing), matching Understanding Mental States/Perception of Others; the contrast directly measures this construct.</td></tr>
<tr><td>20056152_analysis_1</td><td>Bodies &gt; places</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis explicitly assesses representing and reasoning about another&#x27;s mental content (what the avatar is seeing), directly mapping onto Understanding Mental States and Perception of Others. Meets I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>20056152_1</td><td>Bodies &gt; Places; socialcommunication</td><td>20056152_analysis_1</td><td>Bodies &gt; places</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>20056152_2</td><td>Places &gt; Bodies; socialcommunication</td><td>20056152_analysis_0</td><td>Places &gt; bodies</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20096792</strong> | Pred included: 4 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20096792/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20096792_analysis_0</td><td>Live&gt;Recorded</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast probes representation and understanding of others (intentions, gaze, social relevance), recruiting rpSTS and rTPJ; thus it clearly measures perception and understanding of others.</td></tr>
<tr><td>20096792_analysis_1</td><td>Recorded&gt;Live</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast involves perceiving and interpreting another person&#x27;s actions (live vs recorded), tapping representations about others (action/perception and intent relevance). Inclusion criteria for Perception and Understanding of Others are met.</td></tr>
<tr><td>20096792_analysis_2</td><td>JA&gt;SA</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>JA&gt;SA probes representations of others&#x27; actions/intentions (gaze-following, joint attention) and thus measures perception and understanding of others (action perception and mental-state understanding).</td></tr>
<tr><td>20096792_analysis_3</td><td>SA&gt;JA</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Joint vs. solo attention contrasts probe action perception and understanding others&#x27; intentions/mental states (pSTS, TPJ), meeting the criteria for perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>20096792_1</td><td>Live &gt; Recorded; socialcommunication</td><td>20096792_analysis_0</td><td>Live&gt;Recorded</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>20096792_2</td><td>Recorded &gt; Live; socialcommunication</td><td>20096792_analysis_1</td><td>Recorded&gt;Live</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>20096792_3</td><td>JA &gt; SA; socialcommunication</td><td>20096792_analysis_2</td><td>JA&gt;SA</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20119878</strong> | Pred included: 4 | Manual included (accepted matches only): 4 | Correct overlaps: 4 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20119878/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20119878_analysis_0</td><td>Neural regions active to warmth expectancy violation social targets</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast directly measures participants’ perception and evaluation of others’ traits (warmth and competence) and responses to expectancy violations — matching Perception and Understanding of Others.</td></tr>
<tr><td>20119878_analysis_1</td><td>Neural regions active to warmth expectancy consistent social targets</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrasts examine perception and understanding of others’ traits (warmth and competence) and expectancy violations—directly matches Perception and Understanding of Others.</td></tr>
<tr><td>20119878_analysis_2</td><td>Neural regions active to competence expectancy violation social targets</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task explicitly involves representing and evaluating others’ traits (warmth and competence) and responses to expectancy violations about social targets — it measures perception and understanding of others.</td></tr>
<tr><td>20119878_analysis_3</td><td>Neural regions active to competence expectancy consistent social targets</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants evaluated others’ traits (warmth and competence) and expectancy violations about those traits — directly measuring perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>20119878_1</td><td>competence expectancy violation &gt; consistant competence expectency; others</td><td>20119878_analysis_2</td><td>Neural regions active to competence expectancy violation social targets</td><td>0.540</td><td>1.000</td><td>0.862</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>20119878_2</td><td>consistant competence expectency &gt; competence expectancy violation; others</td><td>20119878_analysis_3</td><td>Neural regions active to competence expectancy consistent social targets</td><td>0.507</td><td>1.000</td><td>0.852</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>20119878_3</td><td>consistent warmth expectancy &gt;  warm expectancy violation; others</td><td>20119878_analysis_1</td><td>Neural regions active to warmth expectancy consistent social targets</td><td>0.468</td><td>1.000</td><td>0.840</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>20119878_4</td><td>warm expectancy violation  &gt;  consistent warmth expectancy; others</td><td>20119878_analysis_0</td><td>Neural regions active to warmth expectancy violation social targets</td><td>0.504</td><td>1.000</td><td>0.851</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21249224</strong> | Pred included: 1 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21249224/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21249224_analysis_0</td><td>Brain areas showing increased activity in response to the social interaction condition.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The SI vs NSI contrast targets perception and understanding of others (animacy, action perception, and mentalizing), explicitly engaging mentalizing and action-perception processes, meeting both criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>21249224_1</td><td>social interactions &gt; nonsocial interactions; others</td><td>21249224_analysis_0</td><td>Brain areas showing increased activity in response to the social interaction condition.</td><td>0.397</td><td>1.000</td><td>0.819</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21703352</strong> | Pred included: 3 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21703352/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21703352_analysis_0</td><td>Neural regions engaged during decisions to accept Costly-Donation relative to Noncostly-Reward trials.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast probes mentalizing and understanding others (family members), directly measuring perception and understanding of others&#x27; states and intentions.</td></tr>
<tr><td>21703352_analysis_1</td><td>Neural regions associated with family obligation preferences during decisions to accept Costly-Donation versus Noncostly-Reward trials.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analyses explicitly target mentalizing and understanding others (family members) with TPJ, pSTS, MPFC involvement and regressors related to family obligation—meets criteria for Perception and Understanding of Others.</td></tr>
<tr><td>21703352_analysis_2</td><td>Neural regions associated with the ventral striatum during decisions to accept Costly-Donation relative to Noncostly-Reward trials that correlated positively with participants&#x27; family obligation preferences.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis explicitly reports recruitment of mentalizing regions (TPJ, pSTS, MPFC/DMPFC) and correlates these with family-obligation, directly indexing understanding of others&#x27; mental states in a kinship context.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>21703352_1</td><td>Costly-Donation relative to Noncostly-Reward (accept); others</td><td>21703352_analysis_0</td><td>Neural regions engaged during decisions to accept Costly-Donation relative to Noncostly-Reward trials.</td><td>0.594</td><td>1.000</td><td>0.878</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>21703352_2</td><td>Costly-Donation versus Noncostly-Reward (Family obligtations); others</td><td>21703352_analysis_1</td><td>Neural regions associated with family obligation preferences during decisions to accept Costly-Donation versus Noncostly-Reward trials.</td><td>0.439</td><td>1.000</td><td>0.832</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21955370</strong> | Pred included: 4 | Manual included (accepted matches only): 3 | Correct overlaps: 2 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21955370/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21955370_analysis_0</td><td>I(H + M) &gt; C(H + M)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Understanding others (mentalizing/ToM) would be assessed by contrasts comparing Human vs Machine or interaction effects ([H(I&gt;C)]&gt;[M(I&gt;C)]). The current contrast collapses across Human and Machine and isolates Stroop effects, so it does not meet the inclusion criteria for perception/understanding of others.</td></tr>
<tr><td>21955370_analysis_1</td><td>C(H + M) &gt; I(H + M)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study explicitly aimed to probe understanding of others (ToM) using Human vs Machine contrasts, but the C(H+M) &gt; I(H+M) contrast collapsed across opponent type measures Stroop congruency rather than perception or reasoning about others’ mental states. Thus it does not meet I2 (Understanding Mental States) for this contrast.</td></tr>
<tr><td>21955370_analysis_2</td><td>H(I + C) &gt; M(I + C)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast is explicitly used to index Theory of Mind and understanding others’ mental states during competition, directly matching the ‘understanding mental states’ subconstruct.</td></tr>
<tr><td>21955370_analysis_3</td><td>M(I + C) &gt; H(I + C)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast was explicitly designed to index Theory of Mind/understanding of others in a social competition (human–human &gt; machine), directly measuring perception and understanding of others (mental states).</td></tr>
<tr><td>21955370_analysis_4</td><td>[H(I &gt; C)] &gt; [M(I &gt; C)]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast was explicitly designed to capture Theory of Mind/understanding of the opponent’s mental states (Perception and Understanding of Others), so it meets the criteria.</td></tr>
<tr><td>21955370_analysis_5</td><td>[M(I &gt; C)] &gt; [H(I &gt; C)]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast isolates Theory-of-Mind/understanding-others processes by comparing human–human to human–machine competition, directly probing perception and understanding of others&#x27; mental states.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>21955370_1</td><td>H(I + C) &gt; M(I + C); affiliation</td><td>21955370_analysis_2</td><td>H(I + C) &gt; M(I + C)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>21955370_2</td><td>I(H + M) &gt; C(H + M); affiliation</td><td>21955370_analysis_0</td><td>I(H + M) &gt; C(H + M)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>21955370_3</td><td>[H(I &gt; C)] &gt; [M(I &gt; C)]; affiliation</td><td>21955370_analysis_4</td><td>[H(I &gt; C)] &gt; [M(I &gt; C)]</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 22019860</strong> | Pred included: 2 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/22019860/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> observation of biological motion displays &gt; scrambled displays; others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>22019860_analysis_0</td><td>A</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Biological motion perception is a core measure of Perception and Understanding of Others (action perception/animacy perception); the biological vs scrambled contrast directly measures this construct.</td></tr>
<tr><td>22019860_analysis_1</td><td>B</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Biological motion perception is a form of perceiving and understanding others (action perception/animacy); the contrast (canonical vs scrambled) directly measures this construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>22019860_1</td><td>Observation of biological motion displays &gt; observation of scrambled displays; others</td><td>22019860_analysis_0</td><td>A</td><td>0.026</td><td>1.000</td><td>0.708</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>22019860_2</td><td>observation of biological motion displays &gt; scrambled displays; others</td><td>22019860_analysis_1</td><td>B</td><td>0.032</td><td>0.076</td><td>0.062</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 22726841</strong> | Pred included: 5 | Manual included (accepted matches only): 5 | Correct overlaps: 5 | Match statuses: accepted=5, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/22726841/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>22726841_analysis_0</td><td>Simulated-other&#x27;s reward prediction error</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis explicitly models and measures representations about another’s internal variables (simulated-other reward and action prediction errors), which directly index perception/understanding of others (action/mental state representation).</td></tr>
<tr><td>22726841_analysis_1</td><td>Simulated-other&#x27;s action prediction error</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis explicitly models and measures representations and learning about another person&#x27;s choices (action and reward prediction errors), directly indexing perception/understanding of others.</td></tr>
<tr><td>22726841_analysis_2</td><td>Reward probability</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis directly measures representation and inference about another&#x27;s internal states (simulated-other&#x27;s reward probability, sRPE and sAPE), satisfying perception/understanding of others.</td></tr>
<tr><td>22726841_analysis_3</td><td>Reward prediction error</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis explicitly measures representation and learning about others&#x27; internal states (simulated-other reward and action prediction errors), aligning with Perception and Understanding of Others and Understanding Mental States.</td></tr>
<tr><td>22726841_analysis_4</td><td>Reward probability</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis explicitly models and measures representations of others’ internal states (simulated-other reward probability, sRPE, sAPE) and thus measures perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>22726841_1</td><td>Reward Prediction Error (control) &gt; Reward probability (other); affiliation</td><td>22726841_analysis_3</td><td>Reward prediction error</td><td>0.541</td><td>1.000</td><td>0.862</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>22726841_2</td><td>Reward probability (other) &gt;  Reward Prediction Error (control); affiliation</td><td>22726841_analysis_4</td><td>Reward probability</td><td>0.450</td><td>1.000</td><td>0.835</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>22726841_3</td><td>Reward probability (other) &gt; Reward Prediction Error (other); affiliation</td><td>22726841_analysis_2</td><td>Reward probability</td><td>0.462</td><td>1.000</td><td>0.838</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>22726841_4</td><td>Simulated-other’s action prediction error &gt; Simulated-other’s reward prediction error; affiliation</td><td>22726841_analysis_1</td><td>Simulated-other&#x27;s action prediction error</td><td>0.635</td><td>1.000</td><td>0.890</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>22726841_5</td><td>Simulated-other’s reward prediction error &gt; Simulated-other’s action prediction error; affiliation</td><td>22726841_analysis_0</td><td>Simulated-other&#x27;s reward prediction error</td><td>0.635</td><td>1.000</td><td>0.890</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23063451</strong> | Pred included: 5 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23063451/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23063451_analysis_0</td><td>Self &gt; Other</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task includes an Other (third-person) perspective and the Self vs Other contrast addresses neural representations of self vs other; thus it involves perception/understanding of others (mental state inference).</td></tr>
<tr><td>23063451_analysis_1</td><td>Other &gt; Self</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The Other &gt; Self contrast directly probes representation and understanding of others&#x27; emotional states (third-person perspective-taking), meeting perception-of-others criteria.</td></tr>
<tr><td>23063451_analysis_2</td><td>OtherGood &gt; OtherBad performer</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>OtherGood &gt; OtherBad directly measures representation and understanding of others&#x27; emotional states (successful third-person perspective taking during pain empathy), satisfying Perception and Understanding of Others.</td></tr>
<tr><td>23063451_analysis_3</td><td>OtherBad &gt; OtherGood performer</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Includes an Other perspective condition and contrasts OtherGood vs OtherBad, and analyses of understanding others&#x27; states (empathy, mentalizing)—meets criteria for perception/understanding of others.</td></tr>
<tr><td>23063451_analysis_4</td><td>SelfGood &gt; SelfBad performer</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific analysis contrasts SelfGood vs SelfBad (first-person perspective); it does not measure perception/understanding of others (Other contrasts exist elsewhere but not in this analysis).</td></tr>
<tr><td>23063451_analysis_5</td><td>SelfBad &gt; SelfGood performer</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific contrast compares unsuccessful versus successful self-perspective trials (Self), so it does not measure perception/understanding of others (mentalizing about others) even though the study also has Other conditions; this analysis is Self-specific.</td></tr>
<tr><td>23063451_analysis_6</td><td>analysis_6</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The Other condition requires representing and reasoning about others&#x27; emotional states (third-person perspective, empathy for others&#x27; pain), directly measuring perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>23063451_1</td><td>Conjunction of Other and Self; others</td><td>23063451_analysis_6</td><td>analysis_6</td><td>0.170</td><td>1.000</td><td>0.751</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>23063451_2</td><td>Other &gt; Self; others</td><td>23063451_analysis_1</td><td>Other &gt; Self</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23221019</strong> | Pred included: 5 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=2</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23221019/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Conjunction of all Conditions &gt; Fixation Baseline; socialcommunication, Load x Goal &gt; Fixation Baseline; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23221019_analysis_0</td><td>Conjunction of all conditions versus fixation baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Directly assesses action perception and understanding of others’ mental states (what/how/why goals); matches action perception and mentalizing subconstructs.</td></tr>
<tr><td>23221019_analysis_1</td><td>Conjunction of low-load/how-goal condition &gt; low-load/observe-goal condition and high-load/how-goal condition &gt; high-load/observe-goal condition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Directly assesses action perception and understanding of others’ mental states (how/why goals, mirror and mentalizing systems); meets criteria for Perception and Understanding of Others.</td></tr>
<tr><td>23221019_analysis_2</td><td>Modulation by load for any of the four goals</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrasts explicitly assess perception and understanding of others via action perception (what/how) and mental-state inference (why), matching action perception and understanding mental states.</td></tr>
<tr><td>23221019_analysis_3</td><td>Low-load/why-goal condition compared with high-load/why-goal condition, low-load/how-goal condition, and high-load/how-goal condition combined</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Directly assesses action perception and understanding of others’ mental states (why/how goals), matching this construct.</td></tr>
<tr><td>23221019_analysis_4</td><td>Load-by-goal interaction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The primary contrasts test action perception and understanding of others’ mental states (why/how goals, load-by-goal interaction affecting mentalizing regions), directly matching Perception and Understanding of Others. Satisfies I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>23221019_1</td><td>Conjunction of all Conditions &gt; Fixation Baseline; socialcommunication</td><td>23221019_analysis_0</td><td>Conjunction of all conditions versus fixation baseline</td><td>0.932</td><td>0.200</td><td>0.420</td><td>unmatched</td><td>coord_count_mismatch, low_coord_high_name, low_total_score</td></tr><tr><td>23221019_2</td><td>Conjunction of low-load/how-goal condition &gt; low-load/observe-goal condition and high-load/how-goal condition &gt; high-load/observe-goal condition; socialcommunication</td><td>23221019_analysis_1</td><td>Conjunction of low-load/how-goal condition &gt; low-load/observe-goal condition and high-load/how-goal condition &gt; high-load/observe-goal condition</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23221019_3</td><td>Load x Goal &gt; Fixation Baseline; socialcommunication</td><td>23221019_analysis_4</td><td>Load-by-goal interaction</td><td>0.545</td><td>0.429</td><td>0.464</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>23221019_4</td><td>Low-load/why-goal condition compared with high-load/why-goal condition, low-load/how-goal condition, and high-load/how-goal condition combined; socialcommunication</td><td>23221019_analysis_3</td><td>Low-load/why-goal condition compared with high-load/why-goal condition, low-load/how-goal condition, and high-load/how-goal condition combined</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23221019_5</td><td>Modulation by load for any of the four goals; socialcommunication</td><td>23221019_analysis_2</td><td>Modulation by load for any of the four goals</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23684882</strong> | Pred included: 2 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=3</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23684882/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> [(S-IMIO &gt; NS-IMIO) &gt; (S-CTO &gt; NS-CTO)] &gt; [(S-IMIE &gt; NS-IMIE) &gt; (S-CTE &gt; NS- CTE)]; socialcommunication, [(S-IMIO &gt; S-CTO) + (S-IMIE &gt; S-CTE)] &gt; [(NS-IMIO &gt; NS-CTO) + (NS-IMIE &gt; NS-CTE)]; socialcommunication, [S (IMIO + CTO + IMIE + CTE) &gt; NS (IMIO + CTO + IMIE + CTE)]; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23684882_analysis_0</td><td>(IMI_O &gt; CT_O) ∩ (IMI_E &gt; CT_E)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast captures neural responses to observing and executing others&#x27; gestures and is intended to probe understanding of others&#x27; actions and intentions (mirror and mentalizing systems), matching perception/understanding of others.</td></tr>
<tr><td>23684882_analysis_1</td><td>(S-IMI_O &gt; NS-IMI_O) &gt; (S-CT_O &gt; NS-CT_O)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast contrasts social vs non‑social gestures (communicative intention) during observation/Imitation, directly indexing perception and understanding of others&#x27; actions and intentions.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>23684882_1</td><td>(IMIO &gt; CTO) ∩ (IMIE &gt; CTE); socialcommunication</td><td>23684882_analysis_0</td><td>(IMI_O &gt; CT_O) ∩ (IMI_E &gt; CT_E)</td><td>0.931</td><td>0.950</td><td>0.944</td><td>accepted</td><td>high_coord_match</td></tr><tr><td>23684882_2</td><td>(S-IMIO&gt;NS-IMIO) &gt; (S-CTO &gt; NS-CTO); socialcommunication</td><td>23684882_analysis_1</td><td>(S-IMI_O &gt; NS-IMI_O) &gt; (S-CT_O &gt; NS-CT_O)</td><td>0.949</td><td>1.000</td><td>0.985</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23684882_3</td><td>[(S-IMIO &gt; NS-IMIO) &gt; (S-CTO &gt; NS-CTO)] &gt; [(S-IMIE &gt; NS-IMIE) &gt; (S-CTE &gt; NS- CTE)]; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>23684882_4</td><td>[(S-IMIO &gt; S-CTO) + (S-IMIE &gt; S-CTE)] &gt; [(NS-IMIO &gt; NS-CTO) + (NS-IMIE &gt; NS-CTE)]; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>23684882_5</td><td>[S (IMIO + CTO + IMIE + CTE) &gt; NS (IMIO + CTO + IMIE + CTE)]; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23813661</strong> | Pred included: 5 | Manual included (accepted matches only): 5 | Correct overlaps: 5 | Match statuses: accepted=5, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23813661/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23813661_analysis_0</td><td>Table I. Regions more responsive to meaningful than scrambled videos</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrasts probe animacy/action perception and inferential understanding of others&#x27; intentions (contingency, fluency), matching perception/understanding of others.</td></tr>
<tr><td>23813661_analysis_1</td><td>1. Contingent &gt; Mirrored</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast probes perception and understanding of others (animacy, action perception, intentionality via contingency), meeting the construct criteria.</td></tr>
<tr><td>23813661_analysis_2</td><td>2. Mirrored &gt; Contingent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis probes animacy/action perception and inference about others&#x27; intentions (contingency and kinematics), directly targeting perception and understanding of others.</td></tr>
<tr><td>23813661_analysis_3</td><td>1. Rigid &gt; Smooth</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The study directly examines perception of others (animacy/action perception and inferential mentalizing); the rigid&gt;smooth contrast probes how kinematics influence perception/understanding of others, meeting I1 and I2.</td></tr>
<tr><td>23813661_analysis_4</td><td>2. Interaction: (Contingent &gt; Mirrored) &gt; (Smooth &gt; Rigid)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis targets perception and understanding of others (animacy, action perception, and inference about intentions from kinematics/contingency).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>23813661_1</td><td>(Contingent &gt; Mirrored) &gt; (Smooth &gt; Rigid); others</td><td>23813661_analysis_4</td><td>2. Interaction: (Contingent &gt; Mirrored) &gt; (Smooth &gt; Rigid)</td><td>0.840</td><td>1.000</td><td>0.952</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23813661_2</td><td>Contingent &gt; Mirrored; others</td><td>23813661_analysis_1</td><td>1. Contingent &gt; Mirrored</td><td>0.933</td><td>1.000</td><td>0.980</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23813661_3</td><td>Mirrored &gt; Contingent; others</td><td>23813661_analysis_2</td><td>2. Mirrored &gt; Contingent</td><td>0.933</td><td>1.000</td><td>0.980</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23813661_4</td><td>Rigid &gt; Smooth; others</td><td>23813661_analysis_3</td><td>1. Rigid &gt; Smooth</td><td>0.903</td><td>1.000</td><td>0.971</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>23813661_5</td><td>meaningful videos &gt; scrambled videos; others</td><td>23813661_analysis_0</td><td>Table I. Regions more responsive to meaningful than scrambled videos</td><td>0.538</td><td>1.000</td><td>0.862</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24294841</strong> | Pred included: 8 | Manual included (accepted matches only): 5 | Correct overlaps: 5 | Match statuses: accepted=5, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24294841/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24294841_analysis_0</td><td>Main effect (ES + TS + SRS) - (EN + TN + SRN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Empathy and ToM directly probe understanding others’ emotional and mental states; the analysis contrasts social conditions and thus measures perception/understanding of others.</td></tr>
<tr><td>24294841_analysis_1</td><td>EmpathyES-EN</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The empathy and ToM conditions require representing and reasoning about others’ emotional and mental states (understanding mental states), meeting the inclusion criteria.</td></tr>
<tr><td>24294841_analysis_2</td><td>ToMTS-TN</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Primary contrasts involve inferring others’ emotional states and mental states (empathy and ToM), directly measuring Perception and Understanding of Others.</td></tr>
<tr><td>24294841_analysis_3</td><td>Self-referenceSRS-SRN</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Empathy and ToM conditions explicitly require representing and reasoning about others’ emotional and mental states, meeting the Perception and Understanding of Others criteria.</td></tr>
<tr><td>24294841_analysis_4</td><td>Empathy &gt; ToM(ES + EN)-(TS + TN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Empathy and ToM tasks explicitly probe understanding and reasoning about others’ emotional/mental states, matching the Perception and Understanding of Others construct.</td></tr>
<tr><td>24294841_analysis_5</td><td>ToM &gt; Empathy(TS + TN)-(ES + EN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>ToM &gt; Empathy directly interrogates understanding others’ mental states (the core of the Perception and Understanding of Others construct).</td></tr>
<tr><td>24294841_analysis_6</td><td>Empathy &gt; Self-reference(ES + EN)-(SRS + SRN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Empathy and ToM conditions require perceiving and reasoning about others’ emotional states (understanding mental states), so the contrast measures perception/understanding of others.</td></tr>
<tr><td>24294841_analysis_7</td><td>Self-reference &gt; Empathy(SRS + SRN)-(ES + EN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Empathy and ToM conditions directly assess perception and understanding of others&#x27; emotional and mental states; the contrast involves empathy so pertains to understanding others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24294841_1</td><td>Empathy &gt; Self-reference; others</td><td>24294841_analysis_6</td><td>Empathy &gt; Self-reference(ES + EN)-(SRS + SRN)</td><td>0.727</td><td>1.000</td><td>0.918</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24294841_2</td><td>Empathy ES–EN; others</td><td>24294841_analysis_1</td><td>EmpathyES-EN</td><td>0.880</td><td>0.846</td><td>0.856</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>24294841_3</td><td>Main effect (ES + TS + SRS) – (EN + TN + SRN); others</td><td>24294841_analysis_0</td><td>Main effect (ES + TS + SRS) - (EN + TN + SRN)</td><td>0.978</td><td>1.000</td><td>0.993</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24294841_6</td><td>ToM &gt; Empathy; others</td><td>24294841_analysis_5</td><td>ToM &gt; Empathy(TS + TN)-(ES + EN)</td><td>0.642</td><td>1.000</td><td>0.892</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24294841_7</td><td>ToM TS–TN; others</td><td>24294841_analysis_2</td><td>ToMTS-TN</td><td>0.824</td><td>1.000</td><td>0.947</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24294906</strong> | Pred included: 2 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24294906/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24294906_analysis_0</td><td>Main effect of social value orientation on decision-making</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Study tests mentalizing and inferring others’ intentions (TPJ, medial PFC, precuneus) during cooperative decision-making, directly measuring perception and understanding of others’ mental states.</td></tr>
<tr><td>24294906_analysis_1</td><td>Interaction effect between dispositional trust and social value orientation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The study explicitly probes understanding others’ intentions and mental states (TPJ, mentalizing network, trust effects), satisfying criteria for Perception and Understanding of Others (I1 and I2).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24294906_1</td><td>Interaction effect between dispositional trust and social value orientation; others</td><td>24294906_analysis_1</td><td>Interaction effect between dispositional trust and social value orientation</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24294906_2</td><td>Main effect of social value orientation on decision-making; others</td><td>24294906_analysis_0</td><td>Main effect of social value orientation on decision-making</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24414614</strong> | Pred included: 7 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24414614/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24414614_analysis_0</td><td>Imitating</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The study explicitly investigates perception and understanding of others’ actions and intentions (mirror and mentalizing systems) via contrasts of imitating/being imitated vs observation and PPI connectivity reflecting representation of others’ mental states.</td></tr>
<tr><td>24414614_analysis_1</td><td>Being imitated</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task explicitly assesses perception and understanding of others’ actions and intentions (action perception and mental state understanding) via contrasts of being imitated versus observation and PPI targeting coupling with mentalizing regions.</td></tr>
<tr><td>24414614_analysis_2</td><td>Observation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The experiment and PPI explicitly probe action perception and understanding of others’ intentions (mentalizing) during imitation, directly matching this construct.</td></tr>
<tr><td>24414614_analysis_3</td><td>Imitating-IFG</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study directly probes perception and understanding of others’ actions and intentions (action perception and mentalizing) using imitating/being imitated contrasts and PPI connectivity with mentalizing regions.</td></tr>
<tr><td>24414614_analysis_4</td><td>Being imitated-IFG</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task requires representing and anticipating others’ actions and intentions (action perception and understanding mental states); contrasts and PPI directly probe these processes.</td></tr>
<tr><td>24414614_analysis_5</td><td>Imitating-IPL*</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study targets action perception and understanding others&#x27; intentions (mentalizing). PPI shows coupling between mirror and mentalizing systems when perceiving/anticipating others&#x27; actions.</td></tr>
<tr><td>24414614_analysis_6</td><td>Being imitated-IPL</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study explicitly examines action perception and understanding others&#x27; intentions (Mentalizing) during imitation/being imitated, matching Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24414614_1</td><td>being imitated &gt; rest; socialcommunication</td><td>24414614_analysis_1</td><td>Being imitated</td><td>0.800</td><td>1.000</td><td>0.940</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24414614_2</td><td>imitating &gt; rest; socialcommunication</td><td>24414614_analysis_0</td><td>Imitating</td><td>0.720</td><td>1.000</td><td>0.916</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24414614_3</td><td>observation &gt; rest; socialcommunication</td><td>24414614_analysis_2</td><td>Observation</td><td>0.759</td><td>1.000</td><td>0.928</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24462962</strong> | Pred included: 1 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24462962/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24462962_analysis_0</td><td>Significant clusters of activation in houses and faces fMRI task.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task assesses perception and processing of others&#x27; faces (including emotional expressions) and regressions relate social-cognitive measures to fusiform activity — fitting Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24462962_1</td><td>house + faces; socialcommunication</td><td>24462962_analysis_0</td><td>Significant clusters of activation in houses and faces fMRI task.</td><td>0.308</td><td>1.000</td><td>0.792</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24583253</strong> | Pred included: 5 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24583253/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24583253_analysis_0</td><td>Decrease in activity with the increase in group size</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis measures perception and understanding of others (responses to an individual in need and presence/number of bystanders), fitting perception-of-others constructs.</td></tr>
<tr><td>24583253_analysis_1</td><td>Increase in activity with the increase in group size</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task involves observing others (victim and bystanders) and examines how group-size modulates neural responses—this addresses perception and understanding of others (action/scene perception and social context). Thus I1 and I2 are satisfied.</td></tr>
<tr><td>24583253_analysis_2</td><td>Control &gt; bystander.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The stimuli and contrasts probe perception and understanding of others (observing a person fainting and varying numbers of bystanders), which maps onto action and state perception of others.</td></tr>
<tr><td>24583253_analysis_3</td><td>Falling &gt; standing</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast contrasts a person falling vs standing and manipulates bystander presence; it probes perception/action understanding of others (action perception/empathic response), satisfying both inclusion criteria.</td></tr>
<tr><td>24583253_analysis_4</td><td>Standing &gt; falling</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast involves perceiving others (a person fainting and bystanders) and examines how observers represent/prepare actions in response to others’ states; fits Perception and Understanding of Others (action perception/animacy).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24583253_1</td><td>control &gt; bystander; others</td><td>24583253_analysis_2</td><td>Control &gt; bystander.</td><td>0.974</td><td>1.000</td><td>0.992</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24583253_2</td><td>falling &gt; standing; others</td><td>24583253_analysis_3</td><td>Falling &gt; standing</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24583253_3</td><td>standing &gt;  falling; others</td><td>24583253_analysis_4</td><td>Standing &gt; falling</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24726338</strong> | Pred included: 4 | Manual included (accepted matches only): 4 | Correct overlaps: 2 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24726338/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24726338_analysis_0</td><td>Self &gt; other</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast contrasts self versus third-person/other statements (and the localizer used judgments about another person), so it involves processing about others and meets criteria for perception/understanding of others.</td></tr>
<tr><td>24726338_analysis_1</td><td>Other &gt; self</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast involves third-person (other) versus first-person (self) statements and measures representation of others (out-group/competitor) in a social context, meeting criteria for perception of others.</td></tr>
<tr><td>24726338_analysis_2</td><td>Communication &gt; moral (collapsing across self/other)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Because the contrast collapses across self and other, it does not specifically measure perception or understanding of others (mental state/trait representations) despite the social context.</td></tr>
<tr><td>24726338_analysis_3</td><td>Moral &gt; communication (collapsing across self/other)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Because the contrast is collapsed across self and other, it does not specifically target perception or understanding of others (no explicit other &gt; self contrast), so it does not meet the criterion for perception of others.</td></tr>
<tr><td>24726338_analysis_4</td><td>Self &gt; other (collapsing across moral/communication)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast explicitly compares self versus third-person (other) statements, so it indexes processes of perceiving/understanding others as well as self-other distinctions within a social context; thus it meets criteria for perception and understanding of others.</td></tr>
<tr><td>24726338_analysis_5</td><td>Other &gt; self (collapsing across moral/communication)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The Other&gt;Self contrast measures processing of others’ descriptions (third-person statements) versus self, so it indexes perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24726338_1</td><td>Communication &gt; moral (collapsing across self/other); others</td><td>24726338_analysis_2</td><td>Communication &gt; moral (collapsing across self/other)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24726338_2</td><td>Moral &gt; communication (collapsing across self/other); others</td><td>24726338_analysis_3</td><td>Moral &gt; communication (collapsing across self/other)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24726338_3</td><td>Other &gt; Self; others</td><td>24726338_analysis_1</td><td>Other &gt; self</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24726338_4</td><td>Other &gt; self (collapsing across moral/communication); others</td><td>24726338_analysis_5</td><td>Other &gt; self (collapsing across moral/communication)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24772075</strong> | Pred included: 7 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24772075/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24772075_analysis_0</td><td>MAIN EFFECT: FEEDBACK ONSET: SELF &gt; OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The experiment includes other-related feedback trials and contrasts self vs other, assessing perception and understanding of others’ traits and feedback, meeting the criteria.</td></tr>
<tr><td>24772075_analysis_1</td><td>MAIN EFFECT: FEEDBACK ONSET: OTHER &gt; SELF</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis also measures judgments about another person (other-ratings, updates, neural responses to other-related feedback), satisfying perception/understanding of others criteria.</td></tr>
<tr><td>24772075_analysis_2</td><td>INTERACTION: FEEDBACK ONSET: (SELF &gt; OTHER) × (GERMAN &gt; CHINESE)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis includes ratings and updating about another person and comparison between self and other—measures perception and understanding of others&#x27; traits and mental states.</td></tr>
<tr><td>24772075_analysis_3</td><td>WHOLE-BRAIN CORRELATION WITH OVERALL RELATIVE ABSOLUTE MEAN UPDATES IN THE CONTRAST FEEDBACK ONSET: SELF &gt; OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The experiment includes ratings and feedback about another person and contrasts self vs other, addressing perception and understanding of others (social trait assessment and updating).</td></tr>
<tr><td>24772075_analysis_4</td><td>FEEDBACK RATING (TRIAL-BY-TRIAL CORRELATION): SELF &gt; OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis probes perception and understanding of another person via other-ratings and neural responses to other-related feedback.</td></tr>
<tr><td>24772075_analysis_5</td><td>FEEDBACK DISCREPANCIES (POSITIVE TRIAL-BY-TRIAL CORRELATION): SELF AND OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task measures perception and understanding of another person (initial other-ratings, other-related feedback, updating, and related neural contrasts), meeting criteria for understanding others.</td></tr>
<tr><td>24772075_analysis_6</td><td>FEEDBACK DISCREPANCIES (NEGATIVE TRIAL-BY-TRIAL CORRELATION): SELF AND OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis includes processing of feedback about another person and comparison-related activity (TPJ, STS), measuring perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24772075_2</td><td>OTHER &gt; SELF; others</td><td>24772075_analysis_1</td><td>MAIN EFFECT: FEEDBACK ONSET: OTHER &gt; SELF</td><td>0.453</td><td>1.000</td><td>0.836</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24824165</strong> | Pred included: 2 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24824165/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24824165_analysis_0</td><td>Table 1. Clusters of activation for (atypical-upright+atypical-inverted)-(typical-upright+typical-inverted) assessed by using all data</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast targets perception and understanding of others (action perception/animacy and inference about social interactions/mental states) by comparing atypical vs typical dyadic biological motion.</td></tr>
<tr><td>24824165_analysis_1</td><td>Table 2. Clusters of activation for (atypical-upright+atypical-inverted)-(typical-upright+typical-inverted) assessed by using two separate subsets of data</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task/contrast measures perception and understanding of others (action perception, animacy, and inference about intentions) using point-light dyads and contrasts atypical vs typical interactions.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24824165_1</td><td>(atypical-upright 1 atypical-inverted) 2 (typical-upright 1 typical-inverted); others</td><td>24824165_analysis_1</td><td>Table 2. Clusters of activation for (atypical-upright+atypical-inverted)-(typical-upright+typical-inverted) assessed by using two separate subsets of data</td><td>0.594</td><td>1.000</td><td>0.878</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24825504</strong> | Pred included: 3 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24825504/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24825504_analysis_0</td><td>Brain areas activated in the conjunction triad ∩ self ∩ other.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The study includes an explicit other-referential condition and reports activations in theory-of-mind regions (TPJ, PCC), so it measures perception/understanding of others.</td></tr>
<tr><td>24825504_analysis_1</td><td>Triad &gt; (Self &amp; Other)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Triad trials require representing another person as an intentional agent and dissociating triadic from self/other processing; thus the analysis measures perception and understanding of others (mentalizing/action understanding).</td></tr>
<tr><td>24825504_analysis_2</td><td>Self &gt; (Other &amp; Triad)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the design includes an &#x27;other&#x27; condition, the reported contrast specifically isolates self-related activity (self &gt; other &amp; triad) and is not a contrast measuring perception/understanding of others.</td></tr>
<tr><td>24825504_analysis_3</td><td>Other &gt; (Self &amp; Triad)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The Other &gt; (Self &amp; Triad) contrast directly measures perception and understanding of others (other-referential processing about Angela Merkel), satisfying the construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24825504_1</td><td>Other &gt; (self &amp; triad); others</td><td>24825504_analysis_3</td><td>Other &gt; (Self &amp; Triad)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24825504_3</td><td>Triad &gt; (self &amp; other); others</td><td>24825504_analysis_1</td><td>Triad &gt; (Self &amp; Other)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24825504_4</td><td>triad ∩ self ∩ other; others</td><td>24825504_analysis_0</td><td>Brain areas activated in the conjunction triad ∩ self ∩ other.</td><td>0.488</td><td>1.000</td><td>0.846</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25118071</strong> | Pred included: 1 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25118071/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25118071_analysis_0</td><td>CV&gt;NV contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast and subsequent analyses explicitly probe understanding of others’ emotional and mental states (empathic expression correlated with STS responses), meeting both criteria for perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25118071_1</td><td>CV &gt; NV; others</td><td>25118071_analysis_0</td><td>CV&gt;NV contrast</td><td>0.774</td><td>1.000</td><td>0.932</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25315788</strong> | Pred included: 3 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25315788/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25315788_analysis_0</td><td>A. Univariate analysis: friend minus pc</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast probes understanding of others&#x27; communicative intent/state (friend vs computer) and engages the mentalizing network; it directly measures perception/understanding of others.</td></tr>
<tr><td>25315788_analysis_1</td><td>B. Searchlight-based MVPA: happy versus sad</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires representing content about others&#x27; affective states and engages the mentalizing network; the analysis decodes others-related content and thus meets perception/understanding of others criteria.</td></tr>
<tr><td>25315788_analysis_2</td><td>C. Searchlight-based MVCA: friend minus pc</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast examines how neural representations of messages and their coupling to the mentalizing network change depending on perceived sender (friend vs computer), directly probing perception and understanding of others&#x27; communicative intent/mental states.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25315788_1</td><td>univariate analysis: friend minus pc; others</td><td>25315788_analysis_0</td><td>A. Univariate analysis: friend minus pc</td><td>0.960</td><td>1.000</td><td>0.988</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25697049</strong> | Pred included: 7 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25697049/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25697049_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis directly relates to perception and representation of others (face- and body-selective ROIs and responses to congruent vs incongruent person dyads), satisfying (I1) task relevance and (I2) measurement of perception/understanding of others.</td></tr>
<tr><td>25697049_analysis_1</td><td>Congruent Interactions &gt; Incongruent Interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast examines how perceivers represent and process others in congruent vs incongruent interactions (action perception, animacy/intent inference), fitting perception and understanding of others.</td></tr>
<tr><td>25697049_analysis_2</td><td>Incongruent Interactions &gt; Congruent Interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast examines representations of others (faces/bodies, implied actions, and social interactions) and differences in perceiving others across congruent vs incongruent dyads. This satisfies I1 and I2 for Perception and Understanding of Others (including action perception/animacy).</td></tr>
<tr><td>25697049_analysis_3</td><td>Congruent Interactions &gt; Non-Interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast examines how perceivers represent others’ actions/relations (congruent interactions vs non-interactions) and shows effects in face/body/action-selective regions and pMTG; directly measures perception/understanding of others.</td></tr>
<tr><td>25697049_analysis_4</td><td>Non-Interactions &gt; Congruent Interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task and contrast probe perception and understanding of others (action perception, social interaction cues, and mental-state–relevant information), satisfying this construct.</td></tr>
<tr><td>25697049_analysis_5</td><td>Incongruent Interactions &gt; Non-Interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast directly probes representations and processing of others (their actions, interactions, implied intentions), matching Perception and Understanding of Others. It satisfies I1 and I2.</td></tr>
<tr><td>25697049_analysis_6</td><td>Non-Interactions &gt; Incongruent Interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast evaluates how viewers perceive and represent others in different interaction contexts (action/animacy/mental-state–relevant), matching Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25697049_1</td><td>Incongruent Interactions &gt; Congruent Interactions; others</td><td>25697049_analysis_2</td><td>Incongruent Interactions &gt; Congruent Interactions</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25697049_2</td><td>Incongruent Interactions &gt; Non-Interactions; others</td><td>25697049_analysis_5</td><td>Incongruent Interactions &gt; Non-Interactions</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25729358</strong> | Pred included: 6 | Manual included (accepted matches only): 6 | Correct overlaps: 6 | Match statuses: accepted=6, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25729358/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25729358_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The core manipulation requires inferring the receiver&#x27;s mental state (expectations/beliefs) and engages ToM processes, directly measuring perception and understanding of others (understanding mental states). Meets I1 and I2.</td></tr>
<tr><td>25729358_analysis_1</td><td>Simple deception vs. truth</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrasts explicitly index processes for understanding others&#x27; mental states (right TPJ activation, expectation about receiver choice), directly matching perception/understanding of others.</td></tr>
<tr><td>25729358_analysis_2</td><td>Sophisticated deception vs. truth</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast explicitly probes understanding others&#x27; mental states (anticipating receiver beliefs, ToM), directly satisfying perception and understanding of others.</td></tr>
<tr><td>25729358_analysis_3</td><td>Sophisticated deception vs. simple deception</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Central to the contrast is inferring the receiver&#x27;s beliefs and expectations (theory-of-mind/understanding mental states), so it directly measures perception and understanding of others.</td></tr>
<tr><td>25729358_analysis_4</td><td>analysis_4</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The paradigm explicitly requires representing and reasoning about others&#x27; mental states (expectations of receiver), so the contrast measures understanding of others (ToM).</td></tr>
<tr><td>25729358_analysis_5</td><td>Parametric analysis modeling the incentive to deceive for simple deception trials</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Deception and its parametric modulation depend on inferring the receiver&#x27;s mental state (theory-of-mind); the analysis directly probes understanding of others&#x27; mental states, satisfying the criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25729358_1</td><td>difference between sender and reciever payoff &gt; baseline; others</td><td>25729358_analysis_5</td><td>Parametric analysis modeling the incentive to deceive for simple deception trials</td><td>0.248</td><td>1.000</td><td>0.774</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>25729358_2</td><td>simple &amp; sophisticated deception &gt; truth; others</td><td>25729358_analysis_0</td><td>analysis_0</td><td>0.120</td><td>1.000</td><td>0.736</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>25729358_3</td><td>simple deception &gt; truth; others</td><td>25729358_analysis_1</td><td>Simple deception vs. truth</td><td>0.920</td><td>1.000</td><td>0.976</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25729358_4</td><td>sophisticated deception &gt; simple deception; others</td><td>25729358_analysis_3</td><td>Sophisticated deception vs. simple deception</td><td>0.953</td><td>1.000</td><td>0.986</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25729358_5</td><td>sophisticated deception &gt; truth; others</td><td>25729358_analysis_2</td><td>Sophisticated deception vs. truth</td><td>0.938</td><td>1.000</td><td>0.981</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25729358_6</td><td>truth &gt; simple deception &amp; sophisticated deception; others</td><td>25729358_analysis_4</td><td>analysis_4</td><td>0.100</td><td>1.000</td><td>0.730</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25929599</strong> | Pred included: 5 | Manual included (accepted matches only): 5 | Correct overlaps: 5 | Match statuses: accepted=5, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25929599/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25929599_analysis_0</td><td>Incongruent &gt; Congruent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis assesses representations of others&#x27; actions and emotional/social intent (faces, hand movements) and neural responses to these, matching perception/understanding of others.</td></tr>
<tr><td>25929599_analysis_1</td><td>Happy (Incongruent &gt; Congruent) masked inclusively with Happy (Incongruent &gt; Congruent) &gt; Angry (Incongruent &gt; Congruent)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast examines responses to others&#x27; facial expressions and group-membership (in-/out-group), directly addressing perception and understanding of others (emotion, social group), meeting I1 and I2.</td></tr>
<tr><td>25929599_analysis_2</td><td>Angry (Incongruent &gt; Congruent) masked inclusively with Angry (Incongruent &gt; Congruent) &gt; Happy (Incongruent &gt; Congruent)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast examines responses to others&#x27; emotional expressions and group membership, directly indexing perception and understanding of others&#x27; states and actions.</td></tr>
<tr><td>25929599_analysis_3</td><td>Out-group (Incongruent &gt; Congruent) masked inclusively with Out-group (Incongruent &gt; Congruent) &gt; In-group (Incongruent &gt; Congruent)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis probes representations of others (group membership, emotional expressions) and their influence on action perception/mimicry, directly addressing Perception and Understanding of Others (I1 and I2).</td></tr>
<tr><td>25929599_analysis_4</td><td>In-group (Incongruent &gt; Congruent) masked inclusively with In-group (Incongruent &gt; Congruent) &gt; Out-group (Incongruent &gt; Congruent)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis manipulates and measures perception of others (ethnic group membership, emotional expressions) and how these modulate mimicry, directly probing understanding of others&#x27; social cues.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25929599_1</td><td>Angry (Incongruent &gt; Congruent) masked inclusively with Angry (Incongruent &gt; Congruent) &gt; Happy (Incongruent &gt; Congruent); socialcommunication</td><td>25929599_analysis_2</td><td>Angry (Incongruent &gt; Congruent) masked inclusively with Angry (Incongruent &gt; Congruent) &gt; Happy (Incongruent &gt; Congruent)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25929599_2</td><td>Happy (Incongruent &gt; Congruent) masked inclusively with Happy (Incongruent &gt; Congruent) &gt; Angry (Incongruent &gt; Congruent); socialcommunication</td><td>25929599_analysis_1</td><td>Happy (Incongruent &gt; Congruent) masked inclusively with Happy (Incongruent &gt; Congruent) &gt; Angry (Incongruent &gt; Congruent)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25929599_3</td><td>In-group (Incongruent &gt; Congruent) masked inclusively with In-group (Incongruent &gt; Congruent) &gt; Out-group (Incongruent &gt; Congruent); socialcommunication</td><td>25929599_analysis_4</td><td>In-group (Incongruent &gt; Congruent) masked inclusively with In-group (Incongruent &gt; Congruent) &gt; Out-group (Incongruent &gt; Congruent)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25929599_4</td><td>Incongruent &gt; Congruent; socialcommunication</td><td>25929599_analysis_0</td><td>Incongruent &gt; Congruent</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25929599_5</td><td>Out-group (Incongruent &gt; Congruent) masked inclusively with Out-group (Incongruent &gt; Congruent) &gt; In-group (Incongruent &gt; Congruent);socialcommunication</td><td>25929599_analysis_3</td><td>Out-group (Incongruent &gt; Congruent) masked inclusively with Out-group (Incongruent &gt; Congruent) &gt; In-group (Incongruent &gt; Congruent)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26235682</strong> | Pred included: 1 | Manual included (accepted matches only): 4 | Correct overlaps: 1 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26235682/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26235682_analysis_0</td><td>Positive vs. Neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis involves interpreting reputational judgments provided by others (faces and comments), which entails perceiving and understanding others’ evaluations and traits.</td></tr>
<tr><td>26235682_analysis_1</td><td>Negative vs. Neutral</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The primary outcome is the subject&#x27;s valuation of comments about themselves rather than representing or inferring others’ mental states or traits; it does not primarily measure perception/understanding of others.</td></tr>
<tr><td>26235682_analysis_2</td><td>(Positive &gt; Neutral) inclusively masked with (Negative &gt; Neutral)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although faces are shown, the analysis focuses on participants&#x27; response to reputation about themselves rather than representation or inference about others&#x27; mental states or traits; it does not primarily measure perception/understanding of others.</td></tr>
<tr><td>26235682_analysis_3</td><td>(Positive vs. Neutral) vs. (Negative vs. Neutral)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The primary focus is on the subject’s subjective pleasantness in response to reputation directed at the self, not on representing or reasoning about others’ states or traits; thus it does not target perception/understanding of others as defined.</td></tr>
<tr><td>26235682_analysis_4</td><td>(Negative vs. Neutral) vs. (Positive vs. Neutral)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The primary focus is on the subject&#x27;s response to reputation about themselves rather than representing or reasoning about others&#x27; mental states or traits; it does not target perception/understanding of others as defined by the annotation.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26235682_1</td><td>(Positive &gt; Neutral) inclusively masked with (Negative &gt; Neutral); others</td><td>26235682_analysis_2</td><td>(Positive &gt; Neutral) inclusively masked with (Negative &gt; Neutral)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26235682_2</td><td>(Positive vs. Neutral) &gt; (Negative vs. Neutral); others</td><td>26235682_analysis_3</td><td>(Positive vs. Neutral) vs. (Negative vs. Neutral)</td><td>0.958</td><td>1.000</td><td>0.988</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26235682_3</td><td>Negative &gt; Neutral; others</td><td>26235682_analysis_1</td><td>Negative vs. Neutral</td><td>0.895</td><td>1.000</td><td>0.968</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26235682_4</td><td>Positive &gt; Neutral; others</td><td>26235682_analysis_0</td><td>Positive vs. Neutral</td><td>0.895</td><td>1.000</td><td>0.968</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26254589</strong> | Pred included: 6 | Manual included (accepted matches only): 4 | Correct overlaps: 4 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26254589/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26254589_analysis_0</td><td>EmpaToM: emotionally negative &gt; neutral video</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The EmpaToM explicitly measures understanding others&#x27; emotional and mental states (empathy and ToM), satisfying perception/understanding of others.</td></tr>
<tr><td>26254589_analysis_1</td><td>EmpaToM: ToM &gt; nonToM question</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The ToM contrast directly indexes understanding others&#x27; mental states, which maps onto the &#x27;Understanding Mental States&#x27; subconstruct of Perception and Understanding of Others. This satisfies I1 (task measures perception/understanding of others) and I2 (contrast measures that construct).</td></tr>
<tr><td>26254589_analysis_2</td><td>EmpaToM: emotionally negative &gt; neutral video</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast indexes perception and understanding of others&#x27; emotional states (empathy) and is directly relevant to the &#x27;Understanding Mental States&#x27; subconstruct; it meets both inclusion criteria.</td></tr>
<tr><td>26254589_analysis_3</td><td>EmpaToM: ToM &gt; nonToM questions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The ToM&gt;nonToM contrast specifically measures understanding others&#x27; mental states, fitting Perception and Understanding of Others (Understanding Mental States).</td></tr>
<tr><td>26254589_analysis_4</td><td>EmpaToM: (emotional &gt; neutral video) &gt; (ToM &gt; nonToM questions)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The EmpaToM specifically targets understanding others&#x27; emotional states (empathy) and mental states (ToM), matching the &#x27;Perception and Understanding of Others&#x27; construct, including the &#x27;Understanding Mental States&#x27; subconstruct.</td></tr>
<tr><td>26254589_analysis_5</td><td>EmpaToM: (ToM &gt; nonToM questions) &gt; (emotionally negative &gt; neutral video)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>EmpaToM explicitly measures understanding others&#x27; emotional states (empathy) and mental states (ToM). The ToM contrasts and empathy contrasts directly target ‘Understanding Mental States’, satisfying the criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26254589_1</td><td>EmpaToM: (ToM N nonToM questions) &gt; (emotionally negative N neutral video); others</td><td>26254589_analysis_5</td><td>EmpaToM: (ToM &gt; nonToM questions) &gt; (emotionally negative &gt; neutral video)</td><td>0.973</td><td>1.000</td><td>0.992</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26254589_2</td><td>EmpaToM: (emotional N neutral video) &gt; (ToM N nonToM questions); others</td><td>26254589_analysis_4</td><td>EmpaToM: (emotional &gt; neutral video) &gt; (ToM &gt; nonToM questions)</td><td>0.968</td><td>1.000</td><td>0.990</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26254589_3</td><td>EmpaToM: ToM &gt; nonToM questions; others</td><td>26254589_analysis_3</td><td>EmpaToM: ToM &gt; nonToM questions</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26254589_4</td><td>EmpaToM: emotionally negative &gt; neutral video; others</td><td>26254589_analysis_2</td><td>EmpaToM: emotionally negative &gt; neutral video</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26301900</strong> | Pred included: 2 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26301900/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26301900_analysis_0</td><td>ToM &gt; R</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The ToM task explicitly measures reasoning about others&#x27; mental states (Understanding Mental States subconstruct), satisfying the Perception and Understanding of Others inclusion criteria I1 and I2.</td></tr>
<tr><td>26301900_analysis_1</td><td>Bayesian analysis to evaluate the theory that the response in the mPFC is the same as the response in the right superior temporal gyrus during “ToM &gt; R” (volume of interest (VOI): mPFC-coordinates based on literature, right posterior superior temporal gyrus coordinates based on our sample (MNI: 54-48 16), radius 15 mm; mean and standard deviation (SD) represent the average BOLD-signal within a volume of interest).</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Theory of Mind explicitly measures understanding and reasoning about others&#x27; mental states (subconstruct: Understanding Mental States); the ToM &gt; R contrast directly assesses this construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26301900_1</td><td>ToM &gt; R; others</td><td>26301900_analysis_0</td><td>ToM &gt; R</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26323252</strong> | Pred included: 5 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=1, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26323252/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> ingroup members &gt; outgroup members; others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26323252_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrasts (pain vs neutral; pain×group) index representations of others’ emotional/pain states and how group membership modulates those representations, directly matching Perception and Understanding of Others.</td></tr>
<tr><td>26323252_analysis_1</td><td>Main effect of condition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis targets representations of others’ emotional states (empathy for pain) and group-based perception (ingroup/outgroup), directly addressing perception and understanding of others.</td></tr>
<tr><td>26323252_analysis_2</td><td>Main effect of gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Empathy for pain explicitly involves representing and evaluating others’ emotional/affective states; the pain×group and gender effects reflect perception and understanding of others.</td></tr>
<tr><td>26323252_analysis_3</td><td>Pain×condition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Pain×condition specifically probes representations of others’ emotional/pain states modulated by group membership (ingroup vs outgroup), directly matching Perception and Understanding of Others. Both I1 and I2 are satisfied.</td></tr>
<tr><td>26323252_analysis_4</td><td>Pain×gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Empathy for pain directly involves representing and assessing others’ emotional/physical states; the Pain×gender analysis addresses perception/understanding of others, satisfying this construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26323252_1</td><td>ingroup members &gt; outgroup members; others</td><td>26323252_analysis_2</td><td>Main effect of gender</td><td>0.218</td><td>0.667</td><td>0.532</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>26323252_2</td><td>pain stimuli &gt; neutral stimuli; others</td><td>26323252_analysis_0</td><td>analysis_0</td><td>0.300</td><td>1.000</td><td>0.790</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>26323252_3</td><td>pain x condition &gt; pain x gender; others</td><td>26323252_analysis_3</td><td>Pain×condition</td><td>0.565</td><td>0.778</td><td>0.714</td><td>uncertain</td><td>coord_count_mismatch, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26803059</strong> | Pred included: 10 | Manual included (accepted matches only): 7 | Correct overlaps: 7 | Match statuses: accepted=7, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26803059/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26803059_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Affective ToM explicitly involves reasoning about others&#x27; emotional states; the contrast directly measures perception and understanding of others&#x27; mental states (EFE vs no EFE during ToM judgments).</td></tr>
<tr><td>26803059_analysis_1</td><td>EFE &gt; no EFE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis directly measures understanding of others&#x27; mental and emotional states (affective ToM) and uses stimuli (EFE) to probe perception/understanding of others.</td></tr>
<tr><td>26803059_analysis_2</td><td>No EFE &gt; EFE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Affective ToM explicitly measures reasoning about others&#x27; emotional states (understanding mental states), satisfying perception/understanding of others.</td></tr>
<tr><td>26803059_analysis_3</td><td>Correlation of differential activation EFE &gt; no EFE with RT reduction by EFE, P &lt; 0.001 (uncorrected); k = 10</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task explicitly measures understanding of others&#x27; emotional states (affective ToM) and contrasts presence vs absence of facial expressions, directly indexing perception and understanding of others.</td></tr>
<tr><td>26803059_analysis_4</td><td>Correlation of differential activation no EFE &gt; EFE with RT reduction by EFE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Affective ToM judgments explicitly measure inference about others’ emotional states (understanding mental states), and the contrast examines how EFE influences that process.</td></tr>
<tr><td>26803059_analysis_5</td><td>Greater EFE effects with explicit demand of affective ToM judgments (EFE ToM - no EFE ToM) - (EFE no ToM - no EFE no ToM)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis explicitly measures understanding of others&#x27; mental and emotional states (affective ToM), satisfying criteria for perception and understanding of others.</td></tr>
<tr><td>26803059_analysis_6</td><td>Greater EFE effects with explicit demand of affective ToM judgments (EFE no ToM - no EFE no ToM) - (EFE ToM - no EFE ToM)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis explicitly measures understanding others&#x27; mental and affective states (affective ToM), satisfying perception and understanding of others.</td></tr>
<tr><td>26803059_analysis_7</td><td>PPI</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Affective ToM directly involves representing and reasoning about others&#x27; emotional states; the task and contrasts measure understanding of others&#x27; mental states.</td></tr>
<tr><td>26803059_analysis_8</td><td>Pos. correlation: affective ToM matching with consensus judgments and PPI effects</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The primary task is affective ToM—inferring others&#x27; emotional states—and the analysis examines matching with consensus and neural connectivity, directly measuring Perception and Understanding of Others (Understanding Mental States).</td></tr>
<tr><td>26803059_analysis_9</td><td>Neg. correlation: affective ToM matching with consensus judgments and PPI effects</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Affective ToM judgments directly measure understanding and reasoning about others&#x27; emotional states (Understanding Mental States subconstruct).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26803059_1</td><td>(EFE ToM − no EFE ToM) − (EFE no ToM − no EFE no ToM); others</td><td>26803059_analysis_5</td><td>Greater EFE effects with explicit demand of affective ToM judgments (EFE ToM - no EFE ToM) - (EFE no ToM - no EFE no ToM)</td><td>0.575</td><td>0.900</td><td>0.802</td><td>accepted</td><td>high_coord_match</td></tr><tr><td>26803059_2</td><td>(EFE no ToM − no EFE no ToM) − (EFE ToM − no EFE ToM); others</td><td>26803059_analysis_6</td><td>Greater EFE effects with explicit demand of affective ToM judgments (EFE no ToM - no EFE no ToM) - (EFE ToM - no EFE ToM)</td><td>0.575</td><td>1.000</td><td>0.872</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26803059_3</td><td>3rdppA &gt; 1stppV; others</td><td>26803059_analysis_0</td><td>analysis_0</td><td>0.182</td><td>1.000</td><td>0.755</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>26803059_4</td><td>EFE &gt; no EFE; others</td><td>26803059_analysis_1</td><td>EFE &gt; no EFE</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26803059_5</td><td>EFE &gt; no EFE with RT reduction; others</td><td>26803059_analysis_3</td><td>Correlation of differential activation EFE &gt; no EFE with RT reduction by EFE, P &lt; 0.001 (uncorrected); k = 10</td><td>0.489</td><td>1.000</td><td>0.847</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>26803059_6</td><td>No EFE &gt; EFE; others</td><td>26803059_analysis_2</td><td>No EFE &gt; EFE</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26803059_7</td><td>no EFE &gt; EFE with RT reduction; others</td><td>26803059_analysis_4</td><td>Correlation of differential activation no EFE &gt; EFE with RT reduction by EFE</td><td>0.566</td><td>1.000</td><td>0.870</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26812250</strong> | Pred included: 10 | Manual included (accepted matches only): 11 | Correct overlaps: 10 | Match statuses: accepted=11, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26812250/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26812250_analysis_0</td><td>Negative vs. positive performance feedback.</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast examines responses to feedback rather than representation or inference about others’ mental states; it does not directly measure understanding of others as defined here.</td></tr>
<tr><td>26812250_analysis_1</td><td>(A) Emotional empathic (EE) &gt; emotional unempathic (EN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast entails processing of others’ empathic/unempathic statements and likely engages understanding of others’ mental/emotional states (mentalizing), so it measures perception/understanding of others.</td></tr>
<tr><td>26812250_analysis_2</td><td>(B) Unempathic (EN + CN) &gt; empathic (EE + CE)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrasts probe understanding and inference about others&#x27; communicative intent and mental states (cognitive vs. emotional empathy), matching perception/understanding of others.</td></tr>
<tr><td>26812250_analysis_3</td><td>(C) Cognitive unempathic (CN) &gt; cognitive empathic (CE)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Cognitive empathy and contrasts like CN&gt;CE index understanding and inference about others&#x27; mental/emotional states (mentalizing). This satisfies the Perception and Understanding of Others criteria.</td></tr>
<tr><td>26812250_analysis_4</td><td>(D) Empathic (EE + CE) &gt; high level baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The paradigm involves processing others&#x27; empathic vs. unempathic responses and distinguishes cognitive (mentalizing) and emotional empathy—this indexes perception/understanding of others&#x27; mental states.</td></tr>
<tr><td>26812250_analysis_5</td><td>(E) Emotional empathic (EE) &gt; high level baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrasts and discussion explicitly address inferring the speaker’s intention and differentiating cognitive vs. emotional empathy (understanding others’ mental states), satisfying Perception and Understanding of Others.</td></tr>
<tr><td>26812250_analysis_6</td><td>(F) Cognitive empathic (CE) &gt; high level baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Cognitive empathy involves understanding others&#x27; mental states; the contrast examines neural processing related to being presented with cognitive‑empathic content, indexing perception/understanding of others.</td></tr>
<tr><td>26812250_analysis_7</td><td>(A) Emotional (EE + EN) &gt; cognitive (CE + CN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast examines processing of emotional vs cognitive empathy—constructs that involve understanding others&#x27; emotional/mental states (mentalizing), meeting the criteria for perception and understanding of others.</td></tr>
<tr><td>26812250_analysis_8</td><td>(B) Emotional empathic (EE) &gt; cognitive empathic (CE)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task requires representing and interpreting others&#x27; empathic intent/state (mentalizing) and contrasts cognitive vs emotional empathy, directly measuring perception/understanding of others&#x27; mental states.</td></tr>
<tr><td>26812250_analysis_9</td><td>(C) Emotional unempathic (EN) &gt; cognitive unempathic (CN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis involves representing and interpreting others&#x27; emotional stance (emotional vs cognitive unempathic comments), indexing understanding of others&#x27; mental/emotional states.</td></tr>
<tr><td>26812250_analysis_10</td><td>(D) Cognitive unempathic (CN) &gt; emotional unempathic (EN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast concerns understanding and evaluating others’ empathic vs unempathic statements (mentalizing/understanding others’ states), meeting criteria for Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26812250_1</td><td>Cognitive empathic (CE) &gt; high level baseline; others</td><td>26812250_analysis_6</td><td>(F) Cognitive empathic (CE) &gt; high level baseline</td><td>0.957</td><td>1.000</td><td>0.987</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_10</td><td>Unempathic (EN + CN) &gt; empathic (EE + CE); others</td><td>26812250_analysis_2</td><td>(B) Unempathic (EN + CN) &gt; empathic (EE + CE)</td><td>0.953</td><td>1.000</td><td>0.986</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_11</td><td>negative vs positive performance; others</td><td>26812250_analysis_0</td><td>Negative vs. positive performance feedback.</td><td>0.853</td><td>1.000</td><td>0.956</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_2</td><td>Cognitive unempathic (CN) &gt; cognitive empathic (CE); others</td><td>26812250_analysis_3</td><td>(C) Cognitive unempathic (CN) &gt; cognitive empathic (CE)</td><td>0.962</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_3</td><td>Cognitive unempathic (CN) &gt; emotional unempathic (EN); others</td><td>26812250_analysis_10</td><td>(D) Cognitive unempathic (CN) &gt; emotional unempathic (EN)</td><td>0.964</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_4</td><td>Emotional (EE + EN) &gt; cognitive (CE + CN); others</td><td>26812250_analysis_7</td><td>(A) Emotional (EE + EN) &gt; cognitive (CE + CN)</td><td>0.953</td><td>1.000</td><td>0.986</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_5</td><td>Emotional Empathic (EE) &gt; emotional unempathic (EN); others</td><td>26812250_analysis_1</td><td>(A) Emotional empathic (EE) &gt; emotional unempathic (EN)</td><td>0.962</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_6</td><td>Emotional empathic (EE) &gt; cognitive empathic (CE); others</td><td>26812250_analysis_8</td><td>(B) Emotional empathic (EE) &gt; cognitive empathic (CE)</td><td>0.961</td><td>1.000</td><td>0.988</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_7</td><td>Emotional empathic (EE) &gt; high level baseline; others</td><td>26812250_analysis_5</td><td>(E) Emotional empathic (EE) &gt; high level baseline</td><td>0.957</td><td>1.000</td><td>0.987</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_8</td><td>Emotional unempathic (EN) &gt; cognitive unempathic (CN); others</td><td>26812250_analysis_9</td><td>(C) Emotional unempathic (EN) &gt; cognitive unempathic (CN)</td><td>0.964</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_9</td><td>Empathic (EE + CE) &gt; high level baseline; others</td><td>26812250_analysis_4</td><td>(D) Empathic (EE + CE) &gt; high level baseline</td><td>0.952</td><td>1.000</td><td>0.986</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26908320</strong> | Pred included: 6 | Manual included (accepted matches only): 6 | Correct overlaps: 6 | Match statuses: accepted=6, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26908320/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26908320_analysis_0</td><td>A. Increased activation with more racial trust disparity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis explicitly investigates representations and judgments about others (race perception, trustworthiness, differentiation), directly meeting inclusion criteria for perception and understanding of others.</td></tr>
<tr><td>26908320_analysis_1</td><td>Increased activation with less racial trust disparity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Directly assesses perception and understanding of others&#x27; traits (trustworthiness) and differentiation of faces, matching this construct.</td></tr>
<tr><td>26908320_analysis_2</td><td>B. Increased connectivity with orbitofrontal cortex (18, 66, 0) with less racial trust disparity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis assesses representations and judgments about others&#x27; traits (trustworthiness) and differentiation—directly measures perception/understanding of others.</td></tr>
<tr><td>26908320_analysis_3</td><td>A. Increased activation with more differentiation disparity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Study measures representations and judgments about others&#x27; traits (trustworthiness) and differentiation of faces—directly matches perception and understanding of others.</td></tr>
<tr><td>26908320_analysis_4</td><td>Increased activation with less differentiation disparity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Directly assesses perception and understanding of others (evaluating trustworthiness and differentiating faces by race), satisfying the criteria for this construct.</td></tr>
<tr><td>26908320_analysis_5</td><td>B. Increased connectivity with dorsolateral prefrontal seed (24, 42, 21) with less differentiation disparity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The study examines representations and judgments about others’ traits (trustworthiness) and differentiation of other-race faces—matches Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26908320_1</td><td>Black &gt; White faces: A. Increased activation with less differentiation disparity; socialcommunication</td><td>26908320_analysis_4</td><td>Increased activation with less differentiation disparity</td><td>0.824</td><td>1.000</td><td>0.947</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26908320_2</td><td>Black &gt; White faces: A. Increased activation with less racial trust disparities; socialcommunication</td><td>26908320_analysis_1</td><td>Increased activation with less racial trust disparity</td><td>0.788</td><td>1.000</td><td>0.936</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26908320_3</td><td>Black &gt; White faces: A. Increased activation with more differentiation disparity; socialcommunication</td><td>26908320_analysis_3</td><td>A. Increased activation with more differentiation disparity</td><td>0.849</td><td>1.000</td><td>0.955</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26908320_4</td><td>Black &gt; White faces: A. Increased activation with more racial trust disparities; socialcommunication</td><td>26908320_analysis_0</td><td>A. Increased activation with more racial trust disparity</td><td>0.815</td><td>1.000</td><td>0.944</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26908320_5</td><td>Black &gt; White faces: B. Increassed connectivity with dorsolateral prefrontal seed (24, 42, 21) with less differentiation disparity; socialcommunication</td><td>26908320_analysis_5</td><td>B. Increased connectivity with dorsolateral prefrontal seed (24, 42, 21) with less differentiation disparity</td><td>0.908</td><td>1.000</td><td>0.972</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26908320_6</td><td>Black &gt; White faces: B. Increased connectivity with orbitofrontal cortex (18, 66, 0) with less racial trust disparity; socialcommunication</td><td>26908320_analysis_2</td><td>B. Increased connectivity with orbitofrontal cortex (18, 66, 0) with less racial trust disparity</td><td>0.901</td><td>1.000</td><td>0.970</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27039141</strong> | Pred included: 3 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=1, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27039141/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Physical &gt; Meaning; others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27039141_analysis_0</td><td>A Meaning &gt; Physical</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TN</span></td><td></td><td>Although social-cognition regions associated with understanding others are activated, the contrast itself does not explicitly measure perception or reasoning about other agents or their mental states; it compares interpretive attitudes toward objects.</td></tr>
<tr><td>27039141_analysis_1</td><td>B Physical &gt; Meaning</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The meaning task and own vs other contrasts implicate TPJ/mPFC and involve reasoning about others’ meanings/intentions, satisfying Perception and Understanding of Others (I1, I2).</td></tr>
<tr><td>27039141_analysis_2</td><td>C Own &gt; Other (meaning related task)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast contrasts own versus others&#x27; models in a meaning-related social task and engages regions (TPJ, mPFC) involved in representing others’ mental states; thus it measures perception/understanding of others, satisfying I1 and I2.</td></tr>
<tr><td>27039141_analysis_3</td><td>D Own &gt; Other × Collective &gt; Individual (meaning related task)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The own vs other dimension directly assesses perception/understanding of others’ vs one’s own artifacts; the interaction tests how others’ models differ from own (within collective/individual), meeting I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27039141_1</td><td>Meaning &gt; Physical; others</td><td>27039141_analysis_0</td><td>A Meaning &gt; Physical</td><td>0.947</td><td>0.500</td><td>0.634</td><td>uncertain</td><td>coord_count_mismatch</td></tr><tr><td>27039141_2</td><td>Own &gt; Other (meaning related task); others</td><td>27039141_analysis_2</td><td>C Own &gt; Other (meaning related task)</td><td>0.971</td><td>0.750</td><td>0.816</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>27039141_3</td><td>Own &gt; Other × Collective &gt; Individual (meaning related task); others</td><td>27039141_analysis_3</td><td>D Own &gt; Other × Collective &gt; Individual (meaning related task)</td><td>0.984</td><td>0.667</td><td>0.762</td><td>accepted</td><td>coord_count_mismatch</td></tr><tr><td>27039141_4</td><td>Physical &gt; Meaning; others</td><td>27039141_analysis_1</td><td>B Physical &gt; Meaning</td><td>0.947</td><td>0.333</td><td>0.518</td><td>unmatched</td><td>coord_count_mismatch, low_coord_high_name, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27096431</strong> | Pred included: 11 | Manual included (accepted matches only): 8 | Correct overlaps: 7 | Match statuses: accepted=8, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27096431/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27096431_analysis_0</td><td>Aggressive reaction to provoking opponent &gt; non aggressive reaction to non-provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast contrasts responses to a provoking versus non-provoking opponent, which depends on perceiving and interpreting others’ behavior (provocation). It therefore measures perception/understanding of others.</td></tr>
<tr><td>27096431_analysis_1</td><td>Non aggressive reaction to non-provoking opponent &gt; aggressive reaction to provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast compares behavior toward a provoking versus non‑provoking opponent, reflecting perception/understanding of others’ provocativeness and social intent.</td></tr>
<tr><td>27096431_analysis_2</td><td>Won &gt; lost</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast examines winning versus losing outcomes and reward-related activity rather than representations or inference about others’ mental states or traits, so it does not meet perception-of-others criteria.</td></tr>
<tr><td>27096431_analysis_3</td><td>Lost &gt; won</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis involves processing others&#x27; actions (opponent choices and outcomes) and the neural response to losing likely reflects perception/interpretation of others’ behavior and anticipated feedback, meeting the criteria for understanding others.</td></tr>
<tr><td>27096431_analysis_4</td><td>Won against the provoking opponent &gt; won against the non-provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast compares brain responses when interacting with a provoking versus a non‑provoking opponent, reflecting processing of others’ behavior/traits (provocativeness). The task is social (competition) and the contrast addresses perception/understanding of others, satisfying I1 and I2.</td></tr>
<tr><td>27096431_analysis_5</td><td>Won against the non-provoking opponent &gt; won against the provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The paradigm requires perceiving and interpreting opponents as provoking or non-provoking and the contrast compares outcomes against different opponent types—this indexes perception and understanding of others’ social behavior.</td></tr>
<tr><td>27096431_analysis_6</td><td>Lost against the provoking opponent &gt; lost against the non-provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast explicitly compares responses to a provoking versus non-provoking opponent, indexing perception/understanding of others’ social behavior (provocation), meeting I1 and I2.</td></tr>
<tr><td>27096431_analysis_7</td><td>Lost against the non-provoking opponent &gt; lost against the provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast differentiates brain responses to outcomes depending on opponent type, reflecting perception/understanding of others’ provocative behavior; it meets both inclusion criteria.</td></tr>
<tr><td>27096431_analysis_8</td><td>Retaliation independent of opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The TAP requires participants to perceive and respond to opponents’ provocative behavior; the analysis targets behavior driven by perception of others, meeting criteria for Perception and Understanding of Others.</td></tr>
<tr><td>27096431_analysis_9</td><td>Retaliation interacting with provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast explicitly contrasts provoking vs non‑provoking opponents and participants’ behavioral adaptation—this assesses perception/understanding of others’ provocation states.</td></tr>
<tr><td>27096431_analysis_10</td><td>Retaliation interacting with non-provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis contrasts interactions with provoking vs non-provoking opponents and examines participants’ behavioral responses to opponents’ actions, which reflects perception/understanding of others’ behavior and states.</td></tr>
<tr><td>27096431_analysis_11</td><td>Won</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task contrasts provoking vs non-provoking opponents and participants report perceiving one opponent as more provocative; the analysis measures response to others’ behavior and thus assesses perception/understanding of others.</td></tr>
<tr><td>27096431_analysis_12</td><td>Won against provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the paradigm involves interacting with and perceiving opponents, the &#x27;won against provoking opponent&#x27; outcome contrast does not directly measure representations or inferences about others&#x27; mental states or traits; it primarily indexes outcome-related (winning) neural responses.</td></tr>
<tr><td>27096431_analysis_13</td><td>Won against non-provoking opponent</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>While the paradigm involves interacting with opponents, this specific contrast (winning against non-provoking opponent) does not specifically measure representations or inferences about others’ mental states or traits; it is an outcome/reward contrast rather than a mentalizing/other-perception contrast.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27096431_1</td><td>Aggressive reaction to provoking opponent &gt; non aggressive reaction to non-provoking opponent; others</td><td>27096431_analysis_0</td><td>Aggressive reaction to provoking opponent &gt; non aggressive reaction to non-provoking opponent</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27096431_2</td><td>Lost &gt; Won; others</td><td>27096431_analysis_3</td><td>Lost &gt; won</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27096431_3</td><td>Lost against the non-provoking opponent &gt; lost against the provoking opponent; others</td><td>27096431_analysis_7</td><td>Lost against the non-provoking opponent &gt; lost against the provoking opponent</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27096431_4</td><td>Non aggressive reaction to non-provoking opponent &gt; aggressive reaction to provoking opponent; others</td><td>27096431_analysis_1</td><td>Non aggressive reaction to non-provoking opponent &gt; aggressive reaction to provoking opponent</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27096431_5</td><td>Retaliation independent of opponent; others</td><td>27096431_analysis_8</td><td>Retaliation independent of opponent</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27096431_6</td><td>Retaliation interacting with non-provoking opponent; others</td><td>27096431_analysis_10</td><td>Retaliation interacting with non-provoking opponent</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27096431_7</td><td>Retaliation interacting with provoking opponent; others</td><td>27096431_analysis_9</td><td>Retaliation interacting with provoking opponent</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27096431_8</td><td>Won &gt; Lost; others</td><td>27096431_analysis_2</td><td>Won &gt; lost</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27236373</strong> | Pred included: 8 | Manual included (accepted matches only): 6 | Correct overlaps: 6 | Match statuses: accepted=6, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27236373/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27236373_analysis_0</td><td>Time-point 2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast specifically measures perception and understanding of others&#x27; mental states (Theory of Mind); aligns with inclusion criteria for Understanding Mental States.</td></tr>
<tr><td>27236373_analysis_1</td><td>Response period</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast targets understanding others&#x27; mental states (Theory of Mind), directly matching the &#x27;Understanding Mental States&#x27; subconstruct; satisfies both inclusion criteria.</td></tr>
<tr><td>27236373_analysis_2</td><td>Time-point 1</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The REMICS Social vs Physical contrast at Time-point 1 probes understanding of others&#x27; mental states (Theory of Mind), matching Perception and Understanding of Others.</td></tr>
<tr><td>27236373_analysis_3</td><td>Time-point 2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The REMICS task explicitly measures understanding of others&#x27; mental states (Theory of Mind), matching the &#x27;Perception and Understanding of Others&#x27; construct, including &#x27;Understanding Mental States&#x27;.</td></tr>
<tr><td>27236373_analysis_4</td><td>Response period</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>REMICS directly manipulates and measures understanding of others&#x27; mental states (Theory of Mind contrasts such as Soc&gt;Phy, SocAdj&gt;PhyConf) and thus fits Perception and Understanding of Others.</td></tr>
<tr><td>27236373_analysis_5</td><td>Time-point 2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The REMICS contrasts explicitly test representation and understanding of others&#x27; mental states (Soc &gt; Phy, SocAdj etc.), directly matching &quot;Understanding Mental States&quot; within Perception and Understanding of Others.</td></tr>
<tr><td>27236373_analysis_6</td><td>Response period</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The REMICS contrasts explicitly test Perception and Understanding of Others (mental state inferences: Soc &gt; Phy, SocAdj &gt; PhyConf, and related analyses), satisfying this construct.</td></tr>
<tr><td>27236373_analysis_7</td><td>analysis_7</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study explicitly targets Understanding Mental States (inferring others&#x27; intentions and beliefs), matching this construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27236373_1</td><td>Adj &gt; Conf, Response Period; others</td><td>27236373_analysis_6</td><td>Response period</td><td>0.714</td><td>1.000</td><td>0.914</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27236373_2</td><td>Soc &gt; Phy, Response Period; others</td><td>27236373_analysis_4</td><td>Response period</td><td>0.732</td><td>1.000</td><td>0.920</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27236373_3</td><td>Soc &gt; Phy, Time-Point 1; others</td><td>27236373_analysis_2</td><td>Time-point 1</td><td>0.686</td><td>1.000</td><td>0.906</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27236373_4</td><td>Soc &gt; Phy, Time-Point 2; others</td><td>27236373_analysis_3</td><td>Time-point 2</td><td>0.686</td><td>1.000</td><td>0.906</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27236373_5</td><td>SocAdj &gt; PhyConf, Response Period; others</td><td>27236373_analysis_1</td><td>Response period</td><td>0.625</td><td>1.000</td><td>0.887</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27236373_6</td><td>SocAdj &gt; PhyConf, Time-Point 2; others</td><td>27236373_analysis_0</td><td>Time-point 2</td><td>0.571</td><td>1.000</td><td>0.871</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27375449</strong> | Pred included: 7 | Manual included (accepted matches only): 4 | Correct overlaps: 4 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27375449/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27375449_analysis_0</td><td>Angry &gt; Joyful session</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Directly assesses perception and understanding of others (action perception and recognition of emotional states) via angry vs joyful contrasts and connectivity among regions implicated in social perception.</td></tr>
<tr><td>27375449_analysis_1</td><td>Grasping &gt; Faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task explicitly investigates perception and understanding of others (action perception and emotional context), and the contrast involves action vs face processing—this directly taps Perception and Understanding of Others (including Action Perception).</td></tr>
<tr><td>27375449_analysis_2</td><td>Faces &gt; Grasping</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Study measures perception and understanding of others (action perception and emotional state recognition) via faces vs. grasping contrasts and connectivity analyses; meets criteria.</td></tr>
<tr><td>27375449_analysis_3</td><td>Interaction analysis between grasping and neutral grasping</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task explicitly examines perception and understanding of others (action perception and emotional context effects on interpreting others’ actions).</td></tr>
<tr><td>27375449_analysis_4</td><td>Angry &gt; Joyful run</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis explicitly probes perception and understanding of others’ emotional states and action perception (angry vs joyful grasping), matching this construct.</td></tr>
<tr><td>27375449_analysis_5</td><td>Angry grasping &gt; Joyful grasping</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task explicitly probes perception and understanding of others’ emotional states and actions (action perception and emotion recognition); the contrast (angry &gt; joyful grasping) directly measures this construct.</td></tr>
<tr><td>27375449_analysis_6</td><td>PPI: seed in the right Insula</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task measures perception and understanding of others (action perception and emotion recognition) and uses contrasts (angry vs joyful) directly probing this construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27375449_1</td><td>Angry &gt; Joyful session; socialcommunication</td><td>27375449_analysis_0</td><td>Angry &gt; Joyful session</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27375449_2</td><td>Faces &gt; Grasping; socialcommunication</td><td>27375449_analysis_2</td><td>Faces &gt; Grasping</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27375449_3</td><td>Grasping &gt; Faces; socialcommunication</td><td>27375449_analysis_1</td><td>Grasping &gt; Faces</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27375449_4</td><td>Interaction analysis between grasping and neutral grasping; socialcommunication</td><td>27375449_analysis_3</td><td>Interaction analysis between grasping and neutral grasping</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27477020</strong> | Pred included: 2 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27477020/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27477020_analysis_0</td><td>S-performance</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis examines representations of others’ performance (O-performance) in area 9 and how these are used to judge others and influence self-judgments, directly assessing perception/understanding of others.</td></tr>
<tr><td>27477020_analysis_1</td><td>O-performance</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis includes O-ability ratings, tracking of others’ performance and prediction errors (area 9), directly measuring perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27477020_1</td><td>O-performance &gt; S - performance; others</td><td>27477020_analysis_1</td><td>O-performance</td><td>0.591</td><td>1.000</td><td>0.877</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27477020_2</td><td>S-performance &gt; O - performance; others</td><td>27477020_analysis_0</td><td>S-performance</td><td>0.591</td><td>1.000</td><td>0.877</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27494142</strong> | Pred included: 6 | Manual included (accepted matches only): 4 | Correct overlaps: 4 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27494142/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27494142_analysis_0</td><td>CC-A</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The PD task involves responses to partners’ cooperative vs non-cooperative behavior and therefore assesses perception/understanding of others’ actions/intentions; the contrast (CC vs others) addresses others’ behavior.</td></tr>
<tr><td>27494142_analysis_1</td><td>A-CC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The paradigm requires evaluating partner choices and contrasts neural responses to partner cooperation (CC) versus other outcomes, indexing representations of others’ behavior/states; meets criteria for Perception/Understanding of Others.</td></tr>
<tr><td>27494142_analysis_2</td><td>Gain-Loss</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis assesses participants’ responses to partners’ cooperative/defective behavior (perception and understanding of others), matching inclusion examples.</td></tr>
<tr><td>27494142_analysis_3</td><td>Loss-Gain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires representing and responding to others’ cooperative/defective choices (partner’s actions) and contrasts reflect understanding of others’ behavior, meeting Perception/Understanding of Others criteria.</td></tr>
<tr><td>27494142_analysis_4</td><td>(CC-A) Gain - (CC-A) Loss</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis contrasts outcomes based on partner choices (CC vs other outcomes) and examines neural responses to others’ cooperative or defecting behavior, which indexes perception and understanding of others. Thus it meets I1 and I2.</td></tr>
<tr><td>27494142_analysis_5</td><td>(CC-A) Loss - (CC-A) Gain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Prisoner’s Dilemma requires evaluating others’ choices and the contrasts probe neural responses to partner behavior (mutual cooperation vs other outcomes), satisfying Perception/Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27494142_1</td><td>(CC-A)Gain &gt; (CC-A)Loss; others</td><td>27494142_analysis_4</td><td>(CC-A) Gain - (CC-A) Loss</td><td>0.917</td><td>1.000</td><td>0.975</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27494142_2</td><td>A &gt; CC; others</td><td>27494142_analysis_1</td><td>A-CC</td><td>0.600</td><td>1.000</td><td>0.880</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27494142_3</td><td>CC &gt; A; others</td><td>27494142_analysis_0</td><td>CC-A</td><td>0.600</td><td>1.000</td><td>0.880</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27494142_4</td><td>Gain &gt; Loss; others</td><td>27494142_analysis_2</td><td>Gain-Loss</td><td>0.800</td><td>1.000</td><td>0.940</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27568587</strong> | Pred included: 1 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27568587/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> PAINFUL &gt; NON-PAINFUL and SELF PAINFUL &gt; NON-PAINFUL; others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27568587_analysis_0</td><td>Mentalizing with Dissimilar &gt; Similar person</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This contrast directly indexes Understanding Mental States (mentalizing) about others (similar vs dissimilar), satisfying the inclusion criteria for Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27568587_1</td><td>DISSIMILAR &gt; SIMILAR; others</td><td>27568587_analysis_0</td><td>Mentalizing with Dissimilar &gt; Similar person</td><td>0.667</td><td>1.000</td><td>0.900</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27568587_2</td><td>PAINFUL &gt; NON-PAINFUL and SELF PAINFUL &gt; NON-PAINFUL; others</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 28504911</strong> | Pred included: 4 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/28504911/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>28504911_analysis_0</td><td>Select &gt; content</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Authors report engagement of social-cognition ROIs (TPJ, DMPFC, etc.) for select &gt; content, indicating the contrast involves reasoning about others’ perspectives; it therefore measures perception/understanding of others.</td></tr>
<tr><td>28504911_analysis_1</td><td>Share &gt; content</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast explicitly involves considering others (sharing with others, naming a specific friend) and engages social-cognition ROIs (TPJ, DMPFC), indicating measurement of understanding others’ mental states.</td></tr>
<tr><td>28504911_analysis_2</td><td>Share &gt; select</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast elicits activity in social-cognition/mentalizing regions (DMPFC, TPJ) and specifically examines considering others (sharing with friend/broadcast), so it measures perception and understanding of others.</td></tr>
<tr><td>28504911_analysis_3</td><td>Select</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The select contrast does not explicitly measure perception or reasoning about others’ mental states; although social-cognition regions were engaged, the contrast is focused on self-selection rather than on others, so it does not meet the inclusion criteria for perception/understanding of others.</td></tr>
<tr><td>28504911_analysis_4</td><td>Share</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analyses targeted perception/understanding of others via social-cognition ROIs (DMPFC, TPJ, rSTS, etc.) and contrasts assessing consideration of others when sharing, meeting the perception/others criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>28504911_1</td><td>Select &gt; content; others</td><td>28504911_analysis_0</td><td>Select &gt; content</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>28504911_2</td><td>Share &gt; content; others</td><td>28504911_analysis_1</td><td>Share &gt; content</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>28504911_3</td><td>Share &gt; select; others</td><td>28504911_analysis_2</td><td>Share &gt; select</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29097704</strong> | Pred included: 14 | Manual included (accepted matches only): 8 | Correct overlaps: 8 | Match statuses: accepted=8, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29097704/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29097704_analysis_0</td><td>FT&gt;FA</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task directly assesses perception and interpretation of others’ actions/orientation (animacy/action perception and inference of social intent), meeting the perception-of-others criteria.</td></tr>
<tr><td>29097704_analysis_1</td><td>FA&gt;FT</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast probes action perception and inferring social/intentional states of others (FA vs FT), matching Animacy/Action Perception and Understanding Mental States subcomponents.</td></tr>
<tr><td>29097704_analysis_2</td><td>HL&gt;PLD</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast HL&gt;PLD and related analyses target action perception, animacy and mentalizing (response of MNS and MS to others’ motion), directly measuring perception/understanding of others.</td></tr>
<tr><td>29097704_analysis_3</td><td>PLD&gt;HL</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task explicitly measures perception and interpretation of others&#x27; actions/orientation (action perception/animacy/intent), and the PLD&gt;HL contrast indexes differences in perceiving others.</td></tr>
<tr><td>29097704_analysis_4</td><td>STIMULUS*DIRECTION</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task explicitly measures perception and interpretation of others’ actions/direction (action perception and aspects of mentalizing); contrasts (PLD vs HL, FT vs FA, interaction) target Perception and Understanding of Others.</td></tr>
<tr><td>29097704_analysis_5</td><td>POST-HOC ANALYSIS</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study explicitly investigates perception and understanding of others&#x27; actions, animacy and inferred social intentions (action perception and understanding mental states) via PLD and human-like walkers.</td></tr>
<tr><td>29097704_analysis_6</td><td>FT_PLD&gt;HL</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis assesses perception and interpretation of others&#x27; actions and social orientation (PLD vs HL, FT vs FA), directly indexing Action Perception/understanding others, satisfying I1 and I2.</td></tr>
<tr><td>29097704_analysis_7</td><td>FT_HL&gt;PLD</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrasts probe action perception and attribution of social meaning to observed agents (animacy/action perception and inference about social intent), satisfying Perception and Understanding of Others.</td></tr>
<tr><td>29097704_analysis_8</td><td>FA_PLD&gt;HL</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Task assesses perception and understanding of others’ actions and intentions (point‑light and human‑like walkers, FT/FA), matching action perception and mentalizing subcomponents.</td></tr>
<tr><td>29097704_analysis_9</td><td>FA_HL&gt;PLD</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Task explicitly assesses perception and interpretation of others’ actions/orientation and engages mentalizing/action perception systems (MNS/MS), meeting criteria for Perception and Understanding of Others.</td></tr>
<tr><td>29097704_analysis_10</td><td>PLD_FT&gt;FA</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This analysis probes perception and understanding of others (action/animacy perception and attribution of social involvement) by contrasting PLD facing-toward vs away — directly measuring perception of others’ actions and social meaning.</td></tr>
<tr><td>29097704_analysis_11</td><td>PLD_FA&gt;FT</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants judged others&#x27; motion direction and the study examines action perception and mentalizing systems; this directly targets perception and understanding of others (action perception/understanding mental states).</td></tr>
<tr><td>29097704_analysis_12</td><td>HL_FT&gt;FA</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>HL_FT&gt;FA directly probes perception and understanding of others (orientation, potential for social involvement), mapping onto action perception/animacy and mentalizing processes.</td></tr>
<tr><td>29097704_analysis_13</td><td>HL_FA&gt;FT</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast compares facing-away vs facing-toward human-like walkers, directly probing action perception and inference about others (potential for social involvement/intent), satisfying I1 and I2 for perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29097704_1</td><td>FA &gt; FT; others</td><td>29097704_analysis_1</td><td>FA&gt;FT</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29097704_2</td><td>FA_HL &gt; PLD; others</td><td>29097704_analysis_9</td><td>FA_HL&gt;PLD</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29097704_3</td><td>FA_PLD &gt; HL; others</td><td>29097704_analysis_8</td><td>FA_PLD&gt;HL</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29097704_4</td><td>HL &gt; PLD; others</td><td>29097704_analysis_2</td><td>HL&gt;PLD</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29097704_5</td><td>HL_FT &gt; FA; others</td><td>29097704_analysis_12</td><td>HL_FT&gt;FA</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29097704_6</td><td>PLD &gt; HL; others</td><td>29097704_analysis_3</td><td>PLD&gt;HL</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29097704_7</td><td>PLD_FA &gt; FT; others</td><td>29097704_analysis_11</td><td>PLD_FA&gt;FT</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29097704_8</td><td>Stimulus*Direction; others</td><td>29097704_analysis_4</td><td>STIMULUS*DIRECTION</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29324862</strong> | Pred included: 5 | Manual included (accepted matches only): 7 | Correct overlaps: 5 | Match statuses: accepted=7, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29324862/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29324862_analysis_0</td><td>Partnering</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Following and mutual conditions emphasize perception and tracking of another’s actions; analyses report activation in regions for action perception and mentalizing (pSTS, TPJ, mPFC), addressing perception/understanding of others.</td></tr>
<tr><td>29324862_analysis_1</td><td>Leading &gt; Conjunction</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The Leading &gt; Conjunction contrast primarily emphasizes motor planning and self-initiation rather than processes of perceiving or reasoning about the partner’s mental states; perception-of-others constructs are better captured by Following or Mutual contrasts.</td></tr>
<tr><td>29324862_analysis_2</td><td>Following &gt; Conjunction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Following explicitly involves perception and understanding of another’s actions (motion tracking, pSTS, aIPL, TPJ) and mentalizing, directly matching I1 and I2 for perception and understanding of others.</td></tr>
<tr><td>29324862_analysis_3</td><td>Mutual &gt; Conjunction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Mutual &gt; conjunction engages mentalizing network (TPJ, mPFC, pSTS) that represents others’ intentions and action perception; thus it measures perception and understanding of others.</td></tr>
<tr><td>29324862_analysis_4</td><td>Improvisation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Following and Mutual conditions involve perception and understanding of others’ actions/intentions (pSTS, TPJ, motion-tracking areas) and the study explicitly interprets results in terms of understanding others and mentalizing.</td></tr>
<tr><td>29324862_analysis_5</td><td>Self-initiation</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast emphasizes self-initiation (self-oriented processes) rather than representing or reasoning about others’ mental states; it does not specifically assess Perception and Understanding of Others.</td></tr>
<tr><td>29324862_analysis_6</td><td>Joint improvisation.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis examines perception and understanding of others’ actions and intentions (motion tracking, pSTS, TPJ, mentalizing network), satisfying perception-of-others criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29324862_1</td><td>Following &gt; Conjunction; others</td><td>29324862_analysis_2</td><td>Following &gt; Conjunction</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29324862_2</td><td>Leading + Following + Mutual &gt; Solo + Alone); others</td><td>29324862_analysis_0</td><td>Partnering</td><td>0.148</td><td>1.000</td><td>0.744</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>29324862_3</td><td>Leading + Solo &gt; Mutual + Alone; others</td><td>29324862_analysis_4</td><td>Improvisation</td><td>0.182</td><td>1.000</td><td>0.755</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>29324862_4</td><td>Leading &gt; Conjunction; others</td><td>29324862_analysis_1</td><td>Leading &gt; Conjunction</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29324862_5</td><td>Leading &gt; Solo; others</td><td>29324862_analysis_6</td><td>Joint improvisation.</td><td>0.294</td><td>1.000</td><td>0.788</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>29324862_6</td><td>Mutual &gt; Conjunction; others</td><td>29324862_analysis_3</td><td>Mutual &gt; Conjunction</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29324862_7</td><td>[Leading + Solo + Mutual + Alone] &gt; Following; others</td><td>29324862_analysis_5</td><td>Self-initiation</td><td>0.233</td><td>1.000</td><td>0.770</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29582502</strong> | Pred included: 6 | Manual included (accepted matches only): 4 | Correct overlaps: 4 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29582502/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29582502_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analyses explicitly probe understanding others&#x27; mental states (rTPJ, ToM contrasts, behavioral adaptation to partners), directly measuring perception and understanding of others.</td></tr>
<tr><td>29582502_analysis_1</td><td>Competitive&gt;Cooperative</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast probes understanding of others&#x27; mental states (Theory of Mind) during cooperative vs competitive interactions (COMP&gt;COOP), directly matching Perception_and_Understanding_of_Others. Satisfies I1 (cooperation/competition task) and I2 (measures understanding of others).</td></tr>
<tr><td>29582502_analysis_2</td><td>Cooperative&gt;Competitive</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The study explicitly examines reasoning about others&#x27; mental states (ToM) and neural responses to competitive vs cooperative partners, directly matching perception/understanding of others.</td></tr>
<tr><td>29582502_analysis_3</td><td>Competitive&gt;Cooperative</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast operationalizes understanding and reasoning about others&#x27; mental states (ToM) in cooperative vs competitive contexts; it directly measures Perception and Understanding of Others.</td></tr>
<tr><td>29582502_analysis_4</td><td>Early phase Competitive&gt;Cooperative</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis directly targets understanding others’ mental states (rTPJ responses and ToM), satisfying both inclusion criteria for Perception and Understanding of Others.</td></tr>
<tr><td>29582502_analysis_5</td><td>Dynamic ToM‐Value Competitive&gt;Cooperative</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast explicitly probes Understanding Mental States (Theory of Mind) of others in cooperative vs competitive contexts, satisfying the criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29582502_1</td><td>Competitive &gt; Cooperative; others</td><td>29582502_analysis_3</td><td>Competitive&gt;Cooperative</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29582502_2</td><td>Cooperative &gt; Competitive; others</td><td>29582502_analysis_2</td><td>Cooperative&gt;Competitive</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29582502_3</td><td>Dynamic ToM-Value Competitive &gt; Cooperative; others</td><td>29582502_analysis_5</td><td>Dynamic ToM‐Value Competitive&gt;Cooperative</td><td>0.977</td><td>1.000</td><td>0.993</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29582502_4</td><td>Early phase Competitive &gt; Cooperative; others</td><td>29582502_analysis_4</td><td>Early phase Competitive&gt;Cooperative</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29777673</strong> | Pred included: 1 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29777673/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29777673_analysis_0</td><td>SA vs. SC</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The SA vs. SC contrast focuses on self-evaluation (agentic vs. communal). Although the study includes endorsement-of-others conditions elsewhere, this specific analysis targets self-related judgments rather than representations of others’ states/traits.</td></tr>
<tr><td>29777673_analysis_1</td><td>SC vs. SA</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the study includes endorsement of others’ evaluations, the specific SC vs. SA contrast focuses on self-evaluation (not on representing others’ mental states), so it does not meet criteria for perception/understanding of others.</td></tr>
<tr><td>29777673_analysis_2</td><td>OA vs. OC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The OA vs OC contrast requires representing and evaluating others’ judgments about the participant (processing others’ evaluations), which involves perception/understanding of others’ assessments and thus meets the criteria.</td></tr>
<tr><td>29777673_analysis_3</td><td>OC vs. OA</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although others&#x27; evaluations are presented, the task focuses on endorsement about the self rather than representing or inferring others&#x27; mental states/traits; primary process is self-related.</td></tr>
<tr><td>29777673_analysis_4</td><td>SA vs. Fixation</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This analysis targets self-evaluation rather than representations or judgments about others&#x27; states or traits; it does not meet the perception-of-others criteria.</td></tr>
<tr><td>29777673_analysis_5</td><td>dlPFC as the seed region</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the study includes an endorsement-of-others condition generally, this specific analysis focuses on SA (self-evaluation) and dlPFC seed connectivity during SA vs. Fixation, not on perception/understanding of others, so it does not meet the inclusion criteria.</td></tr>
<tr><td>29777673_analysis_6</td><td>Thalamus as the seed region</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the study contains an endorsement-of-others condition (OA), this specific analysis focuses on SA (self-evaluation) and SA vs Fixation PPI with thalamus; it therefore does not measure perception/understanding of others as defined.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29777673_1</td><td>OA &gt; OC; others</td><td>29777673_analysis_2</td><td>OA vs. OC</td><td>0.750</td><td>1.000</td><td>0.925</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 30649548</strong> | Pred included: 6 | Manual included (accepted matches only): 6 | Correct overlaps: 6 | Match statuses: accepted=6, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/30649548/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>30649548_analysis_0</td><td>Cooperation &gt; competition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis models brain responses to the co-player’s moves (iBBDs) and discusses mentalizing/understanding others’ intentions, directly measuring perception and understanding of others.</td></tr>
<tr><td>30649548_analysis_1</td><td>Competition &gt; cooperation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis models brain responses to a co-player’s moves (iBBDs) and reports activity in mentalizing and social-cognitive regions—directly assessing perception/understanding of others.</td></tr>
<tr><td>30649548_analysis_2</td><td>CN &gt; TB</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis explicitly models brain responses as reactions to the co-player’s behavior (iBBDs) and discusses mentalizing and inference of others’ intentions, meeting criteria for perception and understanding of others.</td></tr>
<tr><td>30649548_analysis_3</td><td>TB (COO &amp;gt; COM) &amp;gt; CN (COO &amp;gt; COM)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis models reactions to the co-player’s actions (iBBDs) and reports activations in mentalizing and action-perception regions, directly indexing perception/understanding of others.</td></tr>
<tr><td>30649548_analysis_4</td><td>Builder (COO &gt; COM) &gt; Other (COO &gt; COM)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis explicitly measures brain responses to another person’s actions and discusses mentalizing and inferring intentions, directly indexing perception and understanding of others.</td></tr>
<tr><td>30649548_analysis_5</td><td>Other (COO &gt; COM) &gt; Builder (COO &gt; COM)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis models participants’ neural reactions to their partner’s actions (iBBDs) during cooperation and competition, which directly measures perception and understanding of others (mentalizing/action perception).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>30649548_1</td><td>Builder (COO &gt; COM) &gt; Other (COO &gt; COM); affiliation</td><td>30649548_analysis_4</td><td>Builder (COO &gt; COM) &gt; Other (COO &gt; COM)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>30649548_2</td><td>CN &gt; TB; affiliation</td><td>30649548_analysis_2</td><td>CN &gt; TB</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>30649548_3</td><td>Competition &gt; Cooperation; affiliation</td><td>30649548_analysis_1</td><td>Competition &gt; cooperation</td><td>1.000</td><td>0.938</td><td>0.956</td><td>accepted</td><td>high_coord_match</td></tr><tr><td>30649548_4</td><td>Cooperation &gt; competition; affiliation</td><td>30649548_analysis_0</td><td>Cooperation &gt; competition</td><td>1.000</td><td>0.943</td><td>0.960</td><td>accepted</td><td>high_coord_match</td></tr><tr><td>30649548_5</td><td>Other (COO &gt; COM) &gt; Builder (COO &gt; COM); affiliation</td><td>30649548_analysis_5</td><td>Other (COO &gt; COM) &gt; Builder (COO &gt; COM)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>30649548_6</td><td>TB (COO &gt; COM) &gt; CN (COO &gt; COM); affiliation</td><td>30649548_analysis_3</td><td>TB (COO &amp;gt; COM) &amp;gt; CN (COO &amp;gt; COM)</td><td>0.789</td><td>1.000</td><td>0.937</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 31090126</strong> | Pred included: 5 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/31090126/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>31090126_analysis_0</td><td>Stimulus type (expression&gt;mosaic)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast assesses perception and understanding of others&#x27; emotional states via dynamic facial expressions, fitting the Perception and Understanding of Others construct.</td></tr>
<tr><td>31090126_analysis_1</td><td>Stimulus type (expression&gt;mosaic)emotion</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis assesses perception and understanding of others (emotion perception, mentalizing-related regions like STS, amygdala, dmPFC), meeting the construct criteria.</td></tr>
<tr><td>31090126_analysis_2</td><td>Stimulus type (expression&gt;mosaic)gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis probes perception and understanding of others (dynamic facial expressions, action perception and potential mentalizing), meeting criteria for this construct. Satisfies I1 and I2.</td></tr>
<tr><td>31090126_analysis_3</td><td>Stimulus type (expression&gt;mosaic)emotiongender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast probes representations of others&#x27; emotional states (anger, happiness) and visual social signals — directly addressing Perception and Understanding of Others (animacy/action/mental states). Meets both inclusion criteria.</td></tr>
<tr><td>31090126_analysis_4</td><td>Stimulus type (expression&gt;mosaic)×laterality</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task explicitly measures perception and understanding of others’ emotional states via dynamic facial expressions (action/animacy/mental state perception). Inclusion criteria I1 and I2 are satisfied.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>31090126_1</td><td>(expression &gt; mosaic) × laterality; socialcommunication</td><td>31090126_analysis_4</td><td>Stimulus type (expression&gt;mosaic)×laterality</td><td>0.800</td><td>1.000</td><td>0.940</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>31090126_2</td><td>expression &gt; mosaic; socialcommunication</td><td>31090126_analysis_0</td><td>Stimulus type (expression&gt;mosaic)</td><td>0.704</td><td>0.959</td><td>0.882</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>
</details></section><section id="bucket-false-positive"><details class="bucket" open><summary><h2>False Positive (61)</h2></summary><p><strong>Match status totals:</strong> accepted=0 | uncertain=1 | unmatched=14</p>
<details class="doc-card">
  <summary><strong>PMID 29079809</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29079809/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29079809_analysis_0</td><td>clusters genetic &gt; non-genetic</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast examines how subjects represent and process others (the sisters), engaging mentalizing and understanding of others’ intentions and moral actions; it matches the construct of Perception and Understanding of Others.</td></tr>
<tr><td>29079809_analysis_1</td><td>clusters non-genetic &gt; genetic</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast probes understanding of others’ mental states and moral behaviour (mentalizing), matching perception/understanding of others criteria.</td></tr>
<tr><td>29079809_analysis_2</td><td>moral dilemma decision task</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Both the ISC movie analysis (with perspective-taking instructions) and the moral-decision GLM engage mentalizing and understanding others’ states (VMPFC, TPJ, precuneus, etc.), directly indexing perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 5660240)</summary><p><strong>Title:</strong> Differential inter-subject correlation of brain activity when kinship is a variable in moral dilemma</p><details><summary>Abstract</summary><pre class="paper-text">Previous behavioural studies have shown that humans act more altruistically towards kin. Whether and how knowledge of genetic relatedness translates into differential neurocognitive evaluation of observed social interactions has remained an open question. Here, we investigated how the human brain is engaged when viewing a moral dilemma between genetic vs. non-genetic sisters. During functional magnetic resonance imaging, a movie was shown, depicting refusal of organ donation between two sisters, with subjects guided to believe the sisters were related either genetically or by adoption. Although 90% of the subjects self-reported that genetic relationship was not relevant, their brain activity told a different story. Comparing correlations of brain activity across all subject pairs between the two viewing conditions, we found significantly stronger inter-subject correlations in insula, cingulate, medial and lateral prefrontal, superior temporal, and superior parietal cortices, when the subjects believed that the sisters were genetically related. Cognitive functions previously associated with these areas include moral and emotional conflict regulation, decision making, and mentalizing, suggesting more similar engagement of such functions when observing refusal of altruism from a genetic sister. Our results show that mere knowledge of a genetic relationship between interacting persons robustly modulates social cognition of the perceiver.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Evaluating and predicting social interactions of others is an integral part of social cognition, one of the most fundamental of human cognitive functions. Indeed, the evolution of social cognition may best explain why humans have a more developed neocortex than other species . So far, social cognition has been predominantly studied with stimuli depicting interactions between strangers, however, most of the significant interactions evaluated in daily life are between one’s family members, friends, and acquaintances 

More importantly, most of our social interactions (and social effort) is directed to a very small number of familiar individuals, 60% of our social effort is directed to just 15 close friends and family . There is also considerable experimental and observational evidence for a “kinship premium” in our interactions with others, especially when those interactions involve altruistic behaviour . We are more likely to help our genetic relatives compared to unrelated individuals, and to do so implicitly, without conscious elaboration . In a trolley dilemma , subjects have to decide if they would push a handle to set a trolley to another track so that instead of killing five people when left without intervention, it will kill a single person on an alternative track. If only strangers are considered, the study subjects favoured the survival of the five over one life; however, their judgement changed if the single person was genetically related to the subject. 

On the other hand it has been shown, that subjects judged incest to be equally morally wrong for a sibling, irrespective of whether this was a genetic, adoptive or step sibling . In these studies, rather co-residence with the sibling in the family, irrespective of genetic status, was the most relevant factor in decisions about the moral reprehensibility of incest. 

These and many other studies, have shown differences in multiple aspects of moral perception/processing, evaluation, judgement and decision making at the behavioural level when processing information about kin vs. non-kin. However, much less is known about the neuronal underpinnings of these effects. Recently, Wlodarski and Dunbar  have shown that different brain regions are active when subjects judge moral dilemmas about kin vs. friends. They found the sensorimotor cortex, ventromedial prefrontal cortex and posterior cingulate cortex to be more strongly activated when the subjects processed social information about their friends than about their kin. These differences imply that the brain processes kinship information differently than information on unrelated individuals. 

We explored this further by comparing subjects’ brain responses to a moral dilemma involving a pair of genetic versus adoptive (  i  .  e  ., unrelated) sisters. During functional magnetic resonance imaging (fMRI) the subjects viewed the same movie involving two sisters, but one group was primed with the information that they were genetic sisters and the other group with the information that they were sisters by adoption. The case of sisters related genetically vs. by adoption is especially suitable for testing whether knowledge of genetic relationship influences perception of a moral dilemma between kin given that there is no potential for shared genetic interest in future generations for adopted siblings . Note, however, that the current study examined perception of a kin relationship that subjects were seeing in a movie, while in Wlodarski and Dunbars  analysis the subjects answered questions about their own kin members and friends. 

In the present study, we utilized inter-subject correlation (ISC) of brain hemodynamic activity as a model-free analysis approach that makes it possible to use movies as ecologically valid stimuli during fMRI. Due to improvements in fMRI acquisition methods and data analysis algorithms , it has become possible to study specific aspects of social cognition between subjects using ecologically valid fMRI paradigms. The ecological validity is particularly important when studying moral dilemmas in order to engage the subjects and make the dilemma as credible and perceptible as possible in order to get authentic reactions. To investigate the degree of similarity in how individual brains respond to the common movie stimulus, the brains of individual subjects are aligned and ISC between the hemodynamic activity time courses for each voxel are calculated across all subject pairs. ISC can be interpreted as reflecting synchronized neural activity and thus similarity of cerebral information processing across individuals . It has been shown that when viewing a feature film during brain scanning, both “higher-order” prefrontal cortical as well as basic sensory cortex regions become synchronized across subjects . Further, ISC may not only reflect mutual neuronal responses, but could provide the basis of inducing a specific common mind set, e.g. built by context information or perspective taking as well as predicting the actions of others . 

The model-free approach of ISC does not require any a priori, pre-designed modeling of the fMRI signal to carry out the analysis and thus provides a powerful tool to investigate neuronal mechanisms as the correlations are exclusively based on similarities between the subjects’ brain activities when they react to the various aspects of the complex movie . At the same time, ISC has been shown to reliably detect involved brain regions in complex experimental setups almost as sensitively as a model-based analysis . 

In study 1, we asked whether people discriminate behaviourally between relatives with genetic vs. non-genetic backgrounds: In an implicit association test (IAT ) the subjects’ reaction time when associating the words “sister” and “adopted sister” to positive or negative connoted adjectives was measured. Further, after watching the movie, the subjects were asked whether genetic vs. non-genetic relationship status mattered to them in the moral dilemma that they observed. 

In study 2, we tested how the subjects perceived moral dilemmas involving genetically related vs. unrelated individuals during fMRI. In a first task, the subjects watched the movie depicting the moral dilemma between two sisters after being primed that they were either genetically related sisters or sisters related only by adoption. Should knowledge of genetic relationship matter, we expect to see differences in the behavioural tests (IAT; questionnaires) as well as the neuronal mechanisms: we predict that brain regions known to be involved in processing of mentalizing , conflict resolution , emotion regulation , and moral dilemmas  would be activated differently under the two viewing conditions. 

Second, following the hypothesis that moral processing is the most relevant factor to distinguish between watching the movie when believing the sisters to be genetic or adopted (i.e., non-genetic), each subject underwent a moral decision task during fMRI scanning to evaluate specifically which brain areas are associated with the perception and processing of moral dilemmas during movie-watching. In this task the subjects had to decide whom to save from a dangerous area and had different choices including their own sister, best friend, and strangers, an experimental design similar to the classical moral trolley dilemma. Again, if the genetic relationship had an effect on the viewers, as has previously been shown behaviourally , we hypothesize that the subjects will show kinship preference by saving their sister over others and that similar brain areas are engaged both in the decision task and when watching the movie believing that the sisters are genetic. 


## Results 
  
### Study 1: Implicit association test (IAT) 
  
To examine if a possible general implicit bias against adopted sisters (that potentially modulates brain functions during movie viewing) underlies the subjects’ perception of the movie, we asked 30 subjects in a behavioural experiment to undergo an IAT . In this test, reaction times during assignment of positive and negative connoted words to the categories of sister and adopted sister showed that there is no such implicit bias: Out of 30 subjects, nine favoured a genetic sister, 13 a non-genetic sister, and eight had no preference (one sampled t-test t = −0.9564 p = 0.3468). The TOST procedure  indicated that the ratings of emotional closeness were significantly similar (observed effect size (d = −0.17) was significantly within the equivalent bounds of d = −0.68 and d = 0.68; t(29) = 2.77, p = 0.005). 


### Study 2: Inter-subject correlation (ISC) of fMRI during movie watching 
  
#### Inter-subject correlation (ISC) across all conditions 
  
During fMRI scanning the subjects watched a movie depicting a moral dilemma between two sisters, either believing the sisters are genetic sisters or that the younger sister was adopted at birth. In a first step, the overall ISC (22) of hemodynamic activity of an independent set of 30 subjects was calculated during first viewing of the movie (Fig.  ). Significant ISC was observed extensively in occipital lobes, posterior parietal areas, and temporal cortices. In the frontal cortex, areas in the lateral inferior frontal gyrus (IFG), lateral middle frontal gyrus (MFG), dorsolateral prefrontal cortex (DLPFC), dorsomedial prefrontal cortex (DMPFC) and ventromedial prefrontal cortex (VMPFC) showed ISC between all subjects. The location of all brain areas were defined using anatomical brain atlases as specifically the Harvard-Oxford Cortical Structural Atlas and the Juelich Histological Atlas.   
Inter-subject correlation (ISC) of all 30 subjects during the first viewing of the movie. On top row are shown lateral and on bottom row medial surfaces of left and right cerebral hemispheres. Red-yellow colours indicate areas of significant ISC during movie watching (FDR q &lt; 0.05). Abbreviations: ACC = anterior cingulate cortex, ANG = angular gyrus, CAL = calcarine gyrus, DLPFC = dorsolateral prefrontal cortex, DMPFC = dorsomedial prefrontal cortex, IFG = inferior frontal gyrus, IOG = inferior occipital gyrus, MFG = middle frontal gyrus, MOG = middle occipital gyrus, MTG = middle temporal gyrus, PCC = posterior cingulate cortex, SOG = superior occipital gyrus, SPL = superior parietal lobe, STS/STG = superior temporal sulcus and gyrus, VMPFC = ventromedial prefrontal cortex, a = anterior, d = dorsal, p = posterior, v = ventral. 
  


#### Differences in ISC between conditions 
  
In a second step, the ISC of all subjects (N = 30) were contrasted between the genetic vs. non-genetic relationship viewing conditions. As each participant watched the movie in the genetic and in the non-genetic condition on two different scanning days in a counterbalanced order, this is a within-subject design. There were robust differences between the two conditions in the ISC of hemodynamic activity of the subjects, despite 90% of the subjects self-reporting that it did not matter to them whether the sisters were related genetically or not. When the subjects watched the movie believing that they were seeing genetically related sisters, the ISC was significantly stronger in the superior temporal sulcus and gyrus (STS/STG), VMPFC, DLPFC, anterior cingulate cortex (ACC) and posterior cingulate cortex (PCC), IFG, insula, cuneus, precuneus, and superior parietal lobule (SPL) (Fig.  , Table  ).   
Differential ISC between the conditions of an assumed genetic and non-genetic sisters and BOLD time series from two exemplary single voxels. (  A  ) Significant differences in brain activity when all subjects watched the movie thinking that the sisters were genetically vs. non-genetically related (FDR q &lt; 0.05, t = 2.1447, for consistent illustration purposes, the figures shows t-values from 3 to 9 and −3 to −9) (N = 30, within subject design). Red-yellow colours indicate areas of significantly higher ISC when the subjects watched the movie as depicting genetically related, as compared to non-genetically related, sisters. Blue colour indicates areas showing significantly higher ISC in the reverse contrast. Abbreviations: ACC = anterior cingulate cortex, CAL = calcarine gyrus, DLPFC = dorsolateral prefrontal cortex, IFG = inferior frontal gyrus, IOG = inferior occipital gyrus, MOG = middle occipital gyrus, PCC = posterior cingulate cortex, SPL = superior parietal lobe, STS/STG = superior temporal sulcus and gyrus, VMPFC = ventromedial prefrontal cortex, a = anterior, d = dorsal, p = posterior, v = ventral. (  B  ) Across subjects averaged BOLD time series of two voxels, one in the area VMPFC that showed significantly higher ISC when the subjects were viewing the sisters as genetic and one time series of a voxel in area V1 (primary visual cortex) that did not show significant between-condition ISC differences. The red line plots the group mean BOLD in the genetic sisters condition and the blue line plots the group mean BOLD in non-genetic sisters condition over the whole length of the movie. Red and blue shades indicate the 25  and 75  percentile of the variance. 
    
Clusters size, peak coordinates and t value of all clusters of Experiment 1 (movie watching task). 
  

When the subjects thought that the sisters were non-genetic, higher ISC was observed mainly in the occipital cortex. Importantly, the movie stimulus was identical in both viewing conditions. 

To illustrate blood oxygenation level dependent (BOLD) time series of specific voxels, in Fig.  , the panel B shows the time series of two exemplary voxels, over the whole length of the movie, from: i) VMPFC that showed higher ISC when the subjects viewed the sisters as genetic and ii) from a voxel in the area V1 (primary visual cortex), which is an early sensory brain area that did not show any between condition differences in ISC. 

Self-ratings of emotional valence and arousal obtained after the scans were not significantly different between the conditions (Valence: r = 0.0075, p = 0.3458, Arousal: r = −0.0189, p = 0.6081) (Fig.  ). Further, the mean ISC of eye-movements (eISC) over time windows showed no significant difference between the groups of participants believing in genetic or non-genetic sisters (p = 0.3918) (see Fig.  ). Likewise, no significant difference could be found in the heart and breathing rate comparing the conditions of assumed genetic versus non-genetic sisters (with bootstrap over 5000 permutations, breathing rate: t-value = 0.430, p = 0.335: heart rate: t-value = −1.12, p = 0.129) (see Fig.  ).   
Experienced emotional valence and arousal as well as physiological parameters when perceiving the sisters in the movie as genetic vs. non-genetic. (  A  ) Shown are dynamic self-ratings of emotional valence and arousal over the whole time course of the movie obtained during re-viewing of the movie after the fMRI sessions when the sisters were viewed as genetically related (red) or non-genetic (blue). The ratings were highly similar and there were no time periods where significant between-condition differences could have been observed. Note that half of the subjects (N = 2 × 15) rated experienced arousal and the rest rated experienced valence after the first fMRI session followed by rating the other emotional dimension after the second fMRI session. Plotted are means for all subjects in the red line for assumed genetic sisters and the blue line for non-genetic sisters condition. Red and blue dashed lines show the 25  and 75  percentile of the variance. (  B  ) Eye gaze behavior (N = 29) in the movies when the sisters were perceived as either genetic (right) or non-genetic (left) shown as a violin plot with the red cross depicting the means and green squares the medians. There were no significant differences between the conditions. (  C  ) Breathing and heart rates (N = 30) when the sisters were perceived as either genetic (red) or non-genetic (blue). There were no significant differences between the conditions. Red line plots the condition with assumed genetic sisters and the blue line non-genetic sisters. Red and blue shade show the 25  and 75  percentile of the variance. 
  



### Comparison with the moral dilemma decision experiment 
  
To further examine which neurocognitive processes might be involved, we studied whether the brain areas showing higher ISC in the genetic condition overlap with those engaged during a modified moral dilemma task  analysed with general linear modelling (GLM) (Fig.  , Tables   and  ). Naturally, it should be kept in mind that while ISC and GLM analyses of brain hemodynamic activity can yield converging results , this is not necessarily the case, as high ISC can be observed also when the BOLD signals are small.   
Having to decide in a simulation between saving one’s sister, friend, and others from crisis regions elicited significant brain activity in the VMPFC, ACC, precuneus, DLPFC, IFG, insula, TPJ and MTG. These activations of subjects (N = 30) were obtained by contrasting the decision phases (from the point of revealing the individuals involved until the decision signaled by the subject’s button press) against non-decision phases (subjects watching the background story depicting the two crisis regions and how the subjects only have resources to save individuals from one of the crisis regions) (FDR q &lt; 0.05, t = 2.0384, for consistent illustration purposes, the figures shows t-values from 3 to 9) Left-lateralized motor and supplementary motor are probably explained by the button press that the subjects performed to announce their decision. Abbreviations: ACC = anterior cingulate cortex, DLPFC = dorsolateral prefrontal cortex, IFG = inferior frontal gyrus, IOG = inferior occipital gyrus, ITG = inferior temporal gyrus, MC = motor cortex, MFC = medial frontal cortex, MOG = middle occipital gyrus, MTG = middle temporal gyrus, PCC = posterior cingulate cortex. SMA = supplementary motor area, SPL = superior parietal lobe, TPJ = temporo-parietal junction,VMPFC = ventromedial prefrontal cortex, a = anterior, d = dorsal, p = posterior, v = ventral. 
    
Clusters size, peak coordinates and t value of all clusters of and Experiment 2 (moral dilemma decision task). 
  

When the same subjects who watched the movie had to decide between saving their sister, best friend, vs. stranger(s), in various combinations, from a crisis region, 93% of the subjects showed a clear kin preference by choosing their sister (even when associated with some strangers) rather than their best female friend (chi-squared test ×  = 43.4 p &lt; 4 × 10 ). Further, as can be seen in Fig.   the VMPFC, ACC, IFG, MTG, SPL, PCC, precuneus, DLPFC, and anterior insula were consistently involved in both making choices of whom to prefer in a morally dilemmatic situation and when observing the moral dilemma between a genetic vs. non-genetic sister. Importantly, ratings of emotional closeness were not significantly different for the subjects’ sisters and their best friends with an average of 9.28 (sisters) and 8.80 (friends) on a 1–10 scale (Wilcoxon signed rank test = 0.12, t-test, t = 1.64 p = 0.11). The TOST procedure  indicated that the ratings of emotional closeness were significantly similar (observed effect size (d = 0.33) was significantly within the equivalent bounds of d = −0.68 and d = 0.68, or in raw scores: −1.07 and 1.07, t(29) = −1.92, p = 0.032).   
Activity during moral dilemma decision making as disclosed by GLM analysis (red) and the ISC when the subjects believed in a genetic relationship between the sisters in the movie (blue), along with the overlap of these two maps (violet), as well as the more strict overlap with a conjunction test (yellow);  , (FDR q &lt; 0.05, t = 2.0384 for GLM and t = 2.1447 for ISC). Abbreviations: ACC = anterior cingulate cortex, DLPFC = dorsolateral prefrontal cortex, IFG = inferior frontal gyrus, MFC = medial frontal cortex, MOG = middle occipital gyrus, MTG = midddle temporal gyrus, PCC = posterior cingulate cortex. SPL = superior parietal lobe, VMPFC = ventromedial prefrontal cortex, a = anterior, d = dorsal, p = posterior, v = ventral. 
  

As measured with an independent group of subjects outside the scanner, reaction times for the moral-dilemma decision were significantly longer in the case of a decision between a group comprised of their friend and four strangers on one side and their sister alone on the other side as in the case that only comprised strangers (on both sides) (paired Wilcoxon rank sum test p = 0.0011973). 



## Discussion 
  
In the present study, we investigated whether refusing altruism from a sister is perceived differently when the viewers think that the sisters are genetically related vs. when they think that one of the sisters has been adopted at young age. The results of the IAT in study 1 suggest that the subjects do not show an implicit bias against adoptive sisters compared to genetic sisters in general. Also, when explicitly asked if the relationship (as genetic or adopted) would matter in the decision of an organ donation, most subjects (90%) report that this knowledge would not affect the decision. Further, heart rate and breathing rate exhibited no significant differences between the two conditions, and self-reported emotional valence and arousal was likewise similar between the conditions (Fig.  ), suggesting that there were no robust differences in experienced emotions between the conditions. 

In contrast, the ISC of the hemodynamic brain activity show a different picture: robust differences were observed in patterns of brain activity due to the mere knowledge of the genetic relationship between the sisters in the movie. Specifically, there were multiple brain regions showing significantly higher ISC when the subjects thought that they are seeing a young girl refusing to donate her organ to save her genetic, as opposed to non-genetic, sister (Fig.  ). These areas included VMPFC, DLPFC, ACC, PCC, insula, precuneus, and SPL. While we would caution against drawing conclusions of specific cognitive functions involved on the basis of observed differences in brain activity, these brain regions have been previously shown to be associated with moral and emotional conflict regulation , decision making , mentalizing , and perspective taking , thus tentatively suggesting more uniform engagement of such neurocognitive functions when observing the dilemma of organ donation between genetic sisters. 

In the reverse contrast (i.e. when the subjects thought the sisters were non-genetic), higher ISC was observed in brain areas in the occipital cortex, conventionally associated mainly with visual perception . One possible explanation could potentially be that in the case of a non-genetic relationship between the sisters, the processing of complex social conflict associated with the moral dilemma is less demanding, and therefore leaves room for the subjects to focus on the visual aspects of the movie. However, eye-movements were not significantly different between the two viewing conditions, suggesting that differences in attention to movie events did not explain the observed robust differences in ISC. 

To specifically test for the possibility that the differences in ISC between the conditions reflected differences in moral evaluation, we compared the areas showing differences in ISC with areas activated when the subjects engaged in a separate moral-decision making control task. In this control task, the subjects had to make choices about saving people (including their sister, best friend, and strangers, in various combinations) from disaster. As each subject made only one decision that contrasted saving the friend over strangers and one decision of saving the friend over the own sister plus four strangers (as well as four decisions that contrasts the sister to groups of others), the statistical power in this experimental design was unfortunately not sufficient to differentiate directly between brain responses during decisions to save the sister vs. the friend. Rather, the results should be viewed as localization of brain regions involved in making moral decisions, yet also modulated by differences in, e.g., executive control, readiness for action, and attention, between the passive perception of the story and active decision making. However, significantly longer reaction times suggested increased difficulty when having to choose between a sister and the friend together with four strangers while areas in the VMPFC, DLPFC, ACC, PCC, precuneus, IFG, MTG, SPL and anterior insula were consistently involved in both making choices of whom to prefer in a morally dilemmatic situation and when observing the moral dilemma between a genetic vs. non-genetic sister (Fig.  ). This overlap in engaged brain regions suggests that processing of moral dilemmas took place during movie watching when the sisters were understood to be genetically related. It is significant that the brain regions flagged up in this analysis are those known to be involved in processing moral dilemmas and mentalizing. The DLPFC has been reported to play a role in overcoming a primary moral judgment in favour of greater welfare  and in cortical emotional processing , while the MTG has been implicated in attributing mental states as well as ingroup/outgroup distinctions , the ACC has been reported to be engaged in resolving conflicts , and the SPL, precuneus, and PCC have been implicated in mentalizing and perspective taking . Further, the VMPFC has been associated with viewing moral conflicts, making moral decisions, attributing mental states to self and others, adopting another person’s perspective, and evaluating their beliefs . 

Thus, the differences in ISC between the conditions appear to have arisen due to the knowledge about the sisters’ relationship influencing cognitive evaluation of the moral dilemma depicted in the movie. The contrast to the behavioural results assessed in study 1, where we find that behavioural decisions were not influenced by knowledge of the relationship, is particularly interesting since it suggests that differential processing is taking place under the surface. 

There could be at least two possible explanations for these findings. First, the study subjects might have purposely been hiding their “real” honest opinions as they might have not been willing to reveal these to the researchers, presumably because of social pressure against discriminating between genetic and adoptive siblings. However, the IAT is an implicit test for biases (it uses differences in reaction times for associations of a specific term with positively and negatively connoted words), so subjects are not aware of their performance on this task. Further, they do not know the exact measures which are used to calculate the IAT score, thus making it difficult to engineer potential biases; hence, it is very difficult, even impossible, to manipulate an IAT response in a desired direction . Thus, while a conscious manipulation of reported opinions would be possible in the open-format questionnaire (when asked if the relationship of the sisters matters in the situation of organ donation), it is very unlikely in the IAT. However, Liberman   et al  .  showed that, when judging incest reprehensibility, the coresidence of siblings is a stronger factor than the assumed relationship status and in when the two parameters are in conflict, the time spent in coresidence outweighs the belief of kin relation. As in this study the subjects were told that the apparent adoption took place as the younger sister was a newborn (implying coresidence of the sisters in both the genetic and the adoption case), we suggest, in accordance with Liberman   et al  ., that the factor of coresidence was given greater account than the kin relationship and thus the subjects’ explicit answers in the questionnaire could probably be seen/taken as truthful i.e. reporting authentic, honest thoughts. 

A second possible explanation for our findings is that, as the results of the implicit testing show, the study subjects indeed did not show any biases behaviourally and still pursued different ways of considering the case of a relationship by genes or by adoption, with resulting differences in brain activity patterns. These results show that an event that is behaviourally counter-intuitive (e.g. refusing to help a sister to prolong her life) needs different and potentially more intensive mental processing when the sisters are related genetically compared to adoptive sisters. As the differences in the brain activity patterns between the genetic and the adopted condition particularly comprise areas known to be involved in processing moral dilemmas and mentalizing as the VMPFC, DLPFC, ACC, PCC, precuneus, IFG, MTG, SPL, and anterior insula, we suggest that the study subjects’ expectations of morality are more strongly violated when close genetic relatives refuse to help each other than when unrelated individuals behave this way, despite their close social relation (adoption). 

Notwithstanding these points, we wish to caution the reader to keep in mind the caveats associated with reverse inference  (although see ). Specifically, even as we are suggesting that an activation observed in a certain region is indicative of a specific cognitive process based on results of previous research documented in the literature, it is possible that the activation of that region in the present study was due to some other cognitive process. This is because in general any given brain region is involved in multiple cognitive functions, thus making it difficult to infer with certainty the cognitive functions involved in a task based on brain regions that are activated. 

An alternative possibility is that the different measures operate at different levels of cognition: The null result in the IAT could be relying on a more basic level of attention to the social knowledge, whereas the questionnaire requires high level explicit cognition and the ISC during movie perception reflects some intermediate level of cognition. Had we thought to include them, other behavioural tests might have revealed more detail and background information on the subjects. Finally, it is always possible that something other than moral considerations could underlie the differences in brain patterns that we found, although, given the brain areas that show differences, this is unlikely. 

In summary, we observed robust differences in brain activity when subjects viewed a movie depicting refusal to donate an organ to a genetic vs. non-genetic sister. These differences in brain activity were observed despite the subjects self-reporting that the relational status of the sisters did not make any difference to them. Areas of increased synchrony in the case of genetic sisters overlapped with those activated in a separate moral dilemma decision task. Taken together, our results suggest that the precuneus, MTG, insula, SPL, and the VMPFC, along with the associated cognitive processes (i.e., moral and emotional conflict regulation), decision making, mentalizing and perspective taking are synchronized across subjects more robustly when they are viewing refusal of altruism from genetic as opposed to non-genetic, kin. Overall, these findings are fundamentally important for understanding social cognition, a pivotal ability that makes us human and, among other things, enables the existence of societies. Our findings point out that the perceived relationship of interacting persons robustly modulates how the brains of spectators process third-party interactions. This is highly significant given that majority of research to date on social cognition has been on strangers, whereas most of our social interactions take place between family members, friends, and acquaintances. 


## Material and Methods 
  
### Subjects 
  
We studied 33 healthy female subjects  (19–39 years, mean age of 26 years, one left-handed, laterality index of right handed 84.5%). None of the subjects reported any history of neurological or psychiatric disorders. When asked, all subjects reported either normal vision or corrected to normal vision by contact lenses. Three subjects were excluded due to discomfort in the scanner, so that the final analysis included 30 subjects. 27 of them were native Finnish speakers and three were native Russian speakers. All subjects were sufficiently proficient in English to follow the dialogue in the movie without subtitles. The experimental protocols were approved by the research ethics committee of the Aalto University and the study was carried out with their permission (Lausunto 9 2013 Sosiaalisen kognition aivomekanismit, 8.10.2013) and in accordance with the guidelines of the declaration of Helsinki . Written informed consent was obtained from each subject prior to participation. 


### Stimuli and Procedure 
  
The study consisted of two experiments. In the first experiment, the feature film   My Sister’s Keeper”   (dir. Nick Cassavetes, 2009, Curmudgeon Films), edited to 23 minutes and 44 s, (of which 14 min 17 s (60%) portray the theme of refusal of the organ donation),with the main story line retained, was shown to the subjects during fMRI. This shortened version of the movie focuses on the moral dilemma of the protagonist Anna to donate one of her kidneys to her sister Kate, who is fatally ill from cancer. In the course of the movie, Anna refuses to donate and Kate dies. The reason for Anna refusing to donate the kidney was not revealed to the subjects until after the experiment. The movie was shown to the subjects in the scanner four times in two separate scanning sessions on two different days. For each viewing of the movie the instructions were varied regarding the information about the sister’s relationship and the perspective to take in this viewing (Fig.  ). Each subject thus watched the movie assuming that the sisters were genetic sisters or that the younger sister Anna had been adopted as a newborn. In addition each subject was asked to take either the perspective of the potential donor (Anna) or the perspective of the potential recipient (Kate) on separate viewings (and both under the condition of a genetic or non-genetic relation background). The order of the different viewing conditions was counterbalanced between the subjects.   
Experimental procedure and ISC analysis in the movie watching task. (  A  ) Every subject watched the movie four times, in a 2 × 2 design assuming that the movie characters are either genetic sisters or not genetically related and taking the perspective of the to-be-donor sister Anna or the to-be-recipient sister Kate. The order of all the conditions were counter-balanced. (  B  ) Time series from each voxel from all the fMRI recordings are compared across subjects in pairwise correlations to obtain the mean inter-subject-correlation (ISC). 
  

In the second experiment, each subject carried out a moral-dilemma decision task during fMRI in order to localize brain regions that are related to moral decision making. For this purpose, a modified version of the classical trolley dilemma , was shown to the subjects. Each subject had to choose between rescuing different individuals, including unknown individuals, their sister and a best female friend. A presentation showing text and pictures told a story about civil unrest in a fictive distant country. This country had two parts: one part very dangerous and the other much less dangerous. Different people are in both parts of the country. Subjects were also told that as they were very rich and owned an airplane, they could go there and rescue some of the people. However, due to the circumstances in the country they had to decide which group of people to rescue. The two choices were always a group of five individuals on one side and a single person on the other. In seven runs the identity of the involved individual(s) was varied using the real names of the subject’s sister and best female friend. The 7 runs were: 1. All persons are unknown; 2. Sister is with four others in the dangerous part of the country, the single person is unknown; 3. Five persons are in the dangerous part of the country, the single person is the sister; 4. Five persons are in the dangerous part of the country, the single person is the friend; 5. Sister is with four others in the dangerous part of the country, the single person is the friend; 6. Friend is with four others in the dangerous part of the country, the single person is the sister; 7. Sister is with four others in the less dangerous part of the country, the single person is unknown. Responses in the moral dilemma decision task were recorded with a button press on a LUMItouch keypad (Photon Control Inc.8363, Canada). For the all questions, it was calculated with which percentage the sister was chosen over the friend and the stranger(s); statistical significance was tested with a Chi  test. 


### fMRI acquisition 
  
Before each scan the subjects were informed about the scanning procedures and asked to avoid bodily movements during the scans. All stimuli were presented to the subject with the Presentation software (Neurobehavioral Systems Inc., Albany, CA, USA), synchronizing the onset of the stimuli with the beginning of the functional scans. The movie was back-projected on a semitransparent screen using a data projector (PT-DZ8700/DZ110X Series, Panasonic, Osaka, Japan). The subjects viewed the screen at 33–35 cm viewing distance   via   a mirror located above their eyes. The audio track of the movie was played to the subjects with a Sensimetrics S14 audio system (Sensimetrics Corporation Malden, USA). The intensity of the auditory stimulation was individually adjusted to be loud enough to be heard over the scanner noise. The brain-imaging data were acquired with a 3T Siemens MAGNETOM Skyra (Siemens Healthcare, Erlangen, Germany), at the Advanced Magnetic Imaging center, Aalto University, using a standard 20-channel receiving head-neck coil. Anatomical images were acquired using a T1-weighted MPRAGE pulse sequence (TR 2530 ms, TE 3.3 ms, TI 1100 ms, flip angle 7°, 256 × 256 matrix, 176 sagittal slices, 1-mm3 resolution). Whole-brain functional data were acquired with T2*-weighted EPI sequence sensitive to the BOLD contrast (TR 2000 ms, TE 30 ms, flip angle 90, 64 × 64 matrix, 35 axial slices, slice thickness 4 mm, 3 × 3 mm in plane resolution). 

A total of 712 whole-brain EPI volumes were thus acquired for each movie viewing. The number of whole-brain EPI volumes for the moral dilemma decision task varied individually depending on the decision made by each subject (median 267 whole-brain EPI volumes). Heart pulse and respiration were monitored with the Biopac system (Biopac Systems Inc., Isla Vista, California, USA) during fMRI. Instantaneous values of heart rate and breathing rate were estimated with Drifter software package  (  http://becs.aalto.fi/en/research/bayes/drifter/  ). 


### fMRI preprocessing 
  
Standard fMRI preprocessing steps were applied using the FSL software (  www.fmrib.ox.ac.uk  ) and custom MATLAB code (available at   https://version.aalto.fi/gitlab/BML/bramila/  ). Briefly, EPI images were corrected for head motion using MCFLIRT. 

Then they were coregistered to the Montreal Neurological Institute 152 2 mm template in a two-step registration procedure using FLIRT: from EPI to subject’s anatomical image after brain extraction (9 degrees of freedom) and from anatomical to standard template (12 degrees of freedom). Further, spatial smoothing was applied with a Gaussian kernel of 6 mm full width at half maximum. High pass temporal filter at a cut-off frequency of 0.01 Hz was used to remove scanner drift. To further control for motion and physiological artefacts, BOLD time series were cleaned using 24 motion-related regressors, signal from deep white matter, ventricles and cerebral spinal fluid locations (see ) for details, cerebral spinal fluid mask from SPM8 file csf.nii, white matter and ventricles masks from Harvard Oxford atlas included with FSL). As a measure of quality control we computed framewise displacement to quantify instantaneous head motion. Out of all the 120 runs (30 subjects, 4 sessions each), 97.5% of the runs (117 runs) had 90% of time points (640 volumes) with framewise displacement under the 0.5 mm threshold suggested in . For the remaining three runs, the number of time points under 0.5 mm were 639 (89.7%), 633 (88.9%), 489 (68.7%), i.e. only one session had a considerable amount of head motion. While head motion is a concern in connectivity studies as it can increase spurious BOLD time series correlations that are affected by the same amount of instantaneous head motion, with across-brain time series correlation, head motion is expected to reduce the SNR. However, to make sure that head motion similarity did not explain any group difference, we computed the same permutation test for the ISC also for average framewise displacement by estimating the similarity of two subjects as the distance between their average framewise displacement value. We found that similarity in average head motion was not different between the two viewing conditions (t-value = 0.255; p = 0.398 obtained with 5000 permutations). 


### Inter-subject correlation (ISC) analysis of brain activity during movie watching 
  
To investigate how similar the brain activity was across subjects in the different experimental conditions, we performed inter-subject correlation (ISC) using the isc-toolbox (  https://www.nitrc.org/projects/isc-toolbox/  ) . For each voxel the toolbox computes a similarity matrix between subject pairs and within same subject in all conditions, with the conditions being: (i) shared assumption that the movie’s sisters are genetically related, (ii) shared assumption that the younger sister was adopted, (iii) shared perspective of the to-be-organ-donor, and (iv) shared perspective of the to-be-organ-recipient. The total size of the similarity matrix is then 120 × 120 (4 conditions × 30 subjects) with each subject having two viewings for the genetic and two viewings for the non-genetic condition. The comparison between the conditions of the sisters to be perceived as either genetic sisters or non-genetic sisters results thus in a total of 1740 pairs per condition, as the similarity of BOLD time series during the two viewings (in either the genetic or the non-genetic condition) of each subject is compared with the two respective viewings of the other N-1 subjects. As the order of subjects does not matter, the final number of pairs in same conditions will be 2*2*(N-1)*N/2 = 1740 with N = 30. Each value of the correlation matrix is a result of the correlation between the BOLD time series of the pair of subjects considered for the selected voxel. We computed differences between experimental conditions by first transforming the correlation values into z-scores with the Fisher Z transform and then computing t-values and corresponding p-values using a permutation based approach . 

The Fisher-Z transformed correlations of the two perspectives were pooled for either the genetic or the non-genetic sisterhood. 

Correction for the multiple comparison was performed with Benjamini-Hochberg false discovery rate (BH-FDR) correction at a q &lt; 0.05, corresponding to a t-value threshold of 2.133. For visualization purposes, all results were also cluster corrected by removing any significant cluster smaller than 4 × 4 × 4 voxels. Summary tables were generated with an increased t-value threshold of 3. For the conjunction or “intersection–union test”  the p values of the ISC and GLM results are pulled together by considering the maximum p-value at each voxel. Then, multiple comparisons correction is performed with the Benjamini-Hochberg false discovery rate procedure with an FDR threshold equal to q &lt; 0.05. 

Unthresholded statistical parametric maps can be found in neurovault:   http://neurovault.org/collections/WGSQZWPH/  . 

#### Perspective taking 
  
In the movie-viewing experiment, in addition to having the subjects to watch the movie in the conditions of sisters related by birth or by adoption, we had altogether four runs, so that on two of the runs the subjects were asked to view the movie from the perspective of the sister who was expected to donate the organ, and on two of the runs from the perspective of the to-be-recipient sister. Thus, there was one run wherein the subjects viewed the movie from the perspective of the to-be- donor thinking that the sisters were genetic, one run wherein the subjects viewed the movie from the perspective of the to-be- donor thinking that the sisters were non-genetic, one run wherein the subjects viewed the movie from the perspective of the to-be- recipient thinking that the sisters were genetic, and one run wherein the subjects viewed the movie from the perspective of the to-be- recipient thinking that the sisters were non-genetic. 

As the results of this task open up a completely other aspect of the experiment with various results to discuss, which go beyond the scope and the space limitation of this article, they will be reported separately elsewhere. These conditions are mentioned here for reasons of describing the experimental procedures thoroughly so that it would be possible for others to replicate the study should they wish to do so. 



### General linear model analysis of the fMRI data acquired during the control task 
  
A moral dilemma decision task was performed by all subjects to localize regions involved in moral processing. The moral dilemma decision task was analyzed with a general linear model approach using the SPM12 software (  www.fil.ion.ucl.ac.uk/spm  ). To distinguish between moments of decision in the moral dilemma and the simple perception of the presentation, we created a temporal model of the occurrence of decision moments during the experiment. The decision regressor included time points from the revelation of the identity of involved individuals to the moment of decision indicated by button press. The activity during these time points was compared to the activity in all other time points of the task, including telling the background story of the moral dilemma in the presentation. Regressors were convolved with canonical hemodynamic response function to account for hemodynamic lag. From the preprocessed input data (see above) low-frequency signal drifts were removed by high-pass filtering (cutoff 128 s). First, individual contrast images were generated for the main effects of the regressors, then first level analyses were subjected to second-level analyses in MATLAB using one-sample   t  -test to test which brain areas showed significant activations in decision vs. no decision moments in a one-sample   t  -test over subjects. Statistical threshold was set at   p   &lt; 0.05 (cluster-corrected using the threshold free cluster enhancement approach implemented by FSL randomize with 5000 permutations). 


### Recording of eye-movements 
  
Eye movements were recorded during fMRI scanning from all subjects with an EyeLink 1000 eye tracker (SR Research, Mississauga, Ontario, Canada; sampling rate 1000 Hz, spatial accuracy better than 0.5°, with a 0.01° resolution in the pupil-tracking mode). Due to technical problems, 4 subjects had to be excluded from the final data analysis (with the rejection criteria of blinks maximum 10% of the duration of the scan and majority of blinks and saccades less than 1 second in duration). In addition, a part of recordings from some additional subjects had to de discarded due to the same criteria mentioned above, resulting in 61 recorded files with sufficient quality, with 35 files remaining in the genetic condition and 26 remaining files for the non-genetic condition. Prior to the experiment the eye tracker was calibrated once with a nine-point calibration. 

Saccade detection was performed using a velocity threshold of 30°/s and an acceleration threshold of 4000°/s2. Because the experiment was relatively long and no intermediate drift correction was performed, we retrospectively corrected the mean effect of the drift. We first calculated the mean of all fixation locations over the entire experiment for each subject, and then rigidly shifted the fixation distributions so that the mean fixation location coincided with the grand mean fixation location over all subjects. 


### Eye-movement analysis 
  
Subject-wise gaze fixation distributions were compared across the genetic vs. non-genetic conditions in the movie viewing task. Individual heat maps were generated by modelling each fixation as a Gaussian function using a Gaussian kernel with a standard deviation of 1degree of visual angle and a radius of 3 standard deviations. The heat maps were generated in time windows of 2 seconds corresponding to the TR used in the fMRI measurements. Spatial similarities between each pair of heat maps across the eye-tracking sessions were calculated using Pearson’s product-moment correlation coefficient (inter-subject correlation of eye gaze, eyeISC ). In the end a similarity matrix was obtained with correlations between each pair for each of the 712 time windows. 

First, the mean eISC scores over all 712 time windows were examined. These mean scores were acquired by extracting the mean of Fisher’s Z-transformed correlation scores and then transforming these mean values back to the correlation scale before the statistical analysis. The statistical significance of the group differences was analysed by contrasting pairs in which both subjects assumed a genetic relationship with pairs in which both subjects assumed the younger sister to be adopted. Non-parametric permutation tests with a total of 100000 permutations were used to avoid making assumptions about the data distribution. In this procedure the data were mixed randomly to change groupings and differences in the resulting new randomised groups were used to form an estimated distribution of the data. A comparison of how many of the permuted random partitions into groups build a more extreme group mean difference that the one observed with the original grouping yielded the final p-values. 


### Behavioral Measurements and Self-reports 
  
#### Valence and Arousal measurements 
  
The subjects self-reported emotions they had experienced during movie viewing. This was carried out after the fMRI experiment by viewing the movie again (Full procedures have been described in an earlier publication ). Two aspects of emotional experience were rated: emotional valence (positive-negative scale) and arousal which were acquired on separate runs. While watching the movie in the middle of the screen, the subjects could move a small cursor on the right side of the screen up and down on a scale using the computer mouse to report their current state of valence or arousal using a web tool   https://version.aalto.fi/gitlab/eglerean/dynamicannotations  . The self-ratings were collected at 5 Hz sampling rate. 


#### Behavioral questionnaires 
  
The subjects were asked after the first fMRI session five short freeform questions about their perception of the movie, specifically about how easy it was to take one or the other perspective, and whether they would have donated their kidney if in place of the movie protagonist. After the second fMRI session all subjects were debriefed by showing them the ending of the original movie, where it is revealed that the sick sister had wished for the healthy sister to refuse donating her kidney. Afterwards they were asked if seeing the real ending changed their opinion on the roles of the two movie protagonists. 

As an additional self-report measure, the subjects’ disposition for catching emotions from others was assessed with two emotional empathy questionnaires: Hatfield’s Emotional Contagion Scale  and the BIS/BAS scale . Every subject also filled in a questionnaire quantifying their social network , including their emotional closeness to their sister and best friend. The names of the sister and best friend were obtained from this questionnaire for the moral dilemma task. 



### Analysis of behavioral measurements 
  
#### Valence and arousal measurements 
  
To test whether dynamic valence and arousal were different between the genetic and non-genetic condition, we first computed inter-subject similarity matrices using valence and arousal rating time-series. These were compared against a similarity matrix for the experimental conditions of the viewing preceding the valence/arousal rating, i.e. the model tests for the case where individuals are more similar within the same condition (genetic or non-genetic), but dissimilar between conditions. Tests were performed using Mantel test with 5000 permutations. We also performed a test to see if subjects who were rating arousal and valence for the genetic condition had a stronger group similarity than subjects who rated arousal and valence for the non-genetic condition. Tests were performed using permutation based t-tests. As dynamic ratings can also be different in specific time points, we also performed a permutation-based t-test on valence and arousal values at each time point corrected for multiple comparisons across time. 


#### Heart rate and breathing rate analysis 
  
Differences between experimental conditions were computed in the same way as in the ISC analysis: Correlation values were first transformed into z-scores with the Fisher Z’s transform and then a permutation based approach was used to compute t-values and corresponding p-values . Correction for the multiple comparisons was performed with Benjamini-Hochberg false discovery rate (BH-FDR) correction at a q &lt; 0.05, corresponding to a t-value threshold of 2.133. 



### Behavioral measurements with a new group of subjects 
  
Subsequent to the fMRI experiments a new group of 30 subjects (all female, and having a sister, 18–33 years, mean age 25.5 years, right handed) were recruited for two further behavioral measurements. The subjects first performed an implicit association test (IAT). The IAT measures attitudes and beliefs that might not be consciously self-recognized by the subject or attitudes that the subjects are unwilling to report. By asking the subjects to sort, as quickly as possible, positively and negatively connoted words into categories, the IAT can measure the reaction times of the association process between the categories and the evaluations (e.g., good, bad). It has been shown in previous studies that making a response is easier and thus faster if the category is matching the implicit evaluation the subject bears in mind . In this study the two categories were “genetic sister” (sisko) and “adopted sister” (adoptiosisko). The two categories were paired in different randomized runs with positive or negative words, thus the experiment comprised separate runs asking the subjects to either match the positive words with the category “genetic sister” and negative words with the category “adopted sister” or vice versa to match positive words with the category “adopted sister” and negative words with the category “genetic sister”. The order in which the runs are presented counter-balanced across subjects and categories switched their localization on the screen in different runs to be on the left or right side of the screen to the same extent. Subjects were asked to press a key with either the right or the left hand and thus assign the evaluation word to one category on either the left or right hand side of the computer screen. With the experiment going on, the number of trials in this part of the IAT is increased in order to minimize the effects of practice. The IAT score is based on how long it takes a person to sort the words with the condition associating positive words and genetic (and negative and adopted) in contrast to negative words and genetic (and positive words and adopted). If an implicit preference exist for one of the categories subjects would be faster to match positive words to that category relative to the reverse. Data were analysed using Matlab. Similarity between subjects’ scores were examined TOST testing . As a second task, reaction times for the moral decision task were measured with the same group of subjects that underwent the IAT. As a difference to the decision task performed during fMRI scanning the order of the decisions was randomized (with easy decision including only strangers and difficult decisions including the sister on one side and the friend on the other). Reaction times were measured as the time between the onset of the slide revealing the identity of the involved individuals and the button press of the subject that related her decision. 


### Data availability 
  
The data that support the findings of this study are available on request from the corresponding author MBT. The data are not publicly available due to a prohibition by the Finnish law: Juridical restrictions set by the Finnish law prevent public access to the collected data, be it anonymized or non-anonymized, when data are recorded from human individuals. As the consent given by the subjects only applies to the specific study reported in our manuscript, no portion of the data collected could be used or released for use by third parties.</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (2)</summary><details class="inner-accordion"><summary>Table 1 (Tab1) - Clusters size, peak coordinates and t value of all clusters of Experiment 1 (movie watching task).</summary><div class="table-html"><table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Clusters size, peak coordinates and t value of all clusters of Experiment 1 (movie watching task).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Cluster Label: movie watching task</th><th colspan="1" rowspan="1">Cluster extent (voxels)</th><th colspan="1" rowspan="1">x MNI (mm)</th><th colspan="1" rowspan="1">y MNI (mm)</th><th colspan="1" rowspan="1">z MNI (mm)</th><th colspan="1" rowspan="1">Peak value (T-value)</th></tr></thead><tbody><tr><td colspan="6" rowspan="1">
<bold>clusters genetic &gt; non-genetic</bold>
</td></tr><tr><td colspan="1" rowspan="1">Superior parietal lobule (R)</td><td colspan="1" rowspan="1">8794</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">−86</td><td colspan="1" rowspan="1">44</td><td colspan="1" rowspan="1">14.6272</td></tr><tr><td colspan="1" rowspan="1">Ventromedial prefrontal cortex (R)</td><td colspan="1" rowspan="1">4099</td><td colspan="1" rowspan="1">0</td><td colspan="1" rowspan="1">54</td><td colspan="1" rowspan="1">−8</td><td colspan="1" rowspan="1">8.3312</td></tr><tr><td colspan="1" rowspan="1">Superior temporal gyrus (L)</td><td colspan="1" rowspan="1">2182</td><td colspan="1" rowspan="1">−42</td><td colspan="1" rowspan="1">−26</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">16.974</td></tr><tr><td colspan="1" rowspan="1">Superior temporal gyrus (R)</td><td colspan="1" rowspan="1">1138</td><td colspan="1" rowspan="1">62</td><td colspan="1" rowspan="1">−26</td><td colspan="1" rowspan="1">4</td><td colspan="1" rowspan="1">13.0493</td></tr><tr><td colspan="1" rowspan="1">Putamen (R)</td><td colspan="1" rowspan="1">616</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">16</td><td colspan="1" rowspan="1">−10</td><td colspan="1" rowspan="1">7.2211</td></tr><tr><td colspan="1" rowspan="1">Insula (L)</td><td colspan="1" rowspan="1">266</td><td colspan="1" rowspan="1">−40</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">−10</td><td colspan="1" rowspan="1">6.9642</td></tr><tr><td colspan="1" rowspan="1">Inferior temporal gyrus (R)</td><td colspan="1" rowspan="1">145</td><td colspan="1" rowspan="1">52</td><td colspan="1" rowspan="1">−46</td><td colspan="1" rowspan="1">−30</td><td colspan="1" rowspan="1">7.432</td></tr><tr><td colspan="1" rowspan="1">Middle temporal gyrus (R)</td><td colspan="1" rowspan="1">110</td><td colspan="1" rowspan="1">66</td><td colspan="1" rowspan="1">−4</td><td colspan="1" rowspan="1">−16</td><td colspan="1" rowspan="1">10.4489</td></tr><tr><td colspan="1" rowspan="1">Inferior frontal gyrus (L)</td><td colspan="1" rowspan="1">89</td><td colspan="1" rowspan="1">−36</td><td colspan="1" rowspan="1">34</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">5.3232</td></tr><tr><td colspan="1" rowspan="1">Superior frontal gyrus (R)</td><td colspan="1" rowspan="1">89</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">62</td><td colspan="1" rowspan="1">26</td><td colspan="1" rowspan="1">6.2544</td></tr><tr><td colspan="1" rowspan="1">Inferior parietal lobule (L)</td><td colspan="1" rowspan="1">86</td><td colspan="1" rowspan="1">−52</td><td colspan="1" rowspan="1">−60</td><td colspan="1" rowspan="1">48</td><td colspan="1" rowspan="1">5.0215</td></tr><tr><td colspan="1" rowspan="1">Postcentral gyrus (R)</td><td colspan="1" rowspan="1">81</td><td colspan="1" rowspan="1">56</td><td colspan="1" rowspan="1">−6</td><td colspan="1" rowspan="1">38</td><td colspan="1" rowspan="1">7.0252</td></tr><tr><td colspan="1" rowspan="1">Superior parietal lobule (L)</td><td colspan="1" rowspan="1">73</td><td colspan="1" rowspan="1">−14</td><td colspan="1" rowspan="1">−58</td><td colspan="1" rowspan="1">72</td><td colspan="1" rowspan="1">7.4174</td></tr><tr><td colspan="6" rowspan="1">
<bold>clusters non-genetic &gt; genetic</bold>
</td></tr><tr><td colspan="1" rowspan="1">Inferior occipital gyrus (R)</td><td colspan="1" rowspan="1">1938</td><td colspan="1" rowspan="1">40</td><td colspan="1" rowspan="1">−66</td><td colspan="1" rowspan="1">−8</td><td colspan="1" rowspan="1">−12.3679</td></tr><tr><td colspan="1" rowspan="1">Middle occipital gyrus (L)</td><td colspan="1" rowspan="1">604</td><td colspan="1" rowspan="1">−32</td><td colspan="1" rowspan="1">−84</td><td colspan="1" rowspan="1">4</td><td colspan="1" rowspan="1">−9.7755</td></tr><tr><td colspan="1" rowspan="1">Cerebellar crus II (R)</td><td colspan="1" rowspan="1">318</td><td colspan="1" rowspan="1">30</td><td colspan="1" rowspan="1">−86</td><td colspan="1" rowspan="1">−32</td><td colspan="1" rowspan="1">−7.3011</td></tr><tr><td colspan="1" rowspan="1">Temporal pole (middle) (R)</td><td colspan="1" rowspan="1">171</td><td colspan="1" rowspan="1">38</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">−34</td><td colspan="1" rowspan="1">−6.6407</td></tr><tr><td colspan="1" rowspan="1">Inferior temporal gyrus (L)</td><td colspan="1" rowspan="1">165</td><td colspan="1" rowspan="1">−54</td><td colspan="1" rowspan="1">−64</td><td colspan="1" rowspan="1">−4</td><td colspan="1" rowspan="1">−8.2858</td></tr><tr><td colspan="1" rowspan="1">Angular gyrus (L)</td><td colspan="1" rowspan="1">147</td><td colspan="1" rowspan="1">−42</td><td colspan="1" rowspan="1">−54</td><td colspan="1" rowspan="1">28</td><td colspan="1" rowspan="1">−5.5752</td></tr><tr><td colspan="1" rowspan="1">Superior temporal gyrus (R)</td><td colspan="1" rowspan="1">95</td><td colspan="1" rowspan="1">64</td><td colspan="1" rowspan="1">−28</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">−8.3381</td></tr><tr><td colspan="1" rowspan="1">Inferior frontal gyrus (R)</td><td colspan="1" rowspan="1">94</td><td colspan="1" rowspan="1">52</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">16</td><td colspan="1" rowspan="1">−5.7279</td></tr><tr><td colspan="1" rowspan="1">Superior temporal gyrus (R)</td><td colspan="1" rowspan="1">87</td><td colspan="1" rowspan="1">64</td><td colspan="1" rowspan="1">0</td><td colspan="1" rowspan="1">−4</td><td colspan="1" rowspan="1">−9.6944</td></tr><tr><td colspan="1" rowspan="1">Precentral gyrus (L)</td><td colspan="1" rowspan="1">86</td><td colspan="1" rowspan="1">−44</td><td colspan="1" rowspan="1">10</td><td colspan="1" rowspan="1">52</td><td colspan="1" rowspan="1">−6.1405</td></tr><tr><td colspan="1" rowspan="1">Superior frontal gyrus (L)</td><td colspan="1" rowspan="1">86</td><td colspan="1" rowspan="1">−8</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">60</td><td colspan="1" rowspan="1">−7.8522</td></tr></tbody></table></table-wrap>
</div></details><details class="inner-accordion"><summary>Table 2 (Tab2) - Clusters size, peak coordinates and t value of all clusters of and Experiment 2 (moral dilemma decision task).</summary><div class="table-html"><table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Clusters size, peak coordinates and t value of all clusters of and Experiment 2 (moral dilemma decision task).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Cluster Label: moral dilemma decision task</th><th colspan="1" rowspan="1">Cluster extent (voxels)</th><th colspan="1" rowspan="1">x MNI (mm)</th><th colspan="1" rowspan="1">y MNI (mm)</th><th colspan="1" rowspan="1">z MNI (mm)</th><th colspan="1" rowspan="1">Peak value (T-value)</th></tr></thead><tbody><tr><td colspan="1" rowspan="1">Cerebellar lobule VI (R)</td><td colspan="1" rowspan="1">10417</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">−54</td><td colspan="1" rowspan="1">−30</td><td colspan="1" rowspan="1">10.1921</td></tr><tr><td colspan="1" rowspan="1">Superior frontal gyrus (L)</td><td colspan="1" rowspan="1">3631</td><td colspan="1" rowspan="1">−8</td><td colspan="1" rowspan="1">10</td><td colspan="1" rowspan="1">48</td><td colspan="1" rowspan="1">8.4473</td></tr><tr><td colspan="1" rowspan="1">Precentral gyrus (L)</td><td colspan="1" rowspan="1">2476</td><td colspan="1" rowspan="1">−34</td><td colspan="1" rowspan="1">−18</td><td colspan="1" rowspan="1">58</td><td colspan="1" rowspan="1">7.2107</td></tr><tr><td colspan="1" rowspan="1">Inferior frontal gyrus (L)</td><td colspan="1" rowspan="1">1207</td><td colspan="1" rowspan="1">−52</td><td colspan="1" rowspan="1">12</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">6.3788</td></tr><tr><td colspan="1" rowspan="1">Superior parietal lobule (L)</td><td colspan="1" rowspan="1">815</td><td colspan="1" rowspan="1">−28</td><td colspan="1" rowspan="1">−56</td><td colspan="1" rowspan="1">48</td><td colspan="1" rowspan="1">6.0945</td></tr><tr><td colspan="1" rowspan="1">Amygdala (L)</td><td colspan="1" rowspan="1">720</td><td colspan="1" rowspan="1">−14</td><td colspan="1" rowspan="1">−12</td><td colspan="1" rowspan="1">−12</td><td colspan="1" rowspan="1">6.2453</td></tr><tr><td colspan="1" rowspan="1">Putamen (R)</td><td colspan="1" rowspan="1">257</td><td colspan="1" rowspan="1">24</td><td colspan="1" rowspan="1">10</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">5.4598</td></tr><tr><td colspan="1" rowspan="1">Superior parietal lobule (R)</td><td colspan="1" rowspan="1">199</td><td colspan="1" rowspan="1">26</td><td colspan="1" rowspan="1">−62</td><td colspan="1" rowspan="1">54</td><td colspan="1" rowspan="1">4.8181</td></tr><tr><td colspan="1" rowspan="1">Angular gyrus (L)</td><td colspan="1" rowspan="1">156</td><td colspan="1" rowspan="1">−48</td><td colspan="1" rowspan="1">−52</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">4.9727</td></tr><tr><td colspan="1" rowspan="1">Insula (R)</td><td colspan="1" rowspan="1">128</td><td colspan="1" rowspan="1">40</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">0</td><td colspan="1" rowspan="1">5.3664</td></tr><tr><td colspan="1" rowspan="1">Middle temporal gyrus (L)</td><td colspan="1" rowspan="1">123</td><td colspan="1" rowspan="1">−62</td><td colspan="1" rowspan="1">−24</td><td colspan="1" rowspan="1">−12</td><td colspan="1" rowspan="1">4.7632</td></tr><tr><td colspan="1" rowspan="1">Temporal pole (middle) (L)</td><td colspan="1" rowspan="1">77</td><td colspan="1" rowspan="1">−56</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">−24</td><td colspan="1" rowspan="1">5.4375</td></tr></tbody></table></table-wrap>
</div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 31056647</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/31056647/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>31056647_analysis_0</td><td>Emotional support group &gt; no support group</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific contrast (emotional support vs no support on exclusion) indexes affective regulation rather than explicit representation or reasoning about others’ mental states (ToM); criteria for Perception of Others are not met.</td></tr>
<tr><td>31056647_analysis_1</td><td>Appraisal support group &gt; no support group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Results explicitly involve ToM/TPJ activity and interpretation of others’ intentions; the contrast measures understanding/updating of others’ mental states, meeting perception-of-others criteria.</td></tr>
<tr><td>31056647_analysis_2</td><td>Appraisal support group &gt; no support group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis explicitly probes neural regions (TPJ, ToM network) involved in understanding others’ minds and the effect of provided information about others; it matches Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 6688450)</summary><p><strong>Title:</strong> Social support modulates the neural correlates underlying social exclusion</p><details><summary>Abstract</summary><pre class="paper-text">Ostracism threatens the human need for social interactions, with negative consequences on cognition, affect and behavior. Understanding the mechanisms that can alleviate these consequences has therefore become an important research agenda. In this study, we used behavioral and fMRI measures to advance our understanding how social support can buffer the negative effects of social exclusion. We focused on two different types of support from a friend: emotional support, conveyed by gentle touch and appraisal support, implemented as informative text messages. Seventy-one female participants underwent fMRI scanning while playing a virtual ball-tossing game in the course of which they were excluded. Two consecutive runs of the game were separated according to the participant’s experimental condition (appraisal support, emotional support and no support). Results showed that the experience of social exclusion is modulated by the type of support received. Specifically, emotional support decreased negative emotions and anterior insula activity, while appraisal support increased negative emotions, with concomitant increase of subgenual anterior cingulate cortex and decrease of temporal-parietal junction activity. These divergent effects of social support point to the necessity to characterize whether and under which conditions it represents an effective and positive resource to alleviate the negative consequences of social exclusion.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
The aim of the present study was to investigate how different types of social support reduce negative feelings associated with social exclusion and its activation at the neural level. Human beings have a fundamental need to interact with each other. Ostracism (social exclusion) threatens this need and has various effects on cognition, affect and behavior ( ). It is often associated with experiences of pain, often called social pain, defined as ‘the distressing experience arising from the perception of actual or potential psychological distance from close others or a social group’ ( ;  ).   refers to it as one of the most painful and emotionally unpleasant conditions that the individual can live with, as it bears the risk of damaging his ability to relate to other individuals. Experimental neuroscientific research over the past decades has extensively focused on the understanding of ostracism’s neurophysiological underpinnings. Mainly investigated via computer-controlled ball-tossing games (the cyberball game, see   for review), the experience of exclusion from the game (social exclusion) usually results in feelings of unpleasantness and discomfort, with concomitant recruitment of a network of brain areas associated with the processing of negative affect, such as the dorsal anterior cingulate cortex (dACC), the subgenual anterior cingulate cortex (subACC) ( ;  ;  ;  ) and the anterior insula (AI) ( ). It is currently a matter of debate if the fingerprint of social exclusion resembles the negative experience associated with pain of physical nature ( ;  ). For example, the experiences of social exclusion and physical pain reflect many common psychological and biological characteristics: from the use of similar words (‘I feel hurt’) ( ), the involvement of overlapping neurochemical ( ;  ) and neural systems ( ), to comparable inflammatory responses and genetic regulation ( ;  ;  ). These commonalities may stem from similar adaptive evolutionary functions ( ). As physical damage to an organism threatens its survival, and the presence of pain lead to protective responses via unpleasant and distressing psychological states, feelings of pain and discomfort after separation from the individual’s social group may serve as protective factors preventing such separation. Consequently, social pain may have promoted safety in a similar manner as physical pain; when a ‘socially painful’ event has occurred, it may drive the individual to repair the social relationship or to seek new ones ( ). However, behaviors that are adaptive when an individual experiences acute pain, e.g. avoiding activities that increase pain, when pain becomes chronic may develop into patterns of behavior that are maladaptive and impair long-term health ( ). Similarly, social pain responses that are situationally appropriate, e.g. feeling angry or avoiding a group after being rejected, may lead to less-effective coping and long-term social isolation, when they become a chronic issue ( ). Given the negative and serious long-term consequences of pain exposure, it is mandatory therefore to understand and promote factors that facilitate the remission or prevent the initiation of such psychological and behavioral effects. In that regard, positive aspects of one’s social world (social support) may improve coping responses and overall well-being. For example, according to  , p. 11), social support is configured as an ‘exchange of resources between two individuals, perceived by the one who provides it - or by those who receive it - as something aimed at increasing the well-being of the recipient’.   describes it in terms of perceived and real, useful and/or significant supplies provided by the community, social networks and trustworthy partners associated to the well-being of the subject.   identifies different types of social support:   `  emotional support’ is associated with sharing life experiences and involves the provision of empathy, love, trust and care; `instrumental support’ involves behaviors that directly help people in need using tangible help (like tangible services and economic benefits); `informational support’ involves the provision of advice, suggestions and information that a person can use to address problems; and finally `appraisal support’ involves the provision of information that is useful for evaluation purposes: constructive feedback, affirmation and social comparison. Several empirical studies ( ;  ;  ) have examined the function of social support on the perception of physical pain, demonstrating a remarkable correlation between social support and the reduction of physical pain experience. Meaningful social connections have also been shown to serve a protective role in reducing neural, physiological and neuroendocrine responses to pain and stress including heart rate, blood pressure and cardiovascular and neuroendocrine responses ( ;  ). Given the strong commonalities between physical and social pain, it is not surprising that the interest on the effects of social support on physical pain has been extended to stressors of social nature, with similar results reported. In particular, psychosocial stress caused by social evaluation ( ) has been observed to be reduced by social support ( ). Interestingly, different types of social supports (verbal support, physical contact) have been associated to different reactions in women ( ), suggesting that not all types of social support are effective in reducing the physiological responses to social stress. In spite of the rich scientific literature on social support and psychological stress upon social evaluation, only few studies have directly examined the effects of social support on the feeling of social pain caused by, for example, social exclusion or ostracism. Similarly, to psychological stress, these studies suggest that the presence of a friend ( ), supportive emotional texts ( ) or gentle slow touch ( ) are able to reduce the negative feelings caused by social exclusion. On the neural level, self-reported supportive daily life interactions have been shown to diminish neuroendocrine stress responses and to correlate with decreased activity in the dACC following ostracism ( ). Similarly,   observed that supportive emotional text leads to reduced AI and enhanced theory of mind (ToM) network activity ( ;  ;  ;  ) during social exclusion. 

To date, however, a single study examining how different types of social support modulates feelings of social pain and how this is represented at the neural level has not been performed yet. Our study aimed, for the first time, at disclosing the role of different support strategies in modulating the behavioral and neural correlates involved in social exclusion. Specifically, we used two different types of support: emotional physical support (emotional support), which we implemented as gentle touch, and informational/appraisal support (appraisal support), which we implemented as informative text messages allowing to better understand the situation. In line with the previous literature, we hypothesized feelings of social pain, induced via exclusion from a virtual ball-tossing game, to be reduced after experiencing social support. Furthermore, we hypothesized such behavior to be associated with reduced activity of the neural network involved during the experience of social exclusion. Finally, we expect different neurophysiological effects depending on the type of social support experienced. In particular, we expected the emotional support group to show modulatory effect in the affective network (e.g. AI, ACC) while the appraisal support group to additionally modulate the ToM network ( ) 


## METHODS 
  
### Participants 
  
In total, 81 Italian female volunteers (age, 21.67 ± 2.29 years) with no history of neurological or psychiatric disorders (assessed with semi-structured interviews conducted by a psychologist) were recruited among undergraduate students at the University of Turin. We chose to include females only, as gender differences on social exclusion are well-documented (see  ;  ). All participants were right-handed according to the Edinburgh Handedness Inventory ( ). Female friends of a similar age as the participants were invited to participate in the experiment, and instructed to act as confederates. Subjects were randomly assigned to one of the three groups: appraisal support group (N = 26), emotional support group (N = 26) and no support group (N = 29). Ten participants were excluded from the study because of excessive movement or lack of compliance during the functional Magnetic Resonance Imaging (fMRI) session, leaving the final sample for the three groups as follows: appraisal support group (N = 23), emotional support group (N = 23) and no support group (N = 25). All participants signed the information
consent after the experimental procedures have been described to them. The study was approved by the Bioethics Committee of the University of Turin. 


### Social pain task 
  
In order to create in the fMRI environment the uncomfortable situation in which participants could experience social exclusion, we used a modified version of the well-known `cyberball game’ ( ), which has been widely used in the literature ( ;  ;  ;  ;  ). Our version was developed by  , who replaced the animated cartoons of the cyberball game by videos showing schematic virtual representations of real people tossing the ball to each other. The task was composed of 10 blocks with two experimental conditions: `social inclusion’ and `social exclusion’. In each block the ball-tossing game included a total of 12 passes, distributed between three players (including the participant). In the five blocks inducing the experience of social inclusion, the participant received at least one third of the total passes, while in the five blocks inducing social exclusion, the participant received less than one third of the total passages (see   for a detailed description of the stimuli preparation and procedure). Once the participant received the ball, she had to decide to whom to throw it back by pressing with her index (left player) or middle (right player) finger on an Magnetic Resonance Imaging (MRI) compatible button box. The presentation of the blocks was equal for all the participants with a pseudorandomized order: the first three and the last two blocks belonged to the inclusion condition, while the five blocks placed in the central position of the task belonged to the exclusion condition. Each ball-tossing game had an average duration of 33.5 s (range, 30–40 s). At the end of each game, the participant was asked to answer the question ‘How are your emotions?’ in order to report the valence and intensity of the emotions experienced during the game on a Likert scale with nine discrete values (from −4 = very negative on 0 to +4 = very positive) displayed for 4 s. The answer was given by using the same button box used to throw the ball (see  ). Of crucial relevance, this sequence of 10 blocks was performed twice, in two separate fMRI runs. In between the runs, emotional or appraisal support was provided by the participant’s friend in the two experimental groups, while no support was provided in the control group. 
  
 Exemplar trial for the social pain task.   In each trial, participants played the game with other two virtual players. During the game, once they receive the ball, they have to decide to whom to throw it back (as illustrated by the two arrows) by pressing the left or right key on the pad. In the inclusion condition, participants received the ball at least one third of the total tosses. In the exclusion condition, participants received the ball less than one third of the total tosses. Immediately after the game, they were asked to answer the question ‘How are your emotions?’ on a 9-point Likert scale, displayed for 4 s. Interstimulus interval was randomly jittered between 1 and 3 s. Arrows in the inclusion condition are inserted only for descriptive purposes and not displayed during the game. 
  

### Social support manipulation 
  
Two experimental groups of social support have been defined: emotional and appraisal. In the emotional support group  ,   each confederate (the female friend) was instructed to gently touch the hand of the participant, with the aim of comforting her. The characteristic of this group was the administration of support only through physical contact, without the use of verbal or expressive linguistic expressions. No specific constraints on how to deliver the touch was given to the confederates. Rather, they should hold, caress and tenderly squeeze her friend’s hand as she would normally do when trying to comfort her. 

In the appraisal support group, social support was given by the participant’s friend through text messages delivered and displayed on the back-projection screen in the scanner. In particular, the participants were told that the phrases they read on the monitor were written and sent directly by their friend from a PC situated in another room, where she could follow the game. Each participant saw 10 pre-prepared phrases meant to give additional information in order to help the understanding of the experience of social exclusion (for example: ‘I think that these two players are actually friends’ or ‘I think that when the experiment will end, we’ll see them go away together’). Importantly, the content of the text was never aimed to comfort the subject but rather to give information allowing the reappraisal of the situation, and it was always emotionally neutral. 

Finally, in order to tease apart the effect of the repetition of the task (adaptation, fatigue, etc.), the control group did the social exclusion task twice but without receiving any kind of support in between. We chose such control condition because the mere presence of a friend, even without delivering any social support, could have affected the following experience of social exclusion ( ). All conditions of social support lasted for 3 min and were delivered between runs 1 and 2 of the cyberball game, while the subject was resting inside the scanner. For the no support group, the same interval was kept between runs 1 and 2, and the subject asked to wait still for the next run to start. 


### Procedure 
  
Each participant, previously randomly assigned to one of three groups, and her friend (except for the no support group in which participants came alone) were received in the fMRI room of the hospital and informed about the study. Specifically, participants were told that they would be connected via Internet to two other players, located in another room of the hospital. After the general information, each confederate was accommodated in the adjacent room for observing through a monitor what happens to her friend during the game. Here she was instructed on what she had to do for the different support conditions. For all participants, after the verbal instruction about the cyberball game, a training session was performed outside of the scanner to ensure that participants understood the game. A second short practice session was administered in the scanner to familiarize the participants with the response recording system. The cyberball game was programmed using Cogent toolbox (2000), running on Matlab 2007 (Mathworks, Cherborn, MA, USA). Inside the scanner, the stimuli were presented via a head coil-mounted display system (Resonance Technology, Inc.). The fMRI session was composed of three phases performed on the same day (see  ): (i) social pain task run 1: each participant was scanned while engaging in the virtual cyberball game, as described above; (ii) social support: each experimental group received social support (e.g. emotional or appraisal), while the control group did not receive any kind of support. During this section, no fMRI scanning was performed. (iii) Social pain task run 2: each participant was scanned for the second time while engaging in the virtual cyberball task, as described above. After the fMRI session, each participant answered a brief interview aimed at investigating the believability of the manipulation. In particular, we asked indirect questions such as: ‘What do you think about the players? How was the game for you? Do you have any comments?’ None of the participants expressed doubts about the veracity of the situation. 
  
 Timeline of the fMRI session.   Each fMRI session was divided into three phases performed on the same day: (i) social pain task run 1, (ii) social support (emotional, appraisal, no support), (iii) social pain task run 2. Social support was either emotional or appraisal for a duration of 3 min. In the case of the no support group, a 3 min break between the two runs was carried out. 
  

### MRI data acquisition 
  
The MRI data were acquired using a 3.0 T MRI Scanner (Philips Ingenia) with a 32-channel array head coil. The study was performed at the Center of Brain Imaging 3 T-NIT, at the Hospital Città della Salute e della Scienza in Turin, Italy. Echo-Planar Image (EPI) sequence [TR/TE, 2000/30 ms; 33 slices, matrix size, 64 × 64; interslice gap, 0.5 mm; field of view (FOV), 230 × 230 mm ; flip angle, 90 degrees; slices aligned to the AC-PC line, 230 volumes/run] for functional images was applied. A total of 226 volumes per subject per run were collected. The first four volumes of each run were discarded to allow the equilibration of T1 saturation effects. T1-weighted sequence MP-RAGE (TR, 8.1 ms; TI, 900 ms; TE, 3.7 ms; voxel size, 1 × 1 × 1 mm ) for structural images of the whole brain was used. 
  
Contrasts of interest 
  
Significant voxels are reported threshold of   P   ≤ 0.05 FWE corrected for small volumes.Peak activity coordinates are given in MNI space. Significant value for   P   &lt; 0.001 uncorrected. 
  

### Data analysis 
  
#### Behavioral analysis 
  
Emotional ratings given by the participants after each round of the cyberball game were analysed in order to investigate differences in the emotional experience between exclusion and inclusion trials and between the first and second run, i.e. before and after receiving social support. We conducted a repeated-measures ANOVA with two within-subjects factors, condition (inclusion, exclusion), time (runs 1 and 2), and one between-subject factor, group (emotional support, appraisal support, no support). Ratings of the exclusion condition were multiplied by −1 in order to carry the same direction as the inclusion ratings, allowing to test the three-way interaction. Statistical analyses were performed with IBM SPSS Statistics version 24. 


#### fMRI data analysis 
  
The MRI data were analysed using Statistical Parametric Mapping 12 (SPM12, Wellcome Department of Cognitive Neurology, London, UK) run on Matlab 2007 (Mathworks, Cherborn, MA, USA). All functional images have been pre-processed following this order: spatially realigned to the first volume, co-registered to the mean image, segmented in cerebrospinal fluid tissues, gray matter and white matter, then normalized to the Montreal Neurological Institute (MNI) space and finally smoothed at the first level with an 8 mm full-width half-maximum Gaussian Kernel, with an additional 6 mm at the second level. Low-frequency drifts, high-pass temporal filtering with a cut-off of 128 s was used. After preprocessing, a General Linear Model ( ) for statistical analysis was used for both functional runs. Regressors of interest were convolved with a canonical hemodynamic response function. For each participant’s first level analysis, six regressors were computed: social inclusion (I), social exclusion (I), emotion rating (I), social inclusion (II), social exclusion (II) and emotion rating (II). In addition, six parametric regressors of no interest were added to the design matrix to correct residual effects of head motion. At the second level, four contrasts of interest from the first-level analyses were fed into a flexible factorial design aiming at investigating the effect of social support on social exclusion, using a random effects analysis ( ). Linear contrast of the repeated-measures ANOVA with the within-subject factors, condition (exclusion, inclusion), time (runs 1 and 2), and the between-subject factor, group (emotional support, appraisal support, no support), were used to assess the interaction between the factors group and time. Given the main research question of our paper, only results for the exclusion condition are reported. We performed whole brain analyses with an initial threshold of   P   &lt; 0.001 uncorrected and reports clusters that survived Family-Wise Error (FWE) correction for small volumes (SVC) at   P   &lt; 0.05. For the SVC, we created two binary masks encompassing, first, the affective network specifically detected in social exclusion paradigms, and second, a network associated to representing other minds and intentions (ToM). Both masks are based on the most recent published meta-analyses on social exclusion and ToM, respectively. More specifically, the first mask included coordinates derived from two meta-analyses on social exclusion published by   and  . In spite of repeated attempts, it was, however, not possible to receive the original maps from both authors. Therefore, spheres of 10 mm radius centered on the reported main activation loci were generated and combined into one mask with the toolbox MarsBaR ( ). The second mask was provided as an image-based mask by ( ), based on their meta-analysis on ToM tasks (see   for more details). Given we did not expect the involvement of the ToM network for the emotional support group, only the first (affective) mask was used to investigate differences in activations between this group and the no support group. To investigate differences in activations between the appraisal support and no support groups and the emotional support and appraisal support groups, both the affective and the ToM masks were used. The MRIcron software package ( ;  ) was used for anatomical and cytoarchitectonic display and interpretation. 


#### Brain–behavior correlation analyses 
  
Pearson correlation analyses between brain activity and behavioral ratings were performed with IBM SPSS Statistics version 24. In particular, the difference in activity (∆) between the first and second run of social exclusion in the regions showing significant statistical difference (see  ) was correlated with the difference in emotional ratings between the two runs (run 1 minus run 2). Activity in these regions was extracted with REX (  http://web.mit.edu/swg/rex/rex.pdf  ). Correlations were performed for each group separately and corrected for the number of ROIs used in each group. 




## RESULTS 
  
### Behavioral results 
  
The ANOVA revealed a significant interaction effect time*condition*group [  F   = 3.39,   P   = 0.040, partial Eta squared = 0.091]. All the other effects and interactions were not significant (  F   &lt; .103) . Post hoc pairwise comparisons were used in order to characterize the effect of the triple interaction. In particular, in the emotional support group, a significant difference between exclusion run 1   vs   run 2 was observed, defined by a reduction of unpleasantness ratings during the second run (  M   = 5.57, SE = 1.91,   P   = 0.005). In the appraisal support group  ,   a significant difference between exclusion run 1   vs   run 2 was also observed, but with an opposite pattern, namely an increase of unpleasant emotions in the second run (  M   = −3.93, SE = 1.91,   P   = 0.044). The no support group did not show any significant difference between runs 1 and 2 for both conditions (see  ). Finally, the differences between the inclusion and exclusion runs (Δ inclusion, Δ exclusion) were entered in a one-way ANOVA to assess whether the groups significantly differed. The analysis revealed a significant difference between the groups in the Δ exclusion only (  F   = 6.22,   P   = 0.003). Post hoc multiple comparisons were used in order to characterize the effect. In particular, we observed a significant difference both between the emotional support group and the no support group (  M   = 0.564, SE = 0.265,   P   = 0.037) and the emotional support group and the appraisal support group (  M   = 0.949, SE = 0.270,   P   = 0.001). 
  
 Behavioral results.   Mean and confidence intervals (95%) divided by group, condition and run. Significant differences are marked with an asterisk (  P   &lt; 0.05, based on post hoc pairwise comparisons) 
  

### fMRI results 
  
  Emotional support group vs. No support group.   

 Emotional support (social exclusion run 1 &gt; social exclusion run 2) &gt; no support (social exclusion run 1 &gt; social exclusion run 2).  

The analysis revealed significantly reduced activation in the right AI (rAI, x = 33, y = 27, z = −8) for the emotional support group compared to the no support group ( ;  ) for the second run compared to the first run of social exclusion . 
  
 FMRI results.   Differences in the neural activation between the emotional support group vs. the no support group for the contrast: social exclusion run 1   &gt;   social exclusion run 2. The bar plots represent contrast estimates and 90% confidence intervals in the right AI. For illustrative purposes, statistical maps are displayed with a threshold of   P   &lt; 0.001 uncorrected and superimposed on a standard T1 template. 
  
 Emotional support (social exclusion run 2 &gt; social exclusion run 1) &gt; No support (social exclusion run 2 &gt; social exclusion run 1).  

No suprathreshold voxels were observed for the reverse contrast. 

  Appraisal support group vs. no support group.   

 Appraisal support (social exclusion run 1 &gt; social exclusion run 2) &gt; No support (social exclusion run 1 &gt; social exclusion run 2).  

The analysis revealed significantly reduced activation in the right temporal parietal junction (rTPJ, x = 46, y = −47, z = 27) for the appraisal support group compared to the no support group ( ;  ) for the second compared to the first run of social exclusion. A more liberal threshold of   P   &lt; 0.001 revealed reduced activation also in the left temporal parietal junction (lTPJ, x = −48, y = −53, z = 34). 
  
 FMRI results.   Differences in the neural activation between the appraisal support group   vs   the no support group for the contrast: social exclusion run 1 &gt; social exclusion run 2. The bar plots represent contrast estimates and 90% confidence intervals in the right TPJ. For illustrative purposes, statistical maps are displayed with a threshold of   P   &lt; 0.001 uncorrected and superimposed on a standard T1 template. 
  
 Appraisal support (social exclusion run 2 &gt; social exclusion run 1) &gt; No support (social exclusion run 2 &gt; social exclusion run 1).  

The analysis revealed significantly increased activation in the subACC (x = −5; y = 32, z = −5) for the appraisal support group compared to the no support group ( .  ) for the second compared to the first run of social exclusion. A more liberal threshold of   P   &lt; 0.001 revealed reduced activation also in the ventromedial prefrontal cortex (vmPFC) (2, y = 37, z = −8). 
  
 FMRI results.   Differences in the neural activation between the appraisal support group   vs   the no support group for the contrast: social exclusion run 2 &gt; social exclusion run 1. The bar plots represent contrast estimates and 90% confidence intervals in the subACC. For illustrative purposes, statistical maps are displayed with a threshold of   P   &lt; 0.001 uncorrected and superimposed on a standard T1 template. 
    
 Correlation results.   Scatterplot of the correlation between the difference in subACC activity between exclusion runs 1 and 2 (∆ subACC) and the difference in unpleasantness ratings between exclusion runs 1 and 2. 
  
  Emotional support group vs. Appraisal support group.   

No suprathreshold voxels were observed in any of the possible combinations. 


### Brain-behavior correlation analyses 
  
The following correlations were performed: (i) for the emotional support group, correlation between ∆ activity in rAI and ∆ unpleasantness ratings and (ii) for the appraisal support group, correlation between ∆ activity in rTPJ, subACC and ∆ unpleasantness ratings. The correlation analyses revealed a significant positive relationship between ∆ subACC and ∆ unpleasantness ratings in the appraisal support group (r(23) = 0.443,   P   &lt; 0.017 one tailed, corrected for the number of correlations performed). This means that in the appraisal support group, the increase of subACC activity observed in the second run of exclusion was associated to increased unpleasantness feelings in the second run ( ). All the other correlations were not significant. 



## DISCUSSION 
  
In the present study, we investigated the effects of different types of social support (emotional and appraisal) on the behavioral and neural correlates of the experience of social exclusion. Seventy-one female participants were scanned twice while playing the cyberball game. Between the two runs of the game, different types of support were delivered by a female friend. At the behavioral level, we observed that, compared to the control group (no support), the sample that received emotional support in the form of gentle touch, reported reduced feeling of unpleasantness during exclusion trials between the first and second run of the game, i.e. after they had received the emotional support. Our results are in line with the findings of  , which showed reduced reported distress associated to ostracism, after being touched with optimal speed (3 cm/s) to induce positive feelings and thereby promoting interpersonal touch and affiliative behavior ( ). By adding these results, our study was able to show for the first time that the experience of emotional support is associated, at the neural level, to a reduction of activity in right AI, a brain area involved in the processing of negative affect during social exclusion and self- and other-directed aversive experiences ( ;  ;  ). The effects of emotional social support on the experience of social pain resemble the findings reported on pain of physical nature ( ;  ). In particular, during the administration of painful stimuli, married women who held the hand of their partners indicated a lower value of perceived pain. The subjective experience was correlated with reduced activation of the brain areas involved in pain processing, including the AI ( ). Moreover, imagined social support, provided through the visualization of images portraying of loved ones, was also able to modify the neural activation of the insula ( ;  ) and reduce the feeling of distress upon physical pain. The similar effect of emotional support on social and physical pain suggests overlapping regulatory mechanisms, possibly associated to the activity of the μ-opioid system and its analgesic properties ( ). 

The more informative type of support yielded instead different results. At the behavioral level, participants reported increased feelings of unpleasantness after receiving information about the other two participants. The subjective experience was accompanied by a reduced activation in the right TPJ, an area included in the ToM network ( ;  ;  ) and involved in incongruency detection and self-other distinction ( ;  ;  ). TPJ is considered a central structure implicated in the representation of mental states of others ( ). A recent study has associated the function of this brain region to the update of the internal models of the situation in order to generate appropriate actions to the social contexts ( ). This function is particularly important when faced with unexpected stimuli that demand attention reorienting and model updates. The findings of the present study suggest that the information received during the support possibly allowed the participants to interpret what was happening during the first run of the game. Indeed, the participants that received information (e.g. ‘the two players are friends’ or ‘there is understanding between them’) leading to a better understating of the social situation, showed an increase of unpleasant emotions (possibly anger) and possibly a reduced need to understand what was happening, indicated by reduced activity in TPJ. To corroborate this hypothesis, we observed increased recruitment of the subACC after receiving the appraisal support. Furthermore, the increased activity in subACC was positively correlated with the increased negative feelings reported during the second run. Interestingly, the subACC is a region involved in affective processes but not in physical pain ( ). Several social pain studies have indicated an increase in activity in the subACC during the negative experience of social exclusion ( ;  ;  ).   indicated the possibility that greater responsivity in the subACC during peer rejection could reflect an inability to properly regulate emotions evocated by negative events. In line with this literature, some studies have shown that this area is more responsive to negative emotional stimuli among depressed patients and correlates to the severity of depressive symptoms ( ;  ). Notably and differently from subACC, the increased negative affect did not result in a concomitant increase of AI activity, suggesting that the effects of ostracism on affective pain-related brain areas were not modulated by this type of support received. These findings point to a different role of these two areas in emotional processing during social exclusion, possible link to affective saliency and the need of emotion regulation. Our results are partially in line with the findings by  , who reported different reactions depending on the type of social support (verbal or physical contact) received. In particular, they observed that only physical contact was effective in reducing the symptoms of distress associated to negative social evaluation, while verbal support did not show any different from the no support condition. In our case, though, the appraisal support group showed increased negative feelings and concomitant neural response. It is possible to speculate that the negative reaction observed after appraisal support could have adaptive functions for the person experiencing it, in that it may drive the individual to seek for new relationship, when the actual ones are dysfunctional ( ). 

In conclusion, our study provides the first neuroimaging evidences that experiences of social support can modulate regions of the brain recruited during social pain and possibly responsible for coding the negative valence and intensity of emotion experience. Furthermore, for the first time, we showed that this effect may be different depending on the type of support received. Social support is a very complex phenomenon in which various factors can influence how it is effective for the receiver (e.g. who is providing it, in which form, etc.). It has been shown that it does not always result in a reduction of the negative experiences associated to social stress ( ) and social pain. Instead, as observed in our study, it can also increase the negative emotional experience, which can still be functional for the individual in the short term. Therefore, it is very important to understand under which conditions (contextual, personal, modality, etc.) social support can represent an effective and positive resource to alleviate the negative consequences of social exclusion. Importantly, our findings are restricted to a female sample; therefore not generalizable to the entire population. Future studied are needed to extend these findings to samples representative of the general population such as male participants and different age groups ( ) and to explore alternative types of social support (e.g. instrumental, informational). 


## Supplementary Material</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (1)</summary><details class="inner-accordion"><summary>Table 1 (TB1) - Contrasts of interest</summary><div class="table-html"><table-wrap id="TB1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Contrasts of interest</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /></colgroup><thead><tr><th rowspan="1" colspan="1" /><th colspan="3" align="left" rowspan="1">
<bold>MNI coordinates</bold>
</th><th align="left" rowspan="1" colspan="1">
<bold><italic toggle="yes">Z</italic>-score</bold>
</th><th align="left" rowspan="1" colspan="1">
<bold><italic toggle="yes">T</italic>-value</bold>
</th><th align="center" rowspan="1" colspan="1">
<bold><italic toggle="yes">P</italic>-value</bold>
</th></tr><tr><th align="left" rowspan="1" colspan="1">
<bold>Anatomical region</bold>
</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">X</italic>
</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">Y</italic>
</th><th align="left" rowspan="1" colspan="1">
<italic toggle="yes">Z</italic>
</th><th rowspan="1" colspan="1" /><th rowspan="1" colspan="1" /><th align="center" rowspan="1" colspan="1">
<italic toggle="yes">FWE corrected</italic>
</th></tr></thead><tbody><tr><td align="left" colspan="7" rowspan="1">
<bold><italic toggle="yes">Emotional support group &gt; no support group</italic></bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Exclusion run 1 &gt; exclusion run 2</italic>
</td><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /></tr><tr><td align="left" rowspan="1" colspan="1">Right AI</td><td align="left" rowspan="1" colspan="1">33</td><td align="left" rowspan="1" colspan="1">27</td><td align="left" rowspan="1" colspan="1">−8</td><td align="left" rowspan="1" colspan="1">3.29</td><td align="left" rowspan="1" colspan="1">3.35</td><td align="left" rowspan="1" colspan="1">.052</td></tr><tr><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /></tr><tr><td align="left" colspan="7" rowspan="1">
<bold><italic toggle="yes">Appraisal support group &gt; no support group</italic></bold>
</td></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Exclusion run 1 &gt; exclusion run 2</italic>
</td><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /></tr><tr><td align="left" rowspan="1" colspan="1">rTPJ</td><td align="left" rowspan="1" colspan="1">46</td><td align="left" rowspan="1" colspan="1">−47</td><td align="left" rowspan="1" colspan="1">27</td><td align="left" rowspan="1" colspan="1">3.42</td><td align="left" rowspan="1" colspan="1">3.48</td><td align="left" rowspan="1" colspan="1">.046</td></tr><tr><td align="left" rowspan="1" colspan="1">lTPJ</td><td align="left" rowspan="1" colspan="1">−48</td><td align="left" rowspan="1" colspan="1">−53</td><td align="left" rowspan="1" colspan="1">34</td><td align="left" rowspan="1" colspan="1">3.22</td><td align="left" rowspan="1" colspan="1">3.27</td><td align="left" rowspan="1" colspan="1">.001<sup>*</sup></td></tr><tr><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /></tr><tr><td align="left" rowspan="1" colspan="1">
<bold><italic toggle="yes">Appraisal support group &gt; no support group</italic></bold>
</td><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /></tr><tr><td align="left" rowspan="1" colspan="1">
<italic toggle="yes">Exclusion run 2 &gt; exclusion run 1</italic>
</td><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /><td rowspan="1" colspan="1" /></tr><tr><td align="left" rowspan="1" colspan="1">Left subACC</td><td align="left" rowspan="1" colspan="1">−5</td><td align="left" rowspan="1" colspan="1">32</td><td align="left" rowspan="1" colspan="1">−5</td><td align="left" rowspan="1" colspan="1">3.34</td><td align="left" rowspan="1" colspan="1">3.39</td><td align="left" rowspan="1" colspan="1">.046</td></tr><tr><td align="left" rowspan="1" colspan="1">Right vmPFC</td><td align="left" rowspan="1" colspan="1">2</td><td align="left" rowspan="1" colspan="1">37</td><td align="left" rowspan="1" colspan="1">−8</td><td align="left" rowspan="1" colspan="1">2.99</td><td align="left" rowspan="1" colspan="1">3.03</td><td align="left" rowspan="1" colspan="1">.001<sup>*</sup></td></tr></tbody></table><table-wrap-foot><p>Significant voxels are reported threshold of <italic toggle="yes">P</italic> ≤ 0.05 FWE corrected for small volumes.Peak activity coordinates are given in MNI space.<sup>*</sup>Significant value for <italic toggle="yes">P</italic> &lt; 0.001 uncorrected.</p></table-wrap-foot></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 31142792</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/31142792/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>31142792_analysis_0</td><td>Increased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast measures representations and evaluations of others (responses to disfigured vs treated faces, discussion of empathy/mentalizing), meeting criteria for perception and understanding of others.</td></tr>
<tr><td>31142792_analysis_1</td><td>Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis assessed representations and responses to others’ appearance and implied traits (dehumanization, empathy-related deactivation), fitting perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 6541618)</summary><p><strong>Title:</strong> Behavioural and Neural Responses to Facial Disfigurement</p><details><summary>Abstract</summary><pre class="paper-text">Faces are among the most salient and relevant visual and social stimuli that humans encounter. Attractive faces are associated with positive character traits and social skills and automatically evoke larger neural responses than faces of average attractiveness in ventral occipito-temporal cortical areas. Little is known about the behavioral and neural responses to disfigured faces. In two experiments, we tested the hypotheses that people harbor a disfigured is bad bias and that ventral visual neural responses, known to be amplified to attractive faces, represent an attentional effect to facial salience rather than to their rewarding properties. In our behavioral study (N = 79), we confirmed the existence of an implicit ‘  disfigured is bad  ’ bias. In our functional MRI experiment (N = 31), neural responses to photographs of disfigured faces before treatment evoked greater neural responses within ventral occipito-temporal cortex and diminished responses within anterior cingulate cortex. The occipito-temporal activity supports the hypothesis that these areas are sensitive to attentional, rather than reward properties of faces. The relative deactivation in anterior cingulate cortex, informed by our behavioral study, may reflect suppressed empathy and social cognition and indicate evidence of a possible neural mechanism underlying dehumanization.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Physical appearance has a profound impact on a person’s life. Beautiful people are preferred and enjoy many advantages compared to average-looking people . While conceptually orthogonal, the correlation of attractiveness and positive character traits indicates the prevalence of a ‘beautiful is good’ stereotype . This stereotype might be innate . Attractive people are seen as more trustworthy, socially competent, dominant, better adjusted, more capable in school and work, and also receive greater rewards and lesser punishments than their average looking peers . Adults and children ascribe desirable personality traits to attractive faces of adults and children and discriminate against unattractive faces even if they are friends and family members . Attractiveness and trustworthiness judgments are consistent across cultures  and are made extremely quickly . Longer exposure to a face does not attenuate these biases and instead only consolidates people’s confidence in a judgement already made . Attractiveness also highly influences visual exploration of faces . 

In this study we examine a corollary to the ‘beautiful is good’ stereotype, that an automatic ‘disfigured is bad’ stereotype also exists. People with facial disfigurement are stigmatized and are often targets of discrimination. Looking at disfigured faces makes observers feel less happy, less in control, less dominant, and more aroused . People with facial disfigurements are not only perceived as less attractive and less likely to be selected as romantic partners, they are also thought of as having unfavourable personality traits (e.g., lack of emotional stability, conscientiousness), internal attributes (e.g., unhappiness, lower intelligence), social qualities (e.g., untrustworthiness, unpopularity)  and are treated poorly in social interactions . In popular culture, facial disfigurement is often used to distinguish good and evil characters . Well known examples of disfigured villains are Scar in the   Lion King   (large facial scar over left eye), Freddy Krueger in   Nightmare on Elm Street   (3rd degree burns and exposed tissue), the   James Bond   villains Le Chiffre (facial scar over left eye), Emilio Largo (missing eye), Ernst Stavro Blofeld (large scar over right eye covering most of his right side of the face), and Alec Trevelyan (facial burn scars), Elle Driver in   Kill Bill   (missing eye), Two Face in the   Batman   Universe (acid scars covering the left side of his head), Hopper in   A Bug’s Life   (scar covering right eye), and the Duchess from   Alice in a Wonderland   (Macrocephaly). This ‘disfigurement is bad’ stereotype is only partially explained by lower attractiveness of disfigured faces . 

Attractiveness of faces –and therefore attribution of a’beauty is good’ stereotype- is highly correlated with typicality or statistical averageness of faces . In addition to being statistical averages of groups, attractive faces are also symmetric . Both facial symmetry and averageness are considered markers of physical health and influence peoples’ choices of partners . Disfigured faces are neither typical nor average, and are usually not symmetric. They often deviate substantially from the norm. If proximity to the norm predicts positive social attributions, being ‘different’ could lead to negative evaluations. Disfigured faces might be linked to unfavourable personality traits, internal attributes, and social qualities because they are less typical and deviate from the population average. The association of disfigurement with negative attributes probably drives stigmatization and discrimination of disfigured people in social, academic, and professional contexts . The stigmatization and discrimination of disfigured people likely contributes to low self-esteem  and long term mental health concerns similar to other stigmatized groups that are subject to dehumanization . Dehumanization deprives a person or a group of people of positive human qualities and has been shown for several stigmatized groups such as homeless people and drug addicts . Dehumanization is used as a propaganda tool in political conflicts . The strongest predictors of dehumanization are hypothesized to be perceived competence and warmth . Faces rated lowest on both competence and warmth most robustly evoke dehumanization - including feelings of disgust and lack of empathy . 

Neuroimaging studies show that seeing attractive faces evokes brain responses in reward, emotion, and visual areas compared to seeing faces of average attractiveness . Attractive faces produce activations in areas associated with reward, like the nucleus accumbens , and orbitofrontal cortex . Moreover, attractiveness correlates with increased activations in areas associated with emotion, empathy, and social cognition like the anterior cingulate cortex and medio-prefrontal cortex  the latter being particularly active in tasks in which people are not making explicit attractiveness judgements . Different regions of the prefrontal cortex are selectively responsive to either attractive or unattractive faces  which is consistent with findings that ventral medio-prefrontal cortex processes stimulus value attributes in coordination with higher order visual areas like fusiform gyri and semantic processing areas (posterior superior temporal sulcus) . Orbital frontal  and medial prefrontal cortices  seem to process both aesthetic and moral values and may represent the biological link between these two kinds of evaluation . 

Left and right amygdala seem to be sensitive to both attractive  and unattractive faces . These non-linear effects for extremes at either end of the attractiveness spectrum suggest that amygdala activation reflects sensitivity to valence intensity rather than positive or negative valence per se . In line with the valence processing hypothesis for the functional role of amygdala, increased activation in the amygdala (bilaterally) is linked to untrustworthiness of faces . A meta-analysis of brain activations to attractiveness and trustworthiness suggests that activation of amygdala and adjacent nucleus accumbens is driven by extremes and atypicality . There is some tentative evidence that face typicality can also account for the activations in medio-prefrontal and anterior cingulate cortex . The authors note that the brain networks activated in response to extremes of attractiveness and trustworthiness are remarkably similar to brain networks that process positive and negative emotions . 

In addition to increased brain activations in reward and emotion areas, attractive faces also evoke larger neural responses in selective visual processing areas within ventral occipito-temporal cortex (such as the fusiform face area) as compared to faces of average attractiveness . These areas remain sensitive to facial attractiveness even when subjects are engaged in tasks in which attractiveness judgements are not queried explicitly. These observations have previously been interpreted as evidence that these areas also process rewards. While a reward response is one possible explanation for this amplified neural response to attractive faces, it is also possible that this reflects sensitivity to the saliency of attractive faces . If this alternate hypothesis is true, other salient features, such as disfigurement, should lead to similarly amplified neural responses in visual processing areas. 

Viewing faces of stigmatized groups fails to activate brain regions associated with empathy and social cognition . Krendl and colleagues reported increased activation in anterior insula and amygdala which correlated with self-reported disgust in response to viewing faces of stigmatized groups . The lack of activation in empathy and social cognition regions of the brain is postulated to be a neural correlate of dehumanization . 

Appearance clearly affects how people are viewed and treated by others. The same mechanisms that benefit attractive people in social interaction, put unattractive people at an unfair disadvantage. The effects of discriminating against people with facial disfigurement seem to extend beyond the specific effects of lower overall attractiveness and may tie in more with the pattern of results that have been shown with stigmatized groups. 

The goal of the present study was to test the behavioural and brain responses to facial disfigurement and investigate whether surgical treatment mitigates these responses. In two experiments, we used a set of photograph pairs of patients with different types of facial disfigurements before and after surgical treatment of the disfigurement. In experiment one we tested if people harbour implicit biases against disfigured faces and if such implicit biases were different from consciously aware self-reported explicit biases. In a follow up functional MRI (fMRI) study, we tested differential automatic brain responses to the same picture pairs when naïve participants were engaged in an unrelated cover task. We hypothesized that people have negative biases against faces with disfigurement. For the neural responses to facial disfigurement we tested competing hypotheses: visual cortices respond to rewards per se, or visual cortices respond to salience. In addition, we expected disfigured faces to show selective responses in emotion and valence areas such as anterior insulae and amygdalae and anterior cingulate and lateral or medial prefrontal areas in line with the research reviewed above. 


## Results and Discussion 
  
The behavioural experiment (N = 79, see method section for details) consisted of an implicit association test  (IAT) and an explicit bias questionnaire (EBQ) to test the hypothesis that people have a negative bias for disfigured faces. For the IAT, we used a stimulus set of photographs of real patients taken before and after treatment for disfigurement. The EBQ consisted of 11 questions which query conscious biases against people with facial disfigurements (see   https://osf.io/ca2u9/   for all items and data). We found no indication of an explicit bias. However, we did find that non-disfigured faces were preferred in the IAT (see Fig.  ). This bias was particularly robust for men, consistent with previous findings . Prior exposure to disfigured faces did not modulate the implicit bias of individuals.   
Female respondents demonstrate significantly less, although still strong, implicit preference for non-disfigured faces than male respondents. Male respondents show a moderate explicit preference for non-disfigured faces while women show no explicit preference. Error bars indicate 95% confidence intervals. 
  

We used the same set of photographs of people before and after surgical treatment that we used in the IAT in the fMRI study (N = 31). Participants viewed these photographs and engaged in a gender judgement task. We measured neural responses to facial disfigurement to test competing hypotheses of reward versus salience in visual areas like fusiform face area. If these visual areas respond to rewards, then non-disfigured faces compared to disfigured faces would show increased activity in visual areas linked to face processing. If these visual areas respond to salience, then we should find the opposite results; disfigured faces compared to non-disfigured faces should show increased activity in these areas. Because people with facial disfigurement are likely treated as an outgroup , neural patterns in response to disfigurement should be similar to previous findings investigating other stigmatized groups . We predicted decreased activation in areas linked to social cognition such as medio-prefrontal cortex and anterior cingulate cortex, as well as increased activations in areas linked to disgust and negative emotion like anterior insula and amygdala. 

We found that images of people with facial disfigurement, as compared to images of the same faces after surgical treatment, evoked greater neural responses within ventral occipito-temporal cortex, particularly bilateral fusiform gyri (see Fig.  ), and right inferior frontal cortex. This observation confirms the hypothesis that face processing and adjacent areas respond automatically to the salience of faces, rather than their attractiveness or rewarding properties per se.   
Increased activations (red yellow) and deactivations (blue-green) in response to faces before treatment. Results were corrected for multiple comparisons by familywise error correction at p &lt; 0.05 with Monte Carlo permutation testing in SnPM with a combined cluster-voxel threshold (cluster defining threshold p &lt; 0.001, T &gt; 3.3852). 
  

In addition to increased responses in visual areas, we found decreases in neural response amplitude to disfigured faces in the medial anterior cingulate gyrus extending towards medial prefrontal cortex (see Figs   and  ), as well as in a region stretching from right cuneus to the right calcarine gyrus and right lingual gyrus. This finding is similar to previous observations of neural responses to other stigmatized outgroups such as drug addicts and homeless people  and could reflect suppression empathy and mentalizing or increased demands in cognitive control, e.g. inhibition of staring at the area of lesion or inhibition of inappropriate social behaviour like obvious avoidance. Both possible hypotheses are not mutually exclusive and could be linked to the increased activation in the left inferior frontal gyrus - a region linked to cognitive control.   
Increased activations (red yellow) and deactivations (blue-green) in response to faces before treatment. Results were corrected for multiple comparisons by familywise error correction at p &lt; 0.05 with Monte Carlo permutation testing in SnPM with a combined cluster-voxel threshold (cluster defining threshold p &lt; 0.001, T &gt; 3.3852). 
  

In previous studies, increased amygdala activation has been reported to both positive and negative valence of faces . Moreover, studies investigating the brain responses to extreme outgroups like homeless people and drug addicts find activations in anterior insula where it is typically interpreted as a disgust response . We did not find statistically significant activations in amygdala and anterior insula. It is possible that this lack of effect is because of our smaller stimulus sample or that the difference between before and after stimulus pairs is not large enough to produce statistically significant results in this before-after contrast of the same face. 

In sum, we found that people have implicit negative biases against faces that are disfigured, without being aware of harbouring such biases. Disfigured faces evoke greater neural responses in ventral occipito-temporal and right inferior frontal regions as compared to non-disfigured faces. This finding refutes the hypothesis that attractiveness and reward per se drives automatic ventral cortical responses and instead confirms the idea that ventral occipito-temporal regions are sensitive to the salience of faces. 

Moreover, disfigured faces evoke lower neural responses in the anterior cingulate and medio-prefrontal cortex, as well as some visual areas. This result is similar to previously reported neural responses to stigmatized outgroups like homeless people and drug addicts . In agreement with this research, we speculate that the de-activation of these brain areas upon seeing disfigured faces as opposed to the same faces after surgical treatment possibly reflects an inhibition of empathy and mentalizing or inhibition of socially inappropriate behaviour. The medial anterior cingulate gyrus and the adjacent medial prefrontal cortex are core areas of the theory of mind and empathy networks  and are crucial for inferring other’s beliefs, feelings, and mental states. Together with previous behavioural research showing a clear association of negative personality traits and our findings of an implicit bias against disfigured faces, we take these response patterns as neural evidence for stigmatization. Future research should investigate if the de-activation of anterior cingulate cortex represents a consistent neural marker for dehumanization of people with disfigured faces or if it reflects social adaptive behaviour to people who deviate from the norm. 

The emphasis of attractiveness, its association with positive attributes and robustness of these associations across cultures  highlights the pervasive effect of attractiveness in social interaction. People who fall towards the lower end of the attractiveness spectrum are disadvantaged or even subject to discrimination and social isolation as in the case of facial disfigurement. Encouragingly, our findings suggest that surgical treatment of disfigurement mitigates the negative effects of disfigurement. Our findings highlight the importance of recognizing that we implicitly and automatically regard flawed faces as flawed people and that corrective surgery confers social and psychological benefits to people with facial disfigurement. Alternative prevention strategies against discrimination of disfigured people and effective support for people with facial conditions should be explored. 


## Methods 
  
### Implicit association test (IAT) and explicit bias questionnaire (EBQ) 
  
#### Participants 
  
80 participants were recruited via an online recruiting system for psychology experiments at the University of Pennsylvania (55 female, 25 male, mean age = 23 years, SD = 6.4, range 18–56). The sample size was determined based on estimates suggested by a meta-analysis on attitudes towards individuals with disabilities as measured by an IAT . Prior to participation, participants were informed that the task was about categorising faces and words but were naïve to the fact that some of those faces might be disfigured. Participation was voluntary, and participants received money as compensation. Study procedures were approved by the Institutional Review Board (IRB) at the University of Pennsylvania (Protocol #806447). IRB approval was in accordance with the International Conference on Harmonization and the Belmont report. All participants gave written informed consent. 

One participant was excluded from the data analysis for the IAT because more than 10% of the total test trials were unreasonably fast (&lt;300 ms). After data exclusion, the data of 79 participants went into the final analysis (55 female, mean age = 23 years, SD = 6.4, range 18–56). 


#### Procedure 
  
Task order between the IAT and the EBQ was counterbalanced so that half of the participants completed the IAT first, and half of the participants completed the EBQ first. Participants were seated in a testing room, in front of a testing laptop. After having been briefed on the order of the tasks, participants gave written informed consent. The entire experiment took about 30 minutes. 

The IAT  was designed using E-Prime software and was modelled after the IATs from   Project Implicit   (  https://implicit.harvard.edu  ). A total of 16 words were used for the IAT: 8 were positive words (attractive, happy, approachable, friendly, adore, lovely, spectacular, excellent), and 8 were negative words (ugly, evil, sickening, rotten, disaster, disgust, pain, despise). 

Participants completed the EBQ as a survey on Qualtrics. Questions were modelled after the Project Implicit and Changing Faces explicit questionnaires . The questionnaire included 11 questions asking about participants’ prior exposure to and conscious biases against people with facial disfigurement. Participants responded on a scale ranging from 1 to 7 (see   https://osf.io/ca2u9/   for details). 


#### Pictures 
  
Images consisted of photographs of patients with facial disfigurements before and after corrective surgery. These photos were collected from craniofacial and dental surgery atlases and compilations of plastic surgery results. The disfigured faces were photos of the individuals before treatment that were affected by one of the following disfigurements: carcinoma, hyperpigmentation, birthmark, scar or small wound, facial paralysis, isolated weight loss, bone disfigurement, or facial trauma. The non-disfigured faces were photographs of the same individuals after treatment (see   https://osf.io/ca2u9/   for all stimulus pairs). Pre-treatment and post-treatment photographs were cropped (to show only faces, with some hair and neck) and colour-corrected to match in size and coloring . The stimulus set consisted of 28 faces, of which 22 were female and 6 were male. 16 of the faces were oriented frontally, 10 were oriented in a three-quarters portrait view, and 2 were profiles (see   https://osf.io/ca2u9/  ). 


#### Implicit association test and explicit bias measure results 
  
Explicit scores range from −3 to +3, with zero indicating no relative preference for non-disfigured vs. disfigured faces. Positive scores indicate a preference for non-disfigured faces, and negative scores indicate a preference for disfigured faces. We found a significant implicit preference for non-disfigured faces (mean difference score = 0.90; SD = 0.58; min = −0.26; max = 2.00;   t   = 13.80; 95% CI = 0.77 to 1.03; p &lt; 0.001; Cohen’s   d   = 1.55). This effect was particularly strong for male respondents (see Table   for details, see Fig.  ). Participants showed no significant explicit preference for non-disfigured vs. disfigured faces (mean explicit score = 0.01; SD = 0.51; min = −1.50; max = 1.08.168;   t   = 0.17; 95% CI = −0.11 to 0.12; p = 0.866; Cohen’s   d =   0.02). Prior exposure had no effect on bias for either the IAT or the EBQ. There was a small to moderate correlation between implicit and explicit scores that was, however, not statistically significant (Pearson’s correlation coefficient, r = 0.22; p = 0.052) making it difficult to draw conclusions as to whether people are aware of their biases.   
Implicit preferences for non-disfigured vs. disfigured faces for all participants by gender. 
  
IAT D scores range from −2 to +2, with zero indicating no relative preference for non-disfigured vs. disfigured faces. Positive scores indicate an implicit preference for non-disfigured faces while negative scores indicate an implicit preference for disfigured faces. D scores were interpreted according to specific, conservative break points based on Cohen’s   d  : ±0.15 (‘slight’ bias), 0.35 (‘moderate’ bias), 0.65 (‘strong’ bias). 

Cohen’s   d   is a standardized effect size, interpreted as   d   of 0.2 = small effect,   d   of 0.5 = medium effect, and   d   ≥ 0.8 = large effect. 
  



### FMRI experiment 
  
#### Participants 
  
We recruited 34 healthy right-handed college students from University of Pennsylvania (24 females, 10 males). Age of participants ranged from 18–35 years. Participants had normal or corrected to normal vision and no prior history of psychiatric or neurological disease. Before participation in the study, each individual gave informed consent approved by the IRB at the University of Pennsylvania (Protocol #806447) in accordance with the International Conference on Harmonization and the Belmont report. 

The data of three participants was excluded from the final analysis. One dataset was excluded because of technical failure which stopped the stimulus presentation halfway through the experiment. Two other datasets were excluded because of synchronization problems between experimenter laptop and the scanner triggers. The data of 31 participants entered the final analysis (22 females, 9 males). 

The EBQ for the participants in the fMRI experiment showed that about half of the participants have a close friend or family member with either a facial disfigurement or a disability. Exposure to people with facial disfigurement was normally distributed in the sample, and most participants reported no to slight preference for non-disfigured over disfigured people (22/28 data entries, 5 missing values). 


#### Procedure and stimulus presentation 
  
The experiment consisted of one session. Participants were presented with 28 pictures of faces in randomized order and were asked to decide whether the displayed face was male or female. Half of the presented pictures were photographs of patients before treatment, and half after treatment. The pictures were identical to the ones used in the behavioural experiment (IAT, see above). Stimuli were presented using E-prime software by projecting them onto a screen using a projector outside the MR scanner room, which could be seen by participants through a mirror mounted over the head coil. Each picture was presented for 6 seconds. Responses were recorded with a 2-button response device. After the experiment, a high-resolution anatomical scan (~7 min) was conducted. After the scanning session, participants were taken out of the scanner and completed the EBQ for disfigurement on a testing computer outside the scanner room. This test was identical to the EBQ in the online sample reported above. 


#### fMRI data acquisition and pre-processing 
  
Images of blood-oxygen level dependent (BOLD) changes were acquired with a 3 T Siemens Magnetom Prisma scanner (Erlangen, Germany) with a 64-channel head coil. We used cushions to minimize participants’ head movement. We used two localizing scans and auto-alignment. Functional images were acquired using a standard BOLD sequence (TR: 2000 ms, TE: 30 ms, flip angle: 60 degrees, voxel size: 2.0 × 2.0 × 2.0 mm, 81 slices). High resolution (0.8 × 0.8 × 0.8 mm) structural (anatomical) images were acquired using an SPC T1 GRAPPA sequence . Data were pre-processed using the Matlab toolbox SPM12 (  http://www.fil.ion.ucl.ac.uk/spm  ). Images were motion corrected and registered to the first image of the scanning block. The mean of the motion-corrected images was co-registered with the individual participants’ anatomical scan. The anatomical and functional scans were spatially normalized to the standard MNI template. Finally, all data were spatially smoothed using an isotropic 8 mm full width at half maximum (FWHM) Gaussian kernel. 


#### fMRI data analysis 
  
At the single-subject level, statistical analysis was performed using a general linear model. The motion estimates of the motion correction algorithm were modelled as regressors of no interest to account for head motion. We performed a whole-brain group analysis by directly contrasting the mean activations per condition in a non-parametric design with SnPM (  https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/nichols/software/snpm  ). Results were corrected for multiple comparisons with a combined voxel-cluster level threshold by familywise error correction at p &lt; 0.05 with Monte Carlo permutation testing. 

In addition to the whole brain group analysis, we performed an item-wise region of interest control analysis to test if the effects in the group analysis are driven by specific items. The two clusters were defined by the group contrasts in the whole brain analysis and consisted of one area comprising of the two large occipital activation clusters, and one comprising the (de-)activation cluster in the anterior cingulate cortex. Mean values for these two regions were extracted for each subject and item. The mean values were normalised with the individual subject’s mean activation in this area to create relative difference scores per subject and item. The data for the item-wise analysis were analysed with linear mixed effect models in RStudio. We built one base model for each dependent variable (occipital cluster activation, anterior cingulate cluster activation) that included condition (pre or post surgery picture) as a predictor and subject and item as random factors with random intercepts. We tested for both random factors whether including random slopes for the condition would improve the model fit and tested interactions with gender and EBQ responses with the best base model (see   https://osf.io/ca2u9/   for details). 


#### FMRI sample results 
  
Participants performed at ceiling for the gender judgment task. 

An ANOVA analysis of the reaction time data in the gender judgement task in the scanner revealed no differences in reaction times between before and after treatment pictures (F  = 0.56, p = 0.45, see Fig.  ) and no differences for item (F  = 1.26, p = 0.17) and no interaction between item and face type (F  = 1.06, p = 0.38).   
Reaction times for gender judgement task per item split by face type. Error bars display 95% confidence intervals. 
  

We found increased activations in temporo-occipital regions encompassing bilateral middle occipital and fusiform gyrus, left inferior occipital gyrus, as well as right inferior temporal and right inferior frontal gyrus (Fig.  ; see Table   for details). An area in the medial anterior cingulate cortex and an area in the right calcarine gyrus showed significant decrease in activation in response to faces before surgery (Fig.  ; see Table   for details). All clusters statistically significant at p &lt; 0.05 FWE at the cluster level corrected by Monte Carlo permutation testing (cluster forming threshold p &lt; 0.001 per voxel).   
Increased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing. 
    
Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing. 
  

The ROI analysis controlling for random effects of items and subjects confirmed the results of the whole brain analysis (see Figs   and  , see   https://osf.io/ca2u9/   for analysis code and full statistical details). Whether the picture of a person was presented from before or after surgery had a significant effect on the BOLD activation level in the anterior cingulate cluster (β = −0.15, s.e. = 0.05, t = −2.95), as well as the occipital cortex (β = 0.17, s.e. = 0.03, t = 5.31). Neither gender of the participant, any of the EBQ measures (see Tables   and   for descriptive statistics), or the gender of the depicted person was found to be related to BOLD activation level differences.   
Itemwise mean activation in the occipital cortex. Stimulus items that do not follow the general activation pattern are Item 2, 7, 12, 25, and 28. 
    
Itemwise mean activation in the anterior cingulate cortex. Stimulus items that do not follow the general activation pattern are Item 1, 25, and 28. 
    
Summary of the EBQ responses I. 
    
Summary of the EBQ responses II. 
  



 ## Data Availability

The datasets generated and analysed during the current study will be made available without restriction on Open Science Framework (DOI 10.17605/OSF.IO/CA2U9) upon acceptance of the article for publication, https://osf.io/ca2u9/. https://osf.io/ca2u9/</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (2)</summary><details class="inner-accordion"><summary>Table 2 (Tab2) - Increased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</summary><div class="table-html"><table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Increased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Location</th><th colspan="1" rowspan="1">k</th><th colspan="1" rowspan="1">T-max</th><th colspan="1" rowspan="1">x</th><th colspan="1" rowspan="1">y</th><th colspan="1" rowspan="1">z</th></tr></thead><tbody><tr><td colspan="1" rowspan="1">left lateral occipital gyrus/BA 18</td><td colspan="1" rowspan="1">3442</td><td colspan="1" rowspan="1">8.08</td><td colspan="1" rowspan="1">−28</td><td colspan="1" rowspan="1">−98</td><td colspan="1" rowspan="1">8</td></tr><tr><td colspan="1" rowspan="1">right lateral occipital gyrus/BA 18</td><td colspan="1" rowspan="1">2377</td><td colspan="1" rowspan="1">6.98</td><td colspan="1" rowspan="1">34</td><td colspan="1" rowspan="1">−90</td><td colspan="1" rowspan="1">2</td></tr><tr><td colspan="1" rowspan="1">right inferior frontal gyrus/BA 44</td><td colspan="1" rowspan="1">230</td><td colspan="1" rowspan="1">5.02</td><td colspan="1" rowspan="1">42</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">26</td></tr></tbody></table></table-wrap></div></details><details class="inner-accordion"><summary>Table 3 (Tab3) - Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</summary><div class="table-html"><table-wrap id="Tab3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Location</th><th colspan="1" rowspan="1">k</th><th colspan="1" rowspan="1">T-max</th><th colspan="1" rowspan="1">x</th><th colspan="1" rowspan="1">y</th><th colspan="1" rowspan="1">z</th></tr></thead><tbody><tr><td colspan="1" rowspan="1">left and right anterior cingulate cortex/BA 24</td><td colspan="1" rowspan="1">765</td><td colspan="1" rowspan="1">4.92</td><td colspan="1" rowspan="1">−2</td><td colspan="1" rowspan="1">36</td><td colspan="1" rowspan="1">10</td></tr><tr><td colspan="1" rowspan="1">right calcarine gyrus/BA 18</td><td colspan="1" rowspan="1">247</td><td colspan="1" rowspan="1">4.10</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">−88</td><td colspan="1" rowspan="1">12</td></tr></tbody></table></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 31598216</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=2</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/31598216/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> decision-making &gt; results; socialcommunication, results &gt; decision-making; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>31598216_analysis_0</td><td>decision-making</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The PDG and associated trustworthiness and mentalizing measures assess understanding of others&#x27; intentions and traits; the decision-making contrast measures perception/understanding of others.</td></tr>
<tr><td>31598216_analysis_1</td><td>results</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The paradigm and reported measures (mentalizing/Reading the Mind in the Eyes, trustworthiness ratings, cooperative choices) directly assess understanding of others&#x27; mental states and social behaviour.</td></tr>
<tr><td>31598216_analysis_2</td><td>PATIENTS</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study assesses mentalizing, trustworthiness judgments, face memory, and behaviour toward others (cooperation) — all map onto Perception and Understanding of Others (including understanding mental states).</td></tr>
<tr><td>31598216_analysis_3</td><td>CONTROLS</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The task requires representing and responding to others&#x27; intentions/behavior (cooperation/defection) and includes trustworthiness ratings and mentalizing measures—consistent with perception/understanding of others (including understanding mental states).</td></tr>
<tr><td>31598216_analysis_4</td><td>main effect of group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study measures understanding of others (mentalizing, trust evaluations, cooperative behaviour) and neural correlates (MPFC), directly matching perception/understanding of others.</td></tr>
<tr><td>31598216_analysis_5</td><td>main effect of condition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The PDG requires representing and responding to others&#x27; actions/intentions (cooperation/defection) and trustworthiness, so the contrast measures perception and understanding of others.</td></tr>
<tr><td>31598216_analysis_6</td><td>ROI-main effect of group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires inferring others&#x27; behaviour/mental states (mentalizing) and the ROI analysis examines neural differences associated with these processes, fitting Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>31598216_1</td><td>decision-making &gt; results; socialcommunication</td><td>31598216_analysis_0</td><td>decision-making</td><td>0.750</td><td>0.093</td><td>0.290</td><td>unmatched</td><td>coord_count_mismatch, low_coord_high_name, low_total_score</td></tr><tr><td>31598216_2</td><td>results &gt; decision-making; socialcommunication</td><td>31598216_analysis_3</td><td>CONTROLS</td><td>0.182</td><td>0.556</td><td>0.443</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr></tbody></table></div>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 6731699)</summary><p><strong>Title:</strong> Social cognition, behaviour and therapy adherence in frontal lobe epilepsy: a study combining neuroeconomic and neuropsychological methods</p><details><summary>Abstract</summary><pre class="paper-text">Social behaviour of healthy humans and its neural correlates have been extensively studied in social neuroscience and neuroeconomics. Whereas it is well established that several types of epilepsies, such as frontal lobe epilepsy, lead to social cognitive impairments, experimental evidence on how these translate into behavioural symptoms is scarce. Furthermore, it is unclear whether social cognitive or behavioural disturbances have an impact on therapy adherence, which is critical for effective disease management, but generally low in these patients. In order to investigate the relationship between social cognition, social behaviour, and therapy adherence in patients with frontal lobe epilepsies (FLE), we designed a study combining conventional neuropsychological with behavioural economic and functional magnetic resonance imaging (fMRI) methodology. Fifteen patients and 15 healthy controls played a prisoners&#x27; dilemma game (an established game to operationalize social behaviour) while undergoing fMRI. Additionally, social cognitive, basic neuropsychological variables, and therapy adherence were assessed. Our results implicate that social behaviour is indeed affected and can be quantified using neuroeconomic methods in patients with FLE. Impaired social behaviour in these patients might be a consequence of altered brain activation in the medial prefrontal cortex and play a role in low therapy adherence. Finally, this study serves as an example of how to integrate neuroeconomic methods in neurology.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Social cognition is a term used for several high-level cognitive functions that determine human behaviour in a social context. Several studies showed impairments of social cognition in frontal lobe epilepsies (FLE), such as Theory-of-Mind/mentalizing and facial emotion recognition [ ], as well as neuropsychiatric co-morbidities [ ], but studies quantifying social behaviour abnormalities of FLE patients in an experimental sense are scarce. Even though behaviour relies on cognitive functions, the relationship between cognition, behaviour and epilepsy variables is complex and not fully understood [ ], making it difficult to predict the impact of altered social cognition on social behaviour, and more specifically, therapy adherence in FLE. 

In neuroeconomics—the science of studying research questions in economics through the application of neuroscientific methods and theories—pro-social behaviours, such as trust or cooperation, have been extensively studied for decades, mainly through game paradigms, such as the trust game or the prisoners&#x27; dilemma game [ ]. Using this approach, several brain areas were found to play a role in pro-social behaviour of healthy humans. Crucial parts of this brain network reside in the frontal cortex such as the ventromedial frontal/orbitofrontal cortex and the anterior cingulate cortex [ ]. These discoveries have led to the adoption of economic methodology to study behaviour in neurology [ ], but to the best of our knowledge, no such study has focused on the behaviour of patients with epilepsy. 

Adherence to anti-epileptic drug therapy is critical for effective disease management. Although its measurement is difficult without a single method that has yet proved to be the gold standard [ ], therapy adherence of patients suffering from epilepsy is low (at about 30–50%) [ ]. This is unfortunate, as low therapy adherence not only leads to poorer seizure control, but also increases the risk of sudden unexpected death in epilepsy [ ]. The reasons for low therapy adherence in general are still a matter of research [ ]. Low therapy adherence seems to have a multifactorial origin with some factors being associated with neuropsychological impairments and psychiatric co-morbidities of chronic diseases [ ], and others reflecting pathophysiological changes of neural networks specifically affected in epilepsy [ ]. 

We thus designed a study combining conventional neuropsychological with neuroeconomic methods to address the following open research questions:
   
Is there a difference in social behaviour between FLE patients and healthy controls that can be experimentally operationalized? 
  
If such a difference exists, is it associated with altered social cognitive functions and their underlying frontal lobe network? 
  
Does social behaviour affect therapy adherence in FLE? 
  


## Methods 
  
### Subjects 
  
We included 15 FLE patients and 15 healthy controls. We established the following inclusion and exclusion criteria to ensure the feasibility of the study and avoid known confounders in behavioural research. 

Inclusion criteria for patients were: age between 18 and 50, right-handedness (Edinburgh handedness inventory; [ ]), no significant anxiety, depression, or obsessive-compulsive symptoms as assessed by a hospital anxiety and depression scale score (HADS) less than 10 [ ] and an obsessive compulsive inventory score (OCIS) less than 40 [ ], sufficient language skills, and a diagnosis of FLE. The latter was based either upon association between typical frontal lobe seizure semiology and existence of an epileptogenic MRI lesion within the frontal lobe or upon direct recording of seizures of frontal lobe origin during a long-term video-EEG monitoring performed in the Department of Functional Neurology and Epileptology at Hospices Civils de Lyon, France, in patients with normal MRI. Exclusion criteria included pregnancy, non-MRI suitable transplants, major perceptive impairments, non-epileptic seizures, a history of intellectual disability, other known neurological diseases, MRI-lesion outside of the frontal lobe or frontal cortical lesions larger than 1 cm in diameter, and past epilepsy or other brain surgery in order to achieve a patient sample of non-resected participants. 

Inclusion criteria for controls were: age between 18 and 50, right-handedness (Edinburgh handedness inventory; [ ]), no significant anxiety, depression, or obsessive-compulsive symptoms assessed by a HADS Score less than 10 [ ] and an OCIS less than 40 [ ] as well as sufficient language skills. Exclusion criteria were pregnancy, non-MRI suitable transplants, major perceptive impairments, medication other than contraceptives, or a history of neurological or psychiatric diseases. 


### Magnetic resonance data acquisition 
  
All images were acquired using the same MRI machine (Siemens Magneton Prisma 3 Tesla) in one session per participant. Structural MRI and shimming (to minimize field inhomogeneities) were performed on all subjects prior to gradient-echo echoplanar imaging that provided blood oxygen level dependent (BOLD) contrast. Each volume comprised 35 AC-PC aligned slices (order of acquisition: interleaved) with a thickness of 2.5 cm, field of view (FOV) 23 cm, parallel imaging parameters GRAPPA/acceleration factor 2, echo time (TE) 26 ms, repetition time (TR) 2260 ms. 


### Paradigm 
  
We designed our paradigm on the basis of similar experiments in the literature [ ] to ensure comparability. When subjects entered the laboratory for the experiment, they received written instructions explaining the prisoners&#x27; dilemma game (PDG) [ ]. In this game, two players decide at the same time whether or not to cooperate with the other player. Depending on the decisions of both players, there is a monetary pay-off, which is equal for both players in the case of mutual cooperation or defection, whereas in the case of divergent strategies, it is not existent for the cooperating player and highest for the defecting player. The pay-off structure of our version of the game is shown in  .
   
Pay-off structure of the prisoners&#x27; dilemma game. 
  

Participants were told that they would play with real money and be paid out their gain. Payment took place at the end of the experiment, but in fact, all participants received the same amount of money (40€) independent of their actual gain due to ethical considerations. The participants did not know the number of rounds played in the game, but in fact all subjects played 32 rounds. Moreover, participants (player 1) were told that they would be playing with four different human beings (two males and two females—here termed player 2) located in another room, but actually played against a computer-generated, randomized, strategy that simulated the other player&#x27;s behaviour. This means that each participant played 32 rounds with each of the four different counterparts (equals 128 decisions to cooperate or to defect in total). Player 2 was illustrated with face images from an established database (Glasgow Unfamiliar Face Database). Once installed in the MRI machine, participants first had to rate the trustworthiness of the presented player 2 on a Likert-scale between 1 and 7 (1 no trustworthiness, 7 highest possible trustworthiness) being shown a face picture of player 2. Next, the actual game began. Each round of the game consisted of 3 screens: first, the pay-off matrix of the game was shown for 2 s. Then, participants were asked to choose to either cooperate or to defect using two input buttons. After a random time interval between 4 and 6 s simulating variable decision times of player 2, the result of the round was shown for 2 s using the pay-off matrix with the result highlighted in yellow colour. After 32 rounds of the game, the face of player 2 was again shown to the participants, who had to rate the trustworthiness of the face another time in analogy to the start. Finally, the overall gain of the game was shown. See   for a visualization.
   
Visualization of the experimental paradigm. Notes: top left—visualization of the beginning and end of the paradigm, bottom right—visualization of the time period in between (corresponds to dashed line in the top left part of the figure); PDG, prisoners&#x27; dilemma game. 
  


### Demographic data, neuropsychological testing, questionnaires and pill counts 
  
We recorded the following variables in all participants via a self-report questionnaire: age, gender, profession and education. For patients, we included the number of seizures (generalized tonic-clonic seizures and other seizure types separately) in the last three months according to a seizure calendar commonly used in clinical routine and current anticonvulsive treatment (number and names of drugs, number of intakes per day, preparation of drugs for intake by the participant or by a carer) as recorded in the patient&#x27;s medical chart. 

Neuropsychological variables captured in all participants included psychomotor speed and mental flexibility (Trail Making test A and B [ ]), memory for faces and working memory (Faces subsets and numbers of the Wechsler Memory Scale, fourth edition [ ]). 

Social cognition was tested through the Reading the Mind in the Eyes test [ ] for Theory-of-Mind/mentalizing (in this test, subjects have to choose the correct word out of a list describing emotions to corresponding photos of a person&#x27;s expression of eyes) and a trust questionnaire ([ ]; French version), additionally to the trustworthiness ratings of faces during the fMRI paradigm. 

Furthermore, therapy adherence was measured in the patient group through pill counts at two consecutive visits [ ] by reviewing pill bottles of a six months period and several questionnaires comprising the Morisky adherence scale ([ , ]; validated French version), the Beliefs about Medicines Questionnaire (BMQ; [ , ]; validated French version) and the SATMED-Q (Treatment Satisfaction with Medicines Questionnaire, [ ]; French version). These questionnaires are commonly used instruments in research on therapy adherence. 


### Data analysis 
  
#### Imaging data 
  
fMRI data were analysed using SPM 12 ( ). Pre-processing involved slice time correction, realignment, normalization and smoothing with an 8 mm full width at half maximum Gaussian kernel. A general linear model with two conditions (decision phase and result phase) was estimated. No subject had to be excluded. The contrast images calculated for individual subjects were entered into a second level or random effects analysis [ ]. We first calculated   t  -tests for the two conditions (decision/result) for all participants as one group and in a second step analysed each group (patients/controls) separately in the same way. In a third step, we calculated two-sample   t  -tests between the two groups for both conditions. We then implemented a 2 × 2 repeated measures ANOVA using a flexible factorial design with the factors ‘group&#x27; (patients/controls) and ‘condition&#x27; (decision/result) to calculate main effects of group and condition as well as the interaction effect of group and condition. The resultant statistical parametric maps were thresholded using an FWE-corrected   p  -value threshold less than 0.001, reporting clusters greater than 20 voxels (  k   = 20) only. Anatomical structures of cluster maxima were labelled in Talairach space using the Talairach Client ( ). 

Based on the previous research, we   a priori   selected the following regions of interest (ROIs) for further analyses: superior, middle and inferior frontal gyri, medial and lateral orbital gyri, posterior orbital gyrus, straight gyrus, anterior cingulate gyrus, amygdala, thalamus and caudate nucleus. We used adult brain maximum probability maps (© Copyright Imperial College of Science, Technology and Medicine 2007. All rights reserved) to obtain the ROIs [ ]. 

In a subsequent step, we computed the % of BOLD signal change extracted from beta images from significant voxels from the second-level analysis within an 8 mm sphere surrounding the activation peak and calculated correlations with all other variables collected during the study. 


#### Neuropsychological and questionnaire data 
  
The statistical analysis was performed with SPSS, v. 20 and involved two-tailed, non-parametric testing (Wilcoxon test,   χ  ²-test), as well as Spearman&#x27;s rho correlations. We corrected for multiple testing using the Bonferroni–Holm procedure and chose a significance level of less than 0.05. 




## Results 
  
### Behavioural data, neuropsychological variables and questionnaires 
  
We included 15 FLE patients and 15 healthy controls. Four patients (26.67%) were seizure free, while the remaining eleven (73.33%) had a median monthly seizure frequency of 4 during a three-month period preceding the experiment. MRI was normal in nine patients (60%) and showed frontal lobe lesions—cavernoma (one patient), focal cortical dysplasia (one patient), diffuse axonal trauma (one patient), and post-haematoma scars (two patients)—in the remaining five patients (33.3%). The remaining patient had undergone skull (but not brain) surgery in childhood for craniostenosis and the current structural MRI showed no obvious pathological changes. Three patients were on monotherapy and 12 patients were under polytherapy (mean number ± s.d. of anti-epileptic drugs: 2.33 ± 0.98). Mean ± s.d. subjective (Morisky adherence scale) and objective (pill counts) measures of treatment adherence during a six-month period were 1.73 ± 0.88 (range: 0–8, lower number indicates higher adherence) and 7.00 ± 12.45 (number of prescribed pills not taken), indicating moderate level of adherence. Patients also showed high belief of treatment necessity (BMQ mean ± s.d.—necessity 21.13 ± 6.30, range: 5–25), average level of concerns and negative views regarding therapy (BMQ: concerns and harms 24.87 ± 9.36, range: 9–45; overuse 7.47 ± 3.27, range: 4–20) and an average treatment satisfaction (SATMED-Q mean ± s.d.: 65.36 ± 7.88, range: 17–85). Questionnaires capturing beliefs about medicines (  p   = 0.256), treatment satisfaction (  p   = 0.776), or adherence (  p   = 0.056) did not correlate significantly with therapy adherence measured through pill counts, although the Morisky adherence scale was close to being significant (  p   = 0.056). 

For all demographic, neuropsychological and neuroeconomic data collected in this study on both patient and control groups, please refer to  . In summary, there was no significant difference between the patient and control groups regarding age, gender, handedness, education, professional status, trust in doctors as captured by the respective questionnaire [ ], trustworthiness of game opponents, or working memory for numbers. By contrast, patients demonstrated statistically significant worse performance than controls in mentalizing measured through the Reading eyes in the mind test, memory for faces, psychomotor speed during Trail making test A, and mental flexibility performance during Trail making test B.
   
Summary of data collected on both participant groups of our study. 
    

Furthermore, there was also a significant difference in the total number of cooperative choices during the PDG, indicating higher cooperation in patients than in controls. The numbers of cooperative choices between games 1 and 4 differed significantly in both groups, indicating that cooperation decreased significantly between the first and the last game of the experiment in both groups ( ).
   
Mean numbers of cooperative choices of the patient and control groups in the four prisoners&#x27; dilemma games. 
  

Mentalizing correlated strongly with memory for faces (  r   = 0.715;   p   = 0.000) and with trust in doctors and the health system [ ] (  r   = 0.437;   p   = 0.026), showing that participants with high mentalizing abilities also better memorized faces and had higher trust in healthcare. Taken together these data supported a positive correlation between several pro-social cognition and behaviour variables. 

There was a significant positive correlation between cooperative choices in the PDG and missed medication intakes with high values representing low therapy adherence (  r   = 0.686;   p   = 0.010), indicating higher cooperative behaviour in patients with low therapy adherence. 


### Imaging results (see tables  ,   and  ) 
  
#### Voxel-based whole brain analysis 
  
 Brain activation during decision-making.   During the decision-making whether to cooperate or defect when playing the PDG, combined event-related fMRI analysis of data from all subjects (one sample   t  -test) showed significantly activated clusters in right inferior parietal lobule, left precuneus, left lingual gyrus, left insula, the middle and inferior frontal gyri bilaterally, left superior frontal gyrus, as well as the right anterior lobe of the cerebellum ( ). In controls, significant activation was observed in the right fusiform gyrus, right superior frontal gyrus, left insula and left superior temporal gyrus, while in patients, significant activation was observed in the left superior frontal gyrus, left precuneus, and left insula.
   
Visualization of brain activation of all participants (according to one sample   t  -test results reported in  ) during decision-making (yellow) and result perception (red)—thresholded at   p   &lt; 0.001, FWE-corrected,   k   = 20. 
    
Summary of analysis of all participants. FWE,   p   = 0.001. 
    
Summary of per group analyses. FWE,   p   = 0.001. 
    
Analysis of group differences and ROI. 
  

 Brain activation during result phase.   During the perception of the results of the game, combined analysis of data from all subjects (one sample   t  -test) showed activation in the inferior parietal lobules bilaterally and the right middle temporal gyrus—these results are visualized in   as well. In controls, significant activation was observed in the left middle frontal gyrus, inferior parietal lobules bilaterally, right inferior temporal gyrus, and left cingulate gyrus. In patients, significant activation was observed in both inferior parietal lobules, left superior temporal gyrus, right middle frontal gyrus, right thalamus, left insula and right precuneus. 

 Individual group comparisons for both conditions.   Two-sample   t  -tests between the patient and control groups did not yield any significant results for both conditions. 

 Integrated comparison across groups and conditions.   In the full factorial analysis, the main group effect (  F  -test) showed significant clusters in the right and left medial prefrontal cortex—Brodmann area 10 ( ). In the main effect of condition (decision-making versus result phase of the game), the following clusters were significant: right anterior lobe of the cerebellum, left precentral gyrus, right inferior frontal gyrus, right insula, left middle temporal gyrus and left superior frontal gyrus. There was no significant interaction between group and condition.
   
Difference in activation between the patient and control groups (main effect of groups,   F  -test) in the whole-brain analysis—thresholded at   p   &lt; 0.001, FWE-corrected,   k   = 20. 
  


#### Region of interest analysis 
  
ROI analysis (  F  -test) showed differences in activation between the patient and control groups in the right and left superior frontal gyrus, while the   t  -test analyses showed no significant differences. The mean % of BOLD signal changes in these ROIs in the result condition showed a significant negative correlation with initial trust (  r   = −0.448;   p   = 0.017), indicating that participants with high initial trust showed lower signal change in these brain areas during result perception. The mean % of BOLD signal changes in the choice condition also correlated significantly with differences between the initial and final trust (  r   = −0.618;   p   = 0.000; first ROI and   r   = −0.545;   p   = 0.003 for the second ROI), meaning that participants with a high decrease of trust from the beginning to the end of the games showed lower signal changes while making their choices in the game. 

Another significant correlation was detected between therapy adherence and the % BOLD signal change in the choice condition (  r   = 0.565;   p   = 0.044 for the first ROI) showing that participants with low therapy adherence had higher signal changes in this brain area. 




## Discussion 
  
In this study, we first confirmed that mentalizing is impaired in patients with FLE, as previously established [ ]. These abnormalities correlated with an altered memory for faces, a previously unreported finding in FLE, which may shed light on the mechanisms underlying dysfunction in social behaviour. Interestingly, there is evidence of frontal lobe contributions to memory for faces [ ]. Furthermore, mentalizing abilities correlated with the results of the trust in healthcare questionnaire implying an overall good correlation of pro-social cognition variables in our study. 

One of the goals of this study was to find out whether deficits in social cognition in FLE patients could lead to differences in social behaviour that can be operationalized. In fact, we showed that FLE patients behave differently from healthy controls in the PDG, but in a counterintuitive way, since they cooperated more than controls. While this finding was unexpected, we observed a frequent pattern of evolving cooperation during the game in both the patients&#x27; and controls&#x27; groups, with higher rates of cooperation in early game phases and less cooperative behaviour in the later ones [ ]. This strategy is consistent with the objective of maximizing profit. Indeed, future possible interactions encourage people to cooperate in the early phases of the game, with the hope to initiate a mutually cooperative relationship. If not reciprocated, strategy will later shift to defection in players who want to maximize their own profit [ ]. 

In this context, higher cooperation in patients might result from impaired negative feedback when cooperative behaviour is not reciprocated, especially because the ability to shift behaviour from cooperation to defection in such a case is dependent on frontal lobe functions [ ]. Alternatively, patients&#x27; behaviour could be interpreted as a preference for higher delayed rewards since higher cooperation leads to higher future profits [ ]. Although evidence on time preference for rewards (also referred to as ‘delay discounting&#x27;) in neurological patients is scarce, altered delay discounting as another behavioural symptom of neurological disorders was reported before [ ]. 

Prior functional brain imaging studies investigating cooperative behaviour through the PDG found, in essence, three brain networks to be active during the game: several frontal brain areas, especially the medial prefrontal cortex, as well as reward and limbic brain regions [ ] reflecting the cognitive functions necessary to successfully play the game, such as decision-making and reward-based learning. Our analyses of brain activation of all participants showed a similar pattern of brain areas to be active during the game. However, when looking at the patient and control group separately, controls, but not patients showed significant activation in several temporal regions during decision-making whether to cooperate or not. Interestingly, those regions have been implicated in both Theory-of-Mind and facial processing tasks (e.g. [ ]) and thus further substantiate our behavioural results of higher mentalizing and face memory abilities in the control group. During the perception of the results during the game, the cingulate gyrus, a region thought to be active in situations of cognitive conflict [ ], was significantly activated in controls, but not in patients. When opponents in an economic game do not reciprocate benevolent behaviour, this is perceived as a conflict between economic self-interest (i.e. rationality) and fairness considerations [ ]. Hypothetically, patients might have not perceived cognitive conflicts to the same extent as controls in such game results, which might also have led to higher cooperative behaviour. 

When comparing our functional imaging results between the patient and control groups on both whole-brain and ROI level, it becomes apparent that the activation of the medial prefrontal cortex (MPFC) differed significantly between the two groups during the game. The MPFC plays a pivotal role in social behaviour and decision-making [ , ], which is reflected in our results of correlating MPFC activation changes and face trustworthiness evaluations, although those have been based upon a low number of data points only. Therefore, a difference in MPFC activation between two groups showing such divergent cooperative behaviour seems plausible. Our fMRI findings do not allow us to infer the directionality of the observed difference in activation, potentially because of either the difference being based on both conditions rather than differences in activation during the individual conditions, the   t  -test comparisons between groups being underpowered, and/or the difference in activation between the groups not being sufficiently strong to pass the set threshold in the   t  -test analyses. Although a pathology-driven lower activation might seem more plausible, a compensatory higher activation is also possible [ ]. Further imaging studies are needed to address this question. 

Another goal of our study was to investigate possible links between cooperative behaviour and therapy adherence in FLE. Our results show that cooperative behaviour in the PDG correlated negatively with therapy adherence in patients (high cooperative behaviour correlating with low therapy adherence). Even though we could not find any previous literature on the relationship between cooperative behaviour and therapy adherence, there is some evidence that decision-making, in general, has effects on therapy adherence with lower decision-making skills leading to lower adherence [ ]. As we have outlined before, higher cooperation can be interpreted as lower social decision-making abilities in the context of our experiment. Thus, the inverse relation between therapy adherence and cooperative behaviour actually fits into the current scientific framework on the interaction of behaviour, decision-making and therapy adherence. Importantly, there was no correlation between questionnaire data about therapy adherence and pill counts in our study, reflecting the known difficulties in measuring adherence in epilepsy (e.g. [ ]). Moreover, the MPFC activation during the game showed a correlation with therapy adherence reflecting the close link between social cognition and therapy adherence. 

We acknowledge that our study design of comparing people with FLE to healthy controls does not allow distinguishing whether the study findings apply solely to FLE or to epilepsy in general. Looking at the published literature on other epilepsy syndromes with frontal lobe dysfunction, such as juvenile myoclonic epilepsy or genetic generalized epilepsies, it becomes apparent that patients with these types of epilepsies also show abnormalities in social functions such as Theory-of-Mind [ , ]. In our view, neuroeconomic methodology could help us to determine the behavioural consequences of these social cognitive impairments. Further studies comparing cooperative behaviour and therapy adherence between FLE and other epileptic disorders are therefore warranted. 


## Conclusion 
  
To conclude, our results implicate that (i) social behaviour is affected by FLE and (ii) can be measured using neuroeconomic methods. Impaired social behaviour in FLE might (iii) be a consequence of differing MPFC activation and (iv) might play a role in low therapy adherence. This is important as therapy adherence is difficult to measure, especially in patients with epilepsy where classical tools of measurement (e.g. questionnaires or electronic devices) have been shown to be imprecise [ ]. Integrating neuroeconomic testing of social behaviour into the neuropsychological testing routine could help to better understand therapy adherence of patients with epilepsies and consequently help to improve patient care through an identification of patients at risk. 


## Supplementary Material</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (3)</summary><details class="inner-accordion"><summary>Table 3. (RSOS180850TB3) - Summary of analysis of all participants. FWE, p = 0.001.</summary><div class="table-html"><table-wrap id="RSOS180850TB3" orientation="portrait" position="float"><label>Table 3.</label><caption><p>Summary of analysis of all participants. FWE, <italic toggle="yes">p</italic> = 0.001.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1">hemisphere</th><th align="left" rowspan="1" colspan="1">lobe</th><th align="left" rowspan="1" colspan="1">cluster covering</th><th align="left" rowspan="1" colspan="1">Brodmann area</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">p</italic>-value</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">K</italic> size</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">T</italic> score</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th></tr></thead><tbody><tr><td colspan="10" rowspan="1"><italic toggle="yes">decision-making</italic></td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">1720</td><td rowspan="1" colspan="1">13.10</td><td rowspan="1" colspan="1">48</td><td rowspan="1" colspan="1">−38</td><td rowspan="1" colspan="1">46</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">precuneus</td><td rowspan="1" colspan="1">BA 7</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">1748</td><td rowspan="1" colspan="1">12.95</td><td rowspan="1" colspan="1">−20</td><td rowspan="1" colspan="1">−66</td><td rowspan="1" colspan="1">52</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">occipital</td><td rowspan="1" colspan="1">lingual gyrus</td><td rowspan="1" colspan="1">BA 18</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">817</td><td rowspan="1" colspan="1">10.63</td><td rowspan="1" colspan="1">−4</td><td rowspan="1" colspan="1">−82</td><td rowspan="1" colspan="1">−6</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">middle frontal gyrus</td><td rowspan="1" colspan="1">BA 6</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">194</td><td rowspan="1" colspan="1">10.58</td><td rowspan="1" colspan="1">−28</td><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">52</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">BA 13</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">130</td><td rowspan="1" colspan="1">10.55</td><td rowspan="1" colspan="1">−30</td><td rowspan="1" colspan="1">22</td><td rowspan="1" colspan="1">0</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">middle frontal gyrus</td><td rowspan="1" colspan="1">BA 9</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">733</td><td rowspan="1" colspan="1">10.43</td><td rowspan="1" colspan="1">46</td><td rowspan="1" colspan="1">34</td><td rowspan="1" colspan="1">30</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">cerebellum anterior lobe</td><td rowspan="1" colspan="1">culmen</td><td rowspan="1" colspan="1">—</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">368</td><td rowspan="1" colspan="1">10.32</td><td rowspan="1" colspan="1">26</td><td rowspan="1" colspan="1">−48</td><td rowspan="1" colspan="1">−14</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">inferior frontal gyrus</td><td rowspan="1" colspan="1">BA 47</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">300</td><td rowspan="1" colspan="1">10.27</td><td rowspan="1" colspan="1">42</td><td rowspan="1" colspan="1">20</td><td rowspan="1" colspan="1">−2</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">inferior frontal gyrus</td><td rowspan="1" colspan="1">BA 6</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">105</td><td rowspan="1" colspan="1">9.90</td><td rowspan="1" colspan="1">28</td><td rowspan="1" colspan="1">16</td><td rowspan="1" colspan="1">56</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">superior frontal gyrus</td><td rowspan="1" colspan="1">BA 6</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">166</td><td rowspan="1" colspan="1">9.62</td><td rowspan="1" colspan="1">−6</td><td rowspan="1" colspan="1">12</td><td rowspan="1" colspan="1">52</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">temporal</td><td rowspan="1" colspan="1">middle temporal gyrus</td><td rowspan="1" colspan="1">BA 37</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">40</td><td rowspan="1" colspan="1">9.36</td><td rowspan="1" colspan="1">56</td><td rowspan="1" colspan="1">−52</td><td rowspan="1" colspan="1">−12</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">results</italic></td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">343</td><td rowspan="1" colspan="1">10.57</td><td rowspan="1" colspan="1">52</td><td rowspan="1" colspan="1">−50</td><td rowspan="1" colspan="1">46</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">temporal</td><td rowspan="1" colspan="1">middle temporal gyrus</td><td rowspan="1" colspan="1">BA 21</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">74</td><td rowspan="1" colspan="1">8.72</td><td rowspan="1" colspan="1">58</td><td rowspan="1" colspan="1">−34</td><td rowspan="1" colspan="1">−8</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">36</td><td rowspan="1" colspan="1">8.14</td><td rowspan="1" colspan="1">−58</td><td rowspan="1" colspan="1">−42</td><td rowspan="1" colspan="1">40</td></tr></tbody></table></table-wrap>
</div></details><details class="inner-accordion"><summary>Table 4. (RSOS180850TB4) - Summary of per group analyses. FWE, p = 0.001.</summary><div class="table-html"><table-wrap id="RSOS180850TB4" orientation="portrait" position="float"><label>Table 4.</label><caption><p>Summary of per group analyses. FWE, <italic toggle="yes">p</italic> = 0.001.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1">hemisphere</th><th align="left" rowspan="1" colspan="1">lobe</th><th align="left" rowspan="1" colspan="1">cluster covering</th><th align="left" rowspan="1" colspan="1">Brodmann area</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">p</italic>-value</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">K</italic> size</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">T</italic> score</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th></tr></thead><tbody><tr><td colspan="10" rowspan="1">PATIENTS</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">decision-making</italic></td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">occipital</td><td rowspan="1" colspan="1">lingual gyrus</td><td rowspan="1" colspan="1">BA 18</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">11 406</td><td rowspan="1" colspan="1">10.64</td><td rowspan="1" colspan="1">24</td><td rowspan="1" colspan="1">−74</td><td rowspan="1" colspan="1">−10</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">superior frontal gyrus</td><td rowspan="1" colspan="1">BA 6</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">8491</td><td rowspan="1" colspan="1">10.28</td><td rowspan="1" colspan="1">−16</td><td rowspan="1" colspan="1">0</td><td rowspan="1" colspan="1">64</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">subcortical insula</td><td rowspan="1" colspan="1">claustrum insula</td><td rowspan="1" colspan="1">—</td><td rowspan="1" colspan="1">0.006</td><td rowspan="1" colspan="1">456</td><td rowspan="1" colspan="1">7.63</td><td rowspan="1" colspan="1">−28</td><td rowspan="1" colspan="1">22</td><td rowspan="1" colspan="1">−2</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">results</italic></td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">4211</td><td rowspan="1" colspan="1">12.48</td><td rowspan="1" colspan="1">−60</td><td rowspan="1" colspan="1">−42</td><td rowspan="1" colspan="1">38</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">5094</td><td rowspan="1" colspan="1">11.53</td><td rowspan="1" colspan="1">58</td><td rowspan="1" colspan="1">−44</td><td rowspan="1" colspan="1">44</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">middle frontal gyrus</td><td rowspan="1" colspan="1">BA 46</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">2994</td><td rowspan="1" colspan="1">10.12</td><td rowspan="1" colspan="1">42</td><td rowspan="1" colspan="1">48</td><td rowspan="1" colspan="1">14</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">cerebellum</td><td rowspan="1" colspan="1">anterior lobe</td><td rowspan="1" colspan="1">—</td><td rowspan="1" colspan="1">0.003</td><td rowspan="1" colspan="1">439</td><td rowspan="1" colspan="1">6.91</td><td rowspan="1" colspan="1">−2</td><td rowspan="1" colspan="1">−54</td><td rowspan="1" colspan="1">−4</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">precuneus</td><td rowspan="1" colspan="1">BA 7</td><td rowspan="1" colspan="1">0.016</td><td rowspan="1" colspan="1">304</td><td rowspan="1" colspan="1">6.35</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">−78</td><td rowspan="1" colspan="1">46</td></tr><tr><td colspan="10" rowspan="1">CONTROLS</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">decision-making</italic></td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">temporal occipital</td><td rowspan="1" colspan="1">fusiform gyrus</td><td rowspan="1" colspan="1">BA 37</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">17309</td><td rowspan="1" colspan="1">13.94</td><td rowspan="1" colspan="1">54</td><td rowspan="1" colspan="1">−54</td><td rowspan="1" colspan="1">−16</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">superior frontal gyrus</td><td rowspan="1" colspan="1">BA 9</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">9140</td><td rowspan="1" colspan="1">11.15</td><td rowspan="1" colspan="1">42</td><td rowspan="1" colspan="1">36</td><td rowspan="1" colspan="1">28</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">BA 13</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">756</td><td rowspan="1" colspan="1">8.1</td><td rowspan="1" colspan="1">−34</td><td rowspan="1" colspan="1">18</td><td rowspan="1" colspan="1">8</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">temporal</td><td rowspan="1" colspan="1">superior temporal gyrus</td><td rowspan="1" colspan="1">BA 22</td><td rowspan="1" colspan="1">0.020</td><td rowspan="1" colspan="1">238</td><td rowspan="1" colspan="1">7.69</td><td rowspan="1" colspan="1">−50</td><td rowspan="1" colspan="1">−48</td><td rowspan="1" colspan="1">14</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">results</italic></td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">middle frontal gyrus</td><td rowspan="1" colspan="1">BA 10</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">67860</td><td rowspan="1" colspan="1">9.12</td><td rowspan="1" colspan="1">−34</td><td rowspan="1" colspan="1">48</td><td rowspan="1" colspan="1">−2</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">3234</td><td rowspan="1" colspan="1">7.88</td><td rowspan="1" colspan="1">−46</td><td rowspan="1" colspan="1">−46</td><td rowspan="1" colspan="1">56</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">parietal</td><td rowspan="1" colspan="1">inferior parietal lobule</td><td rowspan="1" colspan="1">BA 40</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">1086</td><td rowspan="1" colspan="1">7.11</td><td rowspan="1" colspan="1">52</td><td rowspan="1" colspan="1">−50</td><td rowspan="1" colspan="1">46</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">temporal</td><td rowspan="1" colspan="1">inferior temporal gyrus</td><td rowspan="1" colspan="1">BA 20</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">596</td><td rowspan="1" colspan="1">7.08</td><td rowspan="1" colspan="1">58</td><td rowspan="1" colspan="1">−40</td><td rowspan="1" colspan="1">16</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">cingulate gyrus</td><td rowspan="1" colspan="1">BA 24</td><td rowspan="1" colspan="1">0.046</td><td rowspan="1" colspan="1">219</td><td rowspan="1" colspan="1">5.41</td><td rowspan="1" colspan="1">−2</td><td rowspan="1" colspan="1">−14</td><td rowspan="1" colspan="1">38</td></tr></tbody></table></table-wrap>
</div></details><details class="inner-accordion"><summary>Table 5. (RSOS180850TB5) - Analysis of group differences and ROI.</summary><div class="table-html"><table-wrap id="RSOS180850TB5" orientation="portrait" position="float"><label>Table 5.</label><caption><p>Analysis of group differences and ROI.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /><col align="left" span="1" /></colgroup><thead valign="bottom"><tr><th align="left" rowspan="1" colspan="1">hemisphere</th><th align="left" rowspan="1" colspan="1">lobe</th><th align="left" rowspan="1" colspan="1">cluster covering</th><th align="left" rowspan="1" colspan="1">Brodmann area</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">p</italic>-value</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">K</italic> size</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">F</italic> score</th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th align="left" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th></tr></thead><tbody><tr><td colspan="10" rowspan="1"><italic toggle="yes">main effect of group</italic></td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">medial frontal gyrus</td><td rowspan="1" colspan="1">BA 10</td><td rowspan="1" colspan="1">0.034</td><td rowspan="1" colspan="1">214</td><td rowspan="1" colspan="1">22.35</td><td rowspan="1" colspan="1">8</td><td rowspan="1" colspan="1">62</td><td rowspan="1" colspan="1">14</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">main effect of condition</italic></td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">cerebellum</td><td rowspan="1" colspan="1">anterior lobe</td><td rowspan="1" colspan="1">—</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">11343</td><td rowspan="1" colspan="1">108.49</td><td rowspan="1" colspan="1">26</td><td rowspan="1" colspan="1">−50</td><td rowspan="1" colspan="1">−12</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">precentral gyrus</td><td rowspan="1" colspan="1">BA 9</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">1439</td><td rowspan="1" colspan="1">81.83</td><td rowspan="1" colspan="1">−36</td><td rowspan="1" colspan="1">4</td><td rowspan="1" colspan="1">34</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">inferior frontal gyrus</td><td rowspan="1" colspan="1">BA 9</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">1770</td><td rowspan="1" colspan="1">64.81</td><td rowspan="1" colspan="1">42</td><td rowspan="1" colspan="1">10</td><td rowspan="1" colspan="1">26</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">BA 13</td><td rowspan="1" colspan="1">0.002</td><td rowspan="1" colspan="1">414</td><td rowspan="1" colspan="1">56.86</td><td rowspan="1" colspan="1">32</td><td rowspan="1" colspan="1">22</td><td rowspan="1" colspan="1">−2</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">temporal</td><td rowspan="1" colspan="1">middle temporal gyrus</td><td rowspan="1" colspan="1">BA 21</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">6458</td><td rowspan="1" colspan="1">52.55</td><td rowspan="1" colspan="1">−56</td><td rowspan="1" colspan="1">−22</td><td rowspan="1" colspan="1">−14</td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">insula</td><td rowspan="1" colspan="1">BA 13</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">4254</td><td rowspan="1" colspan="1">50.94</td><td rowspan="1" colspan="1">42</td><td rowspan="1" colspan="1">−12</td><td rowspan="1" colspan="1">6</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">cingulate gyrus</td><td rowspan="1" colspan="1">BA 24</td><td rowspan="1" colspan="1">0.000</td><td rowspan="1" colspan="1">1827</td><td rowspan="1" colspan="1">40.86</td><td rowspan="1" colspan="1">−4</td><td rowspan="1" colspan="1">−14</td><td rowspan="1" colspan="1">40</td></tr><tr><td colspan="10" rowspan="1"><italic toggle="yes">ROI-main effect of group</italic></td></tr><tr><td rowspan="1" colspan="1">right</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">superior frontal gyrus</td><td rowspan="1" colspan="1">BA 10</td><td rowspan="1" colspan="1">0.007</td><td rowspan="1" colspan="1">219</td><td rowspan="1" colspan="1">31.57</td><td rowspan="1" colspan="1">14</td><td rowspan="1" colspan="1">60</td><td rowspan="1" colspan="1">14</td></tr><tr><td rowspan="1" colspan="1">left</td><td rowspan="1" colspan="1">frontal</td><td rowspan="1" colspan="1">middle frontal gyrus</td><td rowspan="1" colspan="1">BA 10</td><td rowspan="1" colspan="1">0.046</td><td rowspan="1" colspan="1">124</td><td rowspan="1" colspan="1">19.57</td><td rowspan="1" colspan="1">−32</td><td rowspan="1" colspan="1">54</td><td rowspan="1" colspan="1">8</td></tr></tbody></table></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 15528097</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/15528097/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>15528097_analysis_0</td><td>Brain regions commonly activated by guilt and embarrassment conditions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task and interpretation explicitly engage understanding others&#x27; mental states (Theory of Mind) to judge social emotions; thus it measures perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16055351</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16055351/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16055351_analysis_0</td><td>Anger vs. Neutral (AN + NA) vs. (NN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast captures perception/understanding of others&#x27; emotional states via anger prosody, fitting perception-of-others criteria.</td></tr>
<tr><td>16055351_analysis_1</td><td>Anger to-be-attended vs. Neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast directly assesses perception and understanding of others&#x27; emotional states via angry vs neutral prosody, meeting the criteria.</td></tr>
<tr><td>16055351_analysis_2</td><td>Anger to-be-ignored vs. Neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Anger prosody processing involves representing and evaluating others&#x27; emotional states, meeting Perception and Understanding of Others.</td></tr>
<tr><td>16055351_analysis_3</td><td>Anger to-be-attended vs. to-be-ignored</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast assesses representation and evaluation of others&#x27; emotional states (anger in voice), meeting Perception and Understanding of Others criteria.</td></tr>
<tr><td>16055351_analysis_4</td><td>Anger to-be-ignored vs. to-be-attended</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast probes representations of others&#x27; emotional states via angry prosody (awareness/assessment of others&#x27; emotions), matching perception and understanding of others.</td></tr>
<tr><td>16055351_analysis_5</td><td>Spatial attention towards right vs. left ear</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast measures perception and understanding of others&#x27; emotional states via angry vs. neutral prosody, matching Perception and Understanding of Others.</td></tr>
<tr><td>16055351_analysis_6</td><td>Spatial attention towards left vs. right ear</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast examines perception and evaluation of others&#x27; emotional states via anger prosody, satisfying perception-of-others criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16171833</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16171833/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16171833_analysis_0</td><td>a) Common activations of social interaction (SOC &gt; ARB)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>SOC&gt;ARB involves perceiving socially relevant expressions of (virtual) others and thus relates to perception/understanding of others (action/mental-state-relevant social cues).</td></tr>
<tr><td>16171833_analysis_1</td><td>b) Common activations of arbitrary facial movements (ARB &gt; SOC)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis examines perception of others&#x27; facial movements (social vs arbitrary) and thus measures understanding/perception of others.</td></tr>
<tr><td>16171833_analysis_2</td><td>c) Common activations of self-involvement (ME &gt; OTHER)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific contrast (ME&gt;OTHER) targets self-involvement rather than representations about others; it does not directly probe perception/understanding of others, so inclusion criteria are not met for this analysis.</td></tr>
<tr><td>16171833_analysis_3</td><td>d) Common activations of other-related activity (OTHER &gt; ME)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>OTHER&gt;ME directly probes perception and understanding of others (gaze directed to others, third-person perspective-taking, temporo-parietal activations) and thus satisfies the criteria.</td></tr>
<tr><td>16171833_analysis_4</td><td>e) Common activations of the statistical interaction SOC × ME</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the interaction emphasizes self-involvement, it contrasts ME vs OTHER within social vs arbitrary expressions and engages regions implicated in understanding others (e.g. STG); it therefore relates to perception/understanding of others.</td></tr>
<tr><td>16171833_analysis_5</td><td>f) Common activations of the statistical interaction SOC × OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast explicitly tests perception and interpretation of others&#x27; social signals (SOC directed to OTHER vs ARB), fitting Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16759672</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16759672/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16759672_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis explicitly examines representations and inferences about others’ traits and emotions from faces, meeting both inclusion criteria for perception/understanding of others.</td></tr>
<tr><td>16759672_analysis_1</td><td>Faces associated with disgusting behaviors greater than faces associated with aggressive behaviors</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast directly tests representations about others’ traits and affective states (disgust vs aggression) during face perception, satisfying perception and understanding of others.</td></tr>
<tr><td>16759672_analysis_2</td><td>Faces associated with aggressive behaviors greater than faces associated with disgusting behaviors</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis explicitly measures representations and inferences about others’ traits and emotions from faces (e.g., disgust vs aggression), directly addressing Perception and Understanding of Others, meeting I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 17071110</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/17071110/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>17071110_analysis_0</td><td>Receiving aversive stimuli; parametric modulation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast measures perception and understanding of another’s emotional state (observing opponent suffering, intensity of pain), directly mapping onto perception/understanding of others (mental states, action/animacy perception).</td></tr>
<tr><td>17071110_analysis_1</td><td>Retaliation; parametric modulation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis directly measures representations of others’ emotional states (watching opponent suffering) with parametric modulation by pain intensity (STS, amygdala), satisfying I1 and I2.</td></tr>
<tr><td>17071110_analysis_2</td><td>Conjunction; retaliation and watching the opponent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis explicitly examines observing the opponent’s suffering, empathy-related responses, and mentalizing (amygdala, STS, dorsal/ventral mPFC), directly assessing Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 17408704</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/17408704/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>17408704_analysis_0</td><td>EMO</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Emotion recognition of faces directly assesses perception and understanding of others&#x27; emotional states, satisfying the annotation&#x27;s inclusion criteria.</td></tr>
<tr><td>17408704_analysis_1</td><td>AGE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Age discrimination is the perception and assessment of another person&#x27;s trait (age), fitting the Perception and Understanding of Others construct.</td></tr>
<tr><td>17408704_analysis_2</td><td>EMO-AGE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Emotion recognition of faces involves perceiving and understanding others&#x27; emotional states, satisfying criteria for perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 17627852</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/17627852/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>17627852_analysis_0</td><td>Communication—control</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The communication vs description contrast and familiarity manipulation probe understanding of others&#x27; intentions, traits, and biographical information (theory of mind/mentalization), satisfying perception-of-others criteria.</td></tr>
<tr><td>17627852_analysis_1</td><td>Description—control</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study investigates understanding others’ mental states, intentions, and familiarity effects (theory of mind, biographical information) during communication, meeting criteria for Perception and Understanding of Others.</td></tr>
<tr><td>17627852_analysis_2</td><td>Communication— description</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study targets understanding others&#x27; intentions, biographical/familiarity information, and theory-of-mind-related processing (action and mental-state understanding), satisfying perception-of-others criteria.</td></tr>
<tr><td>17627852_analysis_3</td><td>Familiar communication – unfamiliar communication</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast and task probe understanding of others (theory of mind, biographical knowledge, action observation) and directly assess perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 17964185</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/17964185/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>17964185_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast examines representations of others (friends vs celebrities) and emotional valence toward them, directly addressing perception and understanding of others. Both inclusion criteria are met.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18486491</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18486491/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18486491_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The PD task and contrasts explicitly probe understanding others’ mental states (theory-of-mind, DMPFC/TPJ activity) and in-group/out-group mentalizing, directly matching this construct.</td></tr>
<tr><td>18486491_analysis_1</td><td>analysis_1</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analyses specifically examine theory-of-mind related regions (DMPFC, TPJ), mentalizing, and differences in responses to in-group vs out-group partners — directly indexing perception/understanding of others’ mental states.</td></tr>
<tr><td>18486491_analysis_2</td><td>Regions where the cooperation difference score (in-group-out-group) is significantly correlated with the difference in BOLD signal during in-group and out-group interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis probes neural differences when interacting with in-group vs out-group partners and correlates with behavioral discrimination, directly assessing understanding of others’ behavior/mental states.</td></tr>
<tr><td>18486491_analysis_3</td><td>DMPFC connectivity with in-group - DMPFC connectivity with out-group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast targets mentalizing and understanding of others (in-group vs out-group interactions, theory-of-mind ROIs and connectivity), directly measuring perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18537114</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18537114/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18537114_analysis_0</td><td>P</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires inferring others&#x27; actions/intentions (who will pass the ball) and the authors interpret frontal operculum activity as inference about others&#x27; goals, satisfying Perception/Understanding of Others.</td></tr>
<tr><td>18537114_analysis_1</td><td>S</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Social task requires perceiving and predicting other agents&#x27; actions and intentions (who will pass the ball), which maps onto perception/understanding of others (action perception/understanding mental states).</td></tr>
<tr><td>18537114_analysis_2</td><td>Brain activation in Pavlovian compared with social task</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Social task requires perceiving and predicting other agents&#x27; actions (who will pass the ball) and models learning about others&#x27; behavior, meeting Perception and Understanding of Others criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18603608</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18603608/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> compositions &gt; computer-generated pieces; others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18603608_analysis_0</td><td>Predicted</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The contrast specifically probes understanding of others’ mental states (intention attribution) and activates canonical mentalizing regions (aMFC, STS, temporal poles), matching Perception and Understanding of Others.</td></tr>
<tr><td>18603608_analysis_1</td><td>Not predicted</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis explicitly tests attribution of intentions to a presumed human creator (mental state attribution), matching Perception and Understanding of Others (Understanding Mental States).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>18603608_1</td><td>compositions &gt; computer-generated pieces; others</td><td>18603608_analysis_0</td><td>Predicted</td><td>0.163</td><td>0.667</td><td>0.516</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18633834</strong> | Pred included: 6 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18633834/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18633834_analysis_0</td><td>STEP1
+STEP2 - REST</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task assesses perception and understanding of others (animacy/action perception and inference of intentions/mental states from footsteps); contrasts directly index these processes.</td></tr>
<tr><td>18633834_analysis_1</td><td>NOISE1
+NOISE2 - REST</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Listening to one vs two walkers and related contrasts assesses representations of others (animacy and action perception, inferring intentions/mental states); it meets criteria for Perception and Understanding of Others.</td></tr>
<tr><td>18633834_analysis_2</td><td>(STEP2 - NOISE2) - (STEP1 - NOISE1)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task explicitly involves perceiving other agents (one vs two walkers), biological-motion and mentalizing-related processing (animacy/action perception and understanding others), so meets criteria.</td></tr>
<tr><td>18633834_analysis_3</td><td>STEP1 - NOISE1</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast probes representations about others (action/animacy perception and inferred intentions for two walkers) and elicits regions linked to understanding others (STS, temporal pole, amygdala), satisfying perception-of-others criteria.</td></tr>
<tr><td>18633834_analysis_4</td><td>STEP2 - NOISE2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrasts target perception and understanding of others (number of agents, identity cues, inferred intentions), matching action perception and understanding mental states subconstructs.</td></tr>
<tr><td>18633834_analysis_5</td><td>STEP2 - STEP1</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast probes perception/understanding of others (number of agents, identity cues, intentions/mentalizing) and authors explicitly discuss temporal pole and mentalizing — meets criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18633846</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18633846/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18633846_analysis_0</td><td>Main effect of type of observed action</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast measures perception and understanding of others’ actions and inferred intentions (social vs individual actions), matching this construct (action perception/understanding mental states).</td></tr>
<tr><td>18633846_analysis_1</td><td>Main effect of gaze</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Study examines perception and understanding of others’ actions and intentions via gaze and social vs individual action contrasts, matching this construct.</td></tr>
<tr><td>18633846_analysis_2</td><td>Interaction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires perceiving and interpreting others&#x27; actions and gaze (triadic interactions), matching action perception and understanding others&#x27; mental states.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18783371</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18783371/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18783371_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study explicitly measures Action Perception (observation of hand and foot actions) and neural responses to others’ actions, satisfying the Perception and Understanding of Others criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 19048432</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19048432/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19048432_analysis_0</td><td>Main effect of viewing eyes</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis targets representation and evaluation of others&#x27; pupil signals (inferring others&#x27; dispositional/arousal states), meeting perception/understanding of others criteria.</td></tr>
<tr><td>19048432_analysis_1</td><td>Region of interest analysis</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis explicitly probes representation and interpretation of others&#x27; pupillary signals and inferred dispositional state, matching perception and understanding of others.</td></tr>
<tr><td>19048432_analysis_2</td><td>Main effect of change in observed and observer&#x27;s pupil size</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis explicitly examines perception and interpretation of others&#x27; pupil-size changes and infers dispositional states, meeting criteria for perception and understanding of others.</td></tr>
<tr><td>19048432_analysis_3</td><td>Region of interest analysis</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task explicitly measures participants’ perception and interpretation of others’ dynamic pupillary signals and neural responses to discrepant vs. coherent pupil changes—directly addressing perception and understanding of others.</td></tr>
<tr><td>19048432_analysis_4</td><td>Regions of interest</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis measures representations and interpretation of others&#x27; internal/dispositional state via observed pupil dynamics and related neural responses (amygdala, STS, insula), matching perception/understanding of others. </td></tr>
<tr><td>19048432_analysis_5</td><td>(positive &gt; negative)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis measures perception and interpretation of others&#x27; pupillary signals and how incoherence signals social salience—this directly assesses perception and understanding of others.</td></tr>
<tr><td>19048432_analysis_6</td><td>Regions of interest</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis directly measures perception and understanding of others&#x27; internal state via observed pupillary changes and responses in brain regions associated with social/emotional processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 19347874</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19347874/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19347874_analysis_0</td><td>Brain regions exhibiting significantly increased activation in response to masked sad faces compared to neutral faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis assesses perception and understanding of others&#x27; emotional states (masked sad/happy faces) and correlates with attachment—matches perception-of-others construct.</td></tr>
<tr><td>19347874_analysis_1</td><td>Table II. Brain regions exhibiting significantly increased activation in response to masked happy faces compared to neutral faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis directly measures perception/understanding of others&#x27; emotional states via facial expressions (happy vs neutral), satisfying perception-of-others criteria.</td></tr>
<tr><td>19347874_analysis_2</td><td>Negative correlations of attachment avoidance with brain responses to masked sad faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis examines neural responses to others&#x27; emotional expressions and correlations with attachment—this is perception and understanding of others (emotion/mental state cues). Meets I1 and I2.</td></tr>
<tr><td>19347874_analysis_3</td><td>Positive correlations</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis measures perception and understanding of others&#x27; emotional states via facial expressions and relates this to attachment avoidance.</td></tr>
<tr><td>19347874_analysis_4</td><td>Negative correlations</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Directly measures perception and understanding of others&#x27; emotional states via masked facial expressions and correlates neural responses with attachment, satisfying the criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 19944083</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19944083/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19944083_analysis_0</td><td>Emotional &gt; neutral stimuli</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast directly probes representations and inference about others&#x27; emotional states and social relations (mentalizing, understanding others).</td></tr>
<tr><td>19944083_analysis_1</td><td>Neutral &gt; emotional stimuli</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast directly concerns perceiving and understanding others&#x27; emotional states in social scenes (mental state/emotion inference), satisfying (I1) task measuring perception of others and (I2) measuring perception and understanding of others.</td></tr>
<tr><td>19944083_analysis_2</td><td>Social relation &gt; single person</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast explicitly compares perception/understanding of others in social relations (two vs one person) and probes mentalizing/other-representation; meets the construct criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20188190</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20188190/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20188190_analysis_0</td><td>Preference</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis requires assessing others&#x27; traits/emotional expressions and forming judgments about them (Do I like this person?), fitting Perception and Understanding of Others. It satisfies the inclusion criteria.</td></tr>
<tr><td>20188190_analysis_1</td><td>Gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires assessing others’ faces (liking of other individuals), which involves representing and evaluating others’ social attributes; thus it measures perception and understanding of others.</td></tr>
<tr><td>20188190_analysis_2</td><td>Preference &gt; Gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast assesses representations and evaluations of others (faces with emotional expressions) and reasoning about others’ social/affective properties, meeting criteria for perception and understanding of others.</td></tr>
<tr><td>20188190_analysis_3</td><td>Gender &gt; Preference</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires evaluating other individuals&#x27; faces and forming impressions about others&#x27; traits/valence, engaging perception and understanding of others.</td></tr>
<tr><td>20188190_analysis_4</td><td>aMFC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires assessing others’ social/affective attributes from faces (liking judgments), which indexes perception and understanding of others’ traits/states.</td></tr>
<tr><td>20188190_analysis_5</td><td>vMFC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires assessing others&#x27; characteristics/emotional expressions and making judgments about them, mapping onto perception and understanding of others; the contrast measures this construct.</td></tr>
<tr><td>20188190_analysis_6</td><td>PCC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires assessing other people (faces) and forming judgments about their likeability, which maps onto perception and understanding of others (traits/affective states).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21320516</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21320516/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21320516_analysis_0</td><td>Female and male</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Humor comprehension in this study engages cognitive processes related to understanding others (mentalizing/ToM), with TPJ and related regions reported in conjunction and contrasts, so the contrasts capture perception and understanding of others.</td></tr>
<tr><td>21320516_analysis_1</td><td>Female and male</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Authors report recruitment of TPJ and discuss mentalizing/ToM and understanding others’ mental states in humor comprehension. The contrasts and analyses probe processes related to understanding others (I1) and measure such processing (I2).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21600991</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21600991/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21600991_analysis_0</td><td>Activations in the retrieval of social context</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The design contains ‘other-generation’ conditions and contrasts that compare self vs other and social vs non-social contexts; these probe processing related to another agent’s contributions (perception/understanding of others) during encoding/retrieval.</td></tr>
<tr><td>21600991_analysis_1</td><td>Activations in the retrieval of self-generation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The social-context conditions (SO, SS) require attending to and processing another person&#x27;s contributions; the social vs non-social contrast indexes representation of social interaction and thus relates to perception/understanding of others (e.g., partner actions/roles).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 22174872</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/22174872/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>22174872_analysis_0</td><td>Subtraction of neutral form emotional trials.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task and contrast directly assess representation and inference about others&#x27; emotional/mental states (including complex mental-state decoding), satisfying perception and understanding of others.</td></tr>
<tr><td>22174872_analysis_1</td><td>Subtraction of simple from complex emotion trials.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast isolates processes involved in understanding others&#x27; emotional states and inferring mental states (ToM) for complex emotions, directly matching perception and understanding of others.</td></tr>
<tr><td>22174872_analysis_2</td><td>Subtraction of simple from complex emotion trials (controlled for pitch).</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast isolates processing of complex (mental-state rich) versus simple emotions from vocal cues; authors explicitly interpret mPFC involvement as inferring others&#x27; mental states. This directly measures perception and understanding of others (including Understanding Mental States).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23298748</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23298748/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23298748_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Trustworthiness judgments are assessments of others&#x27; traits and fit Perception and Understanding of Others. Inclusion criteria satisfied.</td></tr>
<tr><td>23298748_analysis_1</td><td>analysis_1</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires representing and judging others&#x27; traits (trustworthiness), which fits Perception and Understanding of Others.</td></tr>
<tr><td>23298748_analysis_2</td><td>analysis_2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task explicitly assesses representations of others&#x27; traits (trustworthiness), meeting criteria for perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23378834</strong> | Pred included: 10 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23378834/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23378834_analysis_0</td><td>FAMOUS FACES vs. BASELINE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Famous face recognition and associated person knowledge involve representing and reasoning about others (their identity/biographical semantic information), meeting the Perception and Understanding of Others criteria.</td></tr>
<tr><td>23378834_analysis_1</td><td>FAMILIAR FACES vs. BASELINE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Familiar-face contrasts and the social-closeness task probe representations and knowledge about other people (their identity, social closeness, biographical information), meeting I1 and I2.</td></tr>
<tr><td>23378834_analysis_2</td><td>FAMOUS ∩ FAMILIAR FACES</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrasts assess representations and judgments about others&#x27; identity and social closeness (Facesfriends vs. Facesnovel; famous vs. novel), matching Perception and Understanding of Others.</td></tr>
<tr><td>23378834_analysis_3</td><td>Faces &gt; Landmarks</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Faces &gt; Landmarks measures perception and understanding of others via face perception/recognition, satisfying the inclusion criteria for Perception and Understanding of Others (I1 and I2).</td></tr>
<tr><td>23378834_analysis_4</td><td>Landmarks &gt; Faces</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast does not probe representations or reasoning about others&#x27; mental states or traits; it is landmarks &gt; faces and does not satisfy Perception/Understanding of Others inclusion criteria (I1/I2).</td></tr>
<tr><td>23378834_analysis_5</td><td>Famous faces &gt; Novel faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Famous &gt; novel faces assesses representations and knowledge about others (identity, biographical semantic information), fitting Perception and Understanding of Others.</td></tr>
<tr><td>23378834_analysis_6</td><td>Novel faces &gt; Famous faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Novel vs famous face contrast assesses representations about other people (identity/familiarity), meeting criteria for perception and understanding of others.</td></tr>
<tr><td>23378834_analysis_7</td><td>Novel faces &gt; Novel landmarks</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrasting novel faces vs novel landmarks assesses perception and representation of other people (face perception/identity), satisfying criteria for perception and understanding of others.</td></tr>
<tr><td>23378834_analysis_8</td><td>Novel landmarks &gt; Novel faces</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Perception/understanding of others refers to inferring others&#x27; mental states, traits, or emotions. The contrast is between landmarks and faces in a perceptual task and does not assess mentalizing or social trait inference, so I1/I2 are not met.</td></tr>
<tr><td>23378834_analysis_9</td><td>Famous faces &gt; Novel landmarks</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Famous face processing entails perception and representation of other people (identity/semantic information); the contrast taps perception/understanding of others compared to non-social stimuli.</td></tr>
<tr><td>23378834_analysis_10</td><td>Novel landmarks &gt; Famous faces</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The specific contrast identifies regions more active for novel landmarks than for famous faces and thus does not directly measure perception/understanding of others (e.g., mental states, animacy or action perception). It does not satisfy I1/I2 for this annotation.</td></tr>
<tr><td>23378834_analysis_11</td><td>Famous landmarks &gt; Novel landmarks</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Famous vs novel landmarks concern place/scene familiarity and not representations or reasoning about other people&#x27;s mental states, actions, or animacy; it does not meet I1 or I2.</td></tr>
<tr><td>23378834_analysis_12</td><td>Novel landmarks &gt; Famous landmarks</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Novel vs famous landmarks are non-social stimuli and do not assess perception or understanding of others’ mental states or traits. The analysis does not meet the inclusion criteria for Perception and Understanding of Others (fails I1 and I2).</td></tr>
<tr><td>23378834_analysis_13</td><td>Familiar faces &gt; Novel faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Familiar &gt; Novel faces measures representations and knowledge about others (identity, familiarity, biographical/affective knowledge), matching Perception and Understanding of Others.</td></tr>
<tr><td>23378834_analysis_14</td><td>Famous ∩ Familiar faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analyses assess representations and judgments about others (familiarity, identity, social closeness), directly matching perception/understanding of others. Meets I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23599165</strong> | Pred included: 8 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23599165/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23599165_analysis_0</td><td>Yes (Match + Unrequited) &gt; No (Rejection + Disinterest)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analyses probe updating representations of others (mentalizing regions pSTS, RMPFC respond to unexpected partner decisions), directly measuring perception and understanding of others&#x27; mental states.</td></tr>
<tr><td>23599165_analysis_1</td><td>No (Rejection + Disinterest) &gt; Yes (Match + Unrequited)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study centers on forming and updating beliefs about others (mentalizing, pSTS/RMPFC activations). The contrast contrasts partners&#x27; decisions, indexing perception/understanding of others&#x27; intentions.</td></tr>
<tr><td>23599165_analysis_2</td><td>Match &gt; Unrequited</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although focused on reward value, the contrast concerns responses to another person&#x27;s expressed interest and involves updating social beliefs about others; thus it measures Perception/Understanding of Others.</td></tr>
<tr><td>23599165_analysis_3</td><td>Rejection &gt; Disinterest</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast involves interpreting others&#x27; decisions and violations of expectations about partners (mentalizing about others), and the paper reports pSTS/RMPFC involvement—consistent with perception/understanding of others.</td></tr>
<tr><td>23599165_analysis_4</td><td>Mismatched (Rejection + Unrequited) &gt; matched (Match + Disinterest)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast targets updating beliefs about others (violations of expected partner decisions) and engages regions (pSTS, RMPFC) linked to understanding others&#x27; mental states, matching the inclusion criteria.</td></tr>
<tr><td>23599165_analysis_5</td><td>Unsigned prediction errors from RL model</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis concerns expectations about partners’ decisions and updating based on others’ behavior; RMPFC/pSTS involvement relates to mentalizing and understanding others’ actions, so it meets criteria for perception/understanding of others.</td></tr>
<tr><td>23599165_analysis_6</td><td>Partners who were given a yes &gt; those given a no</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study examines updating beliefs about others (mentalizing) in response to partners&#x27; decisions; the contrast probes processing of others&#x27; responses and violations of expectations about others, fitting perception/understanding of others.</td></tr>
<tr><td>23599165_analysis_7</td><td>Partners who were given a no &gt; those given a yes</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analyses explicitly probe updating beliefs about partners (mentalizing, pSTS, RMPFC activations) and violations of expectation about others&#x27; decisions, directly measuring perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23667619</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23667619/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23667619_analysis_0</td><td>CSL&gt;TIC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The CSL&gt;TIC contrast targets representations and inference about others&#x27; socio-relational information and mental states (mentalizing), directly matching perception/understanding of others.</td></tr>
<tr><td>23667619_analysis_1</td><td>TIC&gt;CSL</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis examines perception and interpretation of others&#x27; laughter and reports mentalizing-related activations/connectivity; thus it measures perception/understanding of others.</td></tr>
<tr><td>23667619_analysis_2</td><td>CAT&gt;COU</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The CAT&gt;COU contrast targets interpretation of others&#x27; social signals (laughter types) and is tied to mentalizing and understanding others’ intentions; it therefore measures perception and understanding of others.</td></tr>
<tr><td>23667619_analysis_3</td><td>HAP_CAT ∩ TAU_CAT ∩ TIC_CAT ∩ HAP_COU ∩ TAU_COU ∩ TIC_COU</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study investigates participants&#x27; perception and inference of others&#x27; social/intentional states from laughter (mentalizing/arMFC activations and related contrasts), directly addressing understanding of others and meeting both inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23722983</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23722983/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23722983_analysis_0</td><td>Intention effect</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The intention effect primarily indexes the participant’s own intention and self-relevance processing rather than explicit representation or inference about others’ mental states (the salesperson effect, not the intention effect, was associated with mentalizing/TPJ).</td></tr>
<tr><td>23722983_analysis_1</td><td>Salesperson effect</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The salesperson effect specifically reflects interns’ capacity to predict and shape others’ preferences and was associated with TPJ/mentalizing regions (understanding others’ mental states), meeting perception-of-others criteria.</td></tr>
<tr><td>23722983_analysis_2</td><td>Buzz effect</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The buzz effect is explicitly associated with mentalizing regions (DMPFC, TPJ) and indexes understanding/prediction of others’ responses, satisfying perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24243619</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24243619/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24243619_analysis_0</td><td>Main effects of social judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis explicitly assesses others’ traits and emotional states (trustworthiness, attractiveness, happiness, age) from voices, matching Perception and Understanding of Others.</td></tr>
<tr><td>24243619_analysis_1</td><td>Overlapping effects during social and emotional judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis measures perception and understanding of others&#x27; emotional states and traits (trustworthiness, attractiveness, happiness, age), satisfying this construct.</td></tr>
<tr><td>24243619_analysis_2</td><td>Overlapping effects during social and cognitive judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrasts explicitly assess perception and understanding of others’ traits and emotional states (trustworthiness, attractiveness, happiness, age), directly satisfying I1 and I2 for Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24582805</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=3</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24582805/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> OA &gt; CA; others, OP &gt; CP; others, OS &gt; CS; others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24582805_analysis_0</td><td>Main effect of age group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Direct other-evaluations and reflected-perspective tasks involve assessing others’ traits and inferred mental states.</td></tr>
<tr><td>24582805_analysis_1</td><td>Main effect of evaluative perspective</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Direct other-evaluations and reflected self-evaluations require representing and judging others&#x27; (best friend’s) traits/perspective, measuring perception/understanding of others.</td></tr>
<tr><td>24582805_analysis_2</td><td>Age group × evaluative perspective</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Direct other-evaluations and reflected self-evaluations (inferring best friend&#x27;s perspective) require representing and reasoning about others’ traits and perspectives, meeting criteria for perception and understanding of others.</td></tr>
<tr><td>24582805_analysis_3</td><td>Main effect of domain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Direct other-evaluations of the participant&#x27;s best friend and reflected self-evaluations (adopting friend&#x27;s perspective) require reasoning about others&#x27; traits and perspectives, meeting Perception/Understanding of Others.</td></tr>
<tr><td>24582805_analysis_4</td><td>Age group × domain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task includes direct evaluations of a close other and reflected evaluations (inferring others&#x27; perspectives), thus measuring understanding and reasoning about others’ traits and perspectives.</td></tr>
<tr><td>24582805_analysis_5</td><td>Evaluative perspective × domain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Direct other-evaluations about the best friend and reflected evaluations (inferring friend&#x27;s perspective) assess perception and understanding of others&#x27; traits and mental states—meets criteria.</td></tr>
<tr><td>24582805_analysis_6</td><td>Age group × evaluative perspective × domain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Direct other-evaluations about the best friend and reflected self-evaluations from the friend’s perspective require representing and reasoning about others’ traits and perspectives (mentalizing).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24582805_4</td><td>OA &gt; CA; others</td><td>24582805_analysis_5</td><td>Evaluative perspective × domain</td><td>0.217</td><td>0.115</td><td>0.146</td><td>unmatched</td><td>low_total_score</td></tr><tr><td>24582805_5</td><td>OP &gt; CP; others</td><td>24582805_analysis_2</td><td>Age group × evaluative perspective</td><td>0.327</td><td>0.000</td><td>0.098</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>24582805_6</td><td>OS &gt; CS; others</td><td>24582805_analysis_6</td><td>Age group × evaluative perspective × domain</td><td>0.241</td><td>0.000</td><td>0.072</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24814646</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24814646/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24814646_analysis_0</td><td>fMRI contrasts on the basis of the neutral condition as baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires perceiving and interpreting others’ actions/intent (aggressive vs positive behaviors); contrasts measure understanding of others’ states and actions.</td></tr>
<tr><td>24814646_analysis_1</td><td>Initial phase of video-clip: A</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires assessing others&#x27; actions, intentions, and emotional states (aggression, friendliness) from video stimuli; contrasts target perception/understanding of others, meeting I1 and I2.</td></tr>
<tr><td>24814646_analysis_2</td><td>Initial phase of video-clip: B</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The paradigm requires assessing others&#x27; actions and emotional intent (threatening, friendly, neutral) and contrasts examine perception/understanding of others’ states and actions.</td></tr>
<tr><td>24814646_analysis_3</td><td>Final phase of video-clip: C</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task requires assessing others’ emotional and action-related states (aggressive vs friendly behavior), directly engaging perception and understanding of others.</td></tr>
<tr><td>24814646_analysis_4</td><td>Final phase of video-clip: D</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants perceive others&#x27; actions (provocation, friendliness) and make emotional appraisals; contrasts probe perception and understanding of others&#x27; actions/intent, meeting the construct. Satisfies I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24834034</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24834034/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24834034_analysis_0</td><td>FAIR &gt; UNFAIR</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>While the task involves other agents, the contrast indexes responses to fairness violations rather than explicit inference about others&#x27; mental states or traits; it does not clearly meet the Perception and Understanding of Others criteria (e.g., mentalizing).</td></tr>
<tr><td>24834034_analysis_1</td><td>FAIR &lt; UNFAIR</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The unfair vs fair contrast reflects participants&#x27; responses to others&#x27; (proposers&#x27;) behavior and thus taps aspects of perceiving and understanding others&#x27; fairness/intent in a social interaction.</td></tr>
<tr><td>24834034_analysis_2</td><td>HUMAN &gt; COMPUTER</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>While the contrast engages social context, it does not explicitly measure representations of others&#x27; mental states or trait attributions (no explicit mentalizing or theory-of-mind manipulation).</td></tr>
<tr><td>24834034_analysis_3</td><td>HIGH(UNFAIR–FAIR) &gt; LOW(UNFAIR–FAIR)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The UG requires representing and responding to another agent&#x27;s offer (human proposer) and the contrast examines differences in neural responses to others&#x27; fair vs unfair behavior under different stakes, meeting perception-of-others criteria.</td></tr>
<tr><td>24834034_analysis_4</td><td>HIGH(UNFAIR–FAIR) &gt; LOW(UNFAIR–FAIR) WITHIN HUMAN</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the task involves interacting with others, the contrast targets fairness modulation by stake size rather than explicit representation or mentalizing about others&#x27; emotional states or traits; it does not directly measure perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24842782</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24842782/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24842782_analysis_0</td><td>Angry &gt;neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast measures perception and understanding of another&#x27;s emotional state (angry vs. neutral facial expressions) in a social interaction, directly satisfying the construct.</td></tr>
<tr><td>24842782_analysis_1</td><td>Angry &gt;neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Angry&gt;neutral contrast measures representations of others&#x27; emotional states (perception and understanding of others), within a cooperative/competitive social context, satisfying I1 and I2.</td></tr>
<tr><td>24842782_analysis_2</td><td>Parametric modulation angry &gt;neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis assesses representation and evaluation of another&#x27;s emotional state (angry vs neutral) and links neural responses to behavioral aggression, directly addressing perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24936688</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24936688/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24936688_analysis_0</td><td>Status Type by Status Level interaction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The primary task is forming impressions and evaluating others based on inferred moral and financial status — directly measuring perception and understanding of others.</td></tr>
<tr><td>24936688_analysis_1</td><td>Status Type main effect</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis directly tests perception and understanding of others (impression formation and evaluation based on social status). The contrasts measure how targets differing in status are perceived and evaluated, satisfying the criteria.</td></tr>
<tr><td>24936688_analysis_2</td><td>Status Level main effect</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis explicitly examines representations and evaluations of others&#x27; moral and financial status (impression formation), directly matching perception and understanding of others. Meets I1 (task measures perception of others) and I2 (contrast measures this construct).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25281889</strong> | Pred included: 12 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25281889/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25281889_analysis_0</td><td>HA&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast (happy&gt;neutral) measures perception and understanding of others’ emotional states (facial emotion recognition), satisfying the construct.</td></tr>
<tr><td>25281889_analysis_1</td><td>AN&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Directly measures perception and understanding of others&#x27; emotional states via labeling emotional facial expressions (angry/fearful/happy vs neutral).</td></tr>
<tr><td>25281889_analysis_2</td><td>FE&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>FE&gt;NE directly measures perception and understanding of others&#x27; emotional state (fearful faces vs neutral), satisfying the Perception and Understanding of Others criteria.</td></tr>
<tr><td>25281889_analysis_3</td><td>HA&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Labeling emotional facial expressions assesses representations and understanding of others&#x27; emotional states, matching Perception and Understanding of Others.</td></tr>
<tr><td>25281889_analysis_4</td><td>AN&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task involves representing and reasoning about others&#x27; emotional states (labeling angry/fearful/happy faces), fitting Perception and Understanding of Others.</td></tr>
<tr><td>25281889_analysis_5</td><td>FE&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast FE&gt;NE (and other emotion&gt;neutral contrasts) examine representations of others&#x27; emotional states; fits Perception and Understanding of Others.</td></tr>
<tr><td>25281889_analysis_6</td><td>HA&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast assesses representations of others&#x27; emotional states via facial emotion labeling, directly mapping onto Perception and Understanding of Others.</td></tr>
<tr><td>25281889_analysis_7</td><td>AN&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis concerns representing and assessing others&#x27; emotional states (labeling angry vs neutral faces), matching Perception and Understanding of Others.</td></tr>
<tr><td>25281889_analysis_8</td><td>FE&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast compares fearful to neutral faces to probe representations of others&#x27; emotional states, directly addressing Perception and Understanding of Others.</td></tr>
<tr><td>25281889_analysis_9</td><td>HA&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast measures perception and understanding of others&#x27; emotional states (happy vs neutral faces), satisfying Perception and Understanding of Others.</td></tr>
<tr><td>25281889_analysis_10</td><td>AN&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The AN&gt;NE contrast explicitly measures perception and understanding of others’ emotional states (recognition/labeling of angry vs neutral faces), satisfying I1 (task measuring perception/understanding of others) and I2 (measures Perception and Understanding of Others).</td></tr>
<tr><td>25281889_analysis_11</td><td>FE&gt;NE</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast FE&gt;NE measures representations and assessment of others’ emotional states (fearful vs neutral faces), directly matching Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25534111</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25534111/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25534111_analysis_0</td><td>Responding to joint attention (RJA - RJAc)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Responding to joint attention requires representing and interpreting the other&#x27;s attentional focus/intentions; the RJA − RJAc contrast isolates social processes related to understanding others, satisfying the criteria.</td></tr>
<tr><td>25534111_analysis_1</td><td>Initiating joint attention (IJA - IJAc)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task and contrast explicitly probe representation and interpretation of another&#x27;s attention and intentions (gaze following and monitoring the partner), satisfying inclusion criteria for perception and understanding of others.</td></tr>
<tr><td>25534111_analysis_2</td><td>Conjunction of initiating and responding to joint attention (IJA - IJAc) with (RJA - RJAc)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis measures representation and understanding of another&#x27;s attentional state (gaze cues, intention to guide attention), directly mapping onto perception/understanding of others.</td></tr>
<tr><td>25534111_analysis_3</td><td>Initiating Joint attention minus responding to joint attention (IJA - IJAc) - (RJA - RJAc)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrasts isolate processes involved in representing and interpreting another&#x27;s attentional state (gaze cues, readiness to be guided), directly indexing perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25640962</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25640962/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25640962_analysis_0</td><td>IC&gt;NG</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast assesses perception and understanding of others&#x27; communicative intent (gestures, orientation) and engages mentalizing/action-perception processes, satisfying I1 and I2 for Perception and Understanding of Others.</td></tr>
<tr><td>25640962_analysis_1</td><td>ego&gt;allo</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task requires interpreting another&#x27;s communicative intent (body orientation, gestures) and thus involves perception/understanding of others (mentalizing/action perception).</td></tr>
<tr><td>25640962_analysis_2</td><td>allo&gt;ego</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires assessing the actor’s communicative intent using gestures and orientation (inferring others’ intent/mental state), meeting criteria for perception/understanding of others.</td></tr>
<tr><td>25640962_analysis_3</td><td>(IC-ego&gt;IC-allo)&gt;(NG-ego&gt;NG-allo)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast examines how actor orientation and gesture affect perceived communicative intent and involves mentalizing about the actor&#x27;s intent—measuring perception and understanding of others&#x27; mental states and actions.</td></tr>
<tr><td>25640962_analysis_4</td><td>(IC-allo&gt;NG-allo)&gt;(IC-ego&gt;NG-ego)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis involves inferring the speaker&#x27;s communicative intent from gestures and orientation (mentalizing/understanding others); contrast measures perception of others&#x27; social cues.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25716010</strong> | Pred included: 11 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25716010/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25716010_analysis_0</td><td>A. Fear&gt;happy+neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task explicitly measures perception and understanding of others&#x27; emotional states and interaction dynamics (animacy/action perception and emotion), and the contrast (fear&gt;happy+neutral) directly indexes these processes.</td></tr>
<tr><td>25716010_analysis_1</td><td>B. Fear+happy&gt;neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast probes perception and understanding of others’ emotional states and action dynamics (animacy and action perception), directly satisfying I1 and I2.</td></tr>
<tr><td>25716010_analysis_2</td><td>C. Fear+neutral&gt;happy</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis examines perception and understanding of others’ emotions, action dynamics, and animacy/interaction cues (e.g., EBA, STS responses), meeting criteria for perception of others.</td></tr>
<tr><td>25716010_analysis_3</td><td>D. Happy&gt;fear+neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast assesses perception and understanding of others (emotion and interaction in crowds), including action/animacy and emotion perception, satisfying the inclusion criteria.</td></tr>
<tr><td>25716010_analysis_4</td><td>E. Neutral&gt;fear+happy</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis probes perception and understanding of others (animacy/action perception, group emotional states and interactions), meeting criteria for this construct.</td></tr>
<tr><td>25716010_analysis_5</td><td>Main effect of dynamics.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task requires perceiving others&#x27; emotions, action/dynamics, and animacy/interaction cues in crowds (fits action perception and understanding others), satisfying perception-of-others criteria.</td></tr>
<tr><td>25716010_analysis_6</td><td>A. Interactive fear&gt;individual fear</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast probes perception and understanding of others (animacy/action perception and emotional states across interactive vs individual crowds), directly matching perception/understanding of others.</td></tr>
<tr><td>25716010_analysis_7</td><td>B. Interactive fear&gt;individual fear+interactive happy&gt;individual happy</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis targets perception and understanding of others’ emotional states and actions (interactive vs individual crowds), matching perception-of-others criteria.</td></tr>
<tr><td>25716010_analysis_8</td><td>C. Interactive fear&gt;individual fear+interactive neutral&gt;individual neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis targets perception and understanding of others’ emotional states and action dynamics (animacy and action perception), directly satisfying Perception and Understanding of Others. Satisfies I1 and I2.</td></tr>
<tr><td>25716010_analysis_9</td><td>D. Interactive fear&gt;individual fear+individual neutral&gt;interactive neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast directly tests perception and understanding of others’ emotions and interactions (action/animacy perception and social inference), satisfying this construct.</td></tr>
<tr><td>25716010_analysis_10</td><td>E. Interactive neutral&gt;individual neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast directly assesses perception and understanding of others (action/animacy perception, social interactions). It satisfies I1 (task measures perception of others) and I2 (measures perception/understanding of others).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25911123</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=4</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25911123/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Decreasing LOA &gt;  Increasing LOA; others, High-why + High-how &gt; Low-Why + Low-How; others, Increasing LOA &gt; Decreasing LOA; others, Low-Why + Low-How &gt; High-why + High-how; others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25911123_analysis_0</td><td>Response to why &gt; how questions for all stimulus categories</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The why &gt; how contrast targets attributions about others’ mental states (emotions, intentions), directly measuring perception and understanding of others (including understanding mental states).</td></tr>
<tr><td>25911123_analysis_1</td><td>Stronger response to why &gt; how questions for social than for nonsocial stimuli</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The why &gt; how social contrasts explicitly measure perception and understanding of others (mental state attribution, emotion and action understanding), directly matching this construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25911123_1</td><td>Decreasing LOA &gt;  Increasing LOA; others</td><td>25911123_analysis_0</td><td>Response to why &gt; how questions for all stimulus categories</td><td>0.333</td><td>0.000</td><td>0.100</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>25911123_2</td><td>High-why + High-how &gt; Low-Why + Low-How; others</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>25911123_3</td><td>Increasing LOA &gt; Decreasing LOA; others</td><td>25911123_analysis_1</td><td>Stronger response to why &gt; how questions for social than for nonsocial stimuli</td><td>0.312</td><td>0.041</td><td>0.122</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>25911123_4</td><td>Low-Why + Low-How &gt; High-why + High-how; others</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25996424</strong> | Pred included: 6 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25996424/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25996424_analysis_0</td><td>Eye Contact &gt; Averted Gaze</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Study assesses perception and interpretation of others&#x27; gaze and attentional relations (action/mental-state related), meeting criteria for perception of others.</td></tr>
<tr><td>25996424_analysis_1</td><td>Averted Gaze &gt; Eye Contact</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis measures perception and understanding of others (gaze direction, attentional relations, action-related social cues) and fits action/mental-state understanding subcomponents.</td></tr>
<tr><td>25996424_analysis_2</td><td>Congruent Gaze Cues &gt; Incongruent Gaze Cues</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis examines perception and interpretation of others&#x27; gaze (gaze following, inferred attentional relations and communicative intent), directly addressing Perception and Understanding of Others.</td></tr>
<tr><td>25996424_analysis_3</td><td>Incongruent Gaze Cues &gt; Congruent Gaze Cues</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis targets perception and understanding of others (interpreting others&#x27; gaze, attentional relations, action perception and possibly intentional inference), satisfying inclusion criteria.</td></tr>
<tr><td>25996424_analysis_4</td><td>Eye Contact congruent &gt; Averted Gaze congruent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast assesses perception/interpretation of others’ gaze and attentional relations (action/mental-state–relevant social information), meeting the criteria for perception and understanding of others.</td></tr>
<tr><td>25996424_analysis_5</td><td>Averted Gaze incongruent &gt; Averted Gaze congruent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast indexes processing of others&#x27; gaze cues (congruent vs. incongruent) and observed attentional relations, directly reflecting perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26143208</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26143208/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26143208_analysis_0</td><td>Interaction: FAC-Angry × Control</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires perception and understanding of others (gender and emotional expressions of faces), meeting the criteria for this construct.</td></tr>
<tr><td>26143208_analysis_1</td><td>Interaction: FAC-Happy × Control</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis targets perception/understanding of others via facial stimuli (happy faces vs control), meeting criteria for perception of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26262561</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26262561/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26262561_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires perceiving and interpreting others’ gaze direction and political group membership; contrasts examine neural responses to others’ social signals (gaze) and in-group/out-group, matching Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26365506</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26365506/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26365506_analysis_0</td><td>Self-referential &gt; social</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the experiment includes social stimuli, this specific contrast focuses on self-referential &gt; social and therefore does not measure perception/understanding of others (fails I2).</td></tr>
<tr><td>26365506_analysis_1</td><td>Social &gt; self-referential</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast isolates social stimulus processing (images depicting humans) and yields activations in regions associated with perceiving others (bilateral TPJ, pSTS, EBA, FFA), so it measures perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26417673</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26417673/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26417673_analysis_0</td><td>Brain activation of the racial prejudice in disgust perception during the passive viewing task ((disgusted out-group-neutral out-group)-(disgusted in-group-neutral in-group))</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast explicitly measures representations of others’ emotional states (disgust) and group-based perception, matching perception and understanding of others.</td></tr>
<tr><td>26417673_analysis_1</td><td>Positive connectivity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Study explicitly measures representations of others&#x27; emotional states (disgust) and group membership (race); contrast targets perception and understanding of others.</td></tr>
<tr><td>26417673_analysis_2</td><td>Negative connectivity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis directly assesses representations of others&#x27; emotional states (disgust) modulated by group membership, meeting perception and understanding of others criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26481048</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26481048/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26481048_analysis_0</td><td>Compared to implicit baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The stimuli were trait adjectives representing agentic and communal dimensions—content used to form impressions of others—and the analyses probe neural representations of these social-trait categories, satisfying I1 (task measuring perception/understanding of others’ traits) and I2 (the contrast measures perception/understanding of others).</td></tr>
<tr><td>26481048_analysis_1</td><td>Compared to fixation task</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although trait words relate to social traits, the task asked for valence classification rather than judgments about others’ mental states or traits; it does not clearly measure Perception and Understanding of Others per the inclusion criteria.</td></tr>
<tr><td>26481048_analysis_2</td><td>Agency vs. Communion</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Processing of agentic and communal trait words engages mentalizing-related regions and concerns representations of others’ traits/intentions, meeting the criteria for Perception and Understanding of Others.</td></tr>
<tr><td>26481048_analysis_3</td><td>Communion vs. Agency</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Processing of trait words (agency/communion) pertains to representations of others’ traits/mental states and engaged mentalizing-related regions; the contrast measures perception/understanding of others&#x27; social attributes.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26505303</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26505303/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26505303_analysis_0</td><td>Words &gt; Faces</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The experiment requires assessing others&#x27; communicative intentions and action predictions; the Words&gt;Faces contrast engages regions linked to action perception and mental state understanding (e.g., pSTS, TPJ), satisfying I1 and I2.</td></tr>
<tr><td>26505303_analysis_1</td><td>Request &gt; Naming</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task explicitly probes representations about others’ intentions and predicted actions (ToM/action perception), so it measures Perception and Understanding of Others.</td></tr>
<tr><td>26505303_analysis_2</td><td>Request &gt; Naming</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast targets representations of others&#x27; intentions and action predictions (ToM/action-perception), satisfying perception and understanding of others.</td></tr>
<tr><td>26505303_analysis_3</td><td>Naming &gt; Request</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast examines perception and understanding of others&#x27; communicative intentions and action predictions (ToM and action-perception systems), matching the construct. Both inclusion criteria are met.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26567160</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26567160/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26567160_analysis_0</td><td>Attitude &gt; Age</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast probes representations and reasoning about others’ traits/mental states (trust/distrust), directly matching Perception and Understanding of Others.</td></tr>
<tr><td>26567160_analysis_1</td><td>Trust &gt; Age</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires forming impressions and mental-state inferences about others (trust/distrust), directly fitting Perception and Understanding of Others (understanding mental states).</td></tr>
<tr><td>26567160_analysis_2</td><td>Distrust &gt; Age</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires reasoning about others’ traits/intentions (trustworthiness/distrust) and engages mentalizing processes, directly matching Perception and Understanding of Others.</td></tr>
<tr><td>26567160_analysis_3</td><td>Trust</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task and contrasts probe perception and understanding of others (mentalizing, inferring intentions/trustworthiness), meeting I1 and I2.</td></tr>
<tr><td>26567160_analysis_4</td><td>Distrust</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task explicitly requires forming impressions and reasoning about others&#x27; intentions/traits (trust/distrust), directly indexing perception and understanding of others (mentalizing).</td></tr>
<tr><td>26567160_analysis_5</td><td>Precuneus Connectivity-Positive</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis probes perception and understanding of others (trustworthiness judgments, mentalizing network and PrC connectivity) directly, satisfying the criteria for Perception and Understanding of Others.</td></tr>
<tr><td>26567160_analysis_6</td><td>IFG Connectivity-Negative</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrasts and task explicitly assess perception and understanding of others’ mental states/traits (trustworthiness, mentalizing), meeting the criteria for this construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26892859</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26892859/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26892859_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires perceiving and interpreting posers&#x27; evaluative intent and mental states (mentalizing) and the analyses/conjunction highlight regions implicated in understanding others, satisfying the criteria.</td></tr>
<tr><td>26892859_analysis_1</td><td>analysis_1</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task requires assessing others&#x27; evaluative statements and likely engages understanding of others&#x27; mental states/emotions (mentalizing); contrasts measure perception/understanding of others — meets I1 and I2.</td></tr>
<tr><td>26892859_analysis_2</td><td>analysis_2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Task requires representing and interpreting others&#x27; evaluative statements and mental states (mentalizing), directly mapping onto perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27095057</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27095057/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27095057_analysis_0</td><td>Social-Reg &gt; Social-Look</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Social-Reg condition requires understanding the psychotherapist&#x27;s intentions and perspective-taking and shows activation in mentalizing regions (TPJ, dmPFC, precuneus); thus it measures perception/understanding of others.</td></tr>
<tr><td>27095057_analysis_1</td><td>(Reg &gt; Look)_Social ∩ (Reg &gt; Look)_Self</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific analysis is the conjunction of Social- and Self-Reg (common areas). It does not specifically isolate regions for understanding others (mental state attribution); the study’s separate interaction/conjunction analyses did identify Theory-of-Mind/social-specific regions, but this particular contrast does not, so it does not meet the inclusion criteria for Perception and Understanding of Others.</td></tr>
<tr><td>27095057_analysis_2</td><td>Distinct areas recruited by Social-Reg: (Reg &gt; Look)_Social &gt; (Reg &gt; Look)_Self ∩ (Reg &gt; Look)_social</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast reveals regions (dorsomedial PFC, TPJ, precuneus) associated with understanding others&#x27; mental states; the task explicitly measures perception/understanding of others via the psychotherapist-target interaction.</td></tr>
<tr><td>27095057_analysis_3</td><td>Distinct areas recruited by Self-Reg: (Reg &gt; Look)_Self &gt; (Reg &gt; Look)_social ∩ (Reg &gt; Look)_Self</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This analysis highlights Self-Reg-specific activations (Self-Reg &gt; Social-Reg) and therefore does not directly measure processes of perceiving or reasoning about others&#x27; mental states.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27622781</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27622781/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27622781_analysis_0</td><td>Working memory maintenance</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires monitoring/updating information about a competitor and shows activation in mentalizing regions (mPFC, PCC, STS), directly indexing perception/understanding of others (social mentalizing).</td></tr>
<tr><td>27622781_analysis_1</td><td>Effect of performance parametric modulator on no competition</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Perception/understanding of others requires contrasts involving others (e.g., competition or social comparison). The current analysis is of the no-competition parametric modulator and does not measure perception of others.</td></tr>
<tr><td>27622781_analysis_2</td><td>Effect of performance parametric modulator on competition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis assesses mentalizing and updating information about a competitor (PCC/precuneus, STS activity) and contrasts competition vs no-competition, directly addressing perception/understanding of others.</td></tr>
<tr><td>27622781_analysis_3</td><td>Event‐Related Analysis at Feedback</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants track and compare an opponent’s performance; contrasts at feedback show activation in regions implicated in representing others (PCC/precuneus, STS), indicating measurement of perception/understanding of others.</td></tr>
<tr><td>27622781_analysis_4</td><td>Competition&gt;No competition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires monitoring and updating information about a competitor (other), and the authors interpret PCC/precuneus and STS activation as mentalizing about others, satisfying criteria for perception/understanding of others.</td></tr>
<tr><td>27622781_analysis_5</td><td>No Competition&gt;Competition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires representing and updating information about an opponent (mentalizing, PCC/STS engagement), directly probing perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 28477977</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/28477977/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>28477977_analysis_0</td><td>SELF minus OTHER [(SELF_No + SELF_Single + SELF_ Group) - (OTHER_No + OTHER_Single + OTHER_Group)] (Fig. 3)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The specific contrast provided (SELF minus OTHER) isolates self-related activity rather than directly indexing perception/understanding of others; thus it does not meet the inclusion criteria for Perception and Understanding of Others.</td></tr>
<tr><td>28477977_analysis_1</td><td>Group minus No Laughter [(SELF_ Group - SELF_No) + (OTHER_ Group - OTHER_No)]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Group vs No Laughter probes perception/processing of others&#x27; emotional/auditory responses (laughter), directly targeting perception and understanding of others.</td></tr>
<tr><td>28477977_analysis_2</td><td>Single minus No Laughter [(SELF_ Single - SELF_No) + (OTHER_Single - OTHER_No)]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast measures perception of others&#x27; responses (laughter), indexing processing/representation of others&#x27; emotional/behavioral states and thus satisfies I1 and I2.</td></tr>
<tr><td>28477977_analysis_3</td><td>Group minus Single Laughter [(SELF_ Group - SELF_Single) + (OTHER_ Group - OTHER_Single)]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast reflects perception and processing of others&#x27; responses (laughter), directly indexing understanding/perception of others&#x27; behavior and affect.</td></tr>
<tr><td>28477977_analysis_4</td><td>[(SELF_ Group - SELF_No) - (OTHER_Group - OTHER_No)] (Fig. 4)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis examines processing of others&#x27; responses (laughter) and their interaction with self-related signals (auditory cortex and PPI with VS), indexing perception/understanding of others&#x27; responses.</td></tr>
<tr><td>28477977_analysis_5</td><td>[(SELF_ Single - SELF_No) - (OTHER_Single - OTHER_No)]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast involves processing of others&#x27; responses (laughter vs no laughter) and their interaction with self-relevance, tapping perception/understanding of others&#x27; reactions.</td></tr>
<tr><td>28477977_analysis_6</td><td>Physio-physiological interaction (PPI) seeded on the right auditory cortex and mPFC.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analyses include contrasts of listener responses and OTHER conditions and examine processing of others&#x27; responses (auditory cortex) and their integration, meeting perception/understanding of others criteria.</td></tr>
<tr><td>28477977_analysis_7</td><td>Physio-physiological interaction (PPI) analysis in the left auditory cortex and mPFC.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Listener responses (laughter) reflect others&#x27; behavior; auditory cortex seeds and contrasts (Group vs No laughter) index perception of others&#x27; responses, and the PPI examines integration of those signals with self-related mPFC activity.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 28905269</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/28905269/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>28905269_analysis_0</td><td>Regions with increased activation for monogamous men compared to non-monogamous men for the romantic &gt; neutral contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis involves perception and evaluation of others in romantic contexts (romantic &gt; neutral), which maps onto Perception and Understanding of Others. Both I1 (task measuring perception of others) and I2 (contrast measures perception/understanding of others) are satisfied.</td></tr>
<tr><td>28905269_analysis_1</td><td>Regions with increased activation for non-monogamous men compared to monogamous men for the romantic &gt; sexual contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Viewing and differentiating romantic versus sexual images of people taps perception and understanding of others’ social/affiliative states, satisfying the Perception/Understanding of Others criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29221830</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29221830/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29221830_analysis_0</td><td>Collaborative &gt; Arbitrary</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast specifically probes mentalizing/understanding others (authors hypothesize and find TPJ, mPFC, precuneus engagement), satisfying criteria for perception and understanding of others.</td></tr>
<tr><td>29221830_analysis_1</td><td>Individual &gt; Arbitrary</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast does not index processes for representing or reasoning about others (understanding mental states). The Collaborative &gt; Individual contrast would be more relevant to perception/understanding of others; Individual &gt; Arbitrary shows semantic retrieval effects instead.</td></tr>
<tr><td>29221830_analysis_2</td><td>Collaborative &gt; Individual</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast evokes theory-of-mind/understanding of others (right TPJ, mPFC, precuneus) and directly compares collaborative (other-involving) vs individual encoding, meeting criteria for perception/understanding of others.</td></tr>
<tr><td>29221830_analysis_3</td><td>Arbitrary &gt; Individual</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Arbitrary &gt; Individual contrast does not target understanding others or mentalizing. The Collaborative &gt; Individual contrast is the one that implicates theory-of-mind and perception of others; this contrast does not, so it should be excluded.</td></tr>
<tr><td>29221830_analysis_4</td><td>Individual &gt; Collaborative</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast and study explicitly probe understanding of others (theory of mind/mentalizing) and show activation in TPJ, medial PFC and precuneus, satisfying criteria for perception/understanding of others.</td></tr>
<tr><td>29221830_analysis_5</td><td>Arbitrary &gt; Collaborative</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study and contrasts explicitly probe understanding others (theory of mind/mentalizing) and social cognition (e.g., medial PFC, TPJ, precuneus), so it measures perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29265483</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=3</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29265483/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Affective &gt; Cooperative social interactions, Cooperative &amp; Affective social interactions (Conjunction analysis), Cooperative &gt; Affective social interactions</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29265483_analysis_0</td><td>1a. Observation of social interactions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Study measures perception and understanding of others (action perception, inferring mental states via affectivity/cooperativity), directly matching this construct.</td></tr>
<tr><td>29265483_analysis_1</td><td>1b. Cooperativity&gt;affectivity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Study explicitly targets understanding others&#x27; actions, intentions, and mental states (action perception and understanding mental states), matching this annotation.</td></tr>
<tr><td>29265483_analysis_2</td><td>1c. Affectivity&gt;cooperativity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The study explicitly investigates perception and understanding of others’ intentions, affectivity, and cooperativity (mentalizing/action perception), so it meets both I1 and I2.</td></tr>
<tr><td>29265483_analysis_3</td><td>analysis_3</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Primary aim is understanding others&#x27; actions, intentions and mental states (pSTS, vmPFC, mirror/mentalizing systems) — directly matches I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29265483_1</td><td>Affective &gt; Cooperative social interactions</td><td>29265483_analysis_2</td><td>1c. Affectivity&gt;cooperativity</td><td>0.622</td><td>0.222</td><td>0.342</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>29265483_2</td><td>Cooperative &amp; Affective social interactions (Conjunction analysis)</td><td>29265483_analysis_0</td><td>1a. Observation of social interactions</td><td>0.538</td><td>0.351</td><td>0.407</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>29265483_3</td><td>Cooperative &gt; Affective social interactions</td><td>29265483_analysis_1</td><td>1b. Cooperativity&gt;affectivity</td><td>0.622</td><td>0.122</td><td>0.272</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29432769</strong> | Pred included: 8 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=1, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29432769/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29432769_analysis_0</td><td>Self- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast is Self vs Celebrity, which focuses on self-related processing; although it involves a celebrity as the comparator, the analysis is not primarily a measure of Perception and Understanding of Others (the friend-related contrasts would be more directly relevant).</td></tr>
<tr><td>29432769_analysis_1</td><td>Friend- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Friend- vs. celebrity contrast measures perception and understanding of others (representations of a close other versus a non-close other), matching the inclusion criteria for understanding others.</td></tr>
<tr><td>29432769_analysis_2</td><td>Modulation effect of Interdependence</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The friend- vs. celebrity contrast probes perception and understanding of a close other, matching the Perception and Understanding of Others inclusion criteria.</td></tr>
<tr><td>29432769_analysis_3</td><td>Self- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific analysis (Self vs. Celebrity) does not index perception/understanding of others; the relevant friend vs. celebrity contrast would, but it is not the analysis under consideration.</td></tr>
<tr><td>29432769_analysis_4</td><td>Friend- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Friend vs. celebrity judgment directly measures representations and judgments about others’ traits/states, satisfying I1 (others-related task) and I2 (measures Perception and Understanding of Others).</td></tr>
<tr><td>29432769_analysis_5</td><td>Functional connectivity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The friend vs. celebrity contrast measures perception and understanding of others (representation of a close other vs a celebrity).</td></tr>
<tr><td>29432769_analysis_6</td><td>Self- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Friend vs. Celebrity contrast measures perception and understanding of others (judging a friend&#x27;s social attributes), satisfying this construct.</td></tr>
<tr><td>29432769_analysis_7</td><td>Friend- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Friend vs. Celebrity requires representing and judging another person&#x27;s attributes (perception/understanding of others), satisfying the inclusion criteria.</td></tr>
<tr><td>29432769_analysis_8</td><td>Modulation effect of Interdependence</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Friend vs. celebrity contrast measures perception and understanding of another person (close other), matching this construct.</td></tr>
<tr><td>29432769_analysis_9</td><td>Self- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The specific contrast is Self vs Celebrity, which primarily isolates self-related processing rather than characterizing perception/understanding of others (a Friend vs Celebrity or Other vs Celebrity contrast would better match this annotation).</td></tr>
<tr><td>29432769_analysis_10</td><td>Friend- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Friend vs. Celebrity judgments directly probe representations and judgments about others&#x27; traits/attributes, matching Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29432769_1</td><td>Friend vs Celebrity; others</td><td>29432769_analysis_1</td><td>Friend- vs. Celebrity-judgments</td><td>0.760</td><td>0.500</td><td>0.578</td><td>uncertain</td><td>coord_count_mismatch</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29723244</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29723244/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Social vs Non-Social; others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29723244_analysis_0</td><td>“Social” brain regions identified in the NeuroSynth meta-analysis.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The contrast explicitly tests responses to images of people and social interactions versus non-social scenes, indexing perception of others (animacy/action/social scenes); thus it satisfies criteria for Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29723244_1</td><td>Social vs Non-Social; others</td><td>29723244_analysis_0</td><td>“Social” brain regions identified in the NeuroSynth meta-analysis.</td><td>0.362</td><td>0.034</td><td>0.133</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29740753</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29740753/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29740753_analysis_0</td><td>Main effect of modality</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants judged others’ emotional states from facial and vocal cues, satisfying perception and understanding of others.</td></tr>
<tr><td>29740753_analysis_1</td><td>Main effect of emotion</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis assesses representations of others&#x27; emotional states (emotion recognition from face/voice), directly matching perception and understanding of others.</td></tr>
<tr><td>29740753_analysis_2</td><td>3 (Emotion)×3 (Modality) interaction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task requires representing and judging others’ emotional states (anger, happiness, neutral) across modalities; the analysis measures perception and understanding of others’ emotions.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 30056560</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/30056560/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>30056560_analysis_0</td><td>Happy-own&gt;neutral-own</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis probes neural representations of the child’s emotional state (happy vs neutral) and shows engagement of mentalizing and empathy networks, directly assessing Perception and Understanding of Others.</td></tr>
<tr><td>30056560_analysis_1</td><td>ELM: happy-own&gt;neutral-own masked with group×(happy-own&gt;neutral-own)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast probes recognition and neural processing of another’s (the child’s) emotional states and engages mentalizing/perception-of-others networks (STG, precuneus), so it meets criteria for Perception and Understanding of Others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 30272134</strong> | Pred included: 3 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/30272134/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>30272134_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis measures perception and understanding of others&#x27; emotional states (faces and voices, congruent/incongruent), satisfying this construct.</td></tr>
<tr><td>30272134_analysis_1</td><td>(BIMODAL &gt; UNIMODAL FACES) ∩ (BIMODAL &gt; UNIMODAL VOICES)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis measures representations and reasoning about others’ emotional states (faces and voices), meeting perception/understanding of others criteria.</td></tr>
<tr><td>30272134_analysis_2</td><td>INCONGRUENT &gt; CONGRUENT</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast assesses representation and evaluation of others&#x27; emotional states (congruency/incongruency of face and voice), matching perception and understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>
</details></section><section id="bucket-false-negative"><details class="bucket" open><summary><h2>False Negative (5)</h2></summary><p><strong>Match status totals:</strong> accepted=6 | uncertain=0 | unmatched=4</p>
<details class="doc-card">
  <summary><strong>PMID 16406606</strong> | Pred included: 0 | Manual included (accepted matches only): 1 | Correct overlaps: 0 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16406606/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16406606_analysis_0</td><td>Active conditions</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although contrasts involve non-self versus self, the paradigm probes agency/self-recognition rather than representation or reasoning about others&#x27; mental states or social attributes; does not meet criteria for Perception and Understanding of Others.</td></tr>
<tr><td>16406606_analysis_1</td><td>Passive conditions</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although the task contrasts self vs non-self feedback, it primarily assesses self-agency detection rather than representations about others’ states or actions; does not satisfy the Perception/Understanding of Others criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>16406606_1</td><td>Active conditions &gt; passive conditions; others</td><td>16406606_analysis_1</td><td>Passive conditions</td><td>0.643</td><td>1.000</td><td>0.893</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20188182</strong> | Pred included: 0 | Manual included (accepted matches only): 1 | Correct overlaps: 0 | Match statuses: accepted=1, uncertain=0, unmatched=4</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20188182/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> loved one &gt; self; affiliation, loved one &gt; stranger; affiliation, stranger &gt; loved one; others, stranger &gt; self; others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20188182_analysis_0</td><td>Brain regions showing a significant effect of pain.</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although the task involves imagining others, the specific analysis is a global Pain vs Neutral effect across perspectives and does not directly test perception/understanding of others (e.g., Loved-one vs Stranger). Therefore it does not meet the specified inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>20188182_1</td><td>loved one &gt; self; affiliation</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>20188182_2</td><td>loved one &gt; stranger; affiliation</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>20188182_3</td><td>pain &gt; no pain; others</td><td>20188182_analysis_0</td><td>Brain regions showing a significant effect of pain.</td><td>0.369</td><td>1.000</td><td>0.811</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>20188182_6</td><td>stranger &gt; loved one; others</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>20188182_7</td><td>stranger &gt; self; others</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21569855</strong> | Pred included: 0 | Manual included (accepted matches only): 1 | Correct overlaps: 0 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21569855/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21569855_analysis_0</td><td>Incongruent &gt; Congruent</td><td class="decision-cell"><span class="decision-pill decision-none">?</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td>manual+ (accepted)</td><td></td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>21569855_1</td><td>Incongruent &gt; Congruent; others</td><td>21569855_analysis_0</td><td>Incongruent &gt; Congruent</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21664277</strong> | Pred included: 0 | Manual included (accepted matches only): 2 | Correct overlaps: 0 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21664277/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21664277_analysis_0</td><td>a Social alliance vs. ‘number equal’ (see Fig. 3)</td><td class="decision-cell"><span class="decision-pill decision-none">?</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td>manual+ (accepted)</td><td></td></tr>
<tr><td>21664277_analysis_1</td><td>b Social hierarchy vs. fame, age, gender and ‘no. higher / lower’ (see Fig. 4)</td><td class="decision-cell"><span class="decision-pill decision-none">?</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td>manual+ (accepted)</td><td></td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>21664277_1</td><td>Social alliance vs. ‘number equal’; affiliation</td><td>21664277_analysis_0</td><td>a Social alliance vs. ‘number equal’ (see Fig. 3)</td><td>0.819</td><td>1.000</td><td>0.946</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>21664277_2</td><td>Social hierarchy vs. fame, age, gender and ‘no. higher / lower’; affiliation</td><td>21664277_analysis_1</td><td>b Social hierarchy vs. fame, age, gender and ‘no. higher / lower’ (see Fig. 4)</td><td>0.894</td><td>1.000</td><td>0.968</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27090501</strong> | Pred included: 0 | Manual included (accepted matches only): 1 | Correct overlaps: 0 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27090501/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27090501_analysis_0</td><td>Significant activation for the (toss - button press) high-frequency effect.</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast focuses on social reward and self-related responses to increased inclusion rather than representations or inferences about others’ mental states or actions; it does not specifically measure perception/understanding of others.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27090501_1</td><td>Significant activation for the (toss – button press) high-frequency effect; others</td><td>27090501_analysis_0</td><td>Significant activation for the (toss - button press) high-frequency effect.</td><td>0.980</td><td>1.000</td><td>0.994</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>
</details></section>
  
</body>
</html>
