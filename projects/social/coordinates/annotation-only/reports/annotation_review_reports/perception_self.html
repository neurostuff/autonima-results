<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>perception_self review report</title>
  <style>
    :root {
      --bg: #f7f6f2;
      --panel: #ffffff;
      --ink: #1d2730;
      --line: #d8dde3;
    }
    body { margin: 0; padding: 1.25rem; font-family: "IBM Plex Sans", "Segoe UI", sans-serif; background: var(--bg); color: var(--ink); }
    header { background: var(--panel); border: 1px solid var(--line); border-radius: 10px; padding: 1rem; margin-bottom: 1rem; }
    .top-nav { position: sticky; top: 0; z-index: 10; display: flex; flex-wrap: wrap; gap: 0.5rem; background: #eef3f2; border: 1px solid var(--line); border-radius: 10px; padding: 0.6rem; margin-bottom: 1rem; }
    .top-nav a { display: inline-block; padding: 0.35rem 0.6rem; border: 1px solid var(--line); border-radius: 999px; background: #fff; text-decoration: none; font-size: 0.9rem; color: #0e4f85; }
    section { margin-bottom: 1rem; }
    .bucket > summary, .doc-card > summary, .inner-accordion > summary { cursor: pointer; }
    .doc-card { background: var(--panel); border: 1px solid var(--line); border-radius: 10px; padding: 0.85rem; margin-bottom: 0.85rem; }
    .table-wrap, .table-html { overflow-x: auto; }
    .inner-accordion { margin-top: 0.6rem; border-top: 1px dashed var(--line); padding-top: 0.4rem; }
    .paper-text { white-space: pre-wrap; max-height: 26rem; overflow-y: auto; background: #fbfcfe; border: 1px solid var(--line); border-radius: 8px; padding: 0.6rem; font-size: 0.88rem; line-height: 1.35; }
    table { width: 100%; border-collapse: collapse; font-size: 0.9rem; }
    th, td { border: 1px solid var(--line); padding: 0.45rem; vertical-align: top; text-align: left; }
    th { background: #edf2f5; }
    .decision-cell, .confusion-cell { text-align: center; vertical-align: middle; }
    .decision-pill, .confusion-pill {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      min-width: 1.55rem;
      padding: 0.12rem 0.45rem;
      border-radius: 999px;
      font-weight: 700;
      font-size: 0.82rem;
      border: 1px solid transparent;
    }
    .decision-include { background: #e9f8ef; color: #1f7a3d; border-color: #b7e4c6; }
    .decision-exclude { background: #fdecec; color: #9b1c1c; border-color: #f6caca; }
    .decision-none { background: #f2f4f7; color: #5b6775; border-color: #dde3ea; }
    .confusion-good { background: #e9f8ef; color: #166534; border-color: #b7e4c6; }
    .confusion-bad { background: #fdecec; color: #991b1b; border-color: #f6caca; }
    .confusion-na { background: #f2f4f7; color: #5b6775; border-color: #dde3ea; }
    a { color: #0e4f85; }
  </style>
</head>
<body>
  <header>
    <a id="top"></a>
    <h1>perception_self report</h1>
    <p>Manual benchmark is sliced to the auto PMID universe from <code>outputs/nimads_annotation.json</code>. Analysis-level truth uses accepted fuzzy matches only.</p>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Level</th>
            <th>TP</th>
            <th>FP</th>
            <th>FN</th>
            <th>Precision</th>
            <th>Recall</th>
            <th>F1</th>
            <th>Manual Positives</th>
            <th>Predicted Positives</th>
            <th>Universe</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Document bucket overlap</td>
            <td>29</td>
            <td>16</td>
            <td>14</td>
            <td>0.644</td>
            <td>0.674</td>
            <td>0.659</td>
            <td>43</td>
            <td>45</td>
            <td>59</td>
          </tr>
          <tr>
            <td>Study inclusion</td>
            <td>32</td>
            <td>15</td>
            <td>12</td>
            <td>0.681</td>
            <td>0.727</td>
            <td>0.703</td>
            <td>44</td>
            <td>47</td>
            <td>134</td>
          </tr>
          <tr>
            <td>Analysis inclusion (accepted matches only)</td>
            <td>62</td>
            <td>107</td>
            <td>61</td>
            <td>0.367</td>
            <td>0.504</td>
            <td>0.425</td>
            <td>123</td>
            <td>169</td>
            <td>640</td>
          </tr>
        </tbody>
      </table>
    </div>
  </header>
  <nav class="top-nav">
    <a href="#bucket-correct">Correct (29)</a>
    <a href="#bucket-false-positive">False Positive (16)</a>
    <a href="#bucket-false-negative">False Negative (14)</a>
    <a href="#missing-manual">Missing PMIDs (0)</a>
    <a href="#top">Top</a>
  </nav>
  <section id="bucket-correct"><details class="bucket"><summary><h2>Correct (29)</h2></summary><p><strong>Match status totals:</strong> accepted=72 | uncertain=3 | unmatched=11</p>
<details class="doc-card">
  <summary><strong>PMID 14568477</strong> | Pred included: 3 | Manual included (accepted matches only): 4 | Correct overlaps: 3 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/14568477/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>14568477_analysis_0</td><td>Internal &gt; external</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast probes self-responsibility and agency (internal attributions), directly assessing perception/understanding of the self; satisfies both I1 and I2.</td></tr>
<tr><td>14568477_analysis_1</td><td>External &gt; internal</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The specific contrast is External &gt; Internal, which emphasizes attribution to others/situation rather than self-perception/agency; therefore it does not measure perception/understanding of the self.</td></tr>
<tr><td>14568477_analysis_2</td><td>Self-serving bias</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis directly probes perception and understanding of the self (self-responsibility, agency, self-serving attributions), satisfying the self-related criteria.</td></tr>
<tr><td>14568477_analysis_3</td><td>Non-self-serving bias</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast includes internal attributions of negative events (self-blame) and internal vs external attributions, directly indexing perception/understanding of the self and agency.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>14568477_1</td><td>External &gt; internal; others</td><td>14568477_analysis_1</td><td>External &gt; internal</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>14568477_2</td><td>Internal &gt; external; others</td><td>14568477_analysis_0</td><td>Internal &gt; external</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>14568477_3</td><td>Non-self-serving bias &gt; Self-serving bias; others</td><td>14568477_analysis_3</td><td>Non-self-serving bias</td><td>0.677</td><td>1.000</td><td>0.903</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>14568477_4</td><td>Self-serving bias &gt; Non-self-serving bias; others</td><td>14568477_analysis_2</td><td>Self-serving bias</td><td>0.586</td><td>1.000</td><td>0.876</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 15528097</strong> | Pred included: 1 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=4</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/15528097/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Embarrassment &gt; Neutral; socialcommunication, Embarrassment &gt; guilt; socialcommunication, Guilt &gt; Embarrassment; socialcommunication, Guilt &gt; Neutral; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>15528097_analysis_0</td><td>Brain regions commonly activated by guilt and embarrassment conditions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants made first-person evaluative judgments about guilt/embarrassment (self-conscious emotions), engaging self-monitoring/awareness processes; thus it measures perception/understanding of self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>15528097_1</td><td>Embarrassment &gt; Neutral; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>15528097_2</td><td>Embarrassment &gt; guilt; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>15528097_3</td><td>G &gt; N masked E &gt; N; socialcommunication</td><td>15528097_analysis_0</td><td>Brain regions commonly activated by guilt and embarrassment conditions</td><td>0.275</td><td>1.000</td><td>0.783</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>15528097_4</td><td>Guilt &gt; Embarrassment; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>15528097_5</td><td>Guilt &gt; Neutral; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 15808992</strong> | Pred included: 2 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/15808992/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>15808992_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast explicitly targets perception and understanding of the self (self‑face recognition), satisfying the self‑perception construct.</td></tr>
<tr><td>15808992_analysis_1</td><td>other-self contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Self-face recognition directly probes perception and understanding of the self (self as object), satisfying criteria for self-related perception.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>15808992_2</td><td>Self - Other; socialcommunication</td><td>15808992_analysis_0</td><td>analysis_0</td><td>0.140</td><td>1.000</td><td>0.742</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16035037</strong> | Pred included: 3 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16035037/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16035037_analysis_0</td><td>CBF change during distracter minus null contrast</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast does not involve self-related processing (self-face vs familiar face contrasts would, but distracter vs null does not).</td></tr>
<tr><td>16035037_analysis_1</td><td>familiar minus distracter contrast</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast compares familiar other to distracter (unknown) faces and does not measure perception/understanding of the self.</td></tr>
<tr><td>16035037_analysis_2</td><td>Local maxima of CBF change during self minus distracter contrast corrected at P = 0.001 and cluster at P = 0.05</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Directly measures perception/understanding of the self via self-face recognition (self vs distracter), satisfying the self-perception criteria.</td></tr>
<tr><td>16035037_analysis_3</td><td>familiar minus self contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast directly assesses self-face recognition and thus perception/understanding of self.</td></tr>
<tr><td>16035037_analysis_4</td><td>Local maxima of CBF change during self minus familiar contrast</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis directly contrasts self-face against familiar-face, indexing processes of self-perception and self-recognition, satisfying the self-perception criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>16035037_4</td><td>self face &gt; distracter; socialcommunication</td><td>16035037_analysis_2</td><td>Local maxima of CBF change during self minus distracter contrast corrected at P = 0.001 and cluster at P = 0.05</td><td>0.312</td><td>1.000</td><td>0.794</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>16035037_5</td><td>self face &gt; famililar face; socialcommunication</td><td>16035037_analysis_4</td><td>Local maxima of CBF change during self minus familiar contrast</td><td>0.367</td><td>1.000</td><td>0.810</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16406606</strong> | Pred included: 2 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16406606/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16406606_analysis_0</td><td>Active conditions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Direct measure of agency/self-recognition: subjects judged whether cursor was feedback from their own movement; contrasts probe Perception and Understanding of Self (Agency).</td></tr>
<tr><td>16406606_analysis_1</td><td>Passive conditions</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast (passive asynchronous vs synchronous) probes perception/understanding of self, specifically agency and recognition of feedback from one’s own movement—meets I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>16406606_1</td><td>Active conditions &gt; passive conditions; others</td><td>16406606_analysis_1</td><td>Passive conditions</td><td>0.643</td><td>1.000</td><td>0.893</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18501639</strong> | Pred included: 13 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18501639/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18501639_analysis_0</td><td>Main effect [(Sf + Sn)–(Ff + Fn) masked by Sf–Ff and Sn–Fn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast explicitly compares self to friend (self-specific activation) across faces and names, directly measuring perception/understanding of self (self-recognition).</td></tr>
<tr><td>18501639_analysis_1</td><td>Face (simple effect) [Sf–Ff]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Directly tests perception/understanding of the self via self-face and self-name conditions and contrasts (Sf–Ff, Sn–Fn), satisfying self-related processing criteria.</td></tr>
<tr><td>18501639_analysis_2</td><td>Name (simple effect) [Sn–Fn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Sn–Fn contrast directly assesses perception/understanding of the self (self-name vs friend-name), meeting the criteria for self-related perception and judgment.</td></tr>
<tr><td>18501639_analysis_3</td><td>Face specific (interaction) [(Sf–Ff)–(Sn–Fn) masked by Sf–Ff]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrasts explicitly test self-recognition (self-face and self-name vs friend), directly measuring perception and understanding of self (self-knowledge/agency aspects).</td></tr>
<tr><td>18501639_analysis_4</td><td>Name specific (interaction) [(Sn–Fn)–(Sf–Ff) masked by Sn–Fn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Key contrasts explicitly test perception/recognition of the self (Sf, Sn) versus friend/unfamiliar, directly measuring self-related perception and self–other distinction.</td></tr>
<tr><td>18501639_analysis_5</td><td>Main effect [(Ff + Fn)–(Sf + Sn) masked by Ff–Sf and Fn–Sn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task explicitly includes self-face and self-name recognition and contrasts self vs friend, directly measuring processes of self-perception and self-related judgments.</td></tr>
<tr><td>18501639_analysis_6</td><td>Face (simple effect) [Ff–Sf]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task explicitly probes self-recognition (self-face/name) and the contrast involves the self condition (Ff–Sf), thus measuring perception/understanding of self.</td></tr>
<tr><td>18501639_analysis_7</td><td>Name (simple effect) [Fn–Sn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The primary contrasts explicitly test self-recognition (self-face/name vs friend), directly measuring perception and understanding of the self (self-specific activation), satisfying the inclusion criteria.</td></tr>
<tr><td>18501639_analysis_8</td><td>Face specific (interaction) [(Ff–Sf)–(Fn–Sn) masked by Ff–Sf]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrasts explicitly test self-recognition (self vs friend and self vs unfamiliar) for face and name stimuli, directly measuring perception and understanding of self.</td></tr>
<tr><td>18501639_analysis_9</td><td>Name specific (interaction) [(Fn–Sn)–(Ff–Sf) masked by Fn–Sn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The main contrasts test self-recognition (self vs friend, self-face specific activations), directly assessing perception and understanding of self.</td></tr>
<tr><td>18501639_analysis_10</td><td>Main effect [(Sf + Ff + Sn + Fn)–2(Cf + Cn) masked by Sf–Cf, Ff–Cf, Sn–Cn, and Fn–Cn]</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The specific contrast (S+F–2C) indexes familiarity common to self and friend relative to unfamiliar persons; it is not a self-specific contrast and does not isolate self-perception/understanding alone, so it does not meet the Perception/Self criterion.</td></tr>
<tr><td>18501639_analysis_11</td><td>Face (simple effect) [Sf + Ff–2Cf masked by Sf–Cf, and Ff–Cf]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Primary contrasts explicitly test self-recognition (self vs friend, self-face specific effects), directly measuring perception and understanding of self (self-knowledge/agency).</td></tr>
<tr><td>18501639_analysis_12</td><td>Name (simple effect) [Sn + Fn–2Cn masked by Sn–Cn, and Fn–Cn]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrasts explicitly test self-recognition (self-face, self-name) vs friend/unfamiliar, directly measuring perception and understanding of the self.</td></tr>
<tr><td>18501639_analysis_13</td><td>Face specific (interaction) [(Sf + Ff–2Cf)–(Sn + Fn–2Cn) masked by Sf + Ff–2Cf]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study explicitly probes self-recognition (self-face and self-name) and contrasts self versus friend, directly measuring perception and understanding of self.</td></tr>
<tr><td>18501639_analysis_14</td><td>Name specific (interaction) [(Sn + Fn–2Cn)–(Sf + Ff–2Cf) masked by Sn + Fn–2Cn]</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the study addresses self-recognition, this specific contrast pools self and friend (Sn+Fn-2Cn) and contrasts name- vs face-related familiarity; it does not isolate self-specific perception/understanding, so it does not satisfy the self-specific inclusion.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>18501639_3</td><td>Face (simple effect) [Sf–Ff]; socialcommunication</td><td>18501639_analysis_1</td><td>Face (simple effect) [Sf–Ff]</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>18501639_5</td><td>Face specific (interaction) [(Sf–Ff)–(Sn–Fn) masked by Sf–Ff]; socialcommunication</td><td>18501639_analysis_3</td><td>Face specific (interaction) [(Sf–Ff)–(Sn–Fn) masked by Sf–Ff]</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18514546</strong> | Pred included: 4 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18514546/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18514546_analysis_0</td><td>SELF versus high-level baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The SELF-task explicitly requires attending to and judging one&#x27;s own emotional response (self-awareness of internal emotional state), so the contrast measures perception/understanding of self.</td></tr>
<tr><td>18514546_analysis_1</td><td>OTHER versus high-level baseline</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The specific contrast is OTHER vs baseline (evaluating others), so it does not measure perception/understanding of the self (SELF-task would).</td></tr>
<tr><td>18514546_analysis_2</td><td>analysis_2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>SELF-task explicitly asks subjects to report their own emotional response, meeting perception/understanding of self.</td></tr>
<tr><td>18514546_analysis_3</td><td>Interaction (SELF_f – B_f) – (SELF_m – B_m)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast focuses on the SELF-task (subjects report their own emotional response), directly measuring perception/understanding of self (self-awareness of emotional state).</td></tr>
<tr><td>18514546_analysis_4</td><td>Interaction (SELF_m – B_m) – (SELF_f – B_f)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The SELF-task explicitly requires subjects to report their own emotional response, directly measuring perception and understanding of the self (internal emotional state).</td></tr>
<tr><td>18514546_analysis_5</td><td>Interaction (OTHER_f – B_f) – (OTHER_m – B_m)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast is for OTHER (others&#x27; emotions) and a gender interaction; it does not measure self-perception or self-judgments (SELF task would), so it does not meet perception/understanding of self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>18514546_2</td><td>Self &gt; high-level baseline; socialcommunication</td><td>18514546_analysis_0</td><td>SELF versus high-level baseline</td><td>0.877</td><td>1.000</td><td>0.963</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 19107754</strong> | Pred included: 6 | Manual included (accepted matches only): 5 | Correct overlaps: 5 | Match statuses: accepted=5, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19107754/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19107754_analysis_0</td><td>Individualism (IND) &gt; Collectivism (COL)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis directly measures perception and understanding of the self via general and contextual self-judgments and relates neural responses to self-construal scores (satisfies both inclusion criteria).</td></tr>
<tr><td>19107754_analysis_1</td><td>Collectivism (COL) &gt; Individualism (IND)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis directly measures perception and understanding of the self (general vs contextual self-knowledge) and examines neural correlates of self-construal, satisfying the Perception and Understanding of Self criteria.</td></tr>
<tr><td>19107754_analysis_2</td><td>General &gt; Contextual</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrasts explicitly measure perception and understanding of the self (general vs contextual self-judgments) and use self-construal as a covariate, directly matching this construct.</td></tr>
<tr><td>19107754_analysis_3</td><td>Contextual &gt; General</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis directly measures perception and understanding of self (general vs contextual self-knowledge) and examines neural correlates of self-knowledge and self-construal. Satisfies I1 and I2.</td></tr>
<tr><td>19107754_analysis_4</td><td>fMRI results of interaction contrast [(INDgeneral + COLcontextual) - (INDcontextual + COLgeneral)]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Directly measures perception and understanding of the self (general vs contextual self-judgments) and examines neural correlates of self-knowledge modulated by self-construal—meets criteria.</td></tr>
<tr><td>19107754_analysis_5</td><td>Table V. fMRI results of whole-brain correlation analysis of self-judgment contrast image [contextual-general] with self-construal style index [COL score-IND score]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The experiment directly measures perception and understanding of the self (general vs contextual self-judgments) and the contrast assesses self-related processing as a function of self-construal, satisfying I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>19107754_1</td><td>Collectivism &gt; Individualism; self</td><td>19107754_analysis_1</td><td>Collectivism (COL) &gt; Individualism (IND)</td><td>0.824</td><td>1.000</td><td>0.947</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>19107754_2</td><td>Contextual &gt; General; self</td><td>19107754_analysis_3</td><td>Contextual &gt; General</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>19107754_3</td><td>General &gt; Contextual; self</td><td>19107754_analysis_2</td><td>General &gt; Contextual</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>19107754_4</td><td>Individualism &gt; Collectivism; self</td><td>19107754_analysis_0</td><td>Individualism (IND) &gt; Collectivism (COL)</td><td>0.824</td><td>1.000</td><td>0.947</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>19107754_5</td><td>Self Judgement ( Contextual &gt; General) &gt; Self-Construal Style Index ( Collectivism &gt; Individualism); self</td><td>19107754_analysis_5</td><td>Table V. fMRI results of whole-brain correlation analysis of self-judgment contrast image [contextual-general] with self-construal style index [COL score-IND score]</td><td>0.520</td><td>1.000</td><td>0.856</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20045478</strong> | Pred included: 3 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20045478/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20045478_analysis_0</td><td>Self &gt; syllables</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The Self &gt; syllables contrast directly measures perception/understanding of the self (trait judgments about oneself) and satisfies both inclusion criteria.</td></tr>
<tr><td>20045478_analysis_1</td><td>Other &gt; syllables</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific contrast (Other &gt; syllables) targets judgments about another person, not self-related processing, so it does not meet Perception and Understanding of Self criteria.</td></tr>
<tr><td>20045478_analysis_2</td><td>Self &gt; other</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast explicitly probes self-referential processing (trait judgments about oneself) and modulation by self-descriptiveness and memory — directly meets Perception and Understanding of Self.</td></tr>
<tr><td>20045478_analysis_3</td><td>Other &gt; self</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis involves self-referential processing (self trait judgments and contrasts with other) and parametrically modulates self-relevance—meets perception/understanding of self criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>20045478_3</td><td>Self &gt; other; self</td><td>20045478_analysis_2</td><td>Self &gt; other</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>20045478_4</td><td>Self &gt; syllables; self</td><td>20045478_analysis_0</td><td>Self &gt; syllables</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23063451</strong> | Pred included: 5 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23063451/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23063451_analysis_0</td><td>Self &gt; Other</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The Self condition explicitly asks subjects to imagine themselves suffering (first-person perspective); the Self&gt;Other contrast directly addresses self-related perception/agency.</td></tr>
<tr><td>23063451_analysis_1</td><td>Other &gt; Self</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The reported contrast is Other &gt; Self and primarily targets understanding others; although Self trials exist, this analysis focuses on other-related processing and therefore does not primarily index self-perception.</td></tr>
<tr><td>23063451_analysis_2</td><td>OtherGood &gt; OtherBad performer</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast specifically targets Other perspective success (OtherGood vs OtherBad), not self-related perception or self-knowledge/agency, so it does not meet Perception and Understanding of Self.</td></tr>
<tr><td>23063451_analysis_3</td><td>OtherBad &gt; OtherGood performer</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Includes a Self perspective condition and contrasts SelfGood vs SelfBad, with self-evaluations—directly measures perception/understanding of self (agency/self-monitoring).</td></tr>
<tr><td>23063451_analysis_4</td><td>SelfGood &gt; SelfBad performer</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The SelfGood &gt; SelfBad contrast directly probes successful vs unsuccessful adoption of a first-person perspective and self-related evaluative processes, satisfying perception/understanding of self.</td></tr>
<tr><td>23063451_analysis_5</td><td>SelfBad &gt; SelfGood performer</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The SelfBad &gt; SelfGood contrast specifically indexes participants&#x27; ability to adopt a first-person perspective and self-related empathic experience (self-awareness/agency aspects), meeting criteria for perception and understanding of self.</td></tr>
<tr><td>23063451_analysis_6</td><td>analysis_6</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The Self condition explicitly requires adopting one&#x27;s own perspective and distinguishes self-related processing (self/other distinction, agency), satisfying self-perception criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>23063451_1</td><td>Conjunction of Other and Self; others</td><td>23063451_analysis_6</td><td>analysis_6</td><td>0.170</td><td>1.000</td><td>0.751</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>23063451_3</td><td>Self &gt; Other; self</td><td>23063451_analysis_0</td><td>Self &gt; Other</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24294841</strong> | Pred included: 5 | Manual included (accepted matches only): 3 | Correct overlaps: 2 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24294841/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24294841_analysis_0</td><td>Main effect (ES + TS + SRS) - (EN + TN + SRN)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although the experimental design includes a self-reference condition, this specific analysis is the main effect of emotional context across all conditions (sad vs neutral pooled) and does not specifically isolate perception/understanding of the self.</td></tr>
<tr><td>24294841_analysis_1</td><td>EmpathyES-EN</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The self-reference condition explicitly asks participants to imagine how they would feel (self-awareness/self-related judgments), satisfying self-perception criteria.</td></tr>
<tr><td>24294841_analysis_2</td><td>ToMTS-TN</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contains a self-reference condition requiring participants to imagine how they would feel (explicitly measures perception/understanding of self), satisfying both criteria.</td></tr>
<tr><td>24294841_analysis_3</td><td>Self-referenceSRS-SRN</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The self-reference condition requires imagining how one would feel and engages self-reflection/self-knowledge, matching perception and understanding of self.</td></tr>
<tr><td>24294841_analysis_4</td><td>Empathy &gt; ToM(ES + EN)-(TS + TN)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific contrast (Empathy &gt; ToM) targets understanding others’ emotions/mental states rather than self-perception or self-knowledge; thus it does not meet Perception of Self criteria.</td></tr>
<tr><td>24294841_analysis_5</td><td>ToM &gt; Empathy(TS + TN)-(ES + EN)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast ToM &gt; Empathy targets understanding others’ mental states rather than self-perception or self-referential processing (self-reference was a separate condition but not in this contrast).</td></tr>
<tr><td>24294841_analysis_6</td><td>Empathy &gt; Self-reference(ES + EN)-(SRS + SRN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast involves self-reference condition (imagining how one would feel) and directly probes self-related processing/self–other differentiation.</td></tr>
<tr><td>24294841_analysis_7</td><td>Self-reference &gt; Empathy(SRS + SRN)-(ES + EN)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Self-reference condition directly measures self-related processing (imagining how one would feel) and the contrast contrasts self vs other processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24294841_3</td><td>Main effect (ES + TS + SRS) – (EN + TN + SRN); others</td><td>24294841_analysis_0</td><td>Main effect (ES + TS + SRS) - (EN + TN + SRN)</td><td>0.978</td><td>1.000</td><td>0.993</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24294841_4</td><td>Self-reference &gt;  Empathy; self</td><td>24294841_analysis_7</td><td>Self-reference &gt; Empathy(SRS + SRN)-(ES + EN)</td><td>0.720</td><td>1.000</td><td>0.916</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24294841_5</td><td>Self-reference SRS–SRN; self</td><td>24294841_analysis_3</td><td>Self-referenceSRS-SRN</td><td>0.930</td><td>1.000</td><td>0.979</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24294906</strong> | Pred included: 2 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24294906/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24294906_analysis_0</td><td>Main effect of social value orientation on decision-making</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrasts and interpretation explicitly discuss self-referencing and agency (precuneus activation) and decisions about self-relevant outcomes, satisfying constructs related to perception/understanding of self.</td></tr>
<tr><td>24294906_analysis_1</td><td>Interaction effect between dispositional trust and social value orientation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analyses involve self-referencing/agency and comparisons of self-related decision processes (precuneus activation, self-referencing discussed), meeting criteria for Perception and Understanding of Self (I1 and I2).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24294906_1</td><td>Interaction effect between dispositional trust and social value orientation; others</td><td>24294906_analysis_1</td><td>Interaction effect between dispositional trust and social value orientation</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24294906_2</td><td>Main effect of social value orientation on decision-making; others</td><td>24294906_analysis_0</td><td>Main effect of social value orientation on decision-making</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24726338</strong> | Pred included: 4 | Manual included (accepted matches only): 4 | Correct overlaps: 2 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24726338/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24726338_analysis_0</td><td>Self &gt; other</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The Self &gt; Other contrast explicitly measures perception and understanding of the self (self-referential processing in mPFC), satisfying both inclusion criteria.</td></tr>
<tr><td>24726338_analysis_1</td><td>Other &gt; self</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis examines self-referential processing (mPFC response to self vs other moral statements), directly probing perception and understanding of the self.</td></tr>
<tr><td>24726338_analysis_2</td><td>Communication &gt; moral (collapsing across self/other)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although the study contains self-referential measures, this specific contrast (&#x27;communication &gt; moral, collapsing across self/other&#x27;) collapses across self vs other and therefore does not specifically target self-perception/understanding.</td></tr>
<tr><td>24726338_analysis_3</td><td>Moral &gt; communication (collapsing across self/other)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast collapses across self/other and therefore does not specifically index self-perception or self-referential processing (it does not isolate self &gt; other), so it does not meet the self-perception criterion.</td></tr>
<tr><td>24726338_analysis_4</td><td>Self &gt; other (collapsing across moral/communication)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The key contrast is Self &gt; Other and the ROI was defined from a self &gt; other localizer; this directly measures perception and understanding of the self (self-referential processing) in mPFC.</td></tr>
<tr><td>24726338_analysis_5</td><td>Other &gt; self (collapsing across moral/communication)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis directly contrasts self- vs other-referential processing (self-related moral statements) and uses an mPFC self-localizer, meeting self-perception criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24726338_1</td><td>Communication &gt; moral (collapsing across self/other); others</td><td>24726338_analysis_2</td><td>Communication &gt; moral (collapsing across self/other)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24726338_2</td><td>Moral &gt; communication (collapsing across self/other); others</td><td>24726338_analysis_3</td><td>Moral &gt; communication (collapsing across self/other)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24726338_5</td><td>Self &gt; other; self</td><td>24726338_analysis_0</td><td>Self &gt; other</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24726338_6</td><td>Self &gt; other (collapsing across moral/communication); self</td><td>24726338_analysis_4</td><td>Self &gt; other (collapsing across moral/communication)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24772075</strong> | Pred included: 7 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24772075/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24772075_analysis_0</td><td>MAIN EFFECT: FEEDBACK ONSET: SELF &gt; OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrast and analyses explicitly probe self-related perception/understanding (self feedback, self-related MPFC activity), fulfilling self-related criteria.</td></tr>
<tr><td>24772075_analysis_1</td><td>MAIN EFFECT: FEEDBACK ONSET: OTHER &gt; SELF</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis explicitly measures judgments about the self (self-ratings, updates, self-related neural activity), satisfying self-perception criteria.</td></tr>
<tr><td>24772075_analysis_2</td><td>INTERACTION: FEEDBACK ONSET: (SELF &gt; OTHER) × (GERMAN &gt; CHINESE)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis contrasts self vs. other feedback and examines self-related ratings and MPFC activity—directly measures perception and understanding of the self.</td></tr>
<tr><td>24772075_analysis_3</td><td>WHOLE-BRAIN CORRELATION WITH OVERALL RELATIVE ABSOLUTE MEAN UPDATES IN THE CONTRAST FEEDBACK ONSET: SELF &gt; OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast and analysis focus on self-related feedback (self &gt; other) and neural correlates of self-evaluation and updating, meeting perception/understanding of self criteria.</td></tr>
<tr><td>24772075_analysis_4</td><td>FEEDBACK RATING (TRIAL-BY-TRIAL CORRELATION): SELF &gt; OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis explicitly probes self-perception and self-knowledge via self-ratings and neural responses to self-related feedback.</td></tr>
<tr><td>24772075_analysis_5</td><td>FEEDBACK DISCREPANCIES (POSITIVE TRIAL-BY-TRIAL CORRELATION): SELF AND OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis directly measures self-perception and self-related updating (self-ratings before/after feedback) and self-related neural activity (MPFC), satisfying self-perception criteria.</td></tr>
<tr><td>24772075_analysis_6</td><td>FEEDBACK DISCREPANCIES (NEGATIVE TRIAL-BY-TRIAL CORRELATION): SELF AND OTHER</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis explicitly contrasts self-related feedback and self-referential processing (ACC/MPFC activity), measuring self-perception and self-knowledge.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24772075_1</td><td>(SELF &gt; OTHER) × (GERMAN &gt; CHINESE); self</td><td>24772075_analysis_2</td><td>INTERACTION: FEEDBACK ONSET: (SELF &gt; OTHER) × (GERMAN &gt; CHINESE)</td><td>0.707</td><td>1.000</td><td>0.912</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24772075_3</td><td>SELF &gt; OTHER; self</td><td>24772075_analysis_0</td><td>MAIN EFFECT: FEEDBACK ONSET: SELF &gt; OTHER</td><td>0.453</td><td>1.000</td><td>0.836</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>24772075_4</td><td>SELF &gt; OTHER (CORRELATION); self</td><td>24772075_analysis_3</td><td>WHOLE-BRAIN CORRELATION WITH OVERALL RELATIVE ABSOLUTE MEAN UPDATES IN THE CONTRAST FEEDBACK ONSET: SELF &gt; OTHER</td><td>0.174</td><td>1.000</td><td>0.752</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24814646</strong> | Pred included: 5 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=2, unmatched=6</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24814646/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> A&amp;B; socialcommunication, p2&gt;n2; socialcommunication, D&amp;E; socialcommunication, a1&gt; n1; socialcommunication, a2&gt; n2; socialcommunication, p1&gt; n1; socialcommunication</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24814646_analysis_0</td><td>fMRI contrasts on the basis of the neutral condition as baseline</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Participants were instructed to put themselves into the first-person perspective and report own feelings/behavioral tendencies; the contrasts probe self-related perception/awareness during social scenarios (agency/self-appraisal processes).</td></tr>
<tr><td>24814646_analysis_1</td><td>Initial phase of video-clip: A</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Participants were explicitly instructed to adopt a first‑person perspective and empathize (put themselves into the observer role), which probes perception/understanding of the self (agency/self‑related appraisal) during the task. The analysis modeled initial-phase self‑involving responses, satisfying I1 and I2.</td></tr>
<tr><td>24814646_analysis_2</td><td>Initial phase of video-clip: B</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Participants are explicitly instructed to put themselves into the first-person perspective and provided self-reports of feelings/behavioral tendencies; the task probes self-related perception/awareness in the context of social interactions.</td></tr>
<tr><td>24814646_analysis_3</td><td>Final phase of video-clip: C</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Stimuli are presented from a first-person perspective and participants are instructed to put themselves into the observer’s role and report their own feelings and behavioral tendencies, engaging self-perception/agency processes.</td></tr>
<tr><td>24814646_analysis_4</td><td>Final phase of video-clip: D</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants were instructed to adopt the first-person perspective and to report their own feelings/behavioral tendencies; fMRI contrasts therefore reflect self-related perception/understanding (agency/self-awareness). Satisfies I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24814646_1</td><td>A&amp;B; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>24814646_10</td><td>p2&gt;n2; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>24814646_2</td><td>D&amp;E; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>24814646_3</td><td>a1&gt; n1; socialcommunication</td><td>24814646_analysis_0</td><td>fMRI contrasts on the basis of the neutral condition as baseline</td><td>0.304</td><td>0.276</td><td>0.285</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>24814646_4</td><td>a1&gt;p1; socialcommunication</td><td>24814646_analysis_1</td><td>Initial phase of video-clip: A</td><td>0.207</td><td>0.952</td><td>0.729</td><td>uncertain</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>24814646_5</td><td>a2&gt; n2; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>24814646_6</td><td>a2&gt;p2; socialcommunication</td><td>24814646_analysis_3</td><td>Final phase of video-clip: C</td><td>0.286</td><td>1.000</td><td>0.786</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>24814646_7</td><td>p1&gt; n1; socialcommunication</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>24814646_8</td><td>p1&gt;a1; socialcommunication</td><td>24814646_analysis_2</td><td>Initial phase of video-clip: B</td><td>0.172</td><td>0.938</td><td>0.708</td><td>uncertain</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>24814646_9</td><td>p2&gt;a2; socialcommunication</td><td>24814646_analysis_4</td><td>Final phase of video-clip: D</td><td>0.179</td><td>1.000</td><td>0.754</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24825504</strong> | Pred included: 2 | Manual included (accepted matches only): 3 | Correct overlaps: 2 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24825504/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24825504_analysis_0</td><td>Brain areas activated in the conjunction triad ∩ self ∩ other.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>One of the conditions is an explicit self-referential task and the conjunction includes self-related activations in core self-processing (MPFC), satisfying self-perception measures.</td></tr>
<tr><td>24825504_analysis_1</td><td>Triad &gt; (Self &amp; Other)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast Triad &gt; (Self &amp; Other) isolates triadic interaction-related activity rather than self-referential processing; it does not directly measure perception/understanding of the self.</td></tr>
<tr><td>24825504_analysis_2</td><td>Self &gt; (Other &amp; Triad)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast isolates self-referential processing (self &gt; other &amp; triad); the task directly measures perception/understanding of the self.</td></tr>
<tr><td>24825504_analysis_3</td><td>Other &gt; (Self &amp; Triad)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast specifically isolates Other-related processing (Other &gt; Self &amp; Triad), so it does not measure perception/understanding of self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24825504_2</td><td>Self &gt; (other &amp; triad); self</td><td>24825504_analysis_2</td><td>Self &gt; (Other &amp; Triad)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24825504_3</td><td>Triad &gt; (self &amp; other); others</td><td>24825504_analysis_1</td><td>Triad &gt; (Self &amp; Other)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24825504_4</td><td>triad ∩ self ∩ other; others</td><td>24825504_analysis_0</td><td>Brain areas activated in the conjunction triad ∩ self ∩ other.</td><td>0.488</td><td>1.000</td><td>0.846</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26235682</strong> | Pred included: 5 | Manual included (accepted matches only): 4 | Correct overlaps: 4 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26235682/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26235682_analysis_0</td><td>Positive vs. Neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants imagine comments directed at themselves and rate pleasantness, directly engaging self-perception/self-evaluation processes.</td></tr>
<tr><td>26235682_analysis_1</td><td>Negative vs. Neutral</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Subjects imagine comments directed at themselves and rate pleasantness—this involves self-referential perception and judgments about the self.</td></tr>
<tr><td>26235682_analysis_2</td><td>(Positive &gt; Neutral) inclusively masked with (Negative &gt; Neutral)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Task requires self-referential evaluation (imagining receiving comments about oneself and rating pleasantness), directly assessing perception/understanding of the self.</td></tr>
<tr><td>26235682_analysis_3</td><td>(Positive vs. Neutral) vs. (Negative vs. Neutral)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants imagine receiving comments about themselves and rate pleasantness; the contrast isolates self-referential processing of positive vs negative (vs neutral) reputation, satisfying perception/understanding of self.</td></tr>
<tr><td>26235682_analysis_4</td><td>(Negative vs. Neutral) vs. (Positive vs. Neutral)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants imagine comments directed at themselves and rate pleasantness, engaging self-referential/perception-of-self processes. The contrast targets perception/understanding of self-related social feedback.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26235682_1</td><td>(Positive &gt; Neutral) inclusively masked with (Negative &gt; Neutral); others</td><td>26235682_analysis_2</td><td>(Positive &gt; Neutral) inclusively masked with (Negative &gt; Neutral)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26235682_2</td><td>(Positive vs. Neutral) &gt; (Negative vs. Neutral); others</td><td>26235682_analysis_3</td><td>(Positive vs. Neutral) vs. (Negative vs. Neutral)</td><td>0.958</td><td>1.000</td><td>0.988</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26235682_3</td><td>Negative &gt; Neutral; others</td><td>26235682_analysis_1</td><td>Negative vs. Neutral</td><td>0.895</td><td>1.000</td><td>0.968</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26235682_4</td><td>Positive &gt; Neutral; others</td><td>26235682_analysis_0</td><td>Positive vs. Neutral</td><td>0.895</td><td>1.000</td><td>0.968</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26365506</strong> | Pred included: 1 | Manual included (accepted matches only): 2 | Correct overlaps: 1 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26365506/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26365506_analysis_0</td><td>Self-referential &gt; social</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>This contrast directly isolates self-referential processing (self-related judgments &gt; social non-self), meeting both criteria for Perception and Understanding of Self.</td></tr>
<tr><td>26365506_analysis_1</td><td>Social &gt; self-referential</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although the experiment includes self-referential judgments, this specific analysis (Social &gt; self-referential) isolates social processing over self-processing, so it does not measure perception/understanding of self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26365506_1</td><td>Self-referential &gt; social; self</td><td>26365506_analysis_0</td><td>Self-referential &gt; social</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26365506_2</td><td>Social &gt; self-referential; self</td><td>26365506_analysis_1</td><td>Social &gt; self-referential</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26892859</strong> | Pred included: 3 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26892859/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26892859_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants were instructed to imagine the videos directed at themselves and authors report self-referential/mentalizing processes (MPFC/rACC) in the contrasts, indicating the task probes perception/understanding of self.</td></tr>
<tr><td>26892859_analysis_1</td><td>analysis_1</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants were instructed to imagine the statements directed at them and analyses discuss self-referential/MPFC activity, indicating the task probes perception/understanding of self — meets I1 and I2.</td></tr>
<tr><td>26892859_analysis_2</td><td>analysis_2</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Participants were instructed to imagine the statements directed at them and authors interpret MPFC/rACC activity as self-referential processing, indicating the contrasts tap perception/understanding of self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26892859_1</td><td>Negative &gt; neutral; affiliation</td><td>26892859_analysis_0</td><td>analysis_0</td><td>0.214</td><td>1.000</td><td>0.764</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>26892859_2</td><td>positive &gt; neutral; affiliation</td><td>26892859_analysis_1</td><td>analysis_1</td><td>0.143</td><td>1.000</td><td>0.743</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>26892859_3</td><td>positive &gt; neutral and negative &gt; neutral (conjunction analysis); affiliation</td><td>26892859_analysis_2</td><td>analysis_2</td><td>0.216</td><td>1.000</td><td>0.765</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27039141</strong> | Pred included: 3 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=1, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27039141/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Physical &gt; Meaning; others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27039141_analysis_0</td><td>A Meaning &gt; Physical</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TN</span></td><td></td><td>The &#x27;Meaning &gt; Physical&#x27; contrast addresses processing of object meaning, not judgments about the self (agency or self-knowledge). Own&gt;Other contrasts in the paper test self-related processing, but they are distinct from this contrast.</td></tr>
<tr><td>27039141_analysis_1</td><td>B Physical &gt; Meaning</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Contrasts of own &gt; other (meaning task) and activations in mPFC/precuneus index self-related processing (agency/extended self), meeting Perception of Self criteria (I1, I2).</td></tr>
<tr><td>27039141_analysis_2</td><td>C Own &gt; Other (meaning related task)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The &#x27;own &gt; other&#x27; contrast taps self-related processing (episodic memory, agency, extended-self activations in mPFC/precuneus), meeting I1 and I2 for perception/understanding of self.</td></tr>
<tr><td>27039141_analysis_3</td><td>D Own &gt; Other × Collective &gt; Individual (meaning related task)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast explicitly compares own vs other models (and own collective models), engaging self-related processes (autobiographical memory, agency; precuneus, mPFC, ACC), thus meeting I1 and I2.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27039141_1</td><td>Meaning &gt; Physical; others</td><td>27039141_analysis_0</td><td>A Meaning &gt; Physical</td><td>0.947</td><td>0.500</td><td>0.634</td><td>uncertain</td><td>coord_count_mismatch</td></tr><tr><td>27039141_2</td><td>Own &gt; Other (meaning related task); others</td><td>27039141_analysis_2</td><td>C Own &gt; Other (meaning related task)</td><td>0.971</td><td>0.750</td><td>0.816</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr><tr><td>27039141_3</td><td>Own &gt; Other × Collective &gt; Individual (meaning related task); others</td><td>27039141_analysis_3</td><td>D Own &gt; Other × Collective &gt; Individual (meaning related task)</td><td>0.984</td><td>0.667</td><td>0.762</td><td>accepted</td><td>coord_count_mismatch</td></tr><tr><td>27039141_4</td><td>Physical &gt; Meaning; others</td><td>27039141_analysis_1</td><td>B Physical &gt; Meaning</td><td>0.947</td><td>0.333</td><td>0.518</td><td>unmatched</td><td>coord_count_mismatch, low_coord_high_name, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27090501</strong> | Pred included: 1 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27090501/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27090501_analysis_0</td><td>Significant activation for the (toss - button press) high-frequency effect.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis produced precuneus activation interpreted as positive self-image/self-related processing, indicating the contrast measures perception/understanding of the self in the context of social interaction.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27090501_1</td><td>Significant activation for the (toss – button press) high-frequency effect; others</td><td>27090501_analysis_0</td><td>Significant activation for the (toss - button press) high-frequency effect.</td><td>0.980</td><td>1.000</td><td>0.994</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27477020</strong> | Pred included: 2 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27477020/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27477020_analysis_0</td><td>S-performance</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The analysis centers on self-related ability estimates (S-performance) and pgACC representations of self-performance, directly measuring perception/understanding of self.</td></tr>
<tr><td>27477020_analysis_1</td><td>O-performance</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Analysis includes trialwise S-ability ratings and neural correlates of self-performance estimates (pgACC), directly measuring perception/understanding of self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27477020_1</td><td>O-performance &gt; S - performance; others</td><td>27477020_analysis_1</td><td>O-performance</td><td>0.591</td><td>1.000</td><td>0.877</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27477020_2</td><td>S-performance &gt; O - performance; others</td><td>27477020_analysis_0</td><td>S-performance</td><td>0.591</td><td>1.000</td><td>0.877</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27494142</strong> | Pred included: 2 | Manual included (accepted matches only): 4 | Correct overlaps: 1 | Match statuses: accepted=4, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27494142/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27494142_analysis_0</td><td>CC-A</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The analysis focuses on neural responses to social cooperation (reward and context effects) rather than processes of self-perception, agency, or self-knowledge; it does not meet the Perception/Self criteria.</td></tr>
<tr><td>27494142_analysis_1</td><td>A-CC</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although the task is social (cooperation), the reported contrasts focus on responses to interaction outcomes and reward-related activity, not explicit measures of self-perception, agency, or self-knowledge; thus the Perception/Self inclusion (I2) is not satisfied.</td></tr>
<tr><td>27494142_analysis_2</td><td>Gain-Loss</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The experiment involves self-related decision-making in a social context (cooperate/defect) and post-scan self-reported emotions; cooperation tasks are listed as examples for this construct.</td></tr>
<tr><td>27494142_analysis_3</td><td>Loss-Gain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis includes self-referential outcomes and emotional ratings (happiness, trust, shame, guilt) and examines neural responses to outcomes of one’s own choices, fitting Perception/Understanding of Self criteria.</td></tr>
<tr><td>27494142_analysis_4</td><td>(CC-A) Gain - (CC-A) Loss</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although cooperation tasks are listed as example tasks, this particular contrast examines reward-related responses to mutual cooperation and context effects, not explicit self-related judgments (agency or self-knowledge). It does not directly measure Perception/Understanding of Self, so I2 is not satisfied.</td></tr>
<tr><td>27494142_analysis_5</td><td>(CC-A) Loss - (CC-A) Gain</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the task is social, the reported contrasts target responses to cooperative outcomes and context modulation rather than explicit self-perception or self-reference measures; thus it does not meet Perception/Self inclusion.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27494142_1</td><td>(CC-A)Gain &gt; (CC-A)Loss; others</td><td>27494142_analysis_4</td><td>(CC-A) Gain - (CC-A) Loss</td><td>0.917</td><td>1.000</td><td>0.975</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27494142_2</td><td>A &gt; CC; others</td><td>27494142_analysis_1</td><td>A-CC</td><td>0.600</td><td>1.000</td><td>0.880</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27494142_3</td><td>CC &gt; A; others</td><td>27494142_analysis_0</td><td>CC-A</td><td>0.600</td><td>1.000</td><td>0.880</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27494142_4</td><td>Gain &gt; Loss; others</td><td>27494142_analysis_2</td><td>Gain-Loss</td><td>0.800</td><td>1.000</td><td>0.940</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27622781</strong> | Pred included: 5 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27622781/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27622781_analysis_0</td><td>Working memory maintenance</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study measures self-referential and social-comparison processes (mPFC activity, task preference, PDCAS correlations) indicating perception/understanding of self in social context; the contrasts probe these processes.</td></tr>
<tr><td>27622781_analysis_1</td><td>Effect of performance parametric modulator on no competition</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the analysis uses a performance parametric modulator (self performance), the contrast is in the no-competition condition and is not framed as a self-perception/social-comparison measure per the inclusion examples; it does not explicitly measure perception/understanding of self.</td></tr>
<tr><td>27622781_analysis_2</td><td>Effect of performance parametric modulator on competition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study probes self-referential and social-comparison processes (mPFC activation, self-in-comparison-to-others) during competition, so it measures perception/understanding of self.</td></tr>
<tr><td>27622781_analysis_3</td><td>Event‐Related Analysis at Feedback</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The competition condition induces self-referential and social-comparison processes (mPFC activation noted), so the feedback contrast measures perception/understanding of self in a social context.</td></tr>
<tr><td>27622781_analysis_4</td><td>Competition&gt;No competition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The competition manipulation involves self-referential and social-comparison processes (mPFC activity) and thus probes perception/understanding of self as specified in the inclusion examples.</td></tr>
<tr><td>27622781_analysis_5</td><td>No Competition&gt;Competition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Competition involves self-referential and social-comparison processes (mPFC activation) consistent with Perception/Understanding of Self; task examples include competition and social comparison.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27622781_1</td><td>Competition &gt; No Competition; self</td><td>27622781_analysis_4</td><td>Competition&gt;No competition</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>27622781_2</td><td>No Competition &gt; Competition; self</td><td>27622781_analysis_5</td><td>No Competition&gt;Competition</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 28504911</strong> | Pred included: 5 | Manual included (accepted matches only): 3 | Correct overlaps: 3 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/28504911/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>28504911_analysis_0</td><td>Select &gt; content</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Select &gt; content explicitly engages self-related processing (MPFC, PCC) and was tested for self-relevance; thus it measures perception/understanding of self.</td></tr>
<tr><td>28504911_analysis_1</td><td>Share &gt; content</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The share &gt; content contrast engages self-related processing (MPFC, PCC) and the authors report self-relevance/self-reflection during sharing, so the analysis measures perception/understanding of self.</td></tr>
<tr><td>28504911_analysis_2</td><td>Share &gt; select</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The task and contrast engage self-related processing (MPFC, PCC) and the authors report increased self-related ROI activation for sharing versus selecting; thus the contrast measures perception/understanding of self-related processing.</td></tr>
<tr><td>28504911_analysis_3</td><td>Select</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The select condition directly asks participants about their likelihood to read the article themselves (self-relevance). This explicitly measures perception/understanding of self and meets the construct criteria.</td></tr>
<tr><td>28504911_analysis_4</td><td>Share</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analyses explicitly targeted self-related processing (MPFC, PCC) and tested contrasts (select and share) that measured self-relevance and self-related neural activity, meeting the perception/self criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>28504911_1</td><td>Select &gt; content; others</td><td>28504911_analysis_0</td><td>Select &gt; content</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>28504911_2</td><td>Share &gt; content; others</td><td>28504911_analysis_1</td><td>Share &gt; content</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>28504911_3</td><td>Share &gt; select; others</td><td>28504911_analysis_2</td><td>Share &gt; select</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29324862</strong> | Pred included: 6 | Manual included (accepted matches only): 7 | Correct overlaps: 6 | Match statuses: accepted=7, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29324862/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29324862_analysis_0</td><td>Partnering</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The study contrasts self-initiated (leading, solo, mutual, alone) versus externally-triggered (following) actions and discusses self-orientation and agency-related activations (SMA, PMC, DLPFC), directly addressing perception/understanding of self.</td></tr>
<tr><td>29324862_analysis_1</td><td>Leading &gt; Conjunction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Leading is characterized by self-initiated action and self-orientation; the contrast isolates neural correlates of self-initiated motor planning within a social context, matching Perception/Understanding of Self (agency/self-related processing).</td></tr>
<tr><td>29324862_analysis_2</td><td>Following &gt; Conjunction</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The Following-specific contrast emphasizes externally-oriented perception and processing of the partner’s actions (perception of others/ haptic cues) rather than self-related processes or agency; it does not meet the Perception-of-Self inclusion criteria.</td></tr>
<tr><td>29324862_analysis_3</td><td>Mutual &gt; Conjunction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The Mutual condition engages self-related and agency-related regions (PCC, mPFC) and specifically involves balancing self/other attention; the contrast therefore taps perception/understanding of self (agency/self-processing).</td></tr>
<tr><td>29324862_analysis_4</td><td>Improvisation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Leading vs Following contrasts probe self-initiation, agency and self-oriented processing (pre-SMA, SMA, DLPFC, CMA) and explicit analyses target self-initiated versus externally-triggered movement.</td></tr>
<tr><td>29324862_analysis_5</td><td>Self-initiation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast isolates self-initiated versus externally-triggered actions and thus probes agency/self-related processing (self-initiation), satisfying criteria for Perception and Understanding of Self.</td></tr>
<tr><td>29324862_analysis_6</td><td>Joint improvisation.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>Contrasts probe self-initiated versus externally-triggered actions (agency/self-initiation), addressing perception and understanding of self (agency).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29324862_1</td><td>Following &gt; Conjunction; others</td><td>29324862_analysis_2</td><td>Following &gt; Conjunction</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29324862_2</td><td>Leading + Following + Mutual &gt; Solo + Alone); others</td><td>29324862_analysis_0</td><td>Partnering</td><td>0.148</td><td>1.000</td><td>0.744</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>29324862_3</td><td>Leading + Solo &gt; Mutual + Alone; others</td><td>29324862_analysis_4</td><td>Improvisation</td><td>0.182</td><td>1.000</td><td>0.755</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>29324862_4</td><td>Leading &gt; Conjunction; others</td><td>29324862_analysis_1</td><td>Leading &gt; Conjunction</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29324862_5</td><td>Leading &gt; Solo; others</td><td>29324862_analysis_6</td><td>Joint improvisation.</td><td>0.294</td><td>1.000</td><td>0.788</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>29324862_6</td><td>Mutual &gt; Conjunction; others</td><td>29324862_analysis_3</td><td>Mutual &gt; Conjunction</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29324862_7</td><td>[Leading + Solo + Mutual + Alone] &gt; Following; others</td><td>29324862_analysis_5</td><td>Self-initiation</td><td>0.233</td><td>1.000</td><td>0.770</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29432769</strong> | Pred included: 9 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29432769/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29432769_analysis_0</td><td>Self- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Self vs Celebrity is a canonical self-referential judgment contrast assessing Perception and Understanding of Self. Meets I1 (task measuring self-perception) and I2 (measures Perception and Understanding of Self).</td></tr>
<tr><td>29432769_analysis_1</td><td>Friend- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Self- vs. celebrity contrast directly measures perception and understanding of the self (self-referential trait judgments), meeting the self-related inclusion criteria.</td></tr>
<tr><td>29432769_analysis_2</td><td>Modulation effect of Interdependence</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The self- vs. celebrity contrast explicitly targets self-perception and self-referential processing, directly satisfying Perception and Understanding of Self criteria.</td></tr>
<tr><td>29432769_analysis_3</td><td>Self- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The Self vs. Celebrity contrast explicitly assesses Perception and Understanding of Self (self-awareness and self-judgment). It meets both inclusion criteria.</td></tr>
<tr><td>29432769_analysis_4</td><td>Friend- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast is Friend vs. Celebrity (other-related), not a self-related contrast, so it does not meet Perception/Understanding of Self criteria.</td></tr>
<tr><td>29432769_analysis_5</td><td>Functional connectivity</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The self vs. celebrity contrast directly measures perception and understanding of the self (self-referential judgments).</td></tr>
<tr><td>29432769_analysis_6</td><td>Self- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast Self vs. Celebrity directly measures perception and understanding of the self (self-referential judgments).</td></tr>
<tr><td>29432769_analysis_7</td><td>Friend- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Self-judgment (Self vs. Celebrity) directly measures perception and understanding of the self (self-knowledge/self-awareness), meeting both inclusion criteria.</td></tr>
<tr><td>29432769_analysis_8</td><td>Modulation effect of Interdependence</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Self vs. celebrity contrast directly measures perception and understanding of the self (self-referential judgments), satisfying the construct.</td></tr>
<tr><td>29432769_analysis_9</td><td>Self- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Self-judgment (Self vs Celebrity) directly measures Perception and Understanding of Self (self-awareness/self-knowledge), satisfying the listed inclusion criteria.</td></tr>
<tr><td>29432769_analysis_10</td><td>Friend- vs. Celebrity-judgments</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The specific analysis is Friend vs. Celebrity judgments, which concerns others rather than self-related processing, so it does not meet Perception/Understanding of Self criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29432769_2</td><td>Self vs Celebrity; self</td><td>29432769_analysis_3</td><td>Self- vs. Celebrity-judgments</td><td>0.739</td><td>0.800</td><td>0.782</td><td>accepted</td><td>coord_count_mismatch, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 29777673</strong> | Pred included: 7 | Manual included (accepted matches only): 2 | Correct overlaps: 2 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29777673/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29777673_analysis_0</td><td>SA vs. SC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>SA vs. SC directly contrasts self-evaluative judgments about agentic vs. communal traits (self-knowledge/agency), matching Perception and Understanding of Self.</td></tr>
<tr><td>29777673_analysis_1</td><td>SC vs. SA</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>SC vs. SA directly contrasts communal vs. agentic self-evaluation (judgments about the self), satisfying perception/understanding of self criteria.</td></tr>
<tr><td>29777673_analysis_2</td><td>OA vs. OC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis involves self-evaluation and endorsement of others’ evaluation of the self, which directly probes perception and understanding of the self (agency vs. communion).</td></tr>
<tr><td>29777673_analysis_3</td><td>OC vs. OA</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis measures self-related processes (self-evaluation and endorsement of others&#x27; evaluations about the self), directly fitting Perception and Understanding of Self and Agency subcomponents.</td></tr>
<tr><td>29777673_analysis_4</td><td>SA vs. Fixation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast is an explicit self-evaluation (agentic traits), directly measuring perception and understanding of the self; meets the construct inclusion criteria.</td></tr>
<tr><td>29777673_analysis_5</td><td>dlPFC as the seed region</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis specifically targets self-evaluation (agentic self-evaluation, SA) and neural correlates/connectivity (dlPFC seed), directly measuring perception and understanding of the self, satisfying I1 and I2.</td></tr>
<tr><td>29777673_analysis_6</td><td>Thalamus as the seed region</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The SA condition is explicitly agentic self-evaluation (perception/understanding of self, including agency); the contrast (SA vs Fixation) and related PPI target self-related processing, satisfying the inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29777673_2</td><td>SA vs. Fixation; self</td><td>29777673_analysis_4</td><td>SA vs. Fixation</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29777673_3</td><td>SC &gt; SA; self</td><td>29777673_analysis_1</td><td>SC vs. SA</td><td>0.750</td><td>1.000</td><td>0.925</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 30077577</strong> | Pred included: 1 | Manual included (accepted matches only): 1 | Correct overlaps: 1 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/30077577/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>30077577_analysis_0</td><td>self-relevant&gt;control</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TP</span></td><td>manual+ (accepted), correct</td><td>The contrast explicitly probes self-referential recall (putting oneself back into relationship episodes), meeting criteria for perception/understanding of self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>30077577_2</td><td>self-relevant &gt; control; self</td><td>30077577_analysis_0</td><td>self-relevant&gt;control</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>
</details></section><section id="bucket-false-positive"><details class="bucket" open><summary><h2>False Positive (16)</h2></summary><p><strong>Match status totals:</strong> accepted=0 | uncertain=0 | unmatched=6</p>
<details class="doc-card">
  <summary><strong>PMID 15488424</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/15488424/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>15488424_analysis_0</td><td>Cooperation versus independent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The paradigm probes self-related processes (agency, self–other monitoring) during cooperation versus independent play; the contrast examines perception/understanding of self-related agency and monitoring.</td></tr>
<tr><td>15488424_analysis_1</td><td>Competition versus independent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study highlights agency, self–other monitoring and self-related processing in competition vs independent contrasts, thus measuring perception/understanding of self (agency), satisfying criteria.</td></tr>
<tr><td>15488424_analysis_2</td><td>Cooperation versus competition</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis examines agency and self–other monitoring (sense of agency, self/other distinction) relevant to Perception and Understanding of Self.</td></tr>
<tr><td>15488424_analysis_3</td><td>Competition versus cooperation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analyses examine self-related processes (sense of agency, self–other merging/distinction) in cooperation and competition, satisfying criteria for perception/understanding of self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 16171833</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/16171833/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>16171833_analysis_0</td><td>a) Common activations of social interaction (SOC &gt; ARB)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>SOC&gt;ARB contrast does not focus on self-related processing; self-involvement is a separate ME&gt;OTHER contrast, so this analysis does not measure perception/understanding of self.</td></tr>
<tr><td>16171833_analysis_1</td><td>b) Common activations of arbitrary facial movements (ARB &gt; SOC)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific contrast (ARB &gt; SOC) reflects perception of facial movements across addressees and does not target self-related processing (ME vs OTHER) directly.</td></tr>
<tr><td>16171833_analysis_2</td><td>c) Common activations of self-involvement (ME &gt; OTHER)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>ME&gt;OTHER explicitly measures self-involvement/self-relevance (being gazed at) and thus taps self-perception/awareness processes. Satisfies I1 and I2.</td></tr>
<tr><td>16171833_analysis_3</td><td>d) Common activations of other-related activity (OTHER &gt; ME)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast OTHER&gt;ME targets other-related processing (observing others) rather than self-related perception/awareness. Does not satisfy inclusion criteria for self-perception.</td></tr>
<tr><td>16171833_analysis_4</td><td>e) Common activations of the statistical interaction SOC × ME</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The interaction involves the SELF-INVOLVEMENT factor (ME) and examines self-relevant processing in medial PFC; thus it measures perception/understanding of self-related social involvement.</td></tr>
<tr><td>16171833_analysis_5</td><td>f) Common activations of the statistical interaction SOC × OTHER</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific interaction (SOC × OTHER) focuses on stimuli directed to others rather than self-related processing; it does not measure self-perception/awareness as required by the inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 18486491</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/18486491/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>18486491_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study implicates default-mode and self-referential processes (simulation, DMPFC) during the social interaction; cooperation/competition is listed as an example task for this construct, so criteria are met.</td></tr>
<tr><td>18486491_analysis_1</td><td>analysis_1</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the paper discusses default-mode regions and self-referential processes, the reported contrasts and primary analyses focus on social interaction and understanding others rather than explicit measures or contrasts of self-perception/self-judgment.</td></tr>
<tr><td>18486491_analysis_2</td><td>Regions where the cooperation difference score (in-group-out-group) is significantly correlated with the difference in BOLD signal during in-group and out-group interactions</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis focuses on responses to others (in-group vs out-group) and correlations with cooperative behavior, not on self-awareness or self-judgment per se.</td></tr>
<tr><td>18486491_analysis_3</td><td>DMPFC connectivity with in-group - DMPFC connectivity with out-group</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although DMPFC is related to self-referential processing, this analysis targets differences in connectivity when interacting with in-group vs out-group partners (mentalizing about others and affiliation), not an explicit measure of self-perception or self-knowledge.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 19439183</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19439183/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19439183_analysis_0</td><td>Pain &gt; no pain</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the study manipulates perceived agency, the specific contrast pain&gt;no pain is a main effect across contexts and does not target self-related processing or agency judgments.</td></tr>
<tr><td>19439183_analysis_1</td><td>Self + Other &gt; Self</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the design labels conditions as “self” vs “self and other,” the contrast targets processing of another agent and perceived agency rather than self-awareness, agency judgments about the participant, or self-knowledge; thus it does not meet perception-of-self criteria.</td></tr>
<tr><td>19439183_analysis_2</td><td>Self &gt; Self + Other</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although labeled “self” in conditions, participants observed others&#x27; limbs and the contrast probes presence of another agent and perceived agency rather than self-awareness or self-monitoring.</td></tr>
<tr><td>19439183_analysis_3</td><td>PCO &gt; PCS</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although one condition involves self-caused pain, the PCO&gt;PCS contrast is focused on perceived agency/others&#x27; actions rather than on self-awareness, agency judgments about one&#x27;s own actions or self-knowledge.</td></tr>
<tr><td>19439183_analysis_4</td><td>PCS &gt; PCO</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Contrast directly contrasts self-caused versus other-caused pain (agency) and the PPI probes neural processing related to perceived agency/self, satisfying I1 and I2.</td></tr>
<tr><td>19439183_analysis_5</td><td>Emotion contagion score (during PCS trials)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis examines susceptibility to others&#x27; emotions during observation of pain in stimuli, not self-perception, agency, or self-knowledge.</td></tr>
<tr><td>19439183_analysis_6</td><td>Pain ratings (during PCS trials)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the design includes a ‘self’ condition (PCS), the focus is on perceiving others&#x27; pain and the effect of social context/agency rather than self-awareness or self-related agency judgments; it does not assess self-perception constructs.</td></tr>
<tr><td>19439183_analysis_7</td><td>Emotion contagion score (during PCO trials)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast and correlation focus on perceiving others&#x27; pain and trait emotional contagion, not self-related agency or self-knowledge judgments.</td></tr>
<tr><td>19439183_analysis_8</td><td>Pain ratings (during PCO trials)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis concerns ratings of others&#x27; pain (PCO) and perceived agency of another, not self-perception or agency judgments about the self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20188190</strong> | Pred included: 6 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20188190/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20188190_analysis_0</td><td>Preference</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task asks about liking another person (an other-directed preference), not direct self-appraisal or self-knowledge. Thus it does not meet the Perception and Understanding of Self criteria.</td></tr>
<tr><td>20188190_analysis_1</td><td>Gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Preference judgment is explicitly self-referential (“Do I like this person?”), engaging self-evaluation processes; therefore it measures perception/understanding of the self.</td></tr>
<tr><td>20188190_analysis_2</td><td>Preference &gt; Gender</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Preference judgment is self-referential (&quot;Do I like this person?&quot;) and involves self-evaluation/awareness in making the preference decision, meeting criteria for perception/understanding of self.</td></tr>
<tr><td>20188190_analysis_3</td><td>Gender &gt; Preference</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Preference judgment is explicitly self-referential (&#x27;Do I like this person?&#x27;), involving self-evaluation and self-related processing.</td></tr>
<tr><td>20188190_analysis_4</td><td>aMFC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Preference judgment is explicitly self-referential (‘Do I like this person?’), involving self-related evaluation and self-knowledge; meets criteria.</td></tr>
<tr><td>20188190_analysis_5</td><td>vMFC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Preference judgment is explicitly self-referential (&#x27;Do I like this person?&#x27;), engaging self-related appraisal processes; the contrast measures perception/understanding of the self.</td></tr>
<tr><td>20188190_analysis_6</td><td>PCC</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Preference judgment is self-referential (&#x27;Do I like this person?&#x27;) and the authors explicitly discuss self-related processing and engagement of medial prefrontal regions, meeting self-perception criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21320516</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21320516/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21320516_analysis_0</td><td>Female and male</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis uses within-subject subjective funniness ratings as parametric modulators, directly measuring participants&#x27; access to and reporting of their internal emotional state (self-perception).</td></tr>
<tr><td>21320516_analysis_1</td><td>Female and male</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analyses focus on perception of humorous stimuli, emotion and mentalizing, not on self‑related processing, agency, or self‑knowledge; criteria for perception/self are not met.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21600991</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21600991/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21600991_analysis_0</td><td>Activations in the retrieval of social context</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis explicitly isolates self-generation vs other-generation {(rSS+rNS)−(rSO+rNO)}, indexing self-referential/self-related retrieval processes (perception/understanding of self).</td></tr>
<tr><td>21600991_analysis_1</td><td>Activations in the retrieval of self-generation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study explicitly manipulates self-generation vs other-generation and uses contrasts to identify self-generation-related retrieval (left mPFC, MCC). This maps onto perception/understanding of self (self-referential processing/agency).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21955370</strong> | Pred included: 2 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21955370/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21955370_analysis_0</td><td>I(H + M) &gt; C(H + M)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Perception/understanding of self (agency/self-knowledge) is not specifically targeted by this contrast, which assesses cognitive inhibition (Stroop) collapsed across interaction types. The contrast does not measure self-related processing required by this annotation.</td></tr>
<tr><td>21955370_analysis_1</td><td>C(H + M) &gt; I(H + M)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The experiment involves self-related demands in a competition context, but the contrast C(H+M) &gt; I(H+M) isolates congruent vs incongruent (Stroop) effects collapsed across opponent type and does not specifically assess self-perception, agency, or self-knowledge. Therefore it does not satisfy I2.</td></tr>
<tr><td>21955370_analysis_2</td><td>H(I + C) &gt; M(I + C)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although primarily probing ToM, the human–human vs machine contrast involves self-monitoring and self–other distinction (authors discuss self-identification/perspective-taking), so it meets the provided examples and criteria.</td></tr>
<tr><td>21955370_analysis_3</td><td>M(I + C) &gt; H(I + C)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the paradigm involves self–other comparison and some self-related processing is discussed, the contrast primarily targets understanding others (ToM/human–human competition) rather than directly measuring self-perception or agency as the main construct. It therefore does not satisfy I2 for Perception and Understanding of Self.</td></tr>
<tr><td>21955370_analysis_4</td><td>[H(I &gt; C)] &gt; [M(I &gt; C)]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The competition paradigm involves self-monitoring, self–other comparison and agency-related processing (participants track own performance relative to opponent); thus it addresses Perception and Understanding of Self.</td></tr>
<tr><td>21955370_analysis_5</td><td>[M(I &gt; C)] &gt; [H(I &gt; C)]</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>While the task involves self-monitoring in competition, the contrast specifically targets mentalizing/perception of others (human vs. machine) rather than direct measures of self-related processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23599165</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23599165/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23599165_analysis_0</td><td>Yes (Match + Unrequited) &gt; No (Rejection + Disinterest)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analyses explicitly model participants&#x27; beliefs about their own desirability (RL model, prediction errors tied to self-image) and examine neural correlates of those self-related expectations, meeting self-perception criteria.</td></tr>
<tr><td>23599165_analysis_1</td><td>No (Rejection + Disinterest) &gt; Yes (Match + Unrequited)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study models and reports neural responses tied to participants&#x27; self-related beliefs (desirability, self-image) and responses to social feedback about the self; the contrast involves feedback about the self (no vs yes), satisfying self-perception criteria.</td></tr>
<tr><td>23599165_analysis_2</td><td>Match &gt; Unrequited</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast reflects participants&#x27; own decisions and self-related valuation (desire for contact, self-relevant desirability learning), so it measures aspects of Perception/Understanding of Self in this task.</td></tr>
<tr><td>23599165_analysis_3</td><td>Rejection &gt; Disinterest</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The analysis indexes self-relevant processing (updating beliefs about one’s desirability and emotional reactions to being rejected), so it measures perception/understanding of the self in a social context.</td></tr>
<tr><td>23599165_analysis_4</td><td>Mismatched (Rejection + Unrequited) &gt; matched (Match + Disinterest)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the study models self-desirability elsewhere, this specific contrast (mismatched &gt; matched) primarily indexes expectation violations and updating about others rather than direct measures of self-perception or agency.</td></tr>
<tr><td>23599165_analysis_5</td><td>Unsigned prediction errors from RL model</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The RL unsigned prediction errors index learning about one’s own desirability (self-related expectations/self-image). The authors explicitly link RMPFC responses to prediction errors relevant to the personal self-image, satisfying self-perception criteria.</td></tr>
<tr><td>23599165_analysis_6</td><td>Partners who were given a yes &gt; those given a no</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the contrast is partner decision (yes vs no), the analysis explicitly models participants&#x27; expectations about their own desirability and prediction errors (self-relevant learning and self-image), so it measures perception/understanding of self-related social information.</td></tr>
<tr><td>23599165_analysis_7</td><td>Partners who were given a no &gt; those given a yes</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study models participants&#x27; expectations about their own desirability (RL model of self-desirability) and examines neural responses related to self-image and prediction errors; responses to partner rejection/acceptance engage self-related processing, satisfying perception/understanding of self criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 23722983</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/23722983/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>23722983_analysis_0</td><td>Intention effect</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The intention effect reflects interns’ own intentions and correlates strongly with their personal preferences; reported activations (MPFC, PCC) are classical self-relevance/self-processing regions, so this analysis measures perception/understanding of self.</td></tr>
<tr><td>23722983_analysis_1</td><td>Salesperson effect</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The salesperson effect indexes ability to persuade others (mentalizing) rather than self-perception or self-referential processing; it does not measure perception/understanding of self.</td></tr>
<tr><td>23722983_analysis_2</td><td>Buzz effect</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The buzz effect specifically indexes ultimate success across producers (mentalizing and reward) rather than self-perception; although a separate intention analysis targeted self-relevance, the buzz effect does not measure perception/understanding of self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24582805</strong> | Pred included: 7 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=6</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24582805/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> DA &gt; CA; self, DP &gt; CP; self, DS &gt; CS; self, RA &gt; CA; self, RP &gt; CP; self, RS &gt; CS; self</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24582805_analysis_0</td><td>Main effect of age group</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Direct and reflected self-evaluations explicitly measure self-knowledge and self-understanding.</td></tr>
<tr><td>24582805_analysis_1</td><td>Main effect of evaluative perspective</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Direct and reflected self-evaluations explicitly measure perception and understanding of the self (self-knowledge, self-evaluation).</td></tr>
<tr><td>24582805_analysis_2</td><td>Age group × evaluative perspective</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Direct self-evaluations and reflected self-evaluations explicitly probe awareness, traits, and self-knowledge, meeting criteria for perception and understanding of self.</td></tr>
<tr><td>24582805_analysis_3</td><td>Main effect of domain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Direct and reflected self-evaluations explicitly probe self-knowledge, self-evaluation, and self-awareness across domains, meeting Perception/Understanding of Self.</td></tr>
<tr><td>24582805_analysis_4</td><td>Age group × domain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Multiple conditions probe self-knowledge and self-evaluations (direct and reflected self-evaluations), directly measuring perception and understanding of the self.</td></tr>
<tr><td>24582805_analysis_5</td><td>Evaluative perspective × domain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Direct and reflected self-evaluations explicitly probe perception and understanding of the self (self-knowledge, self-evaluation).</td></tr>
<tr><td>24582805_analysis_6</td><td>Age group × evaluative perspective × domain</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Direct self-evaluations and reflected self-evaluations explicitly measure perception and understanding of the self (self-knowledge, self-evaluation).</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24582805_1</td><td>DA &gt; CA; self</td><td>24582805_analysis_0</td><td>Main effect of age group</td><td>0.270</td><td>0.000</td><td>0.081</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>24582805_2</td><td>DP &gt; CP; self</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>24582805_3</td><td>DS &gt; CS; self</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>24582805_7</td><td>RA &gt; CA; self</td><td>24582805_analysis_3</td><td>Main effect of domain</td><td>0.286</td><td>0.040</td><td>0.114</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>24582805_8</td><td>RP &gt; CP; self</td><td>24582805_analysis_4</td><td>Age group × domain</td><td>0.320</td><td>0.000</td><td>0.096</td><td>unmatched</td><td>low_total_score, missing_coords_on_one_side</td></tr><tr><td>24582805_9</td><td>RS &gt; CS; self</td><td>24582805_analysis_1</td><td>Main effect of evaluative perspective</td><td>0.160</td><td>0.152</td><td>0.154</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24936688</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24936688/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24936688_analysis_0</td><td>Status Type by Status Level interaction</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The manipulation is relative to the participant&#x27;s assigned status (higher/same/lower) and post-scan similarity ratings assess self–other comparison, so the analysis involves processes relating to perception/understanding of self (social comparison/self-relevance).</td></tr>
<tr><td>24936688_analysis_1</td><td>Status Type main effect</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The primary task and analysis concern perception and evaluation of others (status of targets). Although similarity ratings to self were collected post-scan, the main contrast assesses others, not self-referential processing per se.</td></tr>
<tr><td>24936688_analysis_2</td><td>Status Level main effect</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Primary analysis addresses perception/evaluation of others rather than self-related processing. Although the design uses relative status (comparison to participant), the reported fMRI contrasts focus on target evaluation, not measures of self-perception. No exclusion criteria specified.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25534111</strong> | Pred included: 1 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25534111/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25534111_analysis_0</td><td>Responding to joint attention (RJA - RJAc)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast focuses on responding to another&#x27;s gaze and representing others&#x27; attention; it does not directly measure self-perception or agency as a primary construct (self-related measures are incidental), so inclusion criteria are not met.</td></tr>
<tr><td>25534111_analysis_1</td><td>Initiating joint attention (IJA - IJAc)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although the study discusses self-other representations and agency, the contrast is focused on joint attention and social interaction rather than direct measures of self-perception/agency or self-knowledge; it does not meet the inclusion criteria.</td></tr>
<tr><td>25534111_analysis_2</td><td>Conjunction of initiating and responding to joint attention (IJA - IJAc) with (RJA - RJAc)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The task and contrasts probe self-related representations (agency, self-perspective in attention) as part of self–other attentional processing, satisfying perception-of-self criteria.</td></tr>
<tr><td>25534111_analysis_3</td><td>Initiating Joint attention minus responding to joint attention (IJA - IJAc) - (RJA - RJAc)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The primary focus is on joint attention and on representing others&#x27; attention; while self/agency is discussed, the analysis does not directly target self-perception or self-knowledge measures as the main construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25640962</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25640962/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25640962_analysis_0</td><td>IC&gt;NG</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although participants gave self-reports of feeling addressed, the IC&gt;NG main effect primarily indexes gesture-related processing and social-communicative perception of others rather than constructs of self-awareness or self-knowledge; it does not meet the Perception and Understanding of Self inclusion criteria.</td></tr>
<tr><td>25640962_analysis_1</td><td>ego&gt;allo</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants make explicit self-referential judgments (“Do you feel addressed?”), indexing perception/understanding of self in a social context.</td></tr>
<tr><td>25640962_analysis_2</td><td>allo&gt;ego</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants make self-referential judgements (‘do I feel addressed’), indexing self-involvement/awareness relevant to perception and understanding of self (agency/self-related evaluation).</td></tr>
<tr><td>25640962_analysis_3</td><td>(IC-ego&gt;IC-allo)&gt;(NG-ego&gt;NG-allo)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants made self-referential judgments (whether they felt addressed), tapping self-awareness/self-referential processing; the contrast probes how stimuli influence perception of self-involvement, satisfying perception-of-self criteria.</td></tr>
<tr><td>25640962_analysis_4</td><td>(IC-allo&gt;NG-allo)&gt;(IC-ego&gt;NG-ego)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Participants judged whether they felt addressed, reflecting self-related perception/awareness and self-involvement; the contrast examines how cues modulate self-referential processing.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25929599</strong> | Pred included: 4 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25929599/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25929599_analysis_0</td><td>Incongruent &gt; Congruent</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Regulation of imitation (Incongruent&gt;Congruent) engages self–other distinction/agency processes (rTPJ) and thus measures aspects of perception/understanding of self.</td></tr>
<tr><td>25929599_analysis_1</td><td>Happy (Incongruent &gt; Congruent) masked inclusively with Happy (Incongruent &gt; Congruent) &gt; Angry (Incongruent &gt; Congruent)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Although primarily social, the analysis and authors interpret engagement of self-other distinction/agency processes (e.g., rTPJ activation) when regulating mimicry, indicating it probes aspects of perception/understanding of self (agency). Therefore it satisfies I1 and I2.</td></tr>
<tr><td>25929599_analysis_2</td><td>Angry (Incongruent &gt; Congruent) masked inclusively with Angry (Incongruent &gt; Congruent) &gt; Happy (Incongruent &gt; Congruent)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The imitation-inhibition paradigm and the incongruent&gt;congruent contrast tap processes of self–other distinction/agency relevant to perception and understanding of self.</td></tr>
<tr><td>25929599_analysis_3</td><td>Out-group (Incongruent &gt; Congruent) masked inclusively with Out-group (Incongruent &gt; Congruent) &gt; In-group (Incongruent &gt; Congruent)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast targets perception/response to others (out-group vs in-group) and mimicry regulation; it does not directly measure self-related perception or self-knowledge. No exclusion criteria specified.</td></tr>
<tr><td>25929599_analysis_4</td><td>In-group (Incongruent &gt; Congruent) masked inclusively with In-group (Incongruent &gt; Congruent) &gt; Out-group (Incongruent &gt; Congruent)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The incongruent&gt;congruent contrast indexes self–other distinction/agency processes (regulation of imitation), which reflects perception/understanding of self (agency). The analysis isolates these processes for in-group stimuli.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 28477977</strong> | Pred included: 5 | Manual included (accepted matches only): 0 | Correct overlaps: 0 | Match statuses: accepted=0, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/28477977/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>28477977_analysis_0</td><td>SELF minus OTHER [(SELF_No + SELF_Single + SELF_ Group) - (OTHER_No + OTHER_Single + OTHER_Group)] (Fig. 3)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast SELF minus OTHER directly targets self-related processing (mPFC activation), satisfying perception/understanding of self.</td></tr>
<tr><td>28477977_analysis_1</td><td>Group minus No Laughter [(SELF_ Group - SELF_No) + (OTHER_ Group - OTHER_No)]</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific contrast (Group vs No Laughter) combines SELF and OTHER conditions and indexes listener response processing rather than self-related perception or self-monitoring; it does not satisfy the perception-of-self criteria.</td></tr>
<tr><td>28477977_analysis_2</td><td>Single minus No Laughter [(SELF_ Single - SELF_No) + (OTHER_Single - OTHER_No)]</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>This specific contrast (Single minus No Laughter collapsed across SELF and OTHER) measures auditory responses to laughter rather than self-related processing or self-awareness; it therefore does not meet the Perception/Self inclusion criteria.</td></tr>
<tr><td>28477977_analysis_3</td><td>Group minus Single Laughter [(SELF_ Group - SELF_Single) + (OTHER_ Group - OTHER_Single)]</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The Group vs Single laughter contrast pools SELF and OTHER conditions and primarily measures listener responses; it does not isolate self-related perception or self-knowledge/agency.</td></tr>
<tr><td>28477977_analysis_4</td><td>[(SELF_ Group - SELF_No) - (OTHER_Group - OTHER_No)] (Fig. 4)</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast explicitly isolates self-related processing (SELF vs OTHER), with mPFC activation reported; it therefore measures perception/understanding of the self.</td></tr>
<tr><td>28477977_analysis_5</td><td>[(SELF_ Single - SELF_No) - (OTHER_Single - OTHER_No)]</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast explicitly manipulates self vs other speaker (SELF vs OTHER), indexing self-related processing.</td></tr>
<tr><td>28477977_analysis_6</td><td>Physio-physiological interaction (PPI) seeded on the right auditory cortex and mPFC.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The SELF vs OTHER contrast isolates self-related processing and mPFC activation, directly measuring perception/understanding of self.</td></tr>
<tr><td>28477977_analysis_7</td><td>Physio-physiological interaction (PPI) analysis in the left auditory cortex and mPFC.</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The study explicitly contrasts SELF vs OTHER and identifies mPFC self-related activity; the PPI uses mPFC seed reflecting self-related processing, thus measuring perception/understanding of self-related signals.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <p>No manual-to-auto match diagnostics for this document.</p>
  </details>
  
  
</details>
</details></section><section id="bucket-false-negative"><details class="bucket" open><summary><h2>False Negative (14)</h2></summary><p><strong>Match status totals:</strong> accepted=51 | uncertain=2 | unmatched=8</p>
<details class="doc-card">
  <summary><strong>PMID 29079809</strong> | Pred included: 0 | Manual included (accepted matches only): 3 | Correct overlaps: 0 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29079809/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29079809_analysis_0</td><td>clusters genetic &gt; non-genetic</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The analysis probes perception/understanding of others and kinship-related social cognition, not self-related processes or self-awareness; it does not measure agency or self-knowledge.</td></tr>
<tr><td>29079809_analysis_1</td><td>clusters non-genetic &gt; genetic</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Task focuses on perceiving and evaluating others (kinship, moral dilemma) rather than self-referential processing or agency/self-knowledge.</td></tr>
<tr><td>29079809_analysis_2</td><td>moral dilemma decision task</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The analyses probe judgments about others (kin, friend, strangers) and mentalizing/perspective-taking, not explicit measures of self-perception, agency, or self-knowledge.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29079809_1</td><td>clusters genetic&gt;non-genetic; self</td><td>29079809_analysis_0</td><td>clusters genetic &gt; non-genetic</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29079809_2</td><td>clusters non-genetic&gt;genetic; self</td><td>29079809_analysis_1</td><td>clusters non-genetic &gt; genetic</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29079809_3</td><td>moral dilemma decision task; self</td><td>29079809_analysis_2</td><td>moral dilemma decision task</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 5660240)</summary><p><strong>Title:</strong> Differential inter-subject correlation of brain activity when kinship is a variable in moral dilemma</p><details><summary>Abstract</summary><pre class="paper-text">Previous behavioural studies have shown that humans act more altruistically towards kin. Whether and how knowledge of genetic relatedness translates into differential neurocognitive evaluation of observed social interactions has remained an open question. Here, we investigated how the human brain is engaged when viewing a moral dilemma between genetic vs. non-genetic sisters. During functional magnetic resonance imaging, a movie was shown, depicting refusal of organ donation between two sisters, with subjects guided to believe the sisters were related either genetically or by adoption. Although 90% of the subjects self-reported that genetic relationship was not relevant, their brain activity told a different story. Comparing correlations of brain activity across all subject pairs between the two viewing conditions, we found significantly stronger inter-subject correlations in insula, cingulate, medial and lateral prefrontal, superior temporal, and superior parietal cortices, when the subjects believed that the sisters were genetically related. Cognitive functions previously associated with these areas include moral and emotional conflict regulation, decision making, and mentalizing, suggesting more similar engagement of such functions when observing refusal of altruism from a genetic sister. Our results show that mere knowledge of a genetic relationship between interacting persons robustly modulates social cognition of the perceiver.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Evaluating and predicting social interactions of others is an integral part of social cognition, one of the most fundamental of human cognitive functions. Indeed, the evolution of social cognition may best explain why humans have a more developed neocortex than other species . So far, social cognition has been predominantly studied with stimuli depicting interactions between strangers, however, most of the significant interactions evaluated in daily life are between one’s family members, friends, and acquaintances 

More importantly, most of our social interactions (and social effort) is directed to a very small number of familiar individuals, 60% of our social effort is directed to just 15 close friends and family . There is also considerable experimental and observational evidence for a “kinship premium” in our interactions with others, especially when those interactions involve altruistic behaviour . We are more likely to help our genetic relatives compared to unrelated individuals, and to do so implicitly, without conscious elaboration . In a trolley dilemma , subjects have to decide if they would push a handle to set a trolley to another track so that instead of killing five people when left without intervention, it will kill a single person on an alternative track. If only strangers are considered, the study subjects favoured the survival of the five over one life; however, their judgement changed if the single person was genetically related to the subject. 

On the other hand it has been shown, that subjects judged incest to be equally morally wrong for a sibling, irrespective of whether this was a genetic, adoptive or step sibling . In these studies, rather co-residence with the sibling in the family, irrespective of genetic status, was the most relevant factor in decisions about the moral reprehensibility of incest. 

These and many other studies, have shown differences in multiple aspects of moral perception/processing, evaluation, judgement and decision making at the behavioural level when processing information about kin vs. non-kin. However, much less is known about the neuronal underpinnings of these effects. Recently, Wlodarski and Dunbar  have shown that different brain regions are active when subjects judge moral dilemmas about kin vs. friends. They found the sensorimotor cortex, ventromedial prefrontal cortex and posterior cingulate cortex to be more strongly activated when the subjects processed social information about their friends than about their kin. These differences imply that the brain processes kinship information differently than information on unrelated individuals. 

We explored this further by comparing subjects’ brain responses to a moral dilemma involving a pair of genetic versus adoptive (  i  .  e  ., unrelated) sisters. During functional magnetic resonance imaging (fMRI) the subjects viewed the same movie involving two sisters, but one group was primed with the information that they were genetic sisters and the other group with the information that they were sisters by adoption. The case of sisters related genetically vs. by adoption is especially suitable for testing whether knowledge of genetic relationship influences perception of a moral dilemma between kin given that there is no potential for shared genetic interest in future generations for adopted siblings . Note, however, that the current study examined perception of a kin relationship that subjects were seeing in a movie, while in Wlodarski and Dunbars  analysis the subjects answered questions about their own kin members and friends. 

In the present study, we utilized inter-subject correlation (ISC) of brain hemodynamic activity as a model-free analysis approach that makes it possible to use movies as ecologically valid stimuli during fMRI. Due to improvements in fMRI acquisition methods and data analysis algorithms , it has become possible to study specific aspects of social cognition between subjects using ecologically valid fMRI paradigms. The ecological validity is particularly important when studying moral dilemmas in order to engage the subjects and make the dilemma as credible and perceptible as possible in order to get authentic reactions. To investigate the degree of similarity in how individual brains respond to the common movie stimulus, the brains of individual subjects are aligned and ISC between the hemodynamic activity time courses for each voxel are calculated across all subject pairs. ISC can be interpreted as reflecting synchronized neural activity and thus similarity of cerebral information processing across individuals . It has been shown that when viewing a feature film during brain scanning, both “higher-order” prefrontal cortical as well as basic sensory cortex regions become synchronized across subjects . Further, ISC may not only reflect mutual neuronal responses, but could provide the basis of inducing a specific common mind set, e.g. built by context information or perspective taking as well as predicting the actions of others . 

The model-free approach of ISC does not require any a priori, pre-designed modeling of the fMRI signal to carry out the analysis and thus provides a powerful tool to investigate neuronal mechanisms as the correlations are exclusively based on similarities between the subjects’ brain activities when they react to the various aspects of the complex movie . At the same time, ISC has been shown to reliably detect involved brain regions in complex experimental setups almost as sensitively as a model-based analysis . 

In study 1, we asked whether people discriminate behaviourally between relatives with genetic vs. non-genetic backgrounds: In an implicit association test (IAT ) the subjects’ reaction time when associating the words “sister” and “adopted sister” to positive or negative connoted adjectives was measured. Further, after watching the movie, the subjects were asked whether genetic vs. non-genetic relationship status mattered to them in the moral dilemma that they observed. 

In study 2, we tested how the subjects perceived moral dilemmas involving genetically related vs. unrelated individuals during fMRI. In a first task, the subjects watched the movie depicting the moral dilemma between two sisters after being primed that they were either genetically related sisters or sisters related only by adoption. Should knowledge of genetic relationship matter, we expect to see differences in the behavioural tests (IAT; questionnaires) as well as the neuronal mechanisms: we predict that brain regions known to be involved in processing of mentalizing , conflict resolution , emotion regulation , and moral dilemmas  would be activated differently under the two viewing conditions. 

Second, following the hypothesis that moral processing is the most relevant factor to distinguish between watching the movie when believing the sisters to be genetic or adopted (i.e., non-genetic), each subject underwent a moral decision task during fMRI scanning to evaluate specifically which brain areas are associated with the perception and processing of moral dilemmas during movie-watching. In this task the subjects had to decide whom to save from a dangerous area and had different choices including their own sister, best friend, and strangers, an experimental design similar to the classical moral trolley dilemma. Again, if the genetic relationship had an effect on the viewers, as has previously been shown behaviourally , we hypothesize that the subjects will show kinship preference by saving their sister over others and that similar brain areas are engaged both in the decision task and when watching the movie believing that the sisters are genetic. 


## Results 
  
### Study 1: Implicit association test (IAT) 
  
To examine if a possible general implicit bias against adopted sisters (that potentially modulates brain functions during movie viewing) underlies the subjects’ perception of the movie, we asked 30 subjects in a behavioural experiment to undergo an IAT . In this test, reaction times during assignment of positive and negative connoted words to the categories of sister and adopted sister showed that there is no such implicit bias: Out of 30 subjects, nine favoured a genetic sister, 13 a non-genetic sister, and eight had no preference (one sampled t-test t = −0.9564 p = 0.3468). The TOST procedure  indicated that the ratings of emotional closeness were significantly similar (observed effect size (d = −0.17) was significantly within the equivalent bounds of d = −0.68 and d = 0.68; t(29) = 2.77, p = 0.005). 


### Study 2: Inter-subject correlation (ISC) of fMRI during movie watching 
  
#### Inter-subject correlation (ISC) across all conditions 
  
During fMRI scanning the subjects watched a movie depicting a moral dilemma between two sisters, either believing the sisters are genetic sisters or that the younger sister was adopted at birth. In a first step, the overall ISC (22) of hemodynamic activity of an independent set of 30 subjects was calculated during first viewing of the movie (Fig.  ). Significant ISC was observed extensively in occipital lobes, posterior parietal areas, and temporal cortices. In the frontal cortex, areas in the lateral inferior frontal gyrus (IFG), lateral middle frontal gyrus (MFG), dorsolateral prefrontal cortex (DLPFC), dorsomedial prefrontal cortex (DMPFC) and ventromedial prefrontal cortex (VMPFC) showed ISC between all subjects. The location of all brain areas were defined using anatomical brain atlases as specifically the Harvard-Oxford Cortical Structural Atlas and the Juelich Histological Atlas.   
Inter-subject correlation (ISC) of all 30 subjects during the first viewing of the movie. On top row are shown lateral and on bottom row medial surfaces of left and right cerebral hemispheres. Red-yellow colours indicate areas of significant ISC during movie watching (FDR q &lt; 0.05). Abbreviations: ACC = anterior cingulate cortex, ANG = angular gyrus, CAL = calcarine gyrus, DLPFC = dorsolateral prefrontal cortex, DMPFC = dorsomedial prefrontal cortex, IFG = inferior frontal gyrus, IOG = inferior occipital gyrus, MFG = middle frontal gyrus, MOG = middle occipital gyrus, MTG = middle temporal gyrus, PCC = posterior cingulate cortex, SOG = superior occipital gyrus, SPL = superior parietal lobe, STS/STG = superior temporal sulcus and gyrus, VMPFC = ventromedial prefrontal cortex, a = anterior, d = dorsal, p = posterior, v = ventral. 
  


#### Differences in ISC between conditions 
  
In a second step, the ISC of all subjects (N = 30) were contrasted between the genetic vs. non-genetic relationship viewing conditions. As each participant watched the movie in the genetic and in the non-genetic condition on two different scanning days in a counterbalanced order, this is a within-subject design. There were robust differences between the two conditions in the ISC of hemodynamic activity of the subjects, despite 90% of the subjects self-reporting that it did not matter to them whether the sisters were related genetically or not. When the subjects watched the movie believing that they were seeing genetically related sisters, the ISC was significantly stronger in the superior temporal sulcus and gyrus (STS/STG), VMPFC, DLPFC, anterior cingulate cortex (ACC) and posterior cingulate cortex (PCC), IFG, insula, cuneus, precuneus, and superior parietal lobule (SPL) (Fig.  , Table  ).   
Differential ISC between the conditions of an assumed genetic and non-genetic sisters and BOLD time series from two exemplary single voxels. (  A  ) Significant differences in brain activity when all subjects watched the movie thinking that the sisters were genetically vs. non-genetically related (FDR q &lt; 0.05, t = 2.1447, for consistent illustration purposes, the figures shows t-values from 3 to 9 and −3 to −9) (N = 30, within subject design). Red-yellow colours indicate areas of significantly higher ISC when the subjects watched the movie as depicting genetically related, as compared to non-genetically related, sisters. Blue colour indicates areas showing significantly higher ISC in the reverse contrast. Abbreviations: ACC = anterior cingulate cortex, CAL = calcarine gyrus, DLPFC = dorsolateral prefrontal cortex, IFG = inferior frontal gyrus, IOG = inferior occipital gyrus, MOG = middle occipital gyrus, PCC = posterior cingulate cortex, SPL = superior parietal lobe, STS/STG = superior temporal sulcus and gyrus, VMPFC = ventromedial prefrontal cortex, a = anterior, d = dorsal, p = posterior, v = ventral. (  B  ) Across subjects averaged BOLD time series of two voxels, one in the area VMPFC that showed significantly higher ISC when the subjects were viewing the sisters as genetic and one time series of a voxel in area V1 (primary visual cortex) that did not show significant between-condition ISC differences. The red line plots the group mean BOLD in the genetic sisters condition and the blue line plots the group mean BOLD in non-genetic sisters condition over the whole length of the movie. Red and blue shades indicate the 25  and 75  percentile of the variance. 
    
Clusters size, peak coordinates and t value of all clusters of Experiment 1 (movie watching task). 
  

When the subjects thought that the sisters were non-genetic, higher ISC was observed mainly in the occipital cortex. Importantly, the movie stimulus was identical in both viewing conditions. 

To illustrate blood oxygenation level dependent (BOLD) time series of specific voxels, in Fig.  , the panel B shows the time series of two exemplary voxels, over the whole length of the movie, from: i) VMPFC that showed higher ISC when the subjects viewed the sisters as genetic and ii) from a voxel in the area V1 (primary visual cortex), which is an early sensory brain area that did not show any between condition differences in ISC. 

Self-ratings of emotional valence and arousal obtained after the scans were not significantly different between the conditions (Valence: r = 0.0075, p = 0.3458, Arousal: r = −0.0189, p = 0.6081) (Fig.  ). Further, the mean ISC of eye-movements (eISC) over time windows showed no significant difference between the groups of participants believing in genetic or non-genetic sisters (p = 0.3918) (see Fig.  ). Likewise, no significant difference could be found in the heart and breathing rate comparing the conditions of assumed genetic versus non-genetic sisters (with bootstrap over 5000 permutations, breathing rate: t-value = 0.430, p = 0.335: heart rate: t-value = −1.12, p = 0.129) (see Fig.  ).   
Experienced emotional valence and arousal as well as physiological parameters when perceiving the sisters in the movie as genetic vs. non-genetic. (  A  ) Shown are dynamic self-ratings of emotional valence and arousal over the whole time course of the movie obtained during re-viewing of the movie after the fMRI sessions when the sisters were viewed as genetically related (red) or non-genetic (blue). The ratings were highly similar and there were no time periods where significant between-condition differences could have been observed. Note that half of the subjects (N = 2 × 15) rated experienced arousal and the rest rated experienced valence after the first fMRI session followed by rating the other emotional dimension after the second fMRI session. Plotted are means for all subjects in the red line for assumed genetic sisters and the blue line for non-genetic sisters condition. Red and blue dashed lines show the 25  and 75  percentile of the variance. (  B  ) Eye gaze behavior (N = 29) in the movies when the sisters were perceived as either genetic (right) or non-genetic (left) shown as a violin plot with the red cross depicting the means and green squares the medians. There were no significant differences between the conditions. (  C  ) Breathing and heart rates (N = 30) when the sisters were perceived as either genetic (red) or non-genetic (blue). There were no significant differences between the conditions. Red line plots the condition with assumed genetic sisters and the blue line non-genetic sisters. Red and blue shade show the 25  and 75  percentile of the variance. 
  



### Comparison with the moral dilemma decision experiment 
  
To further examine which neurocognitive processes might be involved, we studied whether the brain areas showing higher ISC in the genetic condition overlap with those engaged during a modified moral dilemma task  analysed with general linear modelling (GLM) (Fig.  , Tables   and  ). Naturally, it should be kept in mind that while ISC and GLM analyses of brain hemodynamic activity can yield converging results , this is not necessarily the case, as high ISC can be observed also when the BOLD signals are small.   
Having to decide in a simulation between saving one’s sister, friend, and others from crisis regions elicited significant brain activity in the VMPFC, ACC, precuneus, DLPFC, IFG, insula, TPJ and MTG. These activations of subjects (N = 30) were obtained by contrasting the decision phases (from the point of revealing the individuals involved until the decision signaled by the subject’s button press) against non-decision phases (subjects watching the background story depicting the two crisis regions and how the subjects only have resources to save individuals from one of the crisis regions) (FDR q &lt; 0.05, t = 2.0384, for consistent illustration purposes, the figures shows t-values from 3 to 9) Left-lateralized motor and supplementary motor are probably explained by the button press that the subjects performed to announce their decision. Abbreviations: ACC = anterior cingulate cortex, DLPFC = dorsolateral prefrontal cortex, IFG = inferior frontal gyrus, IOG = inferior occipital gyrus, ITG = inferior temporal gyrus, MC = motor cortex, MFC = medial frontal cortex, MOG = middle occipital gyrus, MTG = middle temporal gyrus, PCC = posterior cingulate cortex. SMA = supplementary motor area, SPL = superior parietal lobe, TPJ = temporo-parietal junction,VMPFC = ventromedial prefrontal cortex, a = anterior, d = dorsal, p = posterior, v = ventral. 
    
Clusters size, peak coordinates and t value of all clusters of and Experiment 2 (moral dilemma decision task). 
  

When the same subjects who watched the movie had to decide between saving their sister, best friend, vs. stranger(s), in various combinations, from a crisis region, 93% of the subjects showed a clear kin preference by choosing their sister (even when associated with some strangers) rather than their best female friend (chi-squared test ×  = 43.4 p &lt; 4 × 10 ). Further, as can be seen in Fig.   the VMPFC, ACC, IFG, MTG, SPL, PCC, precuneus, DLPFC, and anterior insula were consistently involved in both making choices of whom to prefer in a morally dilemmatic situation and when observing the moral dilemma between a genetic vs. non-genetic sister. Importantly, ratings of emotional closeness were not significantly different for the subjects’ sisters and their best friends with an average of 9.28 (sisters) and 8.80 (friends) on a 1–10 scale (Wilcoxon signed rank test = 0.12, t-test, t = 1.64 p = 0.11). The TOST procedure  indicated that the ratings of emotional closeness were significantly similar (observed effect size (d = 0.33) was significantly within the equivalent bounds of d = −0.68 and d = 0.68, or in raw scores: −1.07 and 1.07, t(29) = −1.92, p = 0.032).   
Activity during moral dilemma decision making as disclosed by GLM analysis (red) and the ISC when the subjects believed in a genetic relationship between the sisters in the movie (blue), along with the overlap of these two maps (violet), as well as the more strict overlap with a conjunction test (yellow);  , (FDR q &lt; 0.05, t = 2.0384 for GLM and t = 2.1447 for ISC). Abbreviations: ACC = anterior cingulate cortex, DLPFC = dorsolateral prefrontal cortex, IFG = inferior frontal gyrus, MFC = medial frontal cortex, MOG = middle occipital gyrus, MTG = midddle temporal gyrus, PCC = posterior cingulate cortex. SPL = superior parietal lobe, VMPFC = ventromedial prefrontal cortex, a = anterior, d = dorsal, p = posterior, v = ventral. 
  

As measured with an independent group of subjects outside the scanner, reaction times for the moral-dilemma decision were significantly longer in the case of a decision between a group comprised of their friend and four strangers on one side and their sister alone on the other side as in the case that only comprised strangers (on both sides) (paired Wilcoxon rank sum test p = 0.0011973). 



## Discussion 
  
In the present study, we investigated whether refusing altruism from a sister is perceived differently when the viewers think that the sisters are genetically related vs. when they think that one of the sisters has been adopted at young age. The results of the IAT in study 1 suggest that the subjects do not show an implicit bias against adoptive sisters compared to genetic sisters in general. Also, when explicitly asked if the relationship (as genetic or adopted) would matter in the decision of an organ donation, most subjects (90%) report that this knowledge would not affect the decision. Further, heart rate and breathing rate exhibited no significant differences between the two conditions, and self-reported emotional valence and arousal was likewise similar between the conditions (Fig.  ), suggesting that there were no robust differences in experienced emotions between the conditions. 

In contrast, the ISC of the hemodynamic brain activity show a different picture: robust differences were observed in patterns of brain activity due to the mere knowledge of the genetic relationship between the sisters in the movie. Specifically, there were multiple brain regions showing significantly higher ISC when the subjects thought that they are seeing a young girl refusing to donate her organ to save her genetic, as opposed to non-genetic, sister (Fig.  ). These areas included VMPFC, DLPFC, ACC, PCC, insula, precuneus, and SPL. While we would caution against drawing conclusions of specific cognitive functions involved on the basis of observed differences in brain activity, these brain regions have been previously shown to be associated with moral and emotional conflict regulation , decision making , mentalizing , and perspective taking , thus tentatively suggesting more uniform engagement of such neurocognitive functions when observing the dilemma of organ donation between genetic sisters. 

In the reverse contrast (i.e. when the subjects thought the sisters were non-genetic), higher ISC was observed in brain areas in the occipital cortex, conventionally associated mainly with visual perception . One possible explanation could potentially be that in the case of a non-genetic relationship between the sisters, the processing of complex social conflict associated with the moral dilemma is less demanding, and therefore leaves room for the subjects to focus on the visual aspects of the movie. However, eye-movements were not significantly different between the two viewing conditions, suggesting that differences in attention to movie events did not explain the observed robust differences in ISC. 

To specifically test for the possibility that the differences in ISC between the conditions reflected differences in moral evaluation, we compared the areas showing differences in ISC with areas activated when the subjects engaged in a separate moral-decision making control task. In this control task, the subjects had to make choices about saving people (including their sister, best friend, and strangers, in various combinations) from disaster. As each subject made only one decision that contrasted saving the friend over strangers and one decision of saving the friend over the own sister plus four strangers (as well as four decisions that contrasts the sister to groups of others), the statistical power in this experimental design was unfortunately not sufficient to differentiate directly between brain responses during decisions to save the sister vs. the friend. Rather, the results should be viewed as localization of brain regions involved in making moral decisions, yet also modulated by differences in, e.g., executive control, readiness for action, and attention, between the passive perception of the story and active decision making. However, significantly longer reaction times suggested increased difficulty when having to choose between a sister and the friend together with four strangers while areas in the VMPFC, DLPFC, ACC, PCC, precuneus, IFG, MTG, SPL and anterior insula were consistently involved in both making choices of whom to prefer in a morally dilemmatic situation and when observing the moral dilemma between a genetic vs. non-genetic sister (Fig.  ). This overlap in engaged brain regions suggests that processing of moral dilemmas took place during movie watching when the sisters were understood to be genetically related. It is significant that the brain regions flagged up in this analysis are those known to be involved in processing moral dilemmas and mentalizing. The DLPFC has been reported to play a role in overcoming a primary moral judgment in favour of greater welfare  and in cortical emotional processing , while the MTG has been implicated in attributing mental states as well as ingroup/outgroup distinctions , the ACC has been reported to be engaged in resolving conflicts , and the SPL, precuneus, and PCC have been implicated in mentalizing and perspective taking . Further, the VMPFC has been associated with viewing moral conflicts, making moral decisions, attributing mental states to self and others, adopting another person’s perspective, and evaluating their beliefs . 

Thus, the differences in ISC between the conditions appear to have arisen due to the knowledge about the sisters’ relationship influencing cognitive evaluation of the moral dilemma depicted in the movie. The contrast to the behavioural results assessed in study 1, where we find that behavioural decisions were not influenced by knowledge of the relationship, is particularly interesting since it suggests that differential processing is taking place under the surface. 

There could be at least two possible explanations for these findings. First, the study subjects might have purposely been hiding their “real” honest opinions as they might have not been willing to reveal these to the researchers, presumably because of social pressure against discriminating between genetic and adoptive siblings. However, the IAT is an implicit test for biases (it uses differences in reaction times for associations of a specific term with positively and negatively connoted words), so subjects are not aware of their performance on this task. Further, they do not know the exact measures which are used to calculate the IAT score, thus making it difficult to engineer potential biases; hence, it is very difficult, even impossible, to manipulate an IAT response in a desired direction . Thus, while a conscious manipulation of reported opinions would be possible in the open-format questionnaire (when asked if the relationship of the sisters matters in the situation of organ donation), it is very unlikely in the IAT. However, Liberman   et al  .  showed that, when judging incest reprehensibility, the coresidence of siblings is a stronger factor than the assumed relationship status and in when the two parameters are in conflict, the time spent in coresidence outweighs the belief of kin relation. As in this study the subjects were told that the apparent adoption took place as the younger sister was a newborn (implying coresidence of the sisters in both the genetic and the adoption case), we suggest, in accordance with Liberman   et al  ., that the factor of coresidence was given greater account than the kin relationship and thus the subjects’ explicit answers in the questionnaire could probably be seen/taken as truthful i.e. reporting authentic, honest thoughts. 

A second possible explanation for our findings is that, as the results of the implicit testing show, the study subjects indeed did not show any biases behaviourally and still pursued different ways of considering the case of a relationship by genes or by adoption, with resulting differences in brain activity patterns. These results show that an event that is behaviourally counter-intuitive (e.g. refusing to help a sister to prolong her life) needs different and potentially more intensive mental processing when the sisters are related genetically compared to adoptive sisters. As the differences in the brain activity patterns between the genetic and the adopted condition particularly comprise areas known to be involved in processing moral dilemmas and mentalizing as the VMPFC, DLPFC, ACC, PCC, precuneus, IFG, MTG, SPL, and anterior insula, we suggest that the study subjects’ expectations of morality are more strongly violated when close genetic relatives refuse to help each other than when unrelated individuals behave this way, despite their close social relation (adoption). 

Notwithstanding these points, we wish to caution the reader to keep in mind the caveats associated with reverse inference  (although see ). Specifically, even as we are suggesting that an activation observed in a certain region is indicative of a specific cognitive process based on results of previous research documented in the literature, it is possible that the activation of that region in the present study was due to some other cognitive process. This is because in general any given brain region is involved in multiple cognitive functions, thus making it difficult to infer with certainty the cognitive functions involved in a task based on brain regions that are activated. 

An alternative possibility is that the different measures operate at different levels of cognition: The null result in the IAT could be relying on a more basic level of attention to the social knowledge, whereas the questionnaire requires high level explicit cognition and the ISC during movie perception reflects some intermediate level of cognition. Had we thought to include them, other behavioural tests might have revealed more detail and background information on the subjects. Finally, it is always possible that something other than moral considerations could underlie the differences in brain patterns that we found, although, given the brain areas that show differences, this is unlikely. 

In summary, we observed robust differences in brain activity when subjects viewed a movie depicting refusal to donate an organ to a genetic vs. non-genetic sister. These differences in brain activity were observed despite the subjects self-reporting that the relational status of the sisters did not make any difference to them. Areas of increased synchrony in the case of genetic sisters overlapped with those activated in a separate moral dilemma decision task. Taken together, our results suggest that the precuneus, MTG, insula, SPL, and the VMPFC, along with the associated cognitive processes (i.e., moral and emotional conflict regulation), decision making, mentalizing and perspective taking are synchronized across subjects more robustly when they are viewing refusal of altruism from genetic as opposed to non-genetic, kin. Overall, these findings are fundamentally important for understanding social cognition, a pivotal ability that makes us human and, among other things, enables the existence of societies. Our findings point out that the perceived relationship of interacting persons robustly modulates how the brains of spectators process third-party interactions. This is highly significant given that majority of research to date on social cognition has been on strangers, whereas most of our social interactions take place between family members, friends, and acquaintances. 


## Material and Methods 
  
### Subjects 
  
We studied 33 healthy female subjects  (19–39 years, mean age of 26 years, one left-handed, laterality index of right handed 84.5%). None of the subjects reported any history of neurological or psychiatric disorders. When asked, all subjects reported either normal vision or corrected to normal vision by contact lenses. Three subjects were excluded due to discomfort in the scanner, so that the final analysis included 30 subjects. 27 of them were native Finnish speakers and three were native Russian speakers. All subjects were sufficiently proficient in English to follow the dialogue in the movie without subtitles. The experimental protocols were approved by the research ethics committee of the Aalto University and the study was carried out with their permission (Lausunto 9 2013 Sosiaalisen kognition aivomekanismit, 8.10.2013) and in accordance with the guidelines of the declaration of Helsinki . Written informed consent was obtained from each subject prior to participation. 


### Stimuli and Procedure 
  
The study consisted of two experiments. In the first experiment, the feature film   My Sister’s Keeper”   (dir. Nick Cassavetes, 2009, Curmudgeon Films), edited to 23 minutes and 44 s, (of which 14 min 17 s (60%) portray the theme of refusal of the organ donation),with the main story line retained, was shown to the subjects during fMRI. This shortened version of the movie focuses on the moral dilemma of the protagonist Anna to donate one of her kidneys to her sister Kate, who is fatally ill from cancer. In the course of the movie, Anna refuses to donate and Kate dies. The reason for Anna refusing to donate the kidney was not revealed to the subjects until after the experiment. The movie was shown to the subjects in the scanner four times in two separate scanning sessions on two different days. For each viewing of the movie the instructions were varied regarding the information about the sister’s relationship and the perspective to take in this viewing (Fig.  ). Each subject thus watched the movie assuming that the sisters were genetic sisters or that the younger sister Anna had been adopted as a newborn. In addition each subject was asked to take either the perspective of the potential donor (Anna) or the perspective of the potential recipient (Kate) on separate viewings (and both under the condition of a genetic or non-genetic relation background). The order of the different viewing conditions was counterbalanced between the subjects.   
Experimental procedure and ISC analysis in the movie watching task. (  A  ) Every subject watched the movie four times, in a 2 × 2 design assuming that the movie characters are either genetic sisters or not genetically related and taking the perspective of the to-be-donor sister Anna or the to-be-recipient sister Kate. The order of all the conditions were counter-balanced. (  B  ) Time series from each voxel from all the fMRI recordings are compared across subjects in pairwise correlations to obtain the mean inter-subject-correlation (ISC). 
  

In the second experiment, each subject carried out a moral-dilemma decision task during fMRI in order to localize brain regions that are related to moral decision making. For this purpose, a modified version of the classical trolley dilemma , was shown to the subjects. Each subject had to choose between rescuing different individuals, including unknown individuals, their sister and a best female friend. A presentation showing text and pictures told a story about civil unrest in a fictive distant country. This country had two parts: one part very dangerous and the other much less dangerous. Different people are in both parts of the country. Subjects were also told that as they were very rich and owned an airplane, they could go there and rescue some of the people. However, due to the circumstances in the country they had to decide which group of people to rescue. The two choices were always a group of five individuals on one side and a single person on the other. In seven runs the identity of the involved individual(s) was varied using the real names of the subject’s sister and best female friend. The 7 runs were: 1. All persons are unknown; 2. Sister is with four others in the dangerous part of the country, the single person is unknown; 3. Five persons are in the dangerous part of the country, the single person is the sister; 4. Five persons are in the dangerous part of the country, the single person is the friend; 5. Sister is with four others in the dangerous part of the country, the single person is the friend; 6. Friend is with four others in the dangerous part of the country, the single person is the sister; 7. Sister is with four others in the less dangerous part of the country, the single person is unknown. Responses in the moral dilemma decision task were recorded with a button press on a LUMItouch keypad (Photon Control Inc.8363, Canada). For the all questions, it was calculated with which percentage the sister was chosen over the friend and the stranger(s); statistical significance was tested with a Chi  test. 


### fMRI acquisition 
  
Before each scan the subjects were informed about the scanning procedures and asked to avoid bodily movements during the scans. All stimuli were presented to the subject with the Presentation software (Neurobehavioral Systems Inc., Albany, CA, USA), synchronizing the onset of the stimuli with the beginning of the functional scans. The movie was back-projected on a semitransparent screen using a data projector (PT-DZ8700/DZ110X Series, Panasonic, Osaka, Japan). The subjects viewed the screen at 33–35 cm viewing distance   via   a mirror located above their eyes. The audio track of the movie was played to the subjects with a Sensimetrics S14 audio system (Sensimetrics Corporation Malden, USA). The intensity of the auditory stimulation was individually adjusted to be loud enough to be heard over the scanner noise. The brain-imaging data were acquired with a 3T Siemens MAGNETOM Skyra (Siemens Healthcare, Erlangen, Germany), at the Advanced Magnetic Imaging center, Aalto University, using a standard 20-channel receiving head-neck coil. Anatomical images were acquired using a T1-weighted MPRAGE pulse sequence (TR 2530 ms, TE 3.3 ms, TI 1100 ms, flip angle 7°, 256 × 256 matrix, 176 sagittal slices, 1-mm3 resolution). Whole-brain functional data were acquired with T2*-weighted EPI sequence sensitive to the BOLD contrast (TR 2000 ms, TE 30 ms, flip angle 90, 64 × 64 matrix, 35 axial slices, slice thickness 4 mm, 3 × 3 mm in plane resolution). 

A total of 712 whole-brain EPI volumes were thus acquired for each movie viewing. The number of whole-brain EPI volumes for the moral dilemma decision task varied individually depending on the decision made by each subject (median 267 whole-brain EPI volumes). Heart pulse and respiration were monitored with the Biopac system (Biopac Systems Inc., Isla Vista, California, USA) during fMRI. Instantaneous values of heart rate and breathing rate were estimated with Drifter software package  (  http://becs.aalto.fi/en/research/bayes/drifter/  ). 


### fMRI preprocessing 
  
Standard fMRI preprocessing steps were applied using the FSL software (  www.fmrib.ox.ac.uk  ) and custom MATLAB code (available at   https://version.aalto.fi/gitlab/BML/bramila/  ). Briefly, EPI images were corrected for head motion using MCFLIRT. 

Then they were coregistered to the Montreal Neurological Institute 152 2 mm template in a two-step registration procedure using FLIRT: from EPI to subject’s anatomical image after brain extraction (9 degrees of freedom) and from anatomical to standard template (12 degrees of freedom). Further, spatial smoothing was applied with a Gaussian kernel of 6 mm full width at half maximum. High pass temporal filter at a cut-off frequency of 0.01 Hz was used to remove scanner drift. To further control for motion and physiological artefacts, BOLD time series were cleaned using 24 motion-related regressors, signal from deep white matter, ventricles and cerebral spinal fluid locations (see ) for details, cerebral spinal fluid mask from SPM8 file csf.nii, white matter and ventricles masks from Harvard Oxford atlas included with FSL). As a measure of quality control we computed framewise displacement to quantify instantaneous head motion. Out of all the 120 runs (30 subjects, 4 sessions each), 97.5% of the runs (117 runs) had 90% of time points (640 volumes) with framewise displacement under the 0.5 mm threshold suggested in . For the remaining three runs, the number of time points under 0.5 mm were 639 (89.7%), 633 (88.9%), 489 (68.7%), i.e. only one session had a considerable amount of head motion. While head motion is a concern in connectivity studies as it can increase spurious BOLD time series correlations that are affected by the same amount of instantaneous head motion, with across-brain time series correlation, head motion is expected to reduce the SNR. However, to make sure that head motion similarity did not explain any group difference, we computed the same permutation test for the ISC also for average framewise displacement by estimating the similarity of two subjects as the distance between their average framewise displacement value. We found that similarity in average head motion was not different between the two viewing conditions (t-value = 0.255; p = 0.398 obtained with 5000 permutations). 


### Inter-subject correlation (ISC) analysis of brain activity during movie watching 
  
To investigate how similar the brain activity was across subjects in the different experimental conditions, we performed inter-subject correlation (ISC) using the isc-toolbox (  https://www.nitrc.org/projects/isc-toolbox/  ) . For each voxel the toolbox computes a similarity matrix between subject pairs and within same subject in all conditions, with the conditions being: (i) shared assumption that the movie’s sisters are genetically related, (ii) shared assumption that the younger sister was adopted, (iii) shared perspective of the to-be-organ-donor, and (iv) shared perspective of the to-be-organ-recipient. The total size of the similarity matrix is then 120 × 120 (4 conditions × 30 subjects) with each subject having two viewings for the genetic and two viewings for the non-genetic condition. The comparison between the conditions of the sisters to be perceived as either genetic sisters or non-genetic sisters results thus in a total of 1740 pairs per condition, as the similarity of BOLD time series during the two viewings (in either the genetic or the non-genetic condition) of each subject is compared with the two respective viewings of the other N-1 subjects. As the order of subjects does not matter, the final number of pairs in same conditions will be 2*2*(N-1)*N/2 = 1740 with N = 30. Each value of the correlation matrix is a result of the correlation between the BOLD time series of the pair of subjects considered for the selected voxel. We computed differences between experimental conditions by first transforming the correlation values into z-scores with the Fisher Z transform and then computing t-values and corresponding p-values using a permutation based approach . 

The Fisher-Z transformed correlations of the two perspectives were pooled for either the genetic or the non-genetic sisterhood. 

Correction for the multiple comparison was performed with Benjamini-Hochberg false discovery rate (BH-FDR) correction at a q &lt; 0.05, corresponding to a t-value threshold of 2.133. For visualization purposes, all results were also cluster corrected by removing any significant cluster smaller than 4 × 4 × 4 voxels. Summary tables were generated with an increased t-value threshold of 3. For the conjunction or “intersection–union test”  the p values of the ISC and GLM results are pulled together by considering the maximum p-value at each voxel. Then, multiple comparisons correction is performed with the Benjamini-Hochberg false discovery rate procedure with an FDR threshold equal to q &lt; 0.05. 

Unthresholded statistical parametric maps can be found in neurovault:   http://neurovault.org/collections/WGSQZWPH/  . 

#### Perspective taking 
  
In the movie-viewing experiment, in addition to having the subjects to watch the movie in the conditions of sisters related by birth or by adoption, we had altogether four runs, so that on two of the runs the subjects were asked to view the movie from the perspective of the sister who was expected to donate the organ, and on two of the runs from the perspective of the to-be-recipient sister. Thus, there was one run wherein the subjects viewed the movie from the perspective of the to-be- donor thinking that the sisters were genetic, one run wherein the subjects viewed the movie from the perspective of the to-be- donor thinking that the sisters were non-genetic, one run wherein the subjects viewed the movie from the perspective of the to-be- recipient thinking that the sisters were genetic, and one run wherein the subjects viewed the movie from the perspective of the to-be- recipient thinking that the sisters were non-genetic. 

As the results of this task open up a completely other aspect of the experiment with various results to discuss, which go beyond the scope and the space limitation of this article, they will be reported separately elsewhere. These conditions are mentioned here for reasons of describing the experimental procedures thoroughly so that it would be possible for others to replicate the study should they wish to do so. 



### General linear model analysis of the fMRI data acquired during the control task 
  
A moral dilemma decision task was performed by all subjects to localize regions involved in moral processing. The moral dilemma decision task was analyzed with a general linear model approach using the SPM12 software (  www.fil.ion.ucl.ac.uk/spm  ). To distinguish between moments of decision in the moral dilemma and the simple perception of the presentation, we created a temporal model of the occurrence of decision moments during the experiment. The decision regressor included time points from the revelation of the identity of involved individuals to the moment of decision indicated by button press. The activity during these time points was compared to the activity in all other time points of the task, including telling the background story of the moral dilemma in the presentation. Regressors were convolved with canonical hemodynamic response function to account for hemodynamic lag. From the preprocessed input data (see above) low-frequency signal drifts were removed by high-pass filtering (cutoff 128 s). First, individual contrast images were generated for the main effects of the regressors, then first level analyses were subjected to second-level analyses in MATLAB using one-sample   t  -test to test which brain areas showed significant activations in decision vs. no decision moments in a one-sample   t  -test over subjects. Statistical threshold was set at   p   &lt; 0.05 (cluster-corrected using the threshold free cluster enhancement approach implemented by FSL randomize with 5000 permutations). 


### Recording of eye-movements 
  
Eye movements were recorded during fMRI scanning from all subjects with an EyeLink 1000 eye tracker (SR Research, Mississauga, Ontario, Canada; sampling rate 1000 Hz, spatial accuracy better than 0.5°, with a 0.01° resolution in the pupil-tracking mode). Due to technical problems, 4 subjects had to be excluded from the final data analysis (with the rejection criteria of blinks maximum 10% of the duration of the scan and majority of blinks and saccades less than 1 second in duration). In addition, a part of recordings from some additional subjects had to de discarded due to the same criteria mentioned above, resulting in 61 recorded files with sufficient quality, with 35 files remaining in the genetic condition and 26 remaining files for the non-genetic condition. Prior to the experiment the eye tracker was calibrated once with a nine-point calibration. 

Saccade detection was performed using a velocity threshold of 30°/s and an acceleration threshold of 4000°/s2. Because the experiment was relatively long and no intermediate drift correction was performed, we retrospectively corrected the mean effect of the drift. We first calculated the mean of all fixation locations over the entire experiment for each subject, and then rigidly shifted the fixation distributions so that the mean fixation location coincided with the grand mean fixation location over all subjects. 


### Eye-movement analysis 
  
Subject-wise gaze fixation distributions were compared across the genetic vs. non-genetic conditions in the movie viewing task. Individual heat maps were generated by modelling each fixation as a Gaussian function using a Gaussian kernel with a standard deviation of 1degree of visual angle and a radius of 3 standard deviations. The heat maps were generated in time windows of 2 seconds corresponding to the TR used in the fMRI measurements. Spatial similarities between each pair of heat maps across the eye-tracking sessions were calculated using Pearson’s product-moment correlation coefficient (inter-subject correlation of eye gaze, eyeISC ). In the end a similarity matrix was obtained with correlations between each pair for each of the 712 time windows. 

First, the mean eISC scores over all 712 time windows were examined. These mean scores were acquired by extracting the mean of Fisher’s Z-transformed correlation scores and then transforming these mean values back to the correlation scale before the statistical analysis. The statistical significance of the group differences was analysed by contrasting pairs in which both subjects assumed a genetic relationship with pairs in which both subjects assumed the younger sister to be adopted. Non-parametric permutation tests with a total of 100000 permutations were used to avoid making assumptions about the data distribution. In this procedure the data were mixed randomly to change groupings and differences in the resulting new randomised groups were used to form an estimated distribution of the data. A comparison of how many of the permuted random partitions into groups build a more extreme group mean difference that the one observed with the original grouping yielded the final p-values. 


### Behavioral Measurements and Self-reports 
  
#### Valence and Arousal measurements 
  
The subjects self-reported emotions they had experienced during movie viewing. This was carried out after the fMRI experiment by viewing the movie again (Full procedures have been described in an earlier publication ). Two aspects of emotional experience were rated: emotional valence (positive-negative scale) and arousal which were acquired on separate runs. While watching the movie in the middle of the screen, the subjects could move a small cursor on the right side of the screen up and down on a scale using the computer mouse to report their current state of valence or arousal using a web tool   https://version.aalto.fi/gitlab/eglerean/dynamicannotations  . The self-ratings were collected at 5 Hz sampling rate. 


#### Behavioral questionnaires 
  
The subjects were asked after the first fMRI session five short freeform questions about their perception of the movie, specifically about how easy it was to take one or the other perspective, and whether they would have donated their kidney if in place of the movie protagonist. After the second fMRI session all subjects were debriefed by showing them the ending of the original movie, where it is revealed that the sick sister had wished for the healthy sister to refuse donating her kidney. Afterwards they were asked if seeing the real ending changed their opinion on the roles of the two movie protagonists. 

As an additional self-report measure, the subjects’ disposition for catching emotions from others was assessed with two emotional empathy questionnaires: Hatfield’s Emotional Contagion Scale  and the BIS/BAS scale . Every subject also filled in a questionnaire quantifying their social network , including their emotional closeness to their sister and best friend. The names of the sister and best friend were obtained from this questionnaire for the moral dilemma task. 



### Analysis of behavioral measurements 
  
#### Valence and arousal measurements 
  
To test whether dynamic valence and arousal were different between the genetic and non-genetic condition, we first computed inter-subject similarity matrices using valence and arousal rating time-series. These were compared against a similarity matrix for the experimental conditions of the viewing preceding the valence/arousal rating, i.e. the model tests for the case where individuals are more similar within the same condition (genetic or non-genetic), but dissimilar between conditions. Tests were performed using Mantel test with 5000 permutations. We also performed a test to see if subjects who were rating arousal and valence for the genetic condition had a stronger group similarity than subjects who rated arousal and valence for the non-genetic condition. Tests were performed using permutation based t-tests. As dynamic ratings can also be different in specific time points, we also performed a permutation-based t-test on valence and arousal values at each time point corrected for multiple comparisons across time. 


#### Heart rate and breathing rate analysis 
  
Differences between experimental conditions were computed in the same way as in the ISC analysis: Correlation values were first transformed into z-scores with the Fisher Z’s transform and then a permutation based approach was used to compute t-values and corresponding p-values . Correction for the multiple comparisons was performed with Benjamini-Hochberg false discovery rate (BH-FDR) correction at a q &lt; 0.05, corresponding to a t-value threshold of 2.133. 



### Behavioral measurements with a new group of subjects 
  
Subsequent to the fMRI experiments a new group of 30 subjects (all female, and having a sister, 18–33 years, mean age 25.5 years, right handed) were recruited for two further behavioral measurements. The subjects first performed an implicit association test (IAT). The IAT measures attitudes and beliefs that might not be consciously self-recognized by the subject or attitudes that the subjects are unwilling to report. By asking the subjects to sort, as quickly as possible, positively and negatively connoted words into categories, the IAT can measure the reaction times of the association process between the categories and the evaluations (e.g., good, bad). It has been shown in previous studies that making a response is easier and thus faster if the category is matching the implicit evaluation the subject bears in mind . In this study the two categories were “genetic sister” (sisko) and “adopted sister” (adoptiosisko). The two categories were paired in different randomized runs with positive or negative words, thus the experiment comprised separate runs asking the subjects to either match the positive words with the category “genetic sister” and negative words with the category “adopted sister” or vice versa to match positive words with the category “adopted sister” and negative words with the category “genetic sister”. The order in which the runs are presented counter-balanced across subjects and categories switched their localization on the screen in different runs to be on the left or right side of the screen to the same extent. Subjects were asked to press a key with either the right or the left hand and thus assign the evaluation word to one category on either the left or right hand side of the computer screen. With the experiment going on, the number of trials in this part of the IAT is increased in order to minimize the effects of practice. The IAT score is based on how long it takes a person to sort the words with the condition associating positive words and genetic (and negative and adopted) in contrast to negative words and genetic (and positive words and adopted). If an implicit preference exist for one of the categories subjects would be faster to match positive words to that category relative to the reverse. Data were analysed using Matlab. Similarity between subjects’ scores were examined TOST testing . As a second task, reaction times for the moral decision task were measured with the same group of subjects that underwent the IAT. As a difference to the decision task performed during fMRI scanning the order of the decisions was randomized (with easy decision including only strangers and difficult decisions including the sister on one side and the friend on the other). Reaction times were measured as the time between the onset of the slide revealing the identity of the involved individuals and the button press of the subject that related her decision. 


### Data availability 
  
The data that support the findings of this study are available on request from the corresponding author MBT. The data are not publicly available due to a prohibition by the Finnish law: Juridical restrictions set by the Finnish law prevent public access to the collected data, be it anonymized or non-anonymized, when data are recorded from human individuals. As the consent given by the subjects only applies to the specific study reported in our manuscript, no portion of the data collected could be used or released for use by third parties.</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (2)</summary><details class="inner-accordion"><summary>Table 1 (Tab1) - Clusters size, peak coordinates and t value of all clusters of Experiment 1 (movie watching task).</summary><div class="table-html"><table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Clusters size, peak coordinates and t value of all clusters of Experiment 1 (movie watching task).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Cluster Label: movie watching task</th><th colspan="1" rowspan="1">Cluster extent (voxels)</th><th colspan="1" rowspan="1">x MNI (mm)</th><th colspan="1" rowspan="1">y MNI (mm)</th><th colspan="1" rowspan="1">z MNI (mm)</th><th colspan="1" rowspan="1">Peak value (T-value)</th></tr></thead><tbody><tr><td colspan="6" rowspan="1">
<bold>clusters genetic &gt; non-genetic</bold>
</td></tr><tr><td colspan="1" rowspan="1">Superior parietal lobule (R)</td><td colspan="1" rowspan="1">8794</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">−86</td><td colspan="1" rowspan="1">44</td><td colspan="1" rowspan="1">14.6272</td></tr><tr><td colspan="1" rowspan="1">Ventromedial prefrontal cortex (R)</td><td colspan="1" rowspan="1">4099</td><td colspan="1" rowspan="1">0</td><td colspan="1" rowspan="1">54</td><td colspan="1" rowspan="1">−8</td><td colspan="1" rowspan="1">8.3312</td></tr><tr><td colspan="1" rowspan="1">Superior temporal gyrus (L)</td><td colspan="1" rowspan="1">2182</td><td colspan="1" rowspan="1">−42</td><td colspan="1" rowspan="1">−26</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">16.974</td></tr><tr><td colspan="1" rowspan="1">Superior temporal gyrus (R)</td><td colspan="1" rowspan="1">1138</td><td colspan="1" rowspan="1">62</td><td colspan="1" rowspan="1">−26</td><td colspan="1" rowspan="1">4</td><td colspan="1" rowspan="1">13.0493</td></tr><tr><td colspan="1" rowspan="1">Putamen (R)</td><td colspan="1" rowspan="1">616</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">16</td><td colspan="1" rowspan="1">−10</td><td colspan="1" rowspan="1">7.2211</td></tr><tr><td colspan="1" rowspan="1">Insula (L)</td><td colspan="1" rowspan="1">266</td><td colspan="1" rowspan="1">−40</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">−10</td><td colspan="1" rowspan="1">6.9642</td></tr><tr><td colspan="1" rowspan="1">Inferior temporal gyrus (R)</td><td colspan="1" rowspan="1">145</td><td colspan="1" rowspan="1">52</td><td colspan="1" rowspan="1">−46</td><td colspan="1" rowspan="1">−30</td><td colspan="1" rowspan="1">7.432</td></tr><tr><td colspan="1" rowspan="1">Middle temporal gyrus (R)</td><td colspan="1" rowspan="1">110</td><td colspan="1" rowspan="1">66</td><td colspan="1" rowspan="1">−4</td><td colspan="1" rowspan="1">−16</td><td colspan="1" rowspan="1">10.4489</td></tr><tr><td colspan="1" rowspan="1">Inferior frontal gyrus (L)</td><td colspan="1" rowspan="1">89</td><td colspan="1" rowspan="1">−36</td><td colspan="1" rowspan="1">34</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">5.3232</td></tr><tr><td colspan="1" rowspan="1">Superior frontal gyrus (R)</td><td colspan="1" rowspan="1">89</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">62</td><td colspan="1" rowspan="1">26</td><td colspan="1" rowspan="1">6.2544</td></tr><tr><td colspan="1" rowspan="1">Inferior parietal lobule (L)</td><td colspan="1" rowspan="1">86</td><td colspan="1" rowspan="1">−52</td><td colspan="1" rowspan="1">−60</td><td colspan="1" rowspan="1">48</td><td colspan="1" rowspan="1">5.0215</td></tr><tr><td colspan="1" rowspan="1">Postcentral gyrus (R)</td><td colspan="1" rowspan="1">81</td><td colspan="1" rowspan="1">56</td><td colspan="1" rowspan="1">−6</td><td colspan="1" rowspan="1">38</td><td colspan="1" rowspan="1">7.0252</td></tr><tr><td colspan="1" rowspan="1">Superior parietal lobule (L)</td><td colspan="1" rowspan="1">73</td><td colspan="1" rowspan="1">−14</td><td colspan="1" rowspan="1">−58</td><td colspan="1" rowspan="1">72</td><td colspan="1" rowspan="1">7.4174</td></tr><tr><td colspan="6" rowspan="1">
<bold>clusters non-genetic &gt; genetic</bold>
</td></tr><tr><td colspan="1" rowspan="1">Inferior occipital gyrus (R)</td><td colspan="1" rowspan="1">1938</td><td colspan="1" rowspan="1">40</td><td colspan="1" rowspan="1">−66</td><td colspan="1" rowspan="1">−8</td><td colspan="1" rowspan="1">−12.3679</td></tr><tr><td colspan="1" rowspan="1">Middle occipital gyrus (L)</td><td colspan="1" rowspan="1">604</td><td colspan="1" rowspan="1">−32</td><td colspan="1" rowspan="1">−84</td><td colspan="1" rowspan="1">4</td><td colspan="1" rowspan="1">−9.7755</td></tr><tr><td colspan="1" rowspan="1">Cerebellar crus II (R)</td><td colspan="1" rowspan="1">318</td><td colspan="1" rowspan="1">30</td><td colspan="1" rowspan="1">−86</td><td colspan="1" rowspan="1">−32</td><td colspan="1" rowspan="1">−7.3011</td></tr><tr><td colspan="1" rowspan="1">Temporal pole (middle) (R)</td><td colspan="1" rowspan="1">171</td><td colspan="1" rowspan="1">38</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">−34</td><td colspan="1" rowspan="1">−6.6407</td></tr><tr><td colspan="1" rowspan="1">Inferior temporal gyrus (L)</td><td colspan="1" rowspan="1">165</td><td colspan="1" rowspan="1">−54</td><td colspan="1" rowspan="1">−64</td><td colspan="1" rowspan="1">−4</td><td colspan="1" rowspan="1">−8.2858</td></tr><tr><td colspan="1" rowspan="1">Angular gyrus (L)</td><td colspan="1" rowspan="1">147</td><td colspan="1" rowspan="1">−42</td><td colspan="1" rowspan="1">−54</td><td colspan="1" rowspan="1">28</td><td colspan="1" rowspan="1">−5.5752</td></tr><tr><td colspan="1" rowspan="1">Superior temporal gyrus (R)</td><td colspan="1" rowspan="1">95</td><td colspan="1" rowspan="1">64</td><td colspan="1" rowspan="1">−28</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">−8.3381</td></tr><tr><td colspan="1" rowspan="1">Inferior frontal gyrus (R)</td><td colspan="1" rowspan="1">94</td><td colspan="1" rowspan="1">52</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">16</td><td colspan="1" rowspan="1">−5.7279</td></tr><tr><td colspan="1" rowspan="1">Superior temporal gyrus (R)</td><td colspan="1" rowspan="1">87</td><td colspan="1" rowspan="1">64</td><td colspan="1" rowspan="1">0</td><td colspan="1" rowspan="1">−4</td><td colspan="1" rowspan="1">−9.6944</td></tr><tr><td colspan="1" rowspan="1">Precentral gyrus (L)</td><td colspan="1" rowspan="1">86</td><td colspan="1" rowspan="1">−44</td><td colspan="1" rowspan="1">10</td><td colspan="1" rowspan="1">52</td><td colspan="1" rowspan="1">−6.1405</td></tr><tr><td colspan="1" rowspan="1">Superior frontal gyrus (L)</td><td colspan="1" rowspan="1">86</td><td colspan="1" rowspan="1">−8</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">60</td><td colspan="1" rowspan="1">−7.8522</td></tr></tbody></table></table-wrap>
</div></details><details class="inner-accordion"><summary>Table 2 (Tab2) - Clusters size, peak coordinates and t value of all clusters of and Experiment 2 (moral dilemma decision task).</summary><div class="table-html"><table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Clusters size, peak coordinates and t value of all clusters of and Experiment 2 (moral dilemma decision task).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Cluster Label: moral dilemma decision task</th><th colspan="1" rowspan="1">Cluster extent (voxels)</th><th colspan="1" rowspan="1">x MNI (mm)</th><th colspan="1" rowspan="1">y MNI (mm)</th><th colspan="1" rowspan="1">z MNI (mm)</th><th colspan="1" rowspan="1">Peak value (T-value)</th></tr></thead><tbody><tr><td colspan="1" rowspan="1">Cerebellar lobule VI (R)</td><td colspan="1" rowspan="1">10417</td><td colspan="1" rowspan="1">20</td><td colspan="1" rowspan="1">−54</td><td colspan="1" rowspan="1">−30</td><td colspan="1" rowspan="1">10.1921</td></tr><tr><td colspan="1" rowspan="1">Superior frontal gyrus (L)</td><td colspan="1" rowspan="1">3631</td><td colspan="1" rowspan="1">−8</td><td colspan="1" rowspan="1">10</td><td colspan="1" rowspan="1">48</td><td colspan="1" rowspan="1">8.4473</td></tr><tr><td colspan="1" rowspan="1">Precentral gyrus (L)</td><td colspan="1" rowspan="1">2476</td><td colspan="1" rowspan="1">−34</td><td colspan="1" rowspan="1">−18</td><td colspan="1" rowspan="1">58</td><td colspan="1" rowspan="1">7.2107</td></tr><tr><td colspan="1" rowspan="1">Inferior frontal gyrus (L)</td><td colspan="1" rowspan="1">1207</td><td colspan="1" rowspan="1">−52</td><td colspan="1" rowspan="1">12</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">6.3788</td></tr><tr><td colspan="1" rowspan="1">Superior parietal lobule (L)</td><td colspan="1" rowspan="1">815</td><td colspan="1" rowspan="1">−28</td><td colspan="1" rowspan="1">−56</td><td colspan="1" rowspan="1">48</td><td colspan="1" rowspan="1">6.0945</td></tr><tr><td colspan="1" rowspan="1">Amygdala (L)</td><td colspan="1" rowspan="1">720</td><td colspan="1" rowspan="1">−14</td><td colspan="1" rowspan="1">−12</td><td colspan="1" rowspan="1">−12</td><td colspan="1" rowspan="1">6.2453</td></tr><tr><td colspan="1" rowspan="1">Putamen (R)</td><td colspan="1" rowspan="1">257</td><td colspan="1" rowspan="1">24</td><td colspan="1" rowspan="1">10</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">5.4598</td></tr><tr><td colspan="1" rowspan="1">Superior parietal lobule (R)</td><td colspan="1" rowspan="1">199</td><td colspan="1" rowspan="1">26</td><td colspan="1" rowspan="1">−62</td><td colspan="1" rowspan="1">54</td><td colspan="1" rowspan="1">4.8181</td></tr><tr><td colspan="1" rowspan="1">Angular gyrus (L)</td><td colspan="1" rowspan="1">156</td><td colspan="1" rowspan="1">−48</td><td colspan="1" rowspan="1">−52</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">4.9727</td></tr><tr><td colspan="1" rowspan="1">Insula (R)</td><td colspan="1" rowspan="1">128</td><td colspan="1" rowspan="1">40</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">0</td><td colspan="1" rowspan="1">5.3664</td></tr><tr><td colspan="1" rowspan="1">Middle temporal gyrus (L)</td><td colspan="1" rowspan="1">123</td><td colspan="1" rowspan="1">−62</td><td colspan="1" rowspan="1">−24</td><td colspan="1" rowspan="1">−12</td><td colspan="1" rowspan="1">4.7632</td></tr><tr><td colspan="1" rowspan="1">Temporal pole (middle) (L)</td><td colspan="1" rowspan="1">77</td><td colspan="1" rowspan="1">−56</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">−24</td><td colspan="1" rowspan="1">5.4375</td></tr></tbody></table></table-wrap>
</div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 29330483</strong> | Pred included: 0 | Manual included (accepted matches only): 10 | Correct overlaps: 0 | Match statuses: accepted=10, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/29330483/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>29330483_analysis_0</td><td>Heterosexual Women</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>This contrast does not assess self-perception, agency, or self-knowledge; it measures responses to others depicted in erotic stimuli and reward-related activation, not self-related processing.</td></tr>
<tr><td>29330483_analysis_1</td><td>Heterosexual Women</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>Analysis does not probe self-perception, agency, or self-knowledge; it measures responses to others&#x27; erotic stimuli. Does not satisfy I1 or I2.</td></tr>
<tr><td>29330483_analysis_2</td><td>Bisexual Women</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The analysis measures responses to external erotic stimuli and group differences in sexual orientation, not self-perception, agency, or self-knowledge. Inclusion criteria I1/I2 not met.</td></tr>
<tr><td>29330483_analysis_3</td><td>Bisexual Women</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The analysis does not measure self-perception, agency, or self-knowledge. It contrasts neural responses to male vs female erotic stimuli rather than tasks about the self, so it does not meet inclusion criteria.</td></tr>
<tr><td>29330483_analysis_4</td><td>Homosexual Women</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast measures responses to external erotic stimuli (male vs female), not self-related processing, agency, or self-knowledge judgments; thus it does not meet Perception and Understanding of Self criteria.</td></tr>
<tr><td>29330483_analysis_5</td><td>Homosexual Women</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrasts reflect responses to external erotic stimuli (male vs female) and do not probe self-perception, self-knowledge, or agency. Therefore the analysis does not meet Perception and Understanding of Self criteria.</td></tr>
<tr><td>29330483_analysis_6</td><td>Heterosexual Women: Female &gt; Male Videos</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The analysis measures responses to others&#x27; erotic stimuli (male vs female) and ventral striatum activation, not self-perception, agency, or self-knowledge. Does not meet I1 or I2.</td></tr>
<tr><td>29330483_analysis_7</td><td>Heterosexual Women: Male &gt; Female Videos</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Analysis examines responses to erotic videos of others, not self-perception, agency, or self-knowledge; does not meet I1 or I2.</td></tr>
<tr><td>29330483_analysis_8</td><td>Bisexual Women: Female &gt; Male Videos</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast measures responses to external erotic stimuli and ventral striatum activation; it does not assess self-perception, agency, or self-knowledge. Inclusion criteria I1/I2 are not satisfied.</td></tr>
<tr><td>29330483_analysis_9</td><td>Bisexual Women: Male &gt; Female Videos</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The analysis does not assess self-perception, agency, self-knowledge, or self-related judgments. Measures are subjective sexual appeal ratings and ventral striatum activation to others’ erotic stimuli, not introspective self-evaluation tasks.</td></tr>
<tr><td>29330483_analysis_10</td><td>Homosexual Women: Female &gt; Male Videos</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Analysis does not measure self-perception, agency, or self-knowledge; it measures responses to external erotic stimuli. Does not meet I1 or I2.</td></tr>
<tr><td>29330483_analysis_11</td><td>Homosexual Women: Male &gt; Female Videos</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The analysis targets responses to erotic stimuli and does not probe self-perception, agency, or self-knowledge; it does not meet inclusion criteria I1–I2 for perception/understanding of self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>29330483_1</td><td>(Bisexual Women) Female &gt; Male Pictures; self</td><td>29330483_analysis_2</td><td>Bisexual Women</td><td>0.528</td><td>0.923</td><td>0.805</td><td>accepted</td><td>high_coord_match</td></tr><tr><td>29330483_10</td><td>(Homosexual Women) Male &gt; Female Videos; self</td><td>29330483_analysis_11</td><td>Homosexual Women: Male &gt; Female Videos</td><td>0.961</td><td>1.000</td><td>0.988</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29330483_2</td><td>(Bisexual Women) Female &gt; Male Videos; self</td><td>29330483_analysis_8</td><td>Bisexual Women: Female &gt; Male Videos</td><td>0.959</td><td>1.000</td><td>0.988</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29330483_3</td><td>(Bisexual Women) Male &gt; Female Pictures; self</td><td>29330483_analysis_3</td><td>Bisexual Women</td><td>0.528</td><td>1.000</td><td>0.858</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>29330483_4</td><td>(Bisexual Women) Male &gt; Female Videos; self</td><td>29330483_analysis_9</td><td>Bisexual Women: Male &gt; Female Videos</td><td>0.959</td><td>1.000</td><td>0.988</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29330483_5</td><td>(Heterosexual Women) Female &gt; Male Pictures; self</td><td>29330483_analysis_0</td><td>Heterosexual Women</td><td>0.590</td><td>1.000</td><td>0.877</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29330483_6</td><td>(Heterosexual Women) Female &gt; Male Videos; self</td><td>29330483_analysis_6</td><td>Heterosexual Women: Female &gt; Male Videos</td><td>0.963</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29330483_7</td><td>(Heterosexual Women) Male &gt; Female Videos; self</td><td>29330483_analysis_7</td><td>Heterosexual Women: Male &gt; Female Videos</td><td>0.963</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29330483_8</td><td>(Homosexual Women) Female &gt; Male Pictures; self</td><td>29330483_analysis_4</td><td>Homosexual Women</td><td>0.561</td><td>1.000</td><td>0.868</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>29330483_9</td><td>(Homosexual Women) Female &gt; Male Videos; self</td><td>29330483_analysis_10</td><td>Homosexual Women: Female &gt; Male Videos</td><td>0.961</td><td>1.000</td><td>0.988</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 5766543)</summary><p><strong>Title:</strong> Neural Correlates of Sexual Orientation in Heterosexual, Bisexual, and Homosexual Women</p><details><summary>Abstract</summary><pre class="paper-text">We used fMRI to investigate neural correlates of responses to erotic pictures and videos in heterosexual (N = 26), bisexual (N = 26), and homosexual (N = 24) women, ages 25–50. We focused on the ventral striatum, an area of the brain associated with desire, extending previous findings from the sexual psychophysiology literature in which homosexual women had greater category specificity (relative to heterosexual and bisexual women) in their responses to male and female erotic stimuli. We found that homosexual women’s subjective and neural responses reflected greater bias towards female stimuli, compared with bisexual and heterosexual women, whose responses did not significantly differ. These patterns were also suggested by whole brain analyses, with homosexual women showing category-specific activations of greater extents in visual and auditory processing areas. Bisexual women tended to show more mixed patterns, with activations more responsive to female stimuli in sensory processing areas, and activations more responsive to male stimuli in areas associated with social cognition.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Studies using physiological measures have found that women tend to have non-specific patterns of genital arousal . That is, in contrast to men, women tend to show similar degrees of arousal to erotic stimuli depicting either sex. For example, heterosexual women have generally shown equivalent arousal to both erotic stimuli featuring men and erotic stimuli featuring women. This has been repeatedly demonstrated with vaginal photoplethysmography . This pattern has also been found using less direct measures such as looking time , pupil dilation , and fMRI . Notably, homosexual women’s arousal patterns are more category-specific than heterosexual women’s, although less so than men’s . 

The fact that women’s sexual arousal patterns are less category-specific than men’s has been interpreted as a potential contributor to gender differences in “erotic plasticity” , which Baumeister has defined as “the extent to which sex drive is shaped by social, cultural, and situational factors.” 

Baumeister offered three lines of evidence when he initially proposed that women may have greater erotic plasticity compared with men: (1) women show larger effects of social and cultural factors on sexual attitudes, desire, and behavior; (2) sexual attitude-behavior consistency is lower in women than in men; (3) individual women exhibit more variation in sexual behavior across time than men. Women’s less specific arousal patterns may also contribute to their increased “sexual fluidity” , which Diamond has defined as an individual’s “capacity for situation-dependent flexibility in sexual responsiveness, which allows individuals to experience changes in same-sex or other-sex desire across both short-term and long-term time periods” . 

One might hypothesize that arousal patterns of bisexual women should be similar to the non-specific arousal patterns of heterosexual women; however, studies of women’s arousal patterns have mostly neglected to include bisexual women. Heterosexual women’s arousal does not appear to favor erotic stimuli of either sex, and thus may be considered to reflect a bisexual pattern. (We do not mean to imply that heterosexual women are confused or in denial about their “real preferences”; rather, the findings in need of explanation are why heterosexual women show non-heterosexual arousal patterns in the laboratory). The implication of women’s non-specific arousal patterns for their sexual orientations is difficult to interpret. Most women, like most men, behave and identify heterosexually . However, men are more likely than women to identify as completely heterosexual or completely homosexual, and women are more likely than men to identify as bisexual or “mostly heterosexual” . 

If arousal patterns are similar between heterosexual and bisexual women, the question remains what distinguishes the two groups. One possibility, supported by some research, is that bisexual women tend to have greater sexual motivation, which may increase the likelihood of exploring a capacity for attraction to both sexes . Or, bisexual women may be more aware than heterosexual women of their non-specific arousal , which could partially contribute to bisexual sexual motivation. Alternatively, bisexual women may be more likely than heterosexual women to interpret their non-specific arousal states in sexual or romantic terms. 

It is also possible that bisexual women’s arousal patterns differ from those observed in heterosexual women. Consistent with this possibility, recent studies suggest that women with bisexual interests tend to be more aroused by female than by male erotic stimuli . Perhaps for some women with female-biased arousal patterns, this bias can motivate non-heterosexual feelings, behavior, and identity. 

Interpretations of non-specific arousal patterns in women are further complicated by the fact that female genital arousal exhibits relatively low correlations with subjectively reported sexual arousal, in contrast to the high correlations observed in men . Discrepancies between existing genital and subjective measures indicate that some women may report substantial subjective arousal without substantial genital arousal, and vice versa. It has also been suggested that non-specific arousal patterns may not indicate affective responses to erotic stimuli, but may instead reflect a kind of protective preparatory response . 

Neuroimaging assessments may shed light on the neural systems that are involved in responding to a given paradigm. Functional magnetic resonance imaging (fMRI) is a neuroimaging approach that allows for the indirect assessment of brain activity by tracking ratios of oxygenated and deoxygenated blood a proxy for neural firing. When used in the context of presenting erotic stimuli, this non-invasive neural measure could provide a converging line of evidence for interpreting the genital and subjective arousal findings described above. In this study, we used fMRI to specifically focused on the “reward system” in order to address the question: to what extent is there an affective significance to findings from the literature on women’s sexual orientation and genital arousal? 

The part of the “reward system” that we focused on is the ventral striatum, a dopamine-sensitive area of the brain that is a reliable measure of reward-related processing–and in particular, wanting and “incentive motivation” –including with respect to sexual orientation . Most neuroimaging studies of sexual response have focused on men , but the ventral striatum has also been found to reliably activate in studies of women’s responses to erotic stimuli . However, until now, no studies have measured neural responses to erotic stimuli in bisexual women. 

The present investigation primarily focused on two hypotheses: (1) Homosexual women may show greater category-specificity than non-homosexual women in brain activity, as suggested by the genital arousal literature; (2) Bisexual women may show larger biases towards female stimuli, compared with heterosexual women. We tested these hypotheses with respect to subjective and neural responses to erotic pictures and erotic videos. We used two different kinds of erotic stimuli because of their potentially non-overlapping strengths and weaknesses. Erotic pictures may be particularly well-suited for assessing the initial appraisal of sexual stimuli, but their brevity may not reflect the kinds of experiences that drive sexuality in the real world. Erotic videos may allow for the measurement of more intense states, but their extended duration may also provide opportunities for self-regulatory efforts to modify erotic responses. 


## Method 
  
### Participants 
  
Participants were 26 heterosexual women, 26 bisexual women, and 24 homosexual women, recruited from a variety of publicly-posted and online advertisements seeking (paid) volunteers for a neuroimaging study of sexual orientation and arousal. Bisexual women were required to have had at least two previous sexual partners and one romantic partner (of three months or greater duration) of each sex. Homosexual and heterosexual participants all met these criteria with respect to their respective preferred sexes. 

After responding to advertisements, participants were screened for inclusion using online questionnaires. Participants provided information about sexual orientation, sexual interests, and personality, in addition to answering screening questions relevant to medical eligibility for fMRI research. Participants were required to be right handed, non-claustrophobic, free from ferromagnetic implants, and not currently taking psychiatric medications. Participants were informed of the risks and nature of the study and agreed to participate in all portions of the research. This study was approved by the Institutional Review Board of Northwestern University and carried out in accordance with its guidelines. Informed consent was obtained from each participant for every portion of the study in which they participated. 

Participants’ sexual orientation was assessed using self-reported identities (i.e. “Homosexual”/“Gay”, “Bisexual”/“Bi”, “Heterosexual”/“Straight”), as well as with a modified Kinsey score, which asked participants about their sexual fantasies throughout adulthood as well as in the past year. The scale ranged from 0 to 6, with 0 corresponding to an exclusively heterosexual orientation and 6 corresponding to an exclusively homosexual orientation. Responses to questions about adulthood and about the past year were averaged to create a Kinsey score for each participant. The average Kinsey score was 0.8 for heterosexual women (  SD   = 0.7,   range   = 0–2), 2.63 for bisexual women (  SD   = 0.7,   range   = 2–4.5), and 5.2 for homosexual women (  SD   = 0.68,   range   = 4–6). 

Participants’ ages ranged from 21 to 46 years old. Mean ages were 29.7 for heterosexual women (  SD   = 5.86,   range   = 25–46), 30.27 for bisexual women (  SD   = 6.41,   range   = 21–48), and 29 for homosexual women (  SD   = 3.12,   range   = 25–38). The sample of 76 participants was racially and ethnically diverse, with 23 non-Caucasian participants including two Latina participants, ten African-American participants, four Asian-American participants, and seven participants who identified otherwise or who identified as multiethnic/multiracial. Groups did not significantly differ either with respect to age (  F  (2,73) = 0.348,   p   = 0.708) or ethnicity (c (2, N = 76) = 2.94, p = 0.23). We also confirmed that ethnicity did not significantly impact responses to the erotic stimuli. 


### Stimuli and Procedure 
  
Subjects experienced two experimental paradigms in the scanner: first erotic pictures were shown (over a period of ~21 minutes), and then erotic videos were shown (over a period of ~19 minutes) after a brief rest period. Picture stimuli were shown before video stimuli for all participants in an attempt to promote stimulus engagement. That is, it was assumed that potentially less intense stimuli might be better presented earlier in the experimental session while attentional resources are highest. Further, there was concern that first showing more intense stimuli would reduce engagement with subsequent stimuli. As such, pictures and videos stimuli were not counterbalanced with respect to each other. 

Participants watched stimuli while laying down with a combination of earplugs (to minimize scanner noise) and over-ear headphones (for video sound and communication with experimenters). Images were displayed via projector onto a wall, which was made viewable to participants via an angled mirror placed above the eyes. 

#### Erotic pictures paradigm 
  
The present study employed a subset of the picture stimuli used in Safron   et al  .  and Sylva   et al  . . Pictures depicted a nude man, a nude woman, or a same-sex couple (i.e., either two men or two women) engaged in explicit sexual contact. Erotic stimuli featuring both individual nudes and same-sex pairs engaging in explicit sexual interaction is common in research on sexual arousal and sexual orientation , which is not the case when stimuli featuring male-female couples is presented. However, erotic stimuli featuring explicit sexual activity in same-sex couples tends to be substantially more arousing compared with pictures of single nudes . Such stimuli are similar to pictures of nude individuals, in the sense that only men or women, but not both, are depicted in a given picture. Thus, sexual arousal induced by them is relatively unambiguous in terms of the gender to which participants are responding. 

In each of two 10.5-minute runs (ordering counterbalanced), participants viewed 40 erotic pictures featuring male models and 40 erotic pictures featuring female models. Each picture was shown for 3.5 seconds, followed by a variable-duration fixation cross presented for either 1.5, 6.5, or 11.5 seconds. Variable-duration baselines were utilized for superior deconvolution of the BOLD signal in a rapid event-related design for fMRI (in which evoked signals are never allowed to return to baseline levels). During the presentation of each picture, participants used buttons held in their right hands to rate that image on a scale of −2 to +2 (respectively: “strongly disliked,” “disliked,” “liked,” “strongly liked”), with no option of 0 for neutral ratings. Neutral options for ratings were not provided for the sake of consistency with previous research using the same stimuli. Note: Subjective ratings of pictures were lost for some participants due to a button-box equipment error. 


#### Erotic videos paradigm 
  
Following picture assessment, participants were shown six video clips depicting individual masturbating men and six video clips depicting individual masturbating women. Depicted individuals appeared sexually aroused but did not reach orgasm. To estimate baseline responses, six natural landscape videos were shown. 

In each of two 9.25-minute runs (ordering counterbalanced), videos were presented for 15 seconds each, followed by a 15-second distraction task requiring participants to indicate via button-press when a number in a series decreased by an interval other than seven. This task was intended to facilitate a return to emotional and physiological baseline. 15-second stimulus presentations were chosen as a desirable stimulation period in an fMRI block design, which can potentially be more sensitive than event-related designs . 

After leaving the scanner, participants viewed the videos once more and provided ratings of each clip. Videos were rated using a 5-point scale for degree of sexual appeal, ranging from “not at all” (0) to “very much” (4), with a midpoint of “somewhat” (2).’ 



### fMRI signal extraction methods 
  
#### Image acquisition 
  
A Siemens Trio 3 T magnet and 12-channel RF head coil were used to collect T2*-weighted gradient-recalled EPI images from the whole brain (32 3-mm slices with a 0.99-mm interslice gap; TR = 2500 ms; TE = 20 ms; flip angle = 80°; FOV = 200 × 220 mm, 120 × 128 acquisition matrix). Slices were taken along the plane connecting the anterior and posterior commissures, with a 1.72 mm × 1.72 mm × 3.99 mm resolution, with more refined axial dimensions intended to produce less distortion and signal dropout in sub-cortical areas, although possibly at the expense of signal-to-noise ratio. During each picture run, 250 whole-brain volumes were collected, and during each video run, 220 whole-brain volumes were collected, with the first four volumes discarded to account for initial magnetization effects. For anatomical localization, a structural MRI scan consisting of T1-weighted images was conducted after the testing runs (160 1-mm axial slices; TR = 2.1 ms; TE = 4.38 ms; flip angle = 15°; FOV = 220 mm; 256 × 192 matrix). 


#### Image pre-processing 
  
Image pre-processing and analysis was performed using SPM 12b (Wellcome Trust Centre for Neuroimaging, London, UK), and implemented in Matlab v 8.1.604 (The MathWorks Inc., MA, USA). 

Functional (EPI) volumes were first corrected for slice timing. Each participant’s volumes were then registered to the mean slice, after which the registered volumes were resliced, used to create a mean resliced image, and then co-registered to the structural (T1) image. All EPI images, including the mean resliced image, as well as the structural (T1) scans were then spatially normalized to Montreal Neurological Institute (MNI) space, and re-sampled to 3 × 3 × 3 mm (27 mm ) resolution. Normalized functional images were then smoothed to an 8 mm full-width-at-half-maximum Gaussian kernel. 


#### Signal to noise ratio and head coverage exclusions 
  
To exclude participants with poor signal due to either head motion or scanner conditions, average signal-to-noise ratio (SNR) over time was calculated for each subject (after preprocessing, using a mask that included only voxels with appreciable EPI signal). The SNR ratio for each voxel (mean divided by standard deviation) was averaged across all voxels in the brain . Participants whose picture data SNR was less than one standard deviation below the mean were excluded from picture analyses. Similarly, participants whose video data SNR was less than one standard deviation below the mean were excluded from video analyses. 

Based on these criteria, fourteen participants (five heterosexual, five bisexual, and four homosexual) were excluded from fMRI and subjective picture analyses, and sixteen participants (six heterosexual, six bisexual, and four homosexual) were excluded from fMRI and subjective video analyses. After exclusions were performed for SNR, we included a total of twenty-one heterosexual women, twenty-one bisexual women, and twenty homosexual women in fMRI picture analyses. Video analyses after SNR exclusion included eighteen heterosexual women, eighteen bisexual women, and twenty homosexual women. To check the validity of our SNR criterion, head motion plots were visually inspected for all participants (Parrish,   et al  . ). Excluded participants had highly variable head positions as compared to included participants. An additional validity-check was performed using evoked responses to erotic pictures minus a fixation-cross baseline. Excluded participants had substantially reduced activity in visual cortices as compared to included participants. 

An additional thirty-two participants (twelve heterosexual, twelve bisexual, and eight homosexual) were excluded from subjective picture rating analyses due to insufficient subjective data resulting from a button-box equipment error. Five participants (three bisexual and two homosexual) were excluded from subjective video analyses for the same reason. Thus, after exclusions were performed for insufficient subjective data, we included a total of nine heterosexual women, nine bisexual women, and twelve homosexual women in subjective picture analyses, and twenty heterosexual women, seventeen bisexual women, and eighteen homosexual women in subjective video analyses. 

For whole-brain analyses, mean functional scans were individually examined to identify participants with substantial cutoffs in head coverage. As a result, one heterosexual female who had substantial frontal lobe cutoff was excluded from whole-brain analyses in addition to those participants excluded for SNR. 


#### First-level analyses 
  
For both the video and picture assessments, a standard general linear model (GLM)  was used in identifying hemodynamic changes for each participant, and a high-pass filter (cutoff 128 s) was used to remove low-frequency temporal noise. 

Estimated average activity was calculated for each participant’s separate responses to male pictures, female pictures, male videos, and female videos (contrasted with fixation cross for pictures and neutral nature scenes for videos). These estimates were used for region of interest analyses. For whole-brain analyses, estimated average activity was also calculated for each participant’s response to male compared with female pictures and videos. For both the picture and video assessments, each participant’s responses to each stimulus contrast of interest were concatenated within stimulus type, using data from both the 1  and 2  runs. 

Ventral striatum region of interest analyses. An a priori region of interest (ROI) analysis was performed on the ventral striatum—centered on the nucleus accumbens—as this was the area most likely to indicate desire. The ventral striatum and hypothalamus are the only two areas that have been shown to be specifically associated with sexual (as opposed to general) arousal . We focused on the ventral striatum because it likely has higher validity for reflecting sexual incentive value compared with the hypothalamus, which contains a variety of nuclei with heterogeneous functions (including sexual arousal) that would be difficult to disambiguate with the limited spatial resolution of 3 T fMRI. 

The ventral striatum ROI mask used in the present study was drawn on an MNI template brain using the WFU PickAtlas toolbox for SPM 8 . It was anatomically defined as a dilated intersection of the ventral anterior caudate and putamen. The resulting ventral striatum ROI is shown in Fig.  .   
Mask used as the ventral striatum (VS) ROI, drawn using an average brain in the WFU PickAtlas toolbox for SPM 8. MNI coordinates displayed: x = 0, y = 17, z = −8. 
  

Estimates of average ventral striatum activity for each participant were extracted using the MarsBar toolbox for SPM8 . Extracted ventral striatum ROI data were analyzed using JMP Pro v11 (SAS Institute, Cary, NC). 



### Planned contrasts and within-group tests 
  
We constructed separate dependent variables for each combination of stimulus type (i.e. picture or video) and response type (i.e., subjective or ventral striatum activation) by subtracting response to female stimuli from response to male stimuli. That is, we constructed dependent variables for 1) subjective response to pictures, 2) subjective response to videos, 3) ventral striatum activation to pictures, and 4) ventral striatum responses to videos, each of which reflected responses to male stimuli minus responses to female stimuli. We refer to this as the Male-Female contrast. 

Because there were three groups (i.e., heterosexual, homosexual, and bisexual women), two orthogonal between-groups contrasts were constructed to examine what we believe to be the most interesting pair of independent questions based on previous literature . The first question was whether homosexual women differed from the other two groups in their Male-Female contrasts. The second question was whether bisexual women differed from heterosexual women in their Male-Female contrasts. The use of orthogonal planned contrasts allowed us to test these hypotheses with maximum statistical power while simultaneously minimizing the number of overall comparisons. 

Within-group t-tests were also performed separately in each group in order to characterize relative responding to male and female stimuli. 


### Whole-brain analyses 
  
Finally, we examined overall patterns of differential activation in response to male compared with female erotic stimuli across the entire brain. If bisexual and heterosexual women have less specific arousal patterns, then they are likely to exhibit less extensive differential activity between male and female stimuli compared with the activity patterns expected for homosexual women. 

Tests of average group responses to stimulus conditions were performed using one-sample contrasts. Each group (heterosexual women, bisexual women, and homosexual women) was tested individually for clusters of greater activity for male stimuli compared with female stimuli, and female stimuli compared with male stimuli, using a corrected statistical threshold (p &lt; 0.05 FWE). 

For these analyses, cluster reports were generated in SPM. Peak activations and cluster extents (extent threshold k = 5) were visually examined as overlays on slice and render maps. Neuroanatomical descriptions were determined based on agreement between two trained investigators, and checked against designations from the WFU Atlas (Maldjian   et al  ., 2003). 


### Data availability statement 
  
The datasets generated and analyzed during the current study are available from the corresponding author on request. 



## Results 
  
### Between-group planned contrasts 
  
As previously described, planned comparisons for the ventral striatum ROI were conducted via multiple regression using two orthogonal between-groups contrasts: one comparing homosexual women with heterosexual and bisexual women, and one comparing heterosexual with bisexual women. Separate analyses were conducted for each of the Male-Female contrasts (i.e., responses to female stimuli subtracted from responses to male stimuli). Results are presented in Table  .    
Planned contrasts comparing women of different orientation groups. 
  
*Significant p-value &lt; 0.05. **Signifcant p-value &lt; 0.01. 

***Significant p-value &lt; 0.001. 
    
Within-group male – female (male minus female) stimuli difference scores for subjective ratings and ventral striatum (VS) responses, by sexual orientation. Difference scores are defined as a participant’s average response to stimuli depicting males minus average response to stimuli depicting females. Points represent individual participants. Horizontal bars indicate group means and 95% confidence intervals of the means. Horizontal lines at 0 indicate no difference between ratings to erotic stimuli depicting each sex. (  a  ) Difference scores for subjective ratings of picture stimuli. (b) Difference scores for VS activation evoked by picture stimuli. (c) Difference scores for subjective ratings of video stimuli. (d) Difference scores for VS activation evoked by video stimuli. ***p &lt; 0.001, **p &lt; 0.01, *p &lt; 0.05. 
  

#### Homosexual versus non-homosexual women 
  
Subjective ratings. Compared with non-homosexual women, homosexual women had significantly more negative (i.e., gynephilic) Male-Female contrasts for both pictures (p = 0.015) and videos (p &lt; 0.001). That is, homosexual women showed a greater preference for pictures and videos of females relative to males, compared with both bisexual and heterosexual women. 

Ventral striatum activation patterns. Homosexual women had significantly more female-biased ventral striatum responses compared to non-homosexual women for pictures (p = 0.002), but not videos. 


#### Bisexual versus heterosexual women 
  
We compared heterosexual and bisexual women’s subjective and ventral striatum responses to erotic pictures and videos, and observed only one significant difference: for video stimuli, bisexual women had significantly more female-preferring subjective responses than did heterosexual women (p = 0.026). 



### Within-group tests comparing responses to male and female erotic stimuli 
  
Figure   (showing the distribution of Male-Female contrasts for the three groups) shows that heterosexual women exhibited a non-significant trend (p = 0.079) towards favoring female erotic pictures compared with male erotic pictures, and had no differentiation between stimulus sex for other tests. Bisexual women also did not subjectively differentiate among stimulus types based on sex, although they did exhibit (non-significant) marginal female-favoring ventral striatum scores for picture (p = 0.063) and video (p = 0.054) stimuli. Homosexual women, in contrast to heterosexual and bisexual women, showed clear favoring of female stimuli as assessed by subjective liking of pictures (p &lt; 0.001), appeal ratings of videos (p &lt; 0.001), as well as in ventral striatum responses to pictures (p = 0.003) and non-significantly for ventral striatum responses to videos (p = 0.073). Note that these results are presented descriptively. Inferences about differences among the three groups depend on the tests presented in Table  . 


### Whole brain tests comparing responses to male and female erotic stimuli 
  
Note: Activation patterns are described in greater detail in the discussion, with interpretations of possible functional significances. 

#### Picture stimuli 
  
Comparing activation to female versus male erotic pictures, heterosexual women exhibited relatively greater activity for female pictures in occipital (i.e., visual) and occipitotemporal cortices, with no brain areas showing significantly greater activation for male pictures (Fig.  ; Table  ). Bisexual women also showed greater activity in visual cortices for female relative to male pictures, but they showed greater activity for male pictures in other areas including supramarginal and angular gyri, as well as the posterior cingulate. Homosexual women exhibited significant activations for female compared with male pictures in visual cortex, parietal lobes, and parahippocampal cortex, but with no brain areas showing significantly greater activation for male pictures.   
Differential brain activations towards male and female pictures in heterosexual, bisexual, and homosexual women. Whole brain activations are shown for the male picture minus female picture contrasts (with brain activation evoked by viewing neutral stimuli subtracted from activations toward the erotic pictures). Height threshold is set at p &lt; 0.05 FWE with a cluster threshold of k = 5. Axial slice 31, sagittal slice 50, and coronal slice 38 are shown for all groups. 
    
Differential whole-brain activations in response to male vs. female pictures. 
  


#### Video stimuli 
  
When viewing female compared with male erotic videos (Fig.  ; Table  ), all groups showed activity in bilateral superior temporal cortices, likely indicating an auditory confound in which more extensive and substantial vocalizations were present in female erotic videos . However, this effect appeared to vary by sexual orientation, with homosexual women showing the most extensive and robust evoked activity (peak T = 14.22) compared with heterosexual (peak T = 11.71) and bisexual women (peak T = 8.83). In the opposite direction of greater responses to male compared with female erotic videos, heterosexual and bisexual (but not homosexual) women exhibited activations in occipital cortices. While all groups had greater activity towards male videos in (anterior) superior parietal cortices, these activations appeared to be more extensive and robust in bisexual women (peak T = 11.55) compared with heterosexual (peak T = 7.64) and homosexual women (peak T = 7.99).   
Differential brain activations between male and female videos in heterosexual, bisexual, and homosexual women. Whole brain activations are shown for the male video minus female video contrasts (with brain activation evoked by viewing neutral stimuli subtracted from activation toward the erotic videos). Height threshold is set at p &lt; 0.05 FWE with a cluster threshold of k = 5. Axial slice 37, sagittal slice 61, and coronal slice 38 are shown for all groups. 
    
Differential whole-brain activations in response to male vs. female videos. 
  




## Discussion 
  
In this fMRI study of female sexual orientation—the first to include bisexual women—we extended several key findings from the sexual psychophysiology literature . Using the ventral striatum as a neural measure of incentive motivation, we demonstrated that homosexual women have greater gender bias in their responses to male and female erotic stimuli. 

### Main findings: subjective and ventral striatum responses to male and female erotic stimuli 
  
Direct comparisons of bisexual and heterosexual women revealed no significant differences, with the exception of bisexual women having more gynephilic subjective responses to erotic videos. However, bisexual and heterosexual women did not differ with respect to their ventral striatum responses toward these stimuli. When contrasted to bisexual and heterosexual women, homosexual women showed distinctly greater bias toward female stimuli in both their subjective responses to videos and pictures, and also in their ventral striatum responses to pictures. In sum, our planned contrast findings are consistent with the genital arousal literature in which more category-specific responses were observed in homosexual women . 

Another set of tests, comparing male vs. female stimuli within each group, revealed that neither bisexual nor heterosexual women were significantly biased toward stimuli depicting males or stimuli depicting females. This was true both in ventral striatum response and in subjective arousal, for both picture and video stimuli. Homosexual women, however, were uniquely gynephilic (i.e., female-preferring), with significantly greater responses to female stimuli for subjective responses to pictures, subjective responses to videos, and ventral striatum responses to pictures. This gynephilic bias in homosexual women was consistent with our direct comparisons and previous literature. 

Our findings are only partially consistent with observations from the genital arousal literature in which homosexual and bisexual women both had gynephilic responses, but where heterosexual women had non-specific responses . We found significant biases in ventral striatum responses toward female stimuli among homosexual women, but with more indifferent patterns among heterosexual and bisexual women. However, with one exception—bisexual women showing more gynephilic subjective responses to erotic videos than did heterosexual women, in a direct comparison—heterosexual and bisexual women’s patterns of results did not differ significantly. 

Our a priori tests in the ventral striatum allowed us to explore whether women of different sexual orientations also exhibited different degrees of incentive motivation toward male and female erotic stimuli. But fMRI also provides the ability to look at activation patterns across the entire brain, potentially allowing for a more detailed characterization of the neural systems involved. Below we review activation patterns for each group in viewing male compared with female erotic stimuli, along with some reverse inferences as to their functional significance. 


### Whole brain responses to erotic pictures 
  
For heterosexual women viewing erotic pictures, activity was greater for female relative to male stimuli bilaterally in lateral occipital cortices, likely indicative of visual attention , as well as in right-lateralized fusiform cortex, potentially suggesting face or body processing .   In no brain areas did heterosexual women have significantly greater activation for male relative to female erotic pictures  . Rather, they seemed to have a somewhat gynephilic pattern of visual attention, consistent with results from eye-tracking and looking-time studies in which heterosexual women attended to erotic characteristics of female pictures . 

Bisexual women showed more activity in response to female (relative to male) erotic pictures throughout the visual system, including fusiform cortex, which (as described above) is often associated with perception of faces and bodies . Patterns were similar to those observed in heterosexual women (and presumably with similar functional significances), but with larger spatial extents of activation. Although bisexual and heterosexual women were not directly contrasted, this more extensive visual activation could be taken as support for somewhat greater gynephilic interest on the part of bisexual women, consistent with ventral striatum activation patterns. 

For bisexual women viewing erotic pictures, activity was greater for male relative to female stimuli in posterior midcingulate and right retrosplenial cingulate cortices, potentially suggesting greater perceptual salience and emotional memory for male erotic stimuli . Additional male-biased activations were identified bilaterally in supramarginal and angular gyri, indicating processes relating to mental imagery, or possibly mentalizing . 

Thus, in contrast to heterosexual participants, bisexual women showed greater activity towards male (relative to female) erotic pictures in affect-related brain areas. In this way, it seems that it would be overly simplistic to say that bisexual women are similar to heterosexual women, but with the addition of gynephilic interest. Rather,   bisexual women seem to have greater responses to both male and female erotic stimuli, depending on the brain area being considered  . Patterns of greater overall responsiveness are consistent with suggestions that bisexual women may be distinguished by having overall greater degrees of sexual motivation relative to heterosexual women . 

It is also notable that bisexual women uniquely showed greater activations to male stimuli in areas of the brain implicated in higher-order cognition, including mentalizing. Speculatively, these activations could be related to more complex processing of sexual motivation in bisexual women . To the degree that these activation patterns in bisexual women actually specifically reflect social cognition, the question remains open as to why this may be more likely to be observed in bisexual but not heterosexual or homosexual women. 

For homosexual women viewing erotic pictures, greater activations for female (relative to male) stimuli extended throughout the visual system, with additional clusters in occipitotemporal cortices. Clusters in the right inferior precuneus may have indicated mental imagery , and clusters in posterior parahippocampal cortex may have indicated either memory encoding or retrieval .   For homosexual women, no brain areas had significantly greater activation for male relative to female erotic pictures. Thus, homosexual women were the only group that exhibited an overall pattern of differential brain activity (between male and female sexual stimuli) greater only for pictures depicting their preferred gender  . 


### Whole brain responses to erotic videos 
  
For heterosexual women viewing erotic videos, activity was greater for female relative to male stimuli in bilateral superior temporal cortices likely indicating an auditory confound deriving from more extensive and substantial vocalizations being present in female erotic videos . Activity was greater for male relative to female videos in posterior occipital cortex, likely indicating enhanced visual attention . Further clusters (greater for male compared with female videos) in the inferolateral postcentral gyrus and parietal somatosensory association areas may have indicated awareness of bodily sensations, possibly related to sexual imagery . 

For bisexual women viewing erotic videos, activations were greater for female (relative to male) stimuli in superior temporal cortices, likely indicating the same auditory-related activity present in heterosexual women. Bisexual women’s brain activity was greater for male (relative to female) erotic videos in occipital cortex, likely indicating visual attention . Male-biased activations in somatosensory cortices may have indicated processing of bodily sensations , and further activations in bilateral superior parietal lobules, premotor and supplementary motor cortices, and right supramarginal gyrus may have indicated mental imagery or possibly mirroring with the actors shown in the videos . 

Similar to the findings for erotic pictures,   bisexual women were unique in the degree to which male videos produced activations in brain areas associated with more abstract (and possibly complex) processing  . Again, the significance of this pattern remains unclear. 

For homosexual women viewing female relative to male erotic videos, activity in superior temporal cortices likely indicated the same auditory-related processing observed in heterosexual and bisexual women, albeit more robustly and extensively, consistent with enhanced attention to emotionally salient stimulus features. When viewing male relative to female erotic videos, activations in the right somatosensory cortex may have indicated processing of bodily sensations , which may have been either positive or negative in valence. Thus, while emotionally associated brain areas did not exhibit differential activations for videos,   homosexual women’s particularly strong engagement of auditory cortices for female stimuli provided yet further evidence of uniquely gender-biased responding, relative to heterosexual and bisexual women  . 


### Comparisons with previous findings 
  
Few studies have investigated the category-specificity of brain activity in non-heterosexual women. Ponseti   et al  .  found that both heterosexual and homosexual women showed gender-specific patterns of brain activity in multiple areas, including the ventral striatum. Sylva   et al  .  also found some evidence for category-specific responding in women, although not in the ventral striatum, and without specifically testing whether or not heterosexual or homosexual women differed in their responses. 

The patterns observed Ponseti   et al  .  stand in contrast to the present investigation in which homosexual women tended to be the only group showing strongly category-specific responses to erotic stimuli. One possible interpretations for their findings of category-specific responses in all women was the unusual nature of the stimuli (i.e., close-up images of male and female genitalia, isolated from interpersonal contextual factors) . As suggested by Chivers (2017) , it may be the case that sex and gender cues can produce specific responses in heterosexual women, but that these are usually trumped by contextual factors in driving arousal responses in women. The stimuli utilized in the present study contained contextual factors (e.g. body posture, facial expression) that are more typical of those found in the genital arousal literature. 

However, it should be noted that the present study did not find support for greater category-specificity in homosexual women across all stimulus conditions. Rather, planned contrasts in the ventral striatum only revealed significant group differences between homosexual and non-homosexual women for erotic pictures. There were no significant differences in ventral striatum response between homosexual and non-homosexual women for erotic videos (even though subjective evaluations of those stimuli did significantly differ across the groups). 

This pattern of differing results for pictures versus videos may be related to differences in how individuals respond to these stimuli, limitations of our video paradigm, or both. While erotic videos may theoretically allow for the assessment of qualitatively different states of sexual response, it may be the case that incentive motivation is greatest when stimuli are first presented, but then diminishes with longer stimulus presentations . Additionally, erotic pictures may have been more effective at driving ventral striatum responses due to factors such as unpredictably varied presentation times of preferred stimuli contributing to larger magnitude reward-prediction errors . 


### Limitations 
  
One limitation of nearly all studies of erotic responses in women—including this one—is a failure to control for hormonal conditions or contraceptive usage. By default, it can generally be assumed that most women were not measured within the ovulatory window, when responses to erotic stimuli might be greatest . Additionally, a number of women may have been using hormonal contraceptives. Measuring women’s responses outside of the fertile phase of their cycles—or while they were using hormonal contraceptives —may have yielded a restricted range of arousal responses. However, the specificity of erotic responding has not been shown to be influenced by menstrual cycle in previous studies of genital arousal . 

Another source of potential limitations may have been the nature of the stimuli used. Though our stimuli were pilot-tested and rated by individuals of different sexual orientations in order to confirm that they would appeal to a broad participant sample, it is never possible to ensure that common stimuli will evoke the responses intended. This may be especially true for something as emotionally salient and individual as sexual arousal. Thus, it is possible that category specificity patterns could appear to be different if stimuli better reflected participants’ subjective preferences. This is a limitation of many studies of sexual responding, although data gleaned from more individualized stimulus sets are difficult to interpret. 

One more aspect of the stimuli that is difficult to control for is sensory details that are inherently different between male and female stimuli. Differences in actors’ vocalizations (for videos), actors’ body positions (for both videos and pictures), and actors’ body motions (in the videos) were present (on average) between male and female stimuli. These features are difficult to control for and could conceivably lead to differences in both subjective and neural responding when viewing male vs. female stimuli, especially in more primary sensory areas of the brain such as visual and auditory cortices. However, such differences may also serve to reinforce the gendered nature of the stimuli and improve their correspondence with real-world experiences and real-world arousal. 



## Conclusions 
  
Though the neural data presented here align with previously-observed patterns in women’s genital and subjective arousal, much remains unknown about the relationship between arousal patterns, orientation, and the development of sexual motivation towards particular sexes in women. Our study supports past findings indicating that women tend not to have strongly category-specific responses to erotic stimuli, with homosexual women showing somewhat greater specificity than heterosexual and bisexual women. Future research should explore the extent to which women’s non-specific sexual response contributes to erotic plasticity (i.e., change with context) and sexual fluidity (i.e., change over time) .</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (2)</summary><details class="inner-accordion"><summary>Table 2 (Tab2) - Differential whole-brain activations in response to male vs. female pictures.</summary><div class="table-html"><table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Differential whole-brain activations in response to male vs. female pictures.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">R/L</th><th colspan="1" rowspan="1">Region</th><th colspan="1" rowspan="1">BA</th><th colspan="1" rowspan="1">MNI</th><th colspan="1" rowspan="1">voxels</th><th colspan="1" rowspan="1">peak T</th></tr></thead><tbody><tr><td colspan="6" rowspan="1">
<italic toggle="yes">Heterosexual Women</italic>
</td></tr><tr><td colspan="6" rowspan="1">    Female &gt; Male Pictures</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">lingual gyrus, inferolateral occipital</td><td colspan="1" rowspan="1">18, 19</td><td colspan="1" rowspan="1">(−33 −76 −10)</td><td colspan="1" rowspan="1">21</td><td colspan="1" rowspan="1">8.01</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">fusiform gyrus</td><td colspan="1" rowspan="1">37</td><td colspan="1" rowspan="1">(30 −67 −7)</td><td colspan="1" rowspan="1">26</td><td colspan="1" rowspan="1">7.81</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">inferolateral occipital, fusiform gyrus</td><td colspan="1" rowspan="1">19, 37</td><td colspan="1" rowspan="1">(42 −70 −16)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.78</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">fusiform gyrus</td><td colspan="1" rowspan="1">37</td><td colspan="1" rowspan="1">(30 −58 −10)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.59</td></tr><tr><td colspan="6" rowspan="1">    Male &gt; Female Pictures: no differential activations</td></tr><tr><td colspan="6" rowspan="1">
<italic toggle="yes">Bisexual Women</italic>
</td></tr><tr><td colspan="6" rowspan="1">    Female &gt; Male Pictures</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">inferior occipital cortex, middle occipital gyrus, lingual gyrus</td><td colspan="1" rowspan="1">18, 19</td><td colspan="1" rowspan="1">(36 −70 −10)</td><td colspan="1" rowspan="1">86</td><td colspan="1" rowspan="1">10.68</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(48 −64 −10)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.96</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">fusiform gyrus</td><td colspan="1" rowspan="1">37</td><td colspan="1" rowspan="1">(33 −55 −13)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.15</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">inferolateral occipital, middle occipital gyrus</td><td colspan="1" rowspan="1">19, 18</td><td colspan="1" rowspan="1">(−39 −85 −7)</td><td colspan="1" rowspan="1">138</td><td colspan="1" rowspan="1">9.07</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">18, 19</td><td colspan="1" rowspan="1">(−30 −88 5)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.47</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">inferior occipital gyrus, primary visual cortex, lingual gyrus</td><td colspan="1" rowspan="1">18, 17</td><td colspan="1" rowspan="1">(−24 −94 −4)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.06</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">occipital cortex, middle occipital gyrus</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(30 −79 17)</td><td colspan="1" rowspan="1">43</td><td colspan="1" rowspan="1">9.03</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">occipital cortex, middle occipital gyrus</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(30 −82 26)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.38</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">inferior occipital cortex, lingual gyrus</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">(−24 −85 −13)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">8.22</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">primary visual cortex, cuneus</td><td colspan="1" rowspan="1">17</td><td colspan="1" rowspan="1">(15 −100 −4)</td><td colspan="1" rowspan="1">12</td><td colspan="1" rowspan="1">8.09</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">(42 −82 −1)</td><td colspan="1" rowspan="1">27</td><td colspan="1" rowspan="1">8.05</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">(27 −85 2)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.62</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">(33 −91 8)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">6.86</td></tr><tr><td colspan="2" rowspan="1">    Male &gt; Female Pictures</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1" /></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">angular gyrus, supramarginal gyrus</td><td colspan="1" rowspan="1">39, 40</td><td colspan="1" rowspan="1">(−51 −64 41)</td><td colspan="1" rowspan="1">21</td><td colspan="1" rowspan="1">9.37</td></tr><tr><td colspan="1" rowspan="1">      R/L</td><td colspan="1" rowspan="1">posterior cingulate</td><td colspan="1" rowspan="1">23</td><td colspan="1" rowspan="1">(0 −22 35)</td><td colspan="1" rowspan="1">10</td><td colspan="1" rowspan="1">8.23</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">angular gyrus</td><td colspan="1" rowspan="1">39</td><td colspan="1" rowspan="1">(48 −61 35)</td><td colspan="1" rowspan="1">27</td><td colspan="1" rowspan="1">7.94</td></tr><tr><td colspan="1" rowspan="1">       R</td><td colspan="1" rowspan="1">supramarginal gyrus</td><td colspan="1" rowspan="1">40</td><td colspan="1" rowspan="1">(51 −61 44)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.75</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">retrosplenial cingulate</td><td colspan="1" rowspan="1">30</td><td colspan="1" rowspan="1">(15 −52 29)</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">7.68</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">supramarginal gyrus</td><td colspan="1" rowspan="1">40</td><td colspan="1" rowspan="1">(57 −49 35)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">7.38</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">supramarginal gyrus</td><td colspan="1" rowspan="1">40</td><td colspan="1" rowspan="1">(−36 −61 44)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">7.08</td></tr><tr><td colspan="6" rowspan="1">
<italic toggle="yes">Homosexual Women</italic>
</td></tr><tr><td colspan="6" rowspan="1">    Female &gt; Male Pictures</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(−27 −82 14)</td><td colspan="1" rowspan="1">139</td><td colspan="1" rowspan="1">10.01</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">18, 19</td><td colspan="1" rowspan="1">(−33 −88 −1)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.99</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">middle occipital gyrus, primary visual cortex</td><td colspan="1" rowspan="1">18, 17</td><td colspan="1" rowspan="1">(−18 −97 2)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.57</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(33 −79 14)</td><td colspan="1" rowspan="1">89</td><td colspan="1" rowspan="1">9.83</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">inferior lateral occipital cortex</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(42 −85 2)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.19</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">middle occipital gyrus</td><td colspan="1" rowspan="1">19</td><td colspan="1" rowspan="1">(33 −70 8)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.78</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">fusiform gyrus, posterior paraphippocampal gyrus</td><td colspan="1" rowspan="1">37</td><td colspan="1" rowspan="1">(33 −52 −10)</td><td colspan="1" rowspan="1">36</td><td colspan="1" rowspan="1">9.56</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">fusiform gyrus, middle occipital gyrus</td><td colspan="1" rowspan="1">18</td><td colspan="1" rowspan="1">(33 −70 −13)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.03</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">primary visual cortex</td><td colspan="1" rowspan="1">17</td><td colspan="1" rowspan="1">(18 −94 −1)</td><td colspan="1" rowspan="1">11</td><td colspan="1" rowspan="1">8.56</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">fusiform gyrus, lingual gyrus, posterior parahippocampal gyrus</td><td colspan="1" rowspan="1">37</td><td colspan="1" rowspan="1">(−33 −61 −7)</td><td colspan="1" rowspan="1">26</td><td colspan="1" rowspan="1">8.45</td></tr><tr><td colspan="1" rowspan="1">       L</td><td colspan="1" rowspan="1">fusiform gyrus</td><td colspan="1" rowspan="1">37</td><td colspan="1" rowspan="1">(−39 −61 −13)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.62</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">precuneus, occipitoparietal</td><td colspan="1" rowspan="1">7</td><td colspan="1" rowspan="1">(24 −73 41)</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">7.18</td></tr><tr><td colspan="6" rowspan="1">    Male &gt; Female Pictures: no differential activations</td></tr></tbody></table></table-wrap></div></details><details class="inner-accordion"><summary>Table 3 (Tab3) - Differential whole-brain activations in response to male vs. female videos.</summary><div class="table-html"><table-wrap id="Tab3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Differential whole-brain activations in response to male vs. female videos.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">R/L</th><th colspan="1" rowspan="1">Region</th><th colspan="1" rowspan="1">BA</th><th colspan="1" rowspan="1">MNI</th><th colspan="1" rowspan="1">voxels</th><th colspan="1" rowspan="1">peak T</th></tr></thead><tbody><tr><td colspan="6" rowspan="1">
<italic toggle="yes">Heterosexual Women</italic>
</td></tr><tr><td colspan="6" rowspan="1">    Female &gt; Male Videos</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior temporal gyrus, primary and secondary auditory cortex</td><td colspan="1" rowspan="1">22, 41, 42</td><td colspan="1" rowspan="1">(−57 −25 8)</td><td colspan="1" rowspan="1">124</td><td colspan="1" rowspan="1">11.71</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior temporal gyrus, primary and secondary auditory cortex</td><td colspan="1" rowspan="1">22, 41, 42</td><td colspan="1" rowspan="1">(−42 −31 11)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">10.15</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior temporal gyrus</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">(−66 −28 8)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">8.99</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">superior temporal gyrus, primary and secondary auditory cortex</td><td colspan="1" rowspan="1">22, 41, 42</td><td colspan="1" rowspan="1">(54 −16 −1)</td><td colspan="1" rowspan="1">48</td><td colspan="1" rowspan="1">10.42</td></tr><tr><td colspan="6" rowspan="1">    Male &gt; Female Videos</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">primary visual cortex, lingual gyrus</td><td colspan="1" rowspan="1">17, 18</td><td colspan="1" rowspan="1">(−9 −82 2)</td><td colspan="1" rowspan="1">30</td><td colspan="1" rowspan="1">10.14</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">inferolateral postcentral gyrus</td><td colspan="1" rowspan="1">3</td><td colspan="1" rowspan="1">(60 −16 29)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">7.94</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">superior parietal lobule</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">(33 −49 62)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">7.64</td></tr><tr><td colspan="6" rowspan="1">
<italic toggle="yes">Bisexual Women</italic>
</td></tr><tr><td colspan="6" rowspan="1">    Female &gt; Male Videos</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior temporal, primary and secondary auditory cortex</td><td colspan="1" rowspan="1">22, 41, 42</td><td colspan="1" rowspan="1">(−54 −10 5)</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">8.83</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">superior temporal gyrus</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">(63 −25 2)</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">7.26</td></tr><tr><td colspan="6" rowspan="1">    Male &gt; Female Videos</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">superior parietal lobule, extending into supramarginal gyrus</td><td colspan="1" rowspan="1">7, 5, 40</td><td colspan="1" rowspan="1">(36 −46 59)</td><td colspan="1" rowspan="1">211</td><td colspan="1" rowspan="1">11.55</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">postcentral gyrus</td><td colspan="1" rowspan="1">3, 1, 2</td><td colspan="1" rowspan="1">(36 −37 53)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">11.27</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">postcentral sulcus</td><td colspan="1" rowspan="1">1, 2</td><td colspan="1" rowspan="1">(33 −31 41)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">9.51</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">postcentral gyrus, postcentral sulcus</td><td colspan="1" rowspan="1">3, 1, 2</td><td colspan="1" rowspan="1">(−36 −37 56)</td><td colspan="1" rowspan="1">25</td><td colspan="1" rowspan="1">8.45</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior parietal lobule</td><td colspan="1" rowspan="1">5, 7</td><td colspan="1" rowspan="1">(−33 −49 59)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">7.55</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">supplementary motor area</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">(30 −10 59)</td><td colspan="1" rowspan="1">12</td><td colspan="1" rowspan="1">8.14</td></tr><tr><td colspan="1" rowspan="1">       L</td><td colspan="1" rowspan="1">supplementary motor area</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">(−24 −13 62)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">7.73</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">cuneus</td><td colspan="1" rowspan="1">17</td><td colspan="1" rowspan="1">(−9 −82 2)</td><td colspan="1" rowspan="1">9</td><td colspan="1" rowspan="1">7.6</td></tr><tr><td colspan="6" rowspan="1">
<italic toggle="yes">Homosexual Women</italic>
</td></tr><tr><td colspan="6" rowspan="1">    Female &gt; Male Videos</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">superior temporal gyrus</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">(51 −10 2)</td><td colspan="1" rowspan="1">578</td><td colspan="1" rowspan="1">14.22</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">primary and secondary auditory cortex, superior temporal gyrus</td><td colspan="1" rowspan="1">41 42, 22</td><td colspan="1" rowspan="1">(54 −19 5)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">13.91</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">superior temporal gyrus</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">(60 −1 −4)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">13.26</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior temporal gyrus</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">(−63 −19 2)</td><td colspan="1" rowspan="1">247</td><td colspan="1" rowspan="1">12.7</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">primary and secondary auditory cortex</td><td colspan="1" rowspan="1">41, 42</td><td colspan="1" rowspan="1">(−45 −25 8)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">11.97</td></tr><tr><td colspan="1" rowspan="1">      L</td><td colspan="1" rowspan="1">superior temporal gyrus</td><td colspan="1" rowspan="1">22</td><td colspan="1" rowspan="1">(−66 −28 5)</td><td colspan="1" rowspan="1" /><td colspan="1" rowspan="1">10.99</td></tr><tr><td colspan="6" rowspan="1">     Male &gt; Female Videos</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">postcentral gyrus</td><td colspan="1" rowspan="1">3</td><td colspan="1" rowspan="1">(33 −34 47)</td><td colspan="1" rowspan="1">7</td><td colspan="1" rowspan="1">7.99</td></tr><tr><td colspan="1" rowspan="1">      R</td><td colspan="1" rowspan="1">postcentral sulcus</td><td colspan="1" rowspan="1">2</td><td colspan="1" rowspan="1">(42 −28 41)</td><td colspan="1" rowspan="1">5</td><td colspan="1" rowspan="1">7.76</td></tr></tbody></table></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 30349467</strong> | Pred included: 1 | Manual included (accepted matches only): 2 | Correct overlaps: 0 | Match statuses: accepted=2, uncertain=2, unmatched=3</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/30349467/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Activations: Leading &gt; Following (Followers only); others, Activations: Leading &gt; Following (Whole-group); others, Deactivations: Following &gt; Leading (Whole-group); others</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>30349467_analysis_0</td><td>Leaders only (n = 11)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>While leading involves self-initiation/agency-related processes, the analysis focuses primarily on interpersonal role expertise and social/mentalizing networks rather than explicit measures of self-perception or self-knowledge; no direct self-related contrasts were reported.</td></tr>
<tr><td>30349467_analysis_1</td><td>Followers only (n = 10)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although leading involves self-initiation and motor planning, the primary focus of the analyses is on role-specific expertise and on perceiving/responding to a partner; the study does not directly probe self-related perception or self-knowledge constructs as the main measure, so it does not meet the perception-self inclusion criteria.</td></tr>
<tr><td>30349467_analysis_2</td><td>Activations: Leading &gt; Following</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TN</span></td><td></td><td>Although leading involves self-initiation and self-monitoring, the paradigm does not explicitly target self-related judgments or self-knowledge; the contrast is interaction-focused rather than a self-perception task.</td></tr>
<tr><td>30349467_analysis_3</td><td>Deactivations: Following &gt; Leading</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TN</span></td><td></td><td>The paradigm probes joint action roles rather than explicit self-reflection, agency judgments, or self-knowledge tasks; self-processing is not the primary measured construct for this contrast. </td></tr>
<tr><td>30349467_analysis_4</td><td>Activation</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>Leading engages self-monitoring/agency and initiation-related processes (preSMA/SMA, DLPFC noted). The analysis of Leading&gt;Following correlated with leader skill probes these self-related processes.</td></tr>
<tr><td>30349467_analysis_5</td><td>Deactivation</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TN</span></td><td></td><td>The contrast examines role expertise effects on leading vs following, not explicit self-perception, agency judgments, or self-knowledge measures.</td></tr>
<tr><td>30349467_analysis_6</td><td>Leading versus following correlated with skill as a follower.</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TN</span></td><td></td><td>Although leading involves elements of self-initiation/agency, this specific analysis (contrast correlated with follower skill) focuses on role-related social processing and understanding others rather than explicit measures of self-perception or self-knowledge. It therefore does not meet the inclusion criteria for Perception and Understanding of Self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>30349467_1</td><td>Activations: Leading &gt; Following (Followers only); others</td><td>30349467_analysis_6</td><td>Leading versus following correlated with skill as a follower.</td><td>0.491</td><td>0.000</td><td>0.147</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>30349467_2</td><td>Activations: Leading &gt; Following (Leaders only); others</td><td>30349467_analysis_2</td><td>Activations: Leading &gt; Following</td><td>0.810</td><td>0.529</td><td>0.614</td><td>uncertain</td><td>coord_count_mismatch</td></tr><tr><td>30349467_3</td><td>Activations: Leading &gt; Following (Whole-group); others</td><td>30349467_analysis_4</td><td>Activation</td><td>0.357</td><td>0.000</td><td>0.107</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>30349467_4</td><td>Deactivations: Following &gt; Leading (Followers only); others</td><td>30349467_analysis_3</td><td>Deactivations: Following &gt; Leading</td><td>0.800</td><td>0.571</td><td>0.640</td><td>uncertain</td><td>coord_count_mismatch</td></tr><tr><td>30349467_5</td><td>Deactivations: Following &gt; Leading (Whole-group); others</td><td>30349467_analysis_5</td><td>Deactivation</td><td>0.400</td><td>0.000</td><td>0.120</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>30349467_6</td><td>[Leading &gt; Rest] and [Following &gt; Rest] (followers only); others</td><td>30349467_analysis_1</td><td>Followers only (n = 10)</td><td>0.380</td><td>1.000</td><td>0.814</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>30349467_7</td><td>[Leading &gt; Rest] and [Following &gt; Rest] (leaders only); others</td><td>30349467_analysis_0</td><td>Leaders only (n = 11)</td><td>0.347</td><td>1.000</td><td>0.804</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 6186800)</summary><p><strong>Title:</strong> Role-Specific Brain Activations in Leaders and Followers During Joint Action</p><details><summary>Abstract</summary><pre class="paper-text">Much of social interaction in human life requires that individuals perform different roles during joint actions, the most basic distinction being that between a leader and a follower. A number of neuroimaging studies have examined the brain networks for leading and following, but none have examined what effect prior expertise at these roles has on brain activations during joint motor tasks. Couple dancers (e.g., dancers of Tango, Salsa, and swing) are an ideal population in which examine such effects, since leaders and followers of partnered dances have similar overall levels of motor expertise at dancing, but can differ strikingly in their role-specific skill sets. To explore role-specific expertise effects on brain activations for the first time, we recruited nine skilled leaders and nine skilled followers of couple dances for a functional magnetic resonance imaging study. We employed a two-person scanning arrangement that allowed a more naturalistic interaction between two individuals. The dancers interacted physically with an experimenter standing next to the bore of the magnet so as to permit bimanual partnered movements. Together, they alternated between leading and following the joint movements. The results demonstrated that the brain activations during the acts of leading and following were enhanced by prior expertise at being a leader or follower, and that activity in task-specific brain areas tended to be positively correlated with the level of expertise at the corresponding role. These findings provide preliminary evidence that training at one role of a joint motor task can selectively enhance role-related brain activations.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Much joint action between two people involves the contrastive roles of leader and follower ( ). For example, when two people move a sofa, the front person is often the one who navigates the joint movement as well as the one who determines the speed at which the two people move, while the back person responds to these movement-cues and attempts to coordinate his/her actions with the front person. However, the experimental literature that examines joint action in the lab does not give consideration to individual differences, for example the fact that people may be predisposed toward being a leader or follower based on their personality traits or life experiences ( ). In typical studies of joint action, people are randomly assigned to being a leader or follower (or both) of a joint task without assessing individual differences in task expertise that may exist between them. This applies to studies of both experts ( ;  ) and non-experts ( ;  ;  ;  ). This may be problematic since many studies demonstrate that expertise has an effect on behavioral performance and brain activations across many domains ( ;  ;  ). 

An interesting solution to this problem is to examine couple dancers, such as Tango dancers, since such people engage in extensive training to develop expertise at one specific role in the dance, thereby making the assessment of leading/following experience on joint action quite feasible. Leaders and followers of a couple dance have similar overall levels of motor expertise at dancing, but they can differ strikingly in their role-specific skill sets, such that dancers of one role are often unable to dance the opposite role. This applies not merely to the movement patterns themselves, but to the   coordination   skills required for leading (e.g., force conveyance) and following (e.g., responsiveness to force cues). While previous neuroimaging studies have looked separately at the topics of leading/following and expertise, the current study–which is a follow-up analysis to a previously published study from our lab ( )–represents a first attempt at examining   role-specific expertise   at leading and following, doing so using trained leaders and followers of couple dances. The principal aim of the study is to identify role-specific brain activations, namely leading-related activations in trained leaders compared to non-leaders, and following-related activations in trained followers compared to non-followers. 

Previous studies of leading and following have tended to emphasize the networks for leading, more so than those for following. For example, studies of interactive imitation have compared the initiation and imitation of visual actions within the same group of participants, and have highlighted an initiation network involved in self-monitoring, willed action, and decision making ( ;  ;  ;  ). Studies of auditory-entrainment tasks, such as finger tapping, have studied expert leaders or individuals who spontaneously emerge as leaders with in the context of the study, and have similarly identified a network involved in decision making, movement initiation, and self-processing ( ;  ;  ). These studies have provided either no results or inconsistent findings regarding following or expert followers. In a previous publication from our lab ( ), we characterized the networks for leading and following during a joint-action task with physical interaction, using the same dancer participants as those employed in the present study. In accordance with the previous literature, we found that leading showed a motor- and self-oriented profile, engaging areas associated with motor planning, spatial navigation, sequencing, action monitoring, and error correction. In contrast, following showed a far more sensory- and externally oriented profile, revealing areas involved in somatosensation, proprioception, motion tracking, social cognition, and outcome monitoring. However, while that study compared the act of leading with the act of following, it did not assess the influence of prior expertise at being a leader or follower on the brain activations. That was the major objective of the current follow-up analysis, namely to examine role-specific expertise. 

It is well-established that expertise can influence both the structure and function of the brain. There is now a vast literature devoted to various forms of motor, perceptual, and cognitive expertise (reviewed in  ). A general finding of such studies is that brain activations and gray matter volume are enhanced in experts, as compared to non-experts, in areas that process the skills that underlie a person’s domain of expertise ( ;  ;  ). For example, with regard to perceptual tasks, trained musicians and other auditory experts show enhanced effects in auditory cortex ( ,  ;  ;  ), while visual experts show effects in visual cortex ( ,  ;  ). In the motor domain, effects are found in cortical and subcortical motor and premotor areas involved in motor execution, control, planning, and representation ( ;  ). Motor experts, such as athletes, dancers, and musicians, additionally demonstrate changes in perceptual and cognitive areas associated with their trained skills ( ). For example, sensorimotor coupling is enhanced in musicians and athletes ( ;  ). In addition, activations in the action-observation network [including premotor cortex (PMC), superior parietal lobule (SPL), and inferior parietal lobule (IPL)] are enhanced when dancers view specific dance patterns that they are expert in ( ), or when athletes view sports actions that they are expert in ( ), as compared to when the same people view dances or sports movements that they are not trained in.   suggested that this effect was due to motor training, rather than the associated perceptual training. Expertise, in addition to producing enhancements in processing, has also been linked to decreases in the overall number of activated foci in neuroimaging studies, especially in attentional and cognitive-control networks, suggesting an enhancement in automaticity of processing for the trained skill ( ;  ). The “two stage expertise hypothesis” ( ;  ) suggests that short-term training leads to enhancements of brain activations for the trained skill, while long-term training and skill mastering lead instead to decreases or reorganizations in brain activations. 

While previous neuroimaging studies have looked at leading/following and expertise in isolation, no study thus far has combined the two issues, which is the principal objective of the present study. As mentioned above, couple dancers are an ideal cohort for exploring role-specific expertise in leading and following, since they spend many years developing expertise at typically just one of the two roles of the dance. As a result, expert leaders are usually unskilled followers, and vice versa, while both groups have comparable levels of overall motor expertise at the dance. More specifically, leader expertise during couple dancing requires the generation of a motor plan for both the self and the partner, and the efficient conveyance of signals to the partner, while follower expertise requires the tracking of information coming from the leader and its interpretation to construct either an identical or complementary movement pattern in real time. 

In order to assess the effect of role expertise on brain activations during an ecologically valid joint-action task, we carried out an exploratory follow-up analysis to our previous publication that looked at leading and following ( ) in order to examine the effects of role-specific expertise on brain activations. In the previous study, skilled leaders and followers of couple dances performed both a leading and following task in a magnetic resonance imaging (MRI) scanner in interaction with an experimenter standing next to the bore of the magnet. The participant and experimenter were in physical contact at their hands, and alternated between being the leader and follower of joint improvised bimanual movements. The principal aim of the study was to compare brain activity during the acts of leading and following. The current study follows up on those results using the same dataset in order to examine the effects of individual differences on the brain activations, in particular an individual’s expertise at a given role of the dance. The aim was to look for role-specific brain activations, in other words leading-related activations in trained leaders compared to non-leaders (i.e., followers), and following-related activations in trained followers compared to non-followers (i.e., leaders). Based on the literature cited above demonstrating that experts show enhancements in task-specific brain areas compared to non-experts when performing the same tasks, we predicted that leaders, as compared with non-leaders, would show an enhancement of leading-related activations when leading (only), and likewise that followers, as compared with non-followers, would show an enhancement of following-related activations when following (only). Given that we were not able to effectively rule out the influence of gender on dance role in our design, the results need to be viewed as exploratory. 


## Materials and Methods 
  
### Participants 
  
Eighteen participants (nine of each gender) took part in this study after giving their written informed consent in accordance with the Hamilton Integrated Research Ethics Board, who approved the study (St. Joseph’s Healthcare, R. P. #12-3777). They received monetary compensation for their participation. None of them had a past history of neurological or psychiatric disease. An inclusion criterion for the study was that participants have at least 2 years of experience at one or more kinds of couple dances involving leading and following (e.g., Argentine Tango, Salsa, Swing, and Ballroom). Male participants (40.7 ± 14.9 years old) had a mean dance experience of 8.7 ± 7.2 years, principally as leaders, although one male had significant experience as a follower as well. Female participants (40.2 ± 12.3 years old) had a mean dance experience of 5.6 ± 2.9 years, principally as followers, although two females had significant experience as leaders as well. 

On the day of the experiment, participants reported their ability to lead or follow a couple dance using a scale from 0 to 100, where 0 corresponds to no expertise at leading or following, and 100 corresponds to a very high level of expertise at leading or following. Each person did separate ratings for leading and following skill, with results shown in   Figure   . We explained to participants that these scales emphasized the ability to transmit/receive information while dancing with a partner, rather than the ability to perform complex or stylistic movements. Males reported a mean leading ability of 69.8 ± 17.7 (one male was at 35 and the rest ranged from 60 to 90). Likewise, females reported a mean following ability of 77.2 ± 8.3 (ranging from 70 to 90). With regards to the complementary skill, males reported a mean following ability of 33.7 ± 21.6; the male with significant following experience reported his following ability at 78, while all the others males rated it at between 8 and 50. Females reported a mean leading ability of 28.9 ± 25.2; both females with significant leading experience reported their leading ability at 70, while all other females rated themselves at between 5 and 40. Correlations between leading ability, following ability, years of experience at dancing, and age showed that leading ability, but not following ability, correlated with the number of years of experience (  Table   ). Anecdotal evidence suggests that leading skill requires a greater amount of time and effort to achieve than does following skill, which may explain the exclusive correlation of leading skill with years of experience. Since leading and following ability were not anti-correlated in the analysis, participants designated as “leaders” in this study were comprised of all the participants who were primarily trained as leaders for at least 2 years (i.e., all the of males) plus the two participants who, although primarily trained as followers, had significant leading experience and a strong leading ability (two females). Those designated as “followers” were comprised of all the participants who were primarily trained as followers for at least 2 years (i.e., all of the females) plus one participant who, although primarily trained as leader, had significant following experience and a strong following ability (one male). Thus, three participants belonged in both groups. This division was used in only the first set of analyses (see below). 
  
Self-report scales for skills as a leader and follower of couple dances. The   x  -axis shows the self-rating scale for leader skill (left panel) and follower skill (right panel) for couple dancing, where 100 is the highest rating. The   y  -axis of each graph shows the number of participants, from the pool of 18, who rated themselves at the various levels of skill for each role. Female participants are color-coded red and males are color-coded blue, both here and in   Figures  ,   . Participants designated as “leaders” in this study were comprised of all the males plus the two females with strong leading ability, while those designated as “followers” were comprised of all the females plus the male with strong following ability. Leaders are color coded as purple here and in   Figures  –  , whereas followers are color coded as pink here and in   Figures  –   (not to be confused with the color coding of gender). 
    
Correlation between age, years of couple-dance experience, and self-reported leading and following skill. 
    

### Procedure 
  
While the participant was lying supine in the MRI scanner, an experimenter (LASC) stood next to the bore of the scanner in order to have physical contact with the participant’s two hands. The participant’s forearms were fastened to the side of their body such that only their wrists, hands and fingers were able to move. Participants’ hands (palms up) were always below the experimenter’s hands (palms down), so that the participants’ hands could not be passively moved. The experimenter had significant experience both as a follower and a leader of couple dances. Together, the participant and experimenter performed highly controlled joint hand movements in all three planes of motion, alternating between leading and following the joint movement during different task-epochs of the scan. The movement patterns were improvised, rather than pre-learned, in order to maintain an ongoing requirement for motor planning during leading and a comparably heightened sense of responsiveness during following. No external cuing of tempo or rhythm was done with a metronome or with music. Participants performed all conditions with their eyes closed, and were instructed about which task to perform by means of pre-recorded verbal cues delivered through MRI-compatible headphones. Each condition was performed in a random order six times in blocks of 28 s. 

Complete methods and details concerning fMRI acquisition and image analysis, including participant training, are described in  . Briefly, the functional MRI imaging parameters were 2000 ms TR, 35 ms TE, 90° flip angle, 39 axial slices, 4 mm slice thickness, 0 mm gap, 3.75 × 3.75 mm in-plane resolution, 64 × 64 matrix, and 240 mm field of view. An automatic shimming procedure was performed before each scan to minimize inhomogeneities in the static magnetic field. For each of the three functional scans, 216 volumes–corresponding to 12 epochs of 28 s task + 8 s rest–were collected over 7’12”, resulting in a total of 648 volumes. Two magnetic field maps (5 ms then 8 ms TE) with the same imaging parameters as the fMRI were acquired in order to unwarp the data. Unwarping was performed with the relaxation method of “anatabacus”, a plugin in BrainVoyager, in order to correct for non-rigid deformations. In addition, the head-motion parameters were included as nuisance regressors in the analysis. Functional and structural images were processed using BrainVoyager QX 2.8. Coordinate tables were computed using NeuroElf. 


### Analysis 
  
We first performed qualitative analyses on three groups to assess if there were any differences between being a leader and being a follower. Specifically, we carried out three random-effects analyses for the bidirectional contrast “Leading versus Following” (1) for the whole group of 18 participants, (2) for the 11 leaders only, and (3) for the 10 followers only. These were performed at a two-tailed statistical threshold of   p   &lt; 0.005 uncorrected with a cluster-level correction of   k   = 28 voxels determined with Alphasim (family-wise error   p   &lt; 0.05) in NeuroElf. The conjunction of [Leading &gt; Rest] ∩ [Following &gt; Rest] was also performed on these three groups in order to serve as a reference for the general network of brain areas activated by the movement tasks, irrespective of role. It was performed at a two-tailed statistical threshold of   p   &lt; 0.005 uncorrected with a cluster-level correction of   k   = 49 voxels determined with Alphasim. 

Since qualitative differences were found (see Results section), we tested further for the effect of role by performing whole-brain regression analyses on the full group of participants (  n   = 18). We chose to perform statistical regression analyses instead of a direct statistical comparison between leaders and followers for two reasons. First, we consider role expertise to be a continuous trait, rather than a dichotomous one. Dancers can belong to both groups if they are trained at both leading and following. Thus a binary distinction would have led to a “male versus female” contrast, rather than a “leader versus follower” contrast. Second, the number of participants in each group was small (  n   = 10 and 11 for leaders and followers, respectively), whereas the regression involved the full group of 18 participants. Because of the small number of participants in the analysis and because of the small number of female leaders and male followers in the cohort, we consider this an exploratory study. Future studies will need to examine larger numbers of participants who have both leading and following skills, although such dual training tends to be limited to professional teachers of a dance. 

For the whole-brain regression analyses, the self-reported values of leading and following skill were used as covariates in two separate analyses to regress the betas values of the contrast “Leading versus Following”. These regressions were also performed at a two-tailed statistical threshold of   p   &lt; 0.005 uncorrected with a cluster-level correction of   k   = 25 voxels, determined with Alphasim. However, this threshold led to null results, and so we reported the activation at a less stringent threshold of   p   &lt; 0.025 uncorrected with a cluster-level correction of k = 46 voxels, determined with Alphasim. We note that these results should be interpreted with caution and need to be replicated in future analyses. In order to examine the influence of gender, the mean beta value of each activated cluster was extracted for each participant and regressed against his/her corresponding leading or following skill. 



## Results 
  
In order to identify the basic sensorimotor network involved in performing our joint bimanual tasks, we carried out the conjunction of [Leading &gt; Rest] ∩ [Following &gt; Rest], with results shown in   Figure    and Talairach coordinates reported in   Table   . This shared network between leading and following consisted of a widespread sensorimotor cortical (primary motor and somatosensory cortex) and subcortical (thalamus and cerebellum) network, as well as the supplementary motor area (SMA), midcingulate cortex (MCC), SPL, inferior frontal gyrus (IFG), IPL (including the secondary somatosensory cortex [SII] and extending to the insula), and inferior temporal gyrus (ITG), extending to the middle temporal gyrus (MTG). Except for the ITG, which was present in leaders only, this network was found in both leaders and followers. 
  
Shared network for leading and following. The figure shows the results of the conjunction [Leading &gt; Rest] ∩ [Following &gt; Rest] in leaders only (left panel) and followers only (right panel),   p   &lt; 0.005 uncorrected (  k   = 49 voxels). With the exception of the inferior temporal gyrus, the activated network is similar in both followers and leaders. CB, cerebellum; IFG, inferior frontal gyrus; IPL, inferior parietal lobule; ITG, inferior temporal gyrus; MCC, middle cingulate cortex; SMA, supplementary motor area; SMC, sensorimotor cortex; SPL, superior parietal lobule; and Th., thalamus. 
    
The shared network for leading and following. 
    
We next wanted to explore our question of interest, namely whether there was evidence for role-specific activations, in other words activations found only in skilled individuals while performing the role they are trained in. This would reveal whether leaders and followers engage different brain resources during leading and following. As shown in   Figure    and   Table   , we first qualitatively compared three types of analyses of the “Leading &gt; Following” contrast (cyan clusters) and “Following &gt; Leading” contrast (yellow clusters): the whole group of 18 participants; only the leaders (a subset of 11 participants); and only the followers (a subset of 10 participants). Overall, the leaders-only analysis showed basically the same network for leading as the whole group, but no brain areas for following. Likewise, the followers-only analysis showed basically the same network for following as the whole group, but only the dorsolateral prefrontal cortex (DLPFC) for leading (Note that only role-specific activations are labeled in the   Figure   ). 
  
Role-specific brain activations. The figure shows an analysis of the bidirectional “Leading versus Following” contrast in three groupings: the whole group of 18 participants; only the leaders (a subset of 11 participants); and only the followers (a subset of 10 participants). Contrasts are performed at   p   &lt; 0.005 uncorrected (  k   = 28 voxels). The top panel is the midsagittal view, the lower left panel is the left hemisphere, and the lower right panel is the right hemisphere. Each panel is set up as a triad, with the whole group at the top and the restricted analyses of leaders-only and followers-only below that. Cyan clusters and outlines reflect the contrast of “Leading &gt; Following”, whereas yellow clusters and outlines reflect the reverse contrast of “Following &gt; Leading”. In order to facilitate the visualization of role-specific activations, we use colored outlines to represent whole-group activations that are missing in either the leaders-only or the followers-only analyses. More specifically, cyan outlines are regions of whole-group activation that are present in the leaders-only analysis, but not the followers-only analysis, while yellow outlines are regions of whole-group activation that present in the followers-only analysis, but not the leaders-only analysis. The leaders-only analysis shows the same network for leading as the whole group, but no brain areas for following. The followers-only analysis shows the same network for following as the whole group, but only the cerebellum and dorsolateral prefrontal cortex for leading. Only role-specific activations are labeled in this figure. Leading network: CMA: cingulate motor area; PMC, premotor cortex; SMA, supplementary motor area; and SPL, superior parietal lobule. Following network: PCC, posterior cingulate cortex and TPJ, temporo-parietal junction. 
    
Leading versus following in the whole group, the leader-only group, and the follower-only group. 
    
Regarding the leading task, role-specific activations that were found exclusively in skilled leaders (cyan activations in   Figure    in both the leaders-only and whole-group brains that correspond with the cyan outlines in the followers-only brain) were observed in the SMA and cingulate motor area (CMA; top panel), SPL (right and left hemispheres in the lower panels), and PMC(left hemisphere). In addition, while leading, leaders showed a more extended premotor activation than the whole-group, especially in the right hemisphere (  Table   ). 

Regarding the following task, role-specific activations that were found exclusively in skilled followers (yellow activations in   Figure    in both the followers-only and whole-group brains that correspond with the yellow outlines in the leaders-only brain) were observed in the posterior cingulate cortex (PCC; top panel), temporo-parietal junction (TPJ; right and left hemispheres in the lower panels), and parahippocampal cortex (PHC, not shown). In addition, while following, followers showed activity in the posterior superior temporal sulcus (pSTS) that was not present in the whole group (  Table   ). To summarize, the networks associated with leading and following seemed to be more strongly engaged by experts at the corresponding role than non-experts at that role. 

We followed up on these qualitative analyses with whole-brain regressions in which the self-reported expertise at being a leader or follower (see   Figure    above) was used as the covariate for the contrast of leading versus following. Activations for these analyses were only found at a more lenient threshold, but are still reported since they are consistent with both our hypotheses and the qualitative analyses reported above. However, the results should be interpreted with caution.   Figure    shows the regressions with leader skill, and   Figure    shows the regressions with follower skill. The regions where activations during the leading task correlated with leader skill included the SMA, pre-SMA, dorsal PMC (dPMC), superior temporal gyrus (STG), and insula (  Figure    top panel,   Table   ). The regions where activations during the following task correlated with follower skill include the PCC, TPJ, pSTS, and mPFC (  Figure    top panel,   Table   ). For each cluster, the coefficient of determination (  R  ) of the regression of the mean beta value against leader and follower skill is shown in   Tables  ,   , respectively. 
  
Regression of brain activation with leader skill in the whole group of participants. The top panel of this figure shows brain activity that correlates with the contrasts “Leading &gt; Following” (cyan activations) and “Following &gt; Leading” (yellow activations). Contrasts are performed at   p   &lt; 0.02 uncorrected (  k   = 25 voxels). Brain areas for “Leading &gt; Following” that correlate with leader skill include the dorsal premotor cortex (dPMC), insula (Ins.), superior temporal gyrus (STG), and supplementary motor area (SMA). Almost no areas for the contrast “Following &gt; Leading” correlate with leader skill (see   Table   ). The lower plots show mean beta values extracted from the SMA, dPMC, posterior insula and STG against leader skill, where female participants are shown with red dots and male participants with blue dots. Activity for leading increased with increasing leader skill, and this seems to be independent of gender. 
    
Regression of brain activation with the follower skill in the whole group of participants. The top panel of this figure shows brain activity that correlates with the contrasts “Leading &gt; Following” (cyan activations) and “Following &gt; Leading” (yellow activations). Contrasts are performed at   p   &lt; 0.02 uncorrected (  k   = 25 voxels). Brain areas for “Following &gt; Leading” that correlate with follower skill include the medial prefrontal cortex (mPFC), posterior parietal cortex (PCC), posterior superior temporal sulcus (pSTS), and temporo-parietal junction (TPJ). No areas appeared for the contrast “Leading &gt; Following” correlate with follower skill (see   Table   ). The lower plots show mean beta values extracted from the TPJ, PCC, mPFC, and pSTS against follower skill, where female participants are shown with red dots and male participants with blue dots. Activity for following increased with increasing follower skill, and this seems to be independent of gender. 
    
Leading versus following correlated with skill as a leader. 
      
Leading versus following correlated with skill as a follower. 
    
Examples of how the mean beta value in these regions covaries with leader and follower skill are shown in the bottom panels of   Figures  ,   , respectively. The results provide some evidence that activity in these regions might depend on the level of expertise. However, they in no way rule out a gender effect, either alone or in interaction with expertise, and so the results have to be seen as preliminary. In the dPMC and STG (  Figure   , bottom panels), activity for the contrast of “Leading &gt; Following” increased with leader skill, but a male with low leader skill had a low activity, whereas females with high leader skill had a high activity. Other areas that correlated with leader skill had the same trend (not shown). Similarly, in the mPFC and TPJ (  Figure   , bottom panels), activity for the contrast “Leading &gt; Following” decreased with follower skill (that is, “Following &gt; Leading” activity increased with follower skill), but a male with high follower skill had a low activity, similar to females with high follower skill. Other areas that correlated with follower skill had the same trend (not shown). Future studies will be needed to fully exclude the influence of gender on the expertise effects observed here. Hence, the current study must be seen as a pilot study that gives a first glimpse at role-specific expertise effects without being able to effectively factor out the influence of gender. 


## Discussion 
  
This current exploratory study examined for the first time the effect of expertise at the coordinative skills involved in leading and following on brain activations during a joint-action task in a realistic setting. Its results provide support for the existence of role-specific brain activations during joint actions. In particular, we observed that leading-related activations were enhanced in leaders compared to followers when both groups performed the leading task, and that following-related activations were enhanced in followers compared to leaders when both groups performed the following task. Additionally, we showed that leading-related brain regions in the whole group of participants tended to correlate with expertise at being a leader, whereas following-related brain regions tended to correlate with expertise at being a follower. Another way of conceptualizing these results is that the skilled leaders hardly engaged any areas during following that were not already engaged during leading; likewise, the skilled followers hardly engaged any areas during leading that were not already engaged during following. This might explain the null results found in some previous studies when comparing following with leading ( ). These results suggest that expertise at one role of a joint-action task can enhance brain activations for the trained role compared to the untrained role. Hence, not only do the results support the existing literature on expertise effects for motor tasks, but they extend it for the first time to the contrastive roles of leader and follower in joint actions. 

The major finding of the initial qualitative analysis (  Figure   ) was that the brain networks that we observed for leading and following in the whole group seemed to be mainly supported by prior experience at being a leader or follower. In particular, skilled followers strongly engaged the mentalizing and social networks (PCC, TPJ, and STS) while following, which is consistent with a view of following as a process of adapting to one’s partner or as inferring knowledge from one’s partner ( ;  ;  ;  ;  ;  ). In contrast, skilled leaders strongly engaged networks for motor control and planning (SMA, CMA, PMC, and cerebellum) and for spatial navigation and exploration (SPL) while leading, which is consistent with the requirements of the leading role ( ;  ;  ;  ;  ). Interestingly, both skilled leaders and skilled followers activated the DLPFC during leading, which implies that self-initiation and action selection ( ;  ;  ) are probably the most important characteristics of leading, regardless of expertise. 

By performing whole-brain regressions with leading or following skill, we treated being a leader or follower as a continuous trait, rather than a dichotomous one. Although we did not find any activity using our   a priori   threshold, the activations observed at a more lenient threshold were consistent with both our hypotheses and the qualitative results, and are thus reported as exploratory findings. We observed that distinct brain areas tended to correlate with the level of self-reported expertise at being a leader or a follower, respectively. The areas that correlated with follower skill were principally components of the following network, such as the mPFC, PCC, TPJ, and pSTS. Thus, the more that someone is trained at following, the more that s/he will recruit brain regions of the mentalizing and social networks, which might indicate more attention to, or more efficient processing of, social stimuli (i.e., cues coming from the leader) and the mental states of others (i.e., their intentions and action plans). Another characteristic of followers is their ability to track their partner’s movements or other signaling cues so as to produce either imitative or complementary movements. Along these lines, the pSTS has been specifically implicated in the multisensory perception of biological motion ( ;  ), indicating that a trained follower might be specialized in analyzing information coming from the partner’s movement, not least haptic information emanating from body contact ( ). 

In contrast to this profile for following, the areas that tended to correlate with leader skill were mainly part of the leading network, including premotor areas (pre-SMA, SMA, and PMC). Other areas that tended to correlate with leader skill were the insula and STG. This network is quite similar to the one shown to be activated by motor experts in the meta-analysis of  . In addition, all of the areas associated with leader skill in the present study have been previously shown to be involved in improvisation ( ). Since leading requires the ability to improvise movements, we can assume that the better a person is at leading, the better s/he can improvise a motor plan for both the self and the partner, and thus the more s/he recruits premotor areas and the STG. However, it has also been shown that improvisational expertise (in musicians, for example) is related to a deactivation in the DLPFC, TPJ, IFG, and insula ( ;  ), which has been interpreted as indicating an automation of cognitive processing and a greater focus on internal processes during improvisation ( ). The absence of deactivations in these regions in our study can potentially be explained by the fact that our use of a joint task may have precluded the adoption of an internal focus by the participants when leading. Indeed, a study of joint improvisation also found an activation increase in the DLPFC, pre-SMA, and STG ( ), which is quite similar to a situation of improvising with a dance partner when leading. 

Overall, the study integrates two issues in the cognitive neuroscience of motor performance, first the contrast between leading and following, and second the influence of individual differences in motor expertise on brain activations. As mentioned in the Introduction, many experimental studies of joint action randomly assign people to being a leader or follower of a joint task ( ;  ;  ;  ;  ). However, in Western dance culture, people are generally assigned these roles based on their gender, with men tending to be assigned the role of leader in couple dances. Thus, in contrast to a study of piano duetting ( ), for example, people come to a dance study like ours with years of experience at just one role of the joint task. This provides us with the unique ability to examine individual differences in joint action based not on random factors but on role-specific training. Previous studies of expertise processing have demonstrated enhanced brain activations in experts compared to non-experts ( ,  ;  ,  ;  ;  ;  ;  ;  ;  ;  ). However, this has often has been investigated using non-motor tasks, even in motor experts like professional ballet dancers ( ). We have instead probed this using a motor task, with the added benefit of doing this using a joint-action task. The integration of these two issues is that we were able to examine the contrast between leading and following–as per studies of joint action–but to incorporate the factor of prior motor experience, as per studies of expertise processing. The results revealed a clear overlap between these two issues, such that the brain activations during the acts of leading and following were enhanced by prior expertise at being a leader or follower, and that activity in task-specific brain areas tended to be positively correlated with the level of expertise at the corresponding role. In other words, we were able to demonstrate   role-specific enhancements   in brain activation. 

### Limitations 
  
Given that this study was a first attempt to examine the effect of role expertise on brain activations during joint action, we are aware that it has a number of significant limitations. First, we were limited in our ability to measure behavioral performance during task production in the scanner due to an absence of MRI-compatible technologies such as motion capture at our imaging center. Thus, we cannot determine if the differences between leaders and followers seen in the study are due to trait-related differences in activation or behavioral differences as well. The joint-action task performed in this study was quite simple and involved very small hand movements. Hence, it did not require any type of specialized skill, which would foster similar performance in the two groups. In addition, the experimenter was the sole interaction partner for all of the participants in the study and was thus a controlled factor in the interaction. However, the absence of a technology like motion capture means that we are unable to rule out behavioral differences between participants as a source of the results. Further research taking advantage of MRI-compatible technologies will be required to explore this issue. 

Second, the qualitative analyses showed an interesting pattern that was confirmed by the whole-group regression at a more lenient threshold, but not at a standard threshold. Hence, the effects seem to be small. Although the observed activations at the less stringent threshold were consistent with our expectations based on previous studies, the results of this study should be taken with caution and need to be replicated, preferentially with a larger cohort and a wider spread of skill levels. In addition, the skill levels that were used to regress the brain data were self-report data. They might thus have been subject to self-report biases and inaccuracies. However, no objective measure of leadership and/or followership skills exists in the literature. Given the preliminary results of this study, it would be worthwhile to develop such measures in future. Such measures could be used to see if the results of the present study could be replicated based on people’s role expertise in some other motor skill outside of dancing, or even on people’s natural predispositions to be a leader or follower, as related to personality traits and life experiences, rather than the specialized skill of dance training. 

Finally, and importantly, we are unable to rule out gender as a factor in determining the role-specific effects in our study, and hence the results need to be seen as quite preliminary. While the leader and follower groups were not exclusively of one gender, they did have a majority of one gender. Given the evidence for gender effects on a diversity of perceptual, cognitive, and motor tasks ( ;  ;  ;  ;  ), further studies will be required to assess a gender contribution to our results with trained couple dancers. Given the paucity of female leaders and male followers in the world of couple dancing, perhaps the only approach that will be able to address the limitations of the current study is a training study. A study that crosses gender with role during a several-month training program of leading or following for some joint-action task could permit a disentangling of the relative effects of gender and expertise. If female leaders and male followers showed the same role-specific effects as in the current study, this would argue against a gender interpretation in favor of expertise   per se  . Such a study could also reveal potential gender effects as well. 



## Conclusion 
  
This study is the first to look at the influence of prior individual training at being a leader or follower on the brain activations occurring during the acts of leading and following, thereby assessing the effect of role expertise during naturalistic joint action. Our major finding was that leaders and followers do not seem approach leading and following in the same way at the neural level, with leaders engaging more brain resources during leading, and followers during following, thus reflecting role-specific activations. Additionally, we showed that activity in leading-related brain regions tended to correlate with expertise at being a leader, and likewise that activity in following-related brain regions tended to correlate with expertise at being a follower. These findings highlight the fact that the acts of leading and following might be skill-specific, and thus that prior experience at these roles should be assessed when studying leading and following during joint action. However, given our inability to disentangle gender from dance role, the current results must be seen as preliminary. A training study that crosses gender with role will probably be required to truly distinguish dance role from gender. 


## Author Contributions 
  
LC ran the experiment and analyzed the data. LC and SB conceived the experiment, analyzed the results, and wrote the manuscript. 


## Conflict of Interest Statement 
  
The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (4)</summary><details class="inner-accordion"><summary>Table 2 (T2) - The shared network for leading and following.</summary><div class="table-html"><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><p>The shared network for leading and following.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="center" colspan="5" rowspan="1">Leaders only (<italic toggle="yes">n</italic> = 11)</th><th valign="top" align="center" colspan="5" rowspan="1">Followers only (<italic toggle="yes">n</italic> = 10)</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">Area</th><th valign="top" align="left" rowspan="1" colspan="1">BA</th><th valign="top" align="left" rowspan="1" colspan="1">Hemisphere</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">SMC</td><td valign="top" align="left" rowspan="1" colspan="1">1,3,4,5,6,40</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">33</td><td valign="top" align="center" rowspan="1" colspan="1">-40</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">1356</td><td valign="top" align="center" rowspan="1" colspan="1">17.01</td><td valign="top" align="center" rowspan="1" colspan="1">30</td><td valign="top" align="center" rowspan="1" colspan="1">-37</td><td valign="top" align="center" rowspan="1" colspan="1">61</td><td valign="top" align="center" rowspan="1" colspan="1">1149</td><td valign="top" align="center" rowspan="1" colspan="1">16.54</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SMC</td><td valign="top" align="left" rowspan="1" colspan="1">2,3,4,5,6,40</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-39</td><td valign="top" align="center" rowspan="1" colspan="1">-43</td><td valign="top" align="center" rowspan="1" colspan="1">52</td><td valign="top" align="center" rowspan="1" colspan="1">1419</td><td valign="top" align="center" rowspan="1" colspan="1">17.67</td><td valign="top" align="center" rowspan="1" colspan="1">-39</td><td valign="top" align="center" rowspan="1" colspan="1">-37</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">1135</td><td valign="top" align="center" rowspan="1" colspan="1">18.19</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">IFG</td><td valign="top" align="left" rowspan="1" colspan="1">6,13,44</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">54</td><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">19</td><td valign="top" align="center" rowspan="1" colspan="1">174</td><td valign="top" align="center" rowspan="1" colspan="1">5.86</td><td valign="top" align="center" rowspan="1" colspan="1">54</td><td valign="top" align="center" rowspan="1" colspan="1">8</td><td valign="top" align="center" rowspan="1" colspan="1">7</td><td valign="top" align="center" rowspan="1" colspan="1">108</td><td valign="top" align="center" rowspan="1" colspan="1">8.05</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SMA/MCC</td><td valign="top" align="left" rowspan="1" colspan="1">6, 24, 31</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">-13</td><td valign="top" align="center" rowspan="1" colspan="1">52</td><td valign="top" align="center" rowspan="1" colspan="1">610</td><td valign="top" align="center" rowspan="1" colspan="1">14.68</td><td valign="top" align="center" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">-22</td><td valign="top" align="center" rowspan="1" colspan="1">49</td><td valign="top" align="center" rowspan="1" colspan="1">683</td><td valign="top" align="center" rowspan="1" colspan="1">12.14</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">IPL/SII</td><td valign="top" align="left" rowspan="1" colspan="1">13,40,41</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">-28</td><td valign="top" align="center" rowspan="1" colspan="1">25</td><td valign="top" align="center" rowspan="1" colspan="1">294</td><td valign="top" align="center" rowspan="1" colspan="1">7.26</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="center" rowspan="1" colspan="1">-34</td><td valign="top" align="center" rowspan="1" colspan="1">31</td><td valign="top" align="center" rowspan="1" colspan="1">108</td><td valign="top" align="center" rowspan="1" colspan="1">6.82</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">IPL/SII</td><td valign="top" align="left" rowspan="1" colspan="1">13, 22,40</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-51</td><td valign="top" align="center" rowspan="1" colspan="1">-28</td><td valign="top" align="center" rowspan="1" colspan="1">19</td><td valign="top" align="center" rowspan="1" colspan="1">240</td><td valign="top" align="center" rowspan="1" colspan="1">9.52</td><td valign="top" align="center" rowspan="1" colspan="1">-48</td><td valign="top" align="center" rowspan="1" colspan="1">-34</td><td valign="top" align="center" rowspan="1" colspan="1">19</td><td valign="top" align="center" rowspan="1" colspan="1">314</td><td valign="top" align="center" rowspan="1" colspan="1">9.36</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SPL</td><td valign="top" align="left" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">24</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">197</td><td valign="top" align="center" rowspan="1" colspan="1">7.06</td><td valign="top" align="center" rowspan="1" colspan="1">24</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">166</td><td valign="top" align="center" rowspan="1" colspan="1">6.52</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">SPL</td><td valign="top" align="left" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-24</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">453</td><td valign="top" align="center" rowspan="1" colspan="1">10.52</td><td valign="top" align="center" rowspan="1" colspan="1">-27</td><td valign="top" align="center" rowspan="1" colspan="1">-55</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">268</td><td valign="top" align="center" rowspan="1" colspan="1">9.76</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ITG</td><td valign="top" align="left" rowspan="1" colspan="1">37</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">51</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-8</td><td valign="top" align="center" rowspan="1" colspan="1">117</td><td valign="top" align="center" rowspan="1" colspan="1">6.45</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">ITG</td><td valign="top" align="left" rowspan="1" colspan="1">19, 37</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-48</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">93</td><td valign="top" align="center" rowspan="1" colspan="1">6.30</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Thalamus</td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">15</td><td valign="top" align="center" rowspan="1" colspan="1">-25</td><td valign="top" align="center" rowspan="1" colspan="1">10</td><td valign="top" align="center" rowspan="1" colspan="1">105</td><td valign="top" align="center" rowspan="1" colspan="1">7.03</td><td valign="top" align="center" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">16</td><td valign="top" align="center" rowspan="1" colspan="1">49</td><td valign="top" align="center" rowspan="1" colspan="1">6.73</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Thalamus</td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-15</td><td valign="top" align="center" rowspan="1" colspan="1">-19</td><td valign="top" align="center" rowspan="1" colspan="1">10</td><td valign="top" align="center" rowspan="1" colspan="1">139</td><td valign="top" align="center" rowspan="1" colspan="1">7.41</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cerebellum</td><td valign="top" align="left" rowspan="1" colspan="1">Vermis</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">-3</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">308</td><td valign="top" align="center" rowspan="1" colspan="1">10.74</td><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-14</td><td valign="top" align="center" rowspan="1" colspan="1">163</td><td valign="top" align="center" rowspan="1" colspan="1">7.72</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cerebellum</td><td valign="top" align="left" rowspan="1" colspan="1">Culmen/Declive</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">15</td><td valign="top" align="center" rowspan="1" colspan="1">-49</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">164</td><td valign="top" align="center" rowspan="1" colspan="1">9.53</td><td valign="top" align="center" rowspan="1" colspan="1">12</td><td valign="top" align="center" rowspan="1" colspan="1">-46</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">104</td><td valign="top" align="center" rowspan="1" colspan="1">6.19</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cerebellum</td><td valign="top" align="left" rowspan="1" colspan="1">Culmen/Declive</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-21</td><td valign="top" align="center" rowspan="1" colspan="1">-49</td><td valign="top" align="center" rowspan="1" colspan="1">-20</td><td valign="top" align="center" rowspan="1" colspan="1">140</td><td valign="top" align="center" rowspan="1" colspan="1">10.20</td><td valign="top" align="center" rowspan="1" colspan="1">-15</td><td valign="top" align="center" rowspan="1" colspan="1">-52</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">144</td><td valign="top" align="center" rowspan="1" colspan="1">8.87</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">Cerebellum</td><td valign="top" align="left" rowspan="1" colspan="1">Tuber/Declive</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">42</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-23</td><td valign="top" align="center" rowspan="1" colspan="1">94</td><td valign="top" align="center" rowspan="1" colspan="1">7.03</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-20</td><td valign="top" align="center" rowspan="1" colspan="1">156</td><td valign="top" align="center" rowspan="1" colspan="1">7.86</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /></tr></tbody></table><table-wrap-foot><attrib><italic toggle="yes">Talairach coordinates for the conjunction of Leading and Following compared to rest (<italic toggle="yes">p</italic> &gt; 0.005 uncorrected, <italic toggle="yes">k</italic> = 49 voxels). IFG, inferior frontal gyrus; IPL, inferior parietal lobule; ITG, inferior temporal gyrus; SII, secondary somatosensory cortex; SMA, supplementary motor area; SMC, sensorimotor cortex; and SPL, superior parietal lobule.</italic></attrib></table-wrap-foot></table-wrap></div></details><details class="inner-accordion"><summary>Table 3 (T3) - Leading versus following in the whole group, the leader-only group, and the follower-only group.</summary><div class="table-html"><table-wrap id="T3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Leading versus following in the whole group, the leader-only group, and the follower-only group.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="center" colspan="5" rowspan="1">Whole-group (<italic toggle="yes">n</italic> = 18)</th><th valign="top" align="center" colspan="5" rowspan="1">Leaders only (<italic toggle="yes">n</italic> = 11)</th><th valign="top" align="center" colspan="5" rowspan="1">Followers only (<italic toggle="yes">n</italic> = 10)</th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th><th valign="top" align="left" colspan="5" rowspan="1"><hr /></th></tr><tr><th valign="top" align="left" rowspan="1" colspan="1">Area</th><th valign="top" align="center" rowspan="1" colspan="1">BA</th><th valign="top" align="left" rowspan="1" colspan="1">Hemisphere</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th></tr></thead><tbody><tr><td valign="top" align="left" colspan="2" rowspan="1"><bold>Activations: Leading &gt; Following</bold></td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   pre-SMA</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">5</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">29</td><td valign="top" align="center" rowspan="1" colspan="1">5.24</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   SMA</td><td valign="top" align="center" rowspan="1" colspan="1">4,6</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">-4</td><td valign="top" align="center" rowspan="1" colspan="1">-4</td><td valign="top" align="center" rowspan="1" colspan="1">59</td><td valign="top" align="center" rowspan="1" colspan="1">81</td><td valign="top" align="center" rowspan="1" colspan="1">5.97</td><td valign="top" align="center" rowspan="1" colspan="1">-3</td><td valign="top" align="center" rowspan="1" colspan="1">-13</td><td valign="top" align="center" rowspan="1" colspan="1">64</td><td valign="top" align="center" rowspan="1" colspan="1">109</td><td valign="top" align="center" rowspan="1" colspan="1">6.28</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   CMA</td><td valign="top" align="center" rowspan="1" colspan="1">24</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">-6</td><td valign="top" align="center" rowspan="1" colspan="1">8</td><td valign="top" align="center" rowspan="1" colspan="1">37</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">4.84</td><td valign="top" align="center" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">2</td><td valign="top" align="center" rowspan="1" colspan="1">40</td><td valign="top" align="center" rowspan="1" colspan="1">85</td><td valign="top" align="center" rowspan="1" colspan="1">8.41</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PMC</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">21</td><td valign="top" align="center" rowspan="1" colspan="1">-4</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">145</td><td valign="top" align="center" rowspan="1" colspan="1">8.01</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PMC</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-26</td><td valign="top" align="center" rowspan="1" colspan="1">-13</td><td valign="top" align="center" rowspan="1" colspan="1">54</td><td valign="top" align="center" rowspan="1" colspan="1">51</td><td valign="top" align="center" rowspan="1" colspan="1">4.36</td><td valign="top" align="center" rowspan="1" colspan="1">-30</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">64</td><td valign="top" align="center" rowspan="1" colspan="1">32</td><td valign="top" align="center" rowspan="1" colspan="1">5.71</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   DLPFC</td><td valign="top" align="center" rowspan="1" colspan="1">8,9</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-40</td><td valign="top" align="center" rowspan="1" colspan="1">29</td><td valign="top" align="center" rowspan="1" colspan="1">38</td><td valign="top" align="center" rowspan="1" colspan="1">98</td><td valign="top" align="center" rowspan="1" colspan="1">7.35</td><td valign="top" align="center" rowspan="1" colspan="1">-48</td><td valign="top" align="center" rowspan="1" colspan="1">32</td><td valign="top" align="center" rowspan="1" colspan="1">31</td><td valign="top" align="center" rowspan="1" colspan="1">46</td><td valign="top" align="center" rowspan="1" colspan="1">5.84</td><td valign="top" align="center" rowspan="1" colspan="1">-39</td><td valign="top" align="center" rowspan="1" colspan="1">44</td><td valign="top" align="center" rowspan="1" colspan="1">34</td><td valign="top" align="center" rowspan="1" colspan="1">76</td><td valign="top" align="center" rowspan="1" colspan="1">7.08</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   DLPFC</td><td valign="top" align="center" rowspan="1" colspan="1">9</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">-36</td><td valign="top" align="center" rowspan="1" colspan="1">23</td><td valign="top" align="center" rowspan="1" colspan="1">25</td><td valign="top" align="center" rowspan="1" colspan="1">30</td><td valign="top" align="center" rowspan="1" colspan="1">6.91</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   SPL</td><td valign="top" align="center" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">-73</td><td valign="top" align="center" rowspan="1" colspan="1">42</td><td valign="top" align="center" rowspan="1" colspan="1">70</td><td valign="top" align="center" rowspan="1" colspan="1">6.39</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">-73</td><td valign="top" align="center" rowspan="1" colspan="1">49</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">7.56</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   SPL</td><td valign="top" align="center" rowspan="1" colspan="1">7</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">-73</td><td valign="top" align="center" rowspan="1" colspan="1">36</td><td valign="top" align="center" rowspan="1" colspan="1">84</td><td valign="top" align="center" rowspan="1" colspan="1">5.44</td><td valign="top" align="center" rowspan="1" colspan="1">-18</td><td valign="top" align="center" rowspan="1" colspan="1">-79</td><td valign="top" align="center" rowspan="1" colspan="1">43</td><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">6.34</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   Cerebellum</td><td valign="top" align="center" rowspan="1" colspan="1">Tuber</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">-65</td><td valign="top" align="center" rowspan="1" colspan="1">-17</td><td valign="top" align="center" rowspan="1" colspan="1">37</td><td valign="top" align="center" rowspan="1" colspan="1">5.00</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" colspan="2" rowspan="1"><bold>Deactivations: Following &gt; Leading</bold></td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PCC</td><td valign="top" align="center" rowspan="1" colspan="1">7,31</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">-54</td><td valign="top" align="center" rowspan="1" colspan="1">23</td><td valign="top" align="center" rowspan="1" colspan="1">129</td><td valign="top" align="center" rowspan="1" colspan="1">-5.36</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">3</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">31</td><td valign="top" align="center" rowspan="1" colspan="1">92</td><td valign="top" align="center" rowspan="1" colspan="1">-11.12</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   TPJ</td><td valign="top" align="center" rowspan="1" colspan="1">39,40</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">45</td><td valign="top" align="center" rowspan="1" colspan="1">-59</td><td valign="top" align="center" rowspan="1" colspan="1">25</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="center" rowspan="1" colspan="1">-5.33</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">31</td><td valign="top" align="center" rowspan="1" colspan="1">89</td><td valign="top" align="center" rowspan="1" colspan="1">-8.86</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   TPJ</td><td valign="top" align="center" rowspan="1" colspan="1">39,40</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-53</td><td valign="top" align="center" rowspan="1" colspan="1">-63</td><td valign="top" align="center" rowspan="1" colspan="1">23</td><td valign="top" align="center" rowspan="1" colspan="1">105</td><td valign="top" align="center" rowspan="1" colspan="1">-5.70</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   STS</td><td valign="top" align="center" rowspan="1" colspan="1">19,39</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">16</td><td valign="top" align="center" rowspan="1" colspan="1">92</td><td valign="top" align="center" rowspan="1" colspan="1">-10.18</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   STS</td><td valign="top" align="center" rowspan="1" colspan="1">37,39</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">-51</td><td valign="top" align="center" rowspan="1" colspan="1">-52</td><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">66</td><td valign="top" align="center" rowspan="1" colspan="1">-5.94</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   aSTG</td><td valign="top" align="center" rowspan="1" colspan="1">13,22</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">57</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">-2</td><td valign="top" align="center" rowspan="1" colspan="1">72</td><td valign="top" align="center" rowspan="1" colspan="1">-9.70</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   Temporal pole</td><td valign="top" align="center" rowspan="1" colspan="1">38</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">-48</td><td valign="top" align="center" rowspan="1" colspan="1">5</td><td valign="top" align="center" rowspan="1" colspan="1">-23</td><td valign="top" align="center" rowspan="1" colspan="1">30</td><td valign="top" align="center" rowspan="1" colspan="1">-8.09</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PHC</td><td valign="top" align="center" rowspan="1" colspan="1">30,36</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">17</td><td valign="top" align="center" rowspan="1" colspan="1">-33</td><td valign="top" align="center" rowspan="1" colspan="1">0</td><td valign="top" align="center" rowspan="1" colspan="1">39</td><td valign="top" align="center" rowspan="1" colspan="1">-4.72</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">42</td><td valign="top" align="center" rowspan="1" colspan="1">-40</td><td valign="top" align="center" rowspan="1" colspan="1">1</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="center" rowspan="1" colspan="1">-6.13</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   PHC</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-21</td><td valign="top" align="center" rowspan="1" colspan="1">-16</td><td valign="top" align="center" rowspan="1" colspan="1">-6</td><td valign="top" align="center" rowspan="1" colspan="1">103</td><td valign="top" align="center" rowspan="1" colspan="1">-5.76</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">-30</td><td valign="top" align="center" rowspan="1" colspan="1">-22</td><td valign="top" align="center" rowspan="1" colspan="1">-11</td><td valign="top" align="center" rowspan="1" colspan="1">67</td><td valign="top" align="center" rowspan="1" colspan="1">-7.52</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">   Thalamus</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1">21</td><td valign="top" align="center" rowspan="1" colspan="1">-25</td><td valign="top" align="center" rowspan="1" colspan="1">4</td><td valign="top" align="center" rowspan="1" colspan="1">59</td><td valign="top" align="center" rowspan="1" colspan="1">-6.72</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /></tr></tbody></table><table-wrap-foot><attrib><italic toggle="yes">Talairach coordinates for the contrast “Leading versus Following” in the whole group, leaders only, and followers only (<italic toggle="yes">p</italic> &gt; 0.005 uncorrected, <italic toggle="yes">k</italic> = 28 voxels). aSTG, anterior superior temporal gyrus; CMA, cingulate motor area; DLPFC, dorsolateral prefrontal cortex; PCC, posterior cingulate cortex; PHC, parahippocampal cortex; PMC, premotor cortex; SMA, supplementary motor area; SPL, superior parietal lobule; STS, superior temporal sulcus; and TPJ, temporo-parietal junction.</italic></attrib></table-wrap-foot></table-wrap></div></details><details class="inner-accordion"><summary>Table 4 (T4) - Leading versus following correlated with skill as a leader.</summary><div class="table-html"><table-wrap id="T4" position="float" orientation="portrait"><label>Table 4</label><caption><p>Leading versus following correlated with skill as a leader.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1">Area</th><th valign="top" align="center" rowspan="1" colspan="1">BA</th><th valign="top" align="left" rowspan="1" colspan="1">Hemisphere</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sup>2</sup></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1"><bold>Activation</bold></td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">pre-SMA</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">8</td><td valign="top" align="center" rowspan="1" colspan="1">58</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">0.73</td><td valign="top" align="center" rowspan="1" colspan="1">0.37</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">SMA</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">12</td><td valign="top" align="center" rowspan="1" colspan="1">-7</td><td valign="top" align="center" rowspan="1" colspan="1">55</td><td valign="top" align="center" rowspan="1" colspan="1">184</td><td valign="top" align="center" rowspan="1" colspan="1">0.81</td><td valign="top" align="center" rowspan="1" colspan="1">0.46</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">dPMC</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">36</td><td valign="top" align="center" rowspan="1" colspan="1">-1</td><td valign="top" align="center" rowspan="1" colspan="1">37</td><td valign="top" align="center" rowspan="1" colspan="1">46</td><td valign="top" align="center" rowspan="1" colspan="1">0.66</td><td valign="top" align="center" rowspan="1" colspan="1">0.49</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">Insula</td><td valign="top" align="center" rowspan="1" colspan="1">13</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">33</td><td valign="top" align="center" rowspan="1" colspan="1">-25</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="center" rowspan="1" colspan="1">101</td><td valign="top" align="center" rowspan="1" colspan="1">0.71</td><td valign="top" align="center" rowspan="1" colspan="1">0.50</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">STG</td><td valign="top" align="center" rowspan="1" colspan="1">41,22</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">54</td><td valign="top" align="center" rowspan="1" colspan="1">-28</td><td valign="top" align="center" rowspan="1" colspan="1">10</td><td valign="top" align="center" rowspan="1" colspan="1">62</td><td valign="top" align="center" rowspan="1" colspan="1">0.66</td><td valign="top" align="center" rowspan="1" colspan="1">0.48</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"><bold>Deactivation</bold></td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">Cingulate</td><td valign="top" align="center" rowspan="1" colspan="1">13,13</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-21</td><td valign="top" align="center" rowspan="1" colspan="1">-34</td><td valign="top" align="center" rowspan="1" colspan="1">28</td><td valign="top" align="center" rowspan="1" colspan="1">87</td><td valign="top" align="center" rowspan="1" colspan="1">-0.74</td><td valign="top" align="center" rowspan="1" colspan="1">0.37</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">Lingual</td><td valign="top" align="center" rowspan="1" colspan="1">19</td><td valign="top" align="left" rowspan="1" colspan="1">LH</td><td valign="top" align="center" rowspan="1" colspan="1">-33</td><td valign="top" align="center" rowspan="1" colspan="1">-58</td><td valign="top" align="center" rowspan="1" colspan="1">-2</td><td valign="top" align="center" rowspan="1" colspan="1">46</td><td valign="top" align="center" rowspan="1" colspan="1">-0.72</td><td valign="top" align="center" rowspan="1" colspan="1">0.30</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /></tr></tbody></table><table-wrap-foot><attrib><italic toggle="yes">Talairach coordinates for the contrast “Leading versus Following” in the whole group correlated with the covariate “leader skill” (<italic toggle="yes">p</italic> &gt; 0.025 uncorrected, <italic toggle="yes">k</italic> = 46 voxels). <italic toggle="yes">R</italic><sup>2</sup> is the coefficient of determination of the regression of the cluster’s mean beta value against leader skill. dPMC, dorsal premotor cortex; SMA, supplementary motor area; and STG, superior temporal cortex.</italic></attrib></table-wrap-foot></table-wrap></div></details><details class="inner-accordion"><summary>Table 5 (T5) - Leading versus following correlated with skill as a follower.</summary><div class="table-html"><table-wrap id="T5" position="float" orientation="portrait"><label>Table 5</label><caption><p>Leading versus following correlated with skill as a follower.</p></caption><table frame="hsides" rules="groups" cellspacing="5" cellpadding="5"><thead><tr><td valign="top" align="left" rowspan="1" colspan="1" /><th valign="top" align="left" rowspan="1" colspan="1">Area</th><th valign="top" align="center" rowspan="1" colspan="1">BA</th><th valign="top" align="left" rowspan="1" colspan="1">Hemisphere</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">x</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">y</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">z</italic></th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">k</italic></th><th valign="top" align="center" rowspan="1" colspan="1">Max</th><th valign="top" align="center" rowspan="1" colspan="1"><italic toggle="yes">R</italic><sup>2</sup></th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1"><bold>Activation</bold></td><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /><td valign="top" align="center" rowspan="1" colspan="1" /></tr><tr><td valign="top" align="left" rowspan="1" colspan="1"><bold>Deactivation</bold></td><td valign="top" align="left" rowspan="1" colspan="1">PCC</td><td valign="top" align="center" rowspan="1" colspan="1">7,31</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">-55</td><td valign="top" align="center" rowspan="1" colspan="1">37</td><td valign="top" align="center" rowspan="1" colspan="1">78</td><td valign="top" align="center" rowspan="1" colspan="1">-0.75</td><td valign="top" align="center" rowspan="1" colspan="1">0.30</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">mPFC</td><td valign="top" align="center" rowspan="1" colspan="1">9,10</td><td valign="top" align="left" rowspan="1" colspan="1">RH/LH</td><td valign="top" align="center" rowspan="1" colspan="1">6</td><td valign="top" align="center" rowspan="1" colspan="1">47</td><td valign="top" align="center" rowspan="1" colspan="1">22</td><td valign="top" align="center" rowspan="1" colspan="1">64</td><td valign="top" align="center" rowspan="1" colspan="1">-0.73</td><td valign="top" align="center" rowspan="1" colspan="1">0.27</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">pSTS</td><td valign="top" align="center" rowspan="1" colspan="1">19,39</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">42</td><td valign="top" align="center" rowspan="1" colspan="1">-61</td><td valign="top" align="center" rowspan="1" colspan="1">13</td><td valign="top" align="center" rowspan="1" colspan="1">90</td><td valign="top" align="center" rowspan="1" colspan="1">-0.75</td><td valign="top" align="center" rowspan="1" colspan="1">0.61</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /><td valign="top" align="left" rowspan="1" colspan="1">TPJ</td><td valign="top" align="center" rowspan="1" colspan="1">39,40</td><td valign="top" align="left" rowspan="1" colspan="1">RH</td><td valign="top" align="center" rowspan="1" colspan="1">48</td><td valign="top" align="center" rowspan="1" colspan="1">-52</td><td valign="top" align="center" rowspan="1" colspan="1">43</td><td valign="top" align="center" rowspan="1" colspan="1">72</td><td valign="top" align="center" rowspan="1" colspan="1">-0.72</td><td valign="top" align="center" rowspan="1" colspan="1">0.48</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1" /></tr></tbody></table><table-wrap-foot><attrib><italic toggle="yes">Talairach coordinates for the contrast “Leading versus Following” in the whole group correlated with the covariate “follower skill” (<italic toggle="yes">p</italic> &gt; 0.025 uncorrected, <italic toggle="yes">k</italic> = 46 voxels). <italic toggle="yes">R</italic><sup>2</sup> is the coefficient of determination of the regression of the cluster’s mean beta value against follower skill. mPFC, medial prefrontal cortex; PCC, posterior cingulate cortex; pSTS, superior temporal sulcus; and TPJ, temporo-parietal junction.</italic></attrib></table-wrap-foot></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 31142792</strong> | Pred included: 0 | Manual included (accepted matches only): 2 | Correct overlaps: 0 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/31142792/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>31142792_analysis_0</td><td>Increased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The task and contrast probe perception of other people (faces, social evaluation) rather than self-related cognition or self-awareness.</td></tr>
<tr><td>31142792_analysis_1</td><td>Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast measured perception of others (disfigured vs treated faces) and social responses, not self-perception or judgments about the self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>31142792_1</td><td>before treatment &gt; after treatment (decreased activation); self</td><td>31142792_analysis_1</td><td>Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</td><td>0.414</td><td>1.000</td><td>0.824</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>31142792_2</td><td>before treatment &gt; after treatment (increased activation); self</td><td>31142792_analysis_0</td><td>Increased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</td><td>0.414</td><td>1.000</td><td>0.824</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  <details class="inner-accordion"><summary>PMC full text available (PMCID 6541618)</summary><p><strong>Title:</strong> Behavioural and Neural Responses to Facial Disfigurement</p><details><summary>Abstract</summary><pre class="paper-text">Faces are among the most salient and relevant visual and social stimuli that humans encounter. Attractive faces are associated with positive character traits and social skills and automatically evoke larger neural responses than faces of average attractiveness in ventral occipito-temporal cortical areas. Little is known about the behavioral and neural responses to disfigured faces. In two experiments, we tested the hypotheses that people harbor a disfigured is bad bias and that ventral visual neural responses, known to be amplified to attractive faces, represent an attentional effect to facial salience rather than to their rewarding properties. In our behavioral study (N = 79), we confirmed the existence of an implicit ‘  disfigured is bad  ’ bias. In our functional MRI experiment (N = 31), neural responses to photographs of disfigured faces before treatment evoked greater neural responses within ventral occipito-temporal cortex and diminished responses within anterior cingulate cortex. The occipito-temporal activity supports the hypothesis that these areas are sensitive to attentional, rather than reward properties of faces. The relative deactivation in anterior cingulate cortex, informed by our behavioral study, may reflect suppressed empathy and social cognition and indicate evidence of a possible neural mechanism underlying dehumanization.</pre></details><details><summary>Body</summary><pre class="paper-text">## Introduction 
  
Physical appearance has a profound impact on a person’s life. Beautiful people are preferred and enjoy many advantages compared to average-looking people . While conceptually orthogonal, the correlation of attractiveness and positive character traits indicates the prevalence of a ‘beautiful is good’ stereotype . This stereotype might be innate . Attractive people are seen as more trustworthy, socially competent, dominant, better adjusted, more capable in school and work, and also receive greater rewards and lesser punishments than their average looking peers . Adults and children ascribe desirable personality traits to attractive faces of adults and children and discriminate against unattractive faces even if they are friends and family members . Attractiveness and trustworthiness judgments are consistent across cultures  and are made extremely quickly . Longer exposure to a face does not attenuate these biases and instead only consolidates people’s confidence in a judgement already made . Attractiveness also highly influences visual exploration of faces . 

In this study we examine a corollary to the ‘beautiful is good’ stereotype, that an automatic ‘disfigured is bad’ stereotype also exists. People with facial disfigurement are stigmatized and are often targets of discrimination. Looking at disfigured faces makes observers feel less happy, less in control, less dominant, and more aroused . People with facial disfigurements are not only perceived as less attractive and less likely to be selected as romantic partners, they are also thought of as having unfavourable personality traits (e.g., lack of emotional stability, conscientiousness), internal attributes (e.g., unhappiness, lower intelligence), social qualities (e.g., untrustworthiness, unpopularity)  and are treated poorly in social interactions . In popular culture, facial disfigurement is often used to distinguish good and evil characters . Well known examples of disfigured villains are Scar in the   Lion King   (large facial scar over left eye), Freddy Krueger in   Nightmare on Elm Street   (3rd degree burns and exposed tissue), the   James Bond   villains Le Chiffre (facial scar over left eye), Emilio Largo (missing eye), Ernst Stavro Blofeld (large scar over right eye covering most of his right side of the face), and Alec Trevelyan (facial burn scars), Elle Driver in   Kill Bill   (missing eye), Two Face in the   Batman   Universe (acid scars covering the left side of his head), Hopper in   A Bug’s Life   (scar covering right eye), and the Duchess from   Alice in a Wonderland   (Macrocephaly). This ‘disfigurement is bad’ stereotype is only partially explained by lower attractiveness of disfigured faces . 

Attractiveness of faces –and therefore attribution of a’beauty is good’ stereotype- is highly correlated with typicality or statistical averageness of faces . In addition to being statistical averages of groups, attractive faces are also symmetric . Both facial symmetry and averageness are considered markers of physical health and influence peoples’ choices of partners . Disfigured faces are neither typical nor average, and are usually not symmetric. They often deviate substantially from the norm. If proximity to the norm predicts positive social attributions, being ‘different’ could lead to negative evaluations. Disfigured faces might be linked to unfavourable personality traits, internal attributes, and social qualities because they are less typical and deviate from the population average. The association of disfigurement with negative attributes probably drives stigmatization and discrimination of disfigured people in social, academic, and professional contexts . The stigmatization and discrimination of disfigured people likely contributes to low self-esteem  and long term mental health concerns similar to other stigmatized groups that are subject to dehumanization . Dehumanization deprives a person or a group of people of positive human qualities and has been shown for several stigmatized groups such as homeless people and drug addicts . Dehumanization is used as a propaganda tool in political conflicts . The strongest predictors of dehumanization are hypothesized to be perceived competence and warmth . Faces rated lowest on both competence and warmth most robustly evoke dehumanization - including feelings of disgust and lack of empathy . 

Neuroimaging studies show that seeing attractive faces evokes brain responses in reward, emotion, and visual areas compared to seeing faces of average attractiveness . Attractive faces produce activations in areas associated with reward, like the nucleus accumbens , and orbitofrontal cortex . Moreover, attractiveness correlates with increased activations in areas associated with emotion, empathy, and social cognition like the anterior cingulate cortex and medio-prefrontal cortex  the latter being particularly active in tasks in which people are not making explicit attractiveness judgements . Different regions of the prefrontal cortex are selectively responsive to either attractive or unattractive faces  which is consistent with findings that ventral medio-prefrontal cortex processes stimulus value attributes in coordination with higher order visual areas like fusiform gyri and semantic processing areas (posterior superior temporal sulcus) . Orbital frontal  and medial prefrontal cortices  seem to process both aesthetic and moral values and may represent the biological link between these two kinds of evaluation . 

Left and right amygdala seem to be sensitive to both attractive  and unattractive faces . These non-linear effects for extremes at either end of the attractiveness spectrum suggest that amygdala activation reflects sensitivity to valence intensity rather than positive or negative valence per se . In line with the valence processing hypothesis for the functional role of amygdala, increased activation in the amygdala (bilaterally) is linked to untrustworthiness of faces . A meta-analysis of brain activations to attractiveness and trustworthiness suggests that activation of amygdala and adjacent nucleus accumbens is driven by extremes and atypicality . There is some tentative evidence that face typicality can also account for the activations in medio-prefrontal and anterior cingulate cortex . The authors note that the brain networks activated in response to extremes of attractiveness and trustworthiness are remarkably similar to brain networks that process positive and negative emotions . 

In addition to increased brain activations in reward and emotion areas, attractive faces also evoke larger neural responses in selective visual processing areas within ventral occipito-temporal cortex (such as the fusiform face area) as compared to faces of average attractiveness . These areas remain sensitive to facial attractiveness even when subjects are engaged in tasks in which attractiveness judgements are not queried explicitly. These observations have previously been interpreted as evidence that these areas also process rewards. While a reward response is one possible explanation for this amplified neural response to attractive faces, it is also possible that this reflects sensitivity to the saliency of attractive faces . If this alternate hypothesis is true, other salient features, such as disfigurement, should lead to similarly amplified neural responses in visual processing areas. 

Viewing faces of stigmatized groups fails to activate brain regions associated with empathy and social cognition . Krendl and colleagues reported increased activation in anterior insula and amygdala which correlated with self-reported disgust in response to viewing faces of stigmatized groups . The lack of activation in empathy and social cognition regions of the brain is postulated to be a neural correlate of dehumanization . 

Appearance clearly affects how people are viewed and treated by others. The same mechanisms that benefit attractive people in social interaction, put unattractive people at an unfair disadvantage. The effects of discriminating against people with facial disfigurement seem to extend beyond the specific effects of lower overall attractiveness and may tie in more with the pattern of results that have been shown with stigmatized groups. 

The goal of the present study was to test the behavioural and brain responses to facial disfigurement and investigate whether surgical treatment mitigates these responses. In two experiments, we used a set of photograph pairs of patients with different types of facial disfigurements before and after surgical treatment of the disfigurement. In experiment one we tested if people harbour implicit biases against disfigured faces and if such implicit biases were different from consciously aware self-reported explicit biases. In a follow up functional MRI (fMRI) study, we tested differential automatic brain responses to the same picture pairs when naïve participants were engaged in an unrelated cover task. We hypothesized that people have negative biases against faces with disfigurement. For the neural responses to facial disfigurement we tested competing hypotheses: visual cortices respond to rewards per se, or visual cortices respond to salience. In addition, we expected disfigured faces to show selective responses in emotion and valence areas such as anterior insulae and amygdalae and anterior cingulate and lateral or medial prefrontal areas in line with the research reviewed above. 


## Results and Discussion 
  
The behavioural experiment (N = 79, see method section for details) consisted of an implicit association test  (IAT) and an explicit bias questionnaire (EBQ) to test the hypothesis that people have a negative bias for disfigured faces. For the IAT, we used a stimulus set of photographs of real patients taken before and after treatment for disfigurement. The EBQ consisted of 11 questions which query conscious biases against people with facial disfigurements (see   https://osf.io/ca2u9/   for all items and data). We found no indication of an explicit bias. However, we did find that non-disfigured faces were preferred in the IAT (see Fig.  ). This bias was particularly robust for men, consistent with previous findings . Prior exposure to disfigured faces did not modulate the implicit bias of individuals.   
Female respondents demonstrate significantly less, although still strong, implicit preference for non-disfigured faces than male respondents. Male respondents show a moderate explicit preference for non-disfigured faces while women show no explicit preference. Error bars indicate 95% confidence intervals. 
  

We used the same set of photographs of people before and after surgical treatment that we used in the IAT in the fMRI study (N = 31). Participants viewed these photographs and engaged in a gender judgement task. We measured neural responses to facial disfigurement to test competing hypotheses of reward versus salience in visual areas like fusiform face area. If these visual areas respond to rewards, then non-disfigured faces compared to disfigured faces would show increased activity in visual areas linked to face processing. If these visual areas respond to salience, then we should find the opposite results; disfigured faces compared to non-disfigured faces should show increased activity in these areas. Because people with facial disfigurement are likely treated as an outgroup , neural patterns in response to disfigurement should be similar to previous findings investigating other stigmatized groups . We predicted decreased activation in areas linked to social cognition such as medio-prefrontal cortex and anterior cingulate cortex, as well as increased activations in areas linked to disgust and negative emotion like anterior insula and amygdala. 

We found that images of people with facial disfigurement, as compared to images of the same faces after surgical treatment, evoked greater neural responses within ventral occipito-temporal cortex, particularly bilateral fusiform gyri (see Fig.  ), and right inferior frontal cortex. This observation confirms the hypothesis that face processing and adjacent areas respond automatically to the salience of faces, rather than their attractiveness or rewarding properties per se.   
Increased activations (red yellow) and deactivations (blue-green) in response to faces before treatment. Results were corrected for multiple comparisons by familywise error correction at p &lt; 0.05 with Monte Carlo permutation testing in SnPM with a combined cluster-voxel threshold (cluster defining threshold p &lt; 0.001, T &gt; 3.3852). 
  

In addition to increased responses in visual areas, we found decreases in neural response amplitude to disfigured faces in the medial anterior cingulate gyrus extending towards medial prefrontal cortex (see Figs   and  ), as well as in a region stretching from right cuneus to the right calcarine gyrus and right lingual gyrus. This finding is similar to previous observations of neural responses to other stigmatized outgroups such as drug addicts and homeless people  and could reflect suppression empathy and mentalizing or increased demands in cognitive control, e.g. inhibition of staring at the area of lesion or inhibition of inappropriate social behaviour like obvious avoidance. Both possible hypotheses are not mutually exclusive and could be linked to the increased activation in the left inferior frontal gyrus - a region linked to cognitive control.   
Increased activations (red yellow) and deactivations (blue-green) in response to faces before treatment. Results were corrected for multiple comparisons by familywise error correction at p &lt; 0.05 with Monte Carlo permutation testing in SnPM with a combined cluster-voxel threshold (cluster defining threshold p &lt; 0.001, T &gt; 3.3852). 
  

In previous studies, increased amygdala activation has been reported to both positive and negative valence of faces . Moreover, studies investigating the brain responses to extreme outgroups like homeless people and drug addicts find activations in anterior insula where it is typically interpreted as a disgust response . We did not find statistically significant activations in amygdala and anterior insula. It is possible that this lack of effect is because of our smaller stimulus sample or that the difference between before and after stimulus pairs is not large enough to produce statistically significant results in this before-after contrast of the same face. 

In sum, we found that people have implicit negative biases against faces that are disfigured, without being aware of harbouring such biases. Disfigured faces evoke greater neural responses in ventral occipito-temporal and right inferior frontal regions as compared to non-disfigured faces. This finding refutes the hypothesis that attractiveness and reward per se drives automatic ventral cortical responses and instead confirms the idea that ventral occipito-temporal regions are sensitive to the salience of faces. 

Moreover, disfigured faces evoke lower neural responses in the anterior cingulate and medio-prefrontal cortex, as well as some visual areas. This result is similar to previously reported neural responses to stigmatized outgroups like homeless people and drug addicts . In agreement with this research, we speculate that the de-activation of these brain areas upon seeing disfigured faces as opposed to the same faces after surgical treatment possibly reflects an inhibition of empathy and mentalizing or inhibition of socially inappropriate behaviour. The medial anterior cingulate gyrus and the adjacent medial prefrontal cortex are core areas of the theory of mind and empathy networks  and are crucial for inferring other’s beliefs, feelings, and mental states. Together with previous behavioural research showing a clear association of negative personality traits and our findings of an implicit bias against disfigured faces, we take these response patterns as neural evidence for stigmatization. Future research should investigate if the de-activation of anterior cingulate cortex represents a consistent neural marker for dehumanization of people with disfigured faces or if it reflects social adaptive behaviour to people who deviate from the norm. 

The emphasis of attractiveness, its association with positive attributes and robustness of these associations across cultures  highlights the pervasive effect of attractiveness in social interaction. People who fall towards the lower end of the attractiveness spectrum are disadvantaged or even subject to discrimination and social isolation as in the case of facial disfigurement. Encouragingly, our findings suggest that surgical treatment of disfigurement mitigates the negative effects of disfigurement. Our findings highlight the importance of recognizing that we implicitly and automatically regard flawed faces as flawed people and that corrective surgery confers social and psychological benefits to people with facial disfigurement. Alternative prevention strategies against discrimination of disfigured people and effective support for people with facial conditions should be explored. 


## Methods 
  
### Implicit association test (IAT) and explicit bias questionnaire (EBQ) 
  
#### Participants 
  
80 participants were recruited via an online recruiting system for psychology experiments at the University of Pennsylvania (55 female, 25 male, mean age = 23 years, SD = 6.4, range 18–56). The sample size was determined based on estimates suggested by a meta-analysis on attitudes towards individuals with disabilities as measured by an IAT . Prior to participation, participants were informed that the task was about categorising faces and words but were naïve to the fact that some of those faces might be disfigured. Participation was voluntary, and participants received money as compensation. Study procedures were approved by the Institutional Review Board (IRB) at the University of Pennsylvania (Protocol #806447). IRB approval was in accordance with the International Conference on Harmonization and the Belmont report. All participants gave written informed consent. 

One participant was excluded from the data analysis for the IAT because more than 10% of the total test trials were unreasonably fast (&lt;300 ms). After data exclusion, the data of 79 participants went into the final analysis (55 female, mean age = 23 years, SD = 6.4, range 18–56). 


#### Procedure 
  
Task order between the IAT and the EBQ was counterbalanced so that half of the participants completed the IAT first, and half of the participants completed the EBQ first. Participants were seated in a testing room, in front of a testing laptop. After having been briefed on the order of the tasks, participants gave written informed consent. The entire experiment took about 30 minutes. 

The IAT  was designed using E-Prime software and was modelled after the IATs from   Project Implicit   (  https://implicit.harvard.edu  ). A total of 16 words were used for the IAT: 8 were positive words (attractive, happy, approachable, friendly, adore, lovely, spectacular, excellent), and 8 were negative words (ugly, evil, sickening, rotten, disaster, disgust, pain, despise). 

Participants completed the EBQ as a survey on Qualtrics. Questions were modelled after the Project Implicit and Changing Faces explicit questionnaires . The questionnaire included 11 questions asking about participants’ prior exposure to and conscious biases against people with facial disfigurement. Participants responded on a scale ranging from 1 to 7 (see   https://osf.io/ca2u9/   for details). 


#### Pictures 
  
Images consisted of photographs of patients with facial disfigurements before and after corrective surgery. These photos were collected from craniofacial and dental surgery atlases and compilations of plastic surgery results. The disfigured faces were photos of the individuals before treatment that were affected by one of the following disfigurements: carcinoma, hyperpigmentation, birthmark, scar or small wound, facial paralysis, isolated weight loss, bone disfigurement, or facial trauma. The non-disfigured faces were photographs of the same individuals after treatment (see   https://osf.io/ca2u9/   for all stimulus pairs). Pre-treatment and post-treatment photographs were cropped (to show only faces, with some hair and neck) and colour-corrected to match in size and coloring . The stimulus set consisted of 28 faces, of which 22 were female and 6 were male. 16 of the faces were oriented frontally, 10 were oriented in a three-quarters portrait view, and 2 were profiles (see   https://osf.io/ca2u9/  ). 


#### Implicit association test and explicit bias measure results 
  
Explicit scores range from −3 to +3, with zero indicating no relative preference for non-disfigured vs. disfigured faces. Positive scores indicate a preference for non-disfigured faces, and negative scores indicate a preference for disfigured faces. We found a significant implicit preference for non-disfigured faces (mean difference score = 0.90; SD = 0.58; min = −0.26; max = 2.00;   t   = 13.80; 95% CI = 0.77 to 1.03; p &lt; 0.001; Cohen’s   d   = 1.55). This effect was particularly strong for male respondents (see Table   for details, see Fig.  ). Participants showed no significant explicit preference for non-disfigured vs. disfigured faces (mean explicit score = 0.01; SD = 0.51; min = −1.50; max = 1.08.168;   t   = 0.17; 95% CI = −0.11 to 0.12; p = 0.866; Cohen’s   d =   0.02). Prior exposure had no effect on bias for either the IAT or the EBQ. There was a small to moderate correlation between implicit and explicit scores that was, however, not statistically significant (Pearson’s correlation coefficient, r = 0.22; p = 0.052) making it difficult to draw conclusions as to whether people are aware of their biases.   
Implicit preferences for non-disfigured vs. disfigured faces for all participants by gender. 
  
IAT D scores range from −2 to +2, with zero indicating no relative preference for non-disfigured vs. disfigured faces. Positive scores indicate an implicit preference for non-disfigured faces while negative scores indicate an implicit preference for disfigured faces. D scores were interpreted according to specific, conservative break points based on Cohen’s   d  : ±0.15 (‘slight’ bias), 0.35 (‘moderate’ bias), 0.65 (‘strong’ bias). 

Cohen’s   d   is a standardized effect size, interpreted as   d   of 0.2 = small effect,   d   of 0.5 = medium effect, and   d   ≥ 0.8 = large effect. 
  



### FMRI experiment 
  
#### Participants 
  
We recruited 34 healthy right-handed college students from University of Pennsylvania (24 females, 10 males). Age of participants ranged from 18–35 years. Participants had normal or corrected to normal vision and no prior history of psychiatric or neurological disease. Before participation in the study, each individual gave informed consent approved by the IRB at the University of Pennsylvania (Protocol #806447) in accordance with the International Conference on Harmonization and the Belmont report. 

The data of three participants was excluded from the final analysis. One dataset was excluded because of technical failure which stopped the stimulus presentation halfway through the experiment. Two other datasets were excluded because of synchronization problems between experimenter laptop and the scanner triggers. The data of 31 participants entered the final analysis (22 females, 9 males). 

The EBQ for the participants in the fMRI experiment showed that about half of the participants have a close friend or family member with either a facial disfigurement or a disability. Exposure to people with facial disfigurement was normally distributed in the sample, and most participants reported no to slight preference for non-disfigured over disfigured people (22/28 data entries, 5 missing values). 


#### Procedure and stimulus presentation 
  
The experiment consisted of one session. Participants were presented with 28 pictures of faces in randomized order and were asked to decide whether the displayed face was male or female. Half of the presented pictures were photographs of patients before treatment, and half after treatment. The pictures were identical to the ones used in the behavioural experiment (IAT, see above). Stimuli were presented using E-prime software by projecting them onto a screen using a projector outside the MR scanner room, which could be seen by participants through a mirror mounted over the head coil. Each picture was presented for 6 seconds. Responses were recorded with a 2-button response device. After the experiment, a high-resolution anatomical scan (~7 min) was conducted. After the scanning session, participants were taken out of the scanner and completed the EBQ for disfigurement on a testing computer outside the scanner room. This test was identical to the EBQ in the online sample reported above. 


#### fMRI data acquisition and pre-processing 
  
Images of blood-oxygen level dependent (BOLD) changes were acquired with a 3 T Siemens Magnetom Prisma scanner (Erlangen, Germany) with a 64-channel head coil. We used cushions to minimize participants’ head movement. We used two localizing scans and auto-alignment. Functional images were acquired using a standard BOLD sequence (TR: 2000 ms, TE: 30 ms, flip angle: 60 degrees, voxel size: 2.0 × 2.0 × 2.0 mm, 81 slices). High resolution (0.8 × 0.8 × 0.8 mm) structural (anatomical) images were acquired using an SPC T1 GRAPPA sequence . Data were pre-processed using the Matlab toolbox SPM12 (  http://www.fil.ion.ucl.ac.uk/spm  ). Images were motion corrected and registered to the first image of the scanning block. The mean of the motion-corrected images was co-registered with the individual participants’ anatomical scan. The anatomical and functional scans were spatially normalized to the standard MNI template. Finally, all data were spatially smoothed using an isotropic 8 mm full width at half maximum (FWHM) Gaussian kernel. 


#### fMRI data analysis 
  
At the single-subject level, statistical analysis was performed using a general linear model. The motion estimates of the motion correction algorithm were modelled as regressors of no interest to account for head motion. We performed a whole-brain group analysis by directly contrasting the mean activations per condition in a non-parametric design with SnPM (  https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/nichols/software/snpm  ). Results were corrected for multiple comparisons with a combined voxel-cluster level threshold by familywise error correction at p &lt; 0.05 with Monte Carlo permutation testing. 

In addition to the whole brain group analysis, we performed an item-wise region of interest control analysis to test if the effects in the group analysis are driven by specific items. The two clusters were defined by the group contrasts in the whole brain analysis and consisted of one area comprising of the two large occipital activation clusters, and one comprising the (de-)activation cluster in the anterior cingulate cortex. Mean values for these two regions were extracted for each subject and item. The mean values were normalised with the individual subject’s mean activation in this area to create relative difference scores per subject and item. The data for the item-wise analysis were analysed with linear mixed effect models in RStudio. We built one base model for each dependent variable (occipital cluster activation, anterior cingulate cluster activation) that included condition (pre or post surgery picture) as a predictor and subject and item as random factors with random intercepts. We tested for both random factors whether including random slopes for the condition would improve the model fit and tested interactions with gender and EBQ responses with the best base model (see   https://osf.io/ca2u9/   for details). 


#### FMRI sample results 
  
Participants performed at ceiling for the gender judgment task. 

An ANOVA analysis of the reaction time data in the gender judgement task in the scanner revealed no differences in reaction times between before and after treatment pictures (F  = 0.56, p = 0.45, see Fig.  ) and no differences for item (F  = 1.26, p = 0.17) and no interaction between item and face type (F  = 1.06, p = 0.38).   
Reaction times for gender judgement task per item split by face type. Error bars display 95% confidence intervals. 
  

We found increased activations in temporo-occipital regions encompassing bilateral middle occipital and fusiform gyrus, left inferior occipital gyrus, as well as right inferior temporal and right inferior frontal gyrus (Fig.  ; see Table   for details). An area in the medial anterior cingulate cortex and an area in the right calcarine gyrus showed significant decrease in activation in response to faces before surgery (Fig.  ; see Table   for details). All clusters statistically significant at p &lt; 0.05 FWE at the cluster level corrected by Monte Carlo permutation testing (cluster forming threshold p &lt; 0.001 per voxel).   
Increased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing. 
    
Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing. 
  

The ROI analysis controlling for random effects of items and subjects confirmed the results of the whole brain analysis (see Figs   and  , see   https://osf.io/ca2u9/   for analysis code and full statistical details). Whether the picture of a person was presented from before or after surgery had a significant effect on the BOLD activation level in the anterior cingulate cluster (β = −0.15, s.e. = 0.05, t = −2.95), as well as the occipital cortex (β = 0.17, s.e. = 0.03, t = 5.31). Neither gender of the participant, any of the EBQ measures (see Tables   and   for descriptive statistics), or the gender of the depicted person was found to be related to BOLD activation level differences.   
Itemwise mean activation in the occipital cortex. Stimulus items that do not follow the general activation pattern are Item 2, 7, 12, 25, and 28. 
    
Itemwise mean activation in the anterior cingulate cortex. Stimulus items that do not follow the general activation pattern are Item 1, 25, and 28. 
    
Summary of the EBQ responses I. 
    
Summary of the EBQ responses II. 
  



 ## Data Availability

The datasets generated and analysed during the current study will be made available without restriction on Open Science Framework (DOI 10.17605/OSF.IO/CA2U9) upon acceptance of the article for publication, https://osf.io/ca2u9/. https://osf.io/ca2u9/</pre></details></details>
  <details class="inner-accordion"><summary>Coordinate-relevant source tables (2)</summary><details class="inner-accordion"><summary>Table 2 (Tab2) - Increased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</summary><div class="table-html"><table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Increased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Location</th><th colspan="1" rowspan="1">k</th><th colspan="1" rowspan="1">T-max</th><th colspan="1" rowspan="1">x</th><th colspan="1" rowspan="1">y</th><th colspan="1" rowspan="1">z</th></tr></thead><tbody><tr><td colspan="1" rowspan="1">left lateral occipital gyrus/BA 18</td><td colspan="1" rowspan="1">3442</td><td colspan="1" rowspan="1">8.08</td><td colspan="1" rowspan="1">−28</td><td colspan="1" rowspan="1">−98</td><td colspan="1" rowspan="1">8</td></tr><tr><td colspan="1" rowspan="1">right lateral occipital gyrus/BA 18</td><td colspan="1" rowspan="1">2377</td><td colspan="1" rowspan="1">6.98</td><td colspan="1" rowspan="1">34</td><td colspan="1" rowspan="1">−90</td><td colspan="1" rowspan="1">2</td></tr><tr><td colspan="1" rowspan="1">right inferior frontal gyrus/BA 44</td><td colspan="1" rowspan="1">230</td><td colspan="1" rowspan="1">5.02</td><td colspan="1" rowspan="1">42</td><td colspan="1" rowspan="1">8</td><td colspan="1" rowspan="1">26</td></tr></tbody></table></table-wrap></div></details><details class="inner-accordion"><summary>Table 3 (Tab3) - Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</summary><div class="table-html"><table-wrap id="Tab3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Decreased responses to faces before treatment, familywise error corrected with Monte Carlo permutation testing.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Location</th><th colspan="1" rowspan="1">k</th><th colspan="1" rowspan="1">T-max</th><th colspan="1" rowspan="1">x</th><th colspan="1" rowspan="1">y</th><th colspan="1" rowspan="1">z</th></tr></thead><tbody><tr><td colspan="1" rowspan="1">left and right anterior cingulate cortex/BA 24</td><td colspan="1" rowspan="1">765</td><td colspan="1" rowspan="1">4.92</td><td colspan="1" rowspan="1">−2</td><td colspan="1" rowspan="1">36</td><td colspan="1" rowspan="1">10</td></tr><tr><td colspan="1" rowspan="1">right calcarine gyrus/BA 18</td><td colspan="1" rowspan="1">247</td><td colspan="1" rowspan="1">4.10</td><td colspan="1" rowspan="1">6</td><td colspan="1" rowspan="1">−88</td><td colspan="1" rowspan="1">12</td></tr></tbody></table></table-wrap></div></details></details>
</details>


<details class="doc-card">
  <summary><strong>PMID 19944083</strong> | Pred included: 0 | Manual included (accepted matches only): 3 | Correct overlaps: 0 | Match statuses: accepted=3, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/19944083/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>19944083_analysis_0</td><td>Emotional &gt; neutral stimuli</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Contrast examines perception/understanding of others&#x27; emotions and social relations, not self-related processing or judgments about the self.</td></tr>
<tr><td>19944083_analysis_1</td><td>Neutral &gt; emotional stimuli</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast examines perception of others and social scenes, not self-related processing, so it does not meet Perception and Understanding of Self criteria.</td></tr>
<tr><td>19944083_analysis_2</td><td>Social relation &gt; single person</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Contrast addresses perception/understanding of others and social relations, not self-related processes; does not meet self-perception criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>19944083_1</td><td>Emotional &gt; neutral stimuli; self</td><td>19944083_analysis_0</td><td>Emotional &gt; neutral stimuli</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>19944083_2</td><td>Neutral &gt; emotional stimuli; self</td><td>19944083_analysis_1</td><td>Neutral &gt; emotional stimuli</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>19944083_3</td><td>Social relation &gt; single person; self</td><td>19944083_analysis_2</td><td>Social relation &gt; single person</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 20188182</strong> | Pred included: 0 | Manual included (accepted matches only): 1 | Correct overlaps: 0 | Match statuses: accepted=1, uncertain=0, unmatched=2</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/20188182/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> self &gt; loved one; self, self &gt; stranger; self</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>20188182_analysis_0</td><td>Brain regions showing a significant effect of pain.</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Perception-of-self criteria require analyses that measure self-related processing (e.g., contrasts involving Self specifically). This analysis reports Pain vs Neutral across perspectives and does not isolate self-related processing, so it does not satisfy the inclusion criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>20188182_3</td><td>pain &gt; no pain; others</td><td>20188182_analysis_0</td><td>Brain regions showing a significant effect of pain.</td><td>0.369</td><td>1.000</td><td>0.811</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>20188182_4</td><td>self &gt; loved one; self</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr><tr><td>20188182_5</td><td>self &gt; stranger; self</td><td></td><td></td><td>0.000</td><td>0.000</td><td>0.000</td><td>unmatched</td><td>unassigned_by_global_matching, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 21703352</strong> | Pred included: 0 | Manual included (accepted matches only): 2 | Correct overlaps: 0 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/21703352/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>21703352_analysis_0</td><td>Neural regions engaged during decisions to accept Costly-Donation relative to Noncostly-Reward trials.</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The analysis centers on prosocial decision-making and mentalizing about others and self-control, not on explicit measures of self-perception, agency, or self-knowledge; it does not meet the perception-of-self inclusion criteria.</td></tr>
<tr><td>21703352_analysis_1</td><td>Neural regions associated with family obligation preferences during decisions to accept Costly-Donation versus Noncostly-Reward trials.</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although self-control is involved, the contrast does not primarily probe self-perception or self-knowledge processes; it targets decisions about others/family and mentalizing rather than self-assessment.</td></tr>
<tr><td>21703352_analysis_2</td><td>Neural regions associated with the ventral striatum during decisions to accept Costly-Donation relative to Noncostly-Reward trials that correlated positively with participants&#x27; family obligation preferences.</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The primary focus is on mentalizing (understanding others), self-control, and prosocial preferences; it does not primarily measure self-perception or self-knowledge, so Perception and Understanding of Self inclusion criteria are not met.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>21703352_1</td><td>Costly-Donation relative to Noncostly-Reward (accept); others</td><td>21703352_analysis_0</td><td>Neural regions engaged during decisions to accept Costly-Donation relative to Noncostly-Reward trials.</td><td>0.594</td><td>1.000</td><td>0.878</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>21703352_2</td><td>Costly-Donation versus Noncostly-Reward (Family obligtations); others</td><td>21703352_analysis_1</td><td>Neural regions associated with family obligation preferences during decisions to accept Costly-Donation versus Noncostly-Reward trials.</td><td>0.439</td><td>1.000</td><td>0.832</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 24834034</strong> | Pred included: 0 | Manual included (accepted matches only): 5 | Correct overlaps: 0 | Match statuses: accepted=5, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/24834034/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>24834034_analysis_0</td><td>FAIR &gt; UNFAIR</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The FAIR &gt; UNFAIR contrast targets social fairness processing and choices, not self-awareness, agency, or self-knowledge; it does not meet Perception and Understanding of Self criteria.</td></tr>
<tr><td>24834034_analysis_1</td><td>FAIR &lt; UNFAIR</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast addresses responses to offers and fairness norms (interaction between fairness and stake size), not explicit self-perception, agency, or self-knowledge measures.</td></tr>
<tr><td>24834034_analysis_2</td><td>HUMAN &gt; COMPUTER</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast targets partner type (human vs computer) and fairness processing rather than self-awareness, agency, or self-knowledge judgments.</td></tr>
<tr><td>24834034_analysis_3</td><td>HIGH(UNFAIR–FAIR) &gt; LOW(UNFAIR–FAIR)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast targets fairness-related social decision-making and perception of others&#x27; offers rather than explicit self-perception or self-knowledge processes; it does not meet criteria for Perception and Understanding of Self.</td></tr>
<tr><td>24834034_analysis_4</td><td>HIGH(UNFAIR–FAIR) &gt; LOW(UNFAIR–FAIR) WITHIN HUMAN</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The analysis concerns responses to offers and fairness norms, not explicit self-perception, agency, or self-knowledge judgments.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>24834034_1</td><td>FAIR &lt; UNFAIR; self</td><td>24834034_analysis_1</td><td>FAIR &lt; UNFAIR</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24834034_2</td><td>FAIR &gt; UNFAIR; self</td><td>24834034_analysis_0</td><td>FAIR &gt; UNFAIR</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24834034_3</td><td>HIGH(UNFAIR–FAIR) &gt; LOW(UNFAIR–FAIR); self</td><td>24834034_analysis_3</td><td>HIGH(UNFAIR–FAIR) &gt; LOW(UNFAIR–FAIR)</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24834034_4</td><td>HIGH(UNFAIR–FAIR) &gt; LOW(UNFAIR–FAIR) WITHIN HUMAN; self</td><td>24834034_analysis_4</td><td>HIGH(UNFAIR–FAIR) &gt; LOW(UNFAIR–FAIR) WITHIN HUMAN</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>24834034_5</td><td>HUMAN &gt; COMPUTER; self</td><td>24834034_analysis_2</td><td>HUMAN &gt; COMPUTER</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 25729358</strong> | Pred included: 0 | Manual included (accepted matches only): 6 | Correct overlaps: 0 | Match statuses: accepted=6, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/25729358/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>25729358_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrasts index inferences about others&#x27; beliefs and intentions rather than self-awareness, agency, or self-knowledge; does not measure perception/understanding of self. </td></tr>
<tr><td>25729358_analysis_1</td><td>Simple deception vs. truth</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Primary focus is on inferring the receiver&#x27;s beliefs and deceptive intent (other-focused ToM), not on self-awareness, agency, or self-knowledge.</td></tr>
<tr><td>25729358_analysis_2</td><td>Sophisticated deception vs. truth</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast targets inferring the receiver&#x27;s beliefs and intention to deceive (theory-of-mind/external social cognition) rather than self-awareness, agency, or self-knowledge; it does not measure Perception and Understanding of Self.</td></tr>
<tr><td>25729358_analysis_3</td><td>Sophisticated deception vs. simple deception</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although participants report intentions, the contrast targets inference about others and deception, not core self-perception (agency or self-knowledge) measures; it does not primarily assess perception/understanding of self.</td></tr>
<tr><td>25729358_analysis_4</td><td>analysis_4</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast and task primarily probe inference about the receiver and intention to deceive (others), not self-awareness or self-knowledge processes.</td></tr>
<tr><td>25729358_analysis_5</td><td>Parametric analysis modeling the incentive to deceive for simple deception trials</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The analysis focuses on anticipating the receiver&#x27;s choices and incentives to deceive (other-oriented processes), not on self-perception, agency, or self-knowledge.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>25729358_1</td><td>difference between sender and reciever payoff &gt; baseline; others</td><td>25729358_analysis_5</td><td>Parametric analysis modeling the incentive to deceive for simple deception trials</td><td>0.248</td><td>1.000</td><td>0.774</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>25729358_2</td><td>simple &amp; sophisticated deception &gt; truth; others</td><td>25729358_analysis_0</td><td>analysis_0</td><td>0.120</td><td>1.000</td><td>0.736</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr><tr><td>25729358_3</td><td>simple deception &gt; truth; others</td><td>25729358_analysis_1</td><td>Simple deception vs. truth</td><td>0.920</td><td>1.000</td><td>0.976</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25729358_4</td><td>sophisticated deception &gt; simple deception; others</td><td>25729358_analysis_3</td><td>Sophisticated deception vs. simple deception</td><td>0.953</td><td>1.000</td><td>0.986</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25729358_5</td><td>sophisticated deception &gt; truth; others</td><td>25729358_analysis_2</td><td>Sophisticated deception vs. truth</td><td>0.938</td><td>1.000</td><td>0.981</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>25729358_6</td><td>truth &gt; simple deception &amp; sophisticated deception; others</td><td>25729358_analysis_4</td><td>analysis_4</td><td>0.100</td><td>1.000</td><td>0.730</td><td>accepted</td><td>accepted_exact_coord_override, exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26262561</strong> | Pred included: 0 | Manual included (accepted matches only): 1 | Correct overlaps: 0 | Match statuses: accepted=1, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26262561/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26262561_analysis_0</td><td>analysis_0</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although participants provided self-ratings to compute perceived similarity, the neuroimaging contrasts focus on processing others’ gaze and in-group effects rather than on self-referential processing per se; the analysis does not directly probe self-awareness/self-knowledge neural processes.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26262561_1</td><td>[incongruent &gt; congruent trials]; self</td><td>26262561_analysis_0</td><td>analysis_0</td><td>0.208</td><td>1.000</td><td>0.762</td><td>accepted</td><td>exact_coord_set, high_coord_match, low_name_with_exact_coords</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26481048</strong> | Pred included: 0 | Manual included (accepted matches only): 2 | Correct overlaps: 0 | Match statuses: accepted=2, uncertain=0, unmatched=2</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26481048/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Activation &gt; Deactivation; self, Deactivation &gt; Activation; self</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26481048_analysis_0</td><td>Compared to implicit baseline</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TN</span></td><td></td><td>Participants classified word valence and were not asked to reflect on themselves; the study explicitly dissociated content from perspective and did not probe self-related judgments, so it does not satisfy perception/understanding of self criteria.</td></tr>
<tr><td>26481048_analysis_1</td><td>Compared to fixation task</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-good">TN</span></td><td></td><td>The task did not require self-referential judgments or measures of self-perception; participants classified word valence, so it does not meet Perception and Understanding of Self criteria.</td></tr>
<tr><td>26481048_analysis_2</td><td>Agency vs. Communion</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The study explicitly dissociates perspective and does not ask participants to judge self-related material; it targets content (agency vs. communion) rather than self-perception, so it does not meet Perception/Self criteria.</td></tr>
<tr><td>26481048_analysis_3</td><td>Communion vs. Agency</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The study did not probe self-related processing or self‑referential judgments (task dissociated perspective and was a generic valence classification), so it does not measure Perception/Understanding of Self.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26481048_1</td><td>Activation &gt; Deactivation; self</td><td>26481048_analysis_1</td><td>Compared to fixation task</td><td>0.400</td><td>0.123</td><td>0.206</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>26481048_2</td><td>Agency vs. Communion; self</td><td>26481048_analysis_2</td><td>Agency vs. Communion</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26481048_3</td><td>Communion vs. Agency; self</td><td>26481048_analysis_3</td><td>Communion vs. Agency</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26481048_4</td><td>Deactivation &gt; Activation; self</td><td>26481048_analysis_0</td><td>Compared to implicit baseline</td><td>0.167</td><td>0.571</td><td>0.450</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 26812250</strong> | Pred included: 0 | Manual included (accepted matches only): 11 | Correct overlaps: 0 | Match statuses: accepted=11, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/26812250/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>26812250_analysis_0</td><td>Negative vs. positive performance feedback.</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast indexes emotional response to feedback, not explicit self-perception, self-judgment, or self-monitoring processes as defined by this construct.</td></tr>
<tr><td>26812250_analysis_1</td><td>(A) Emotional empathic (EE) &gt; emotional unempathic (EN)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The primary contrast addresses processing of others&#x27; empathic statements and social cognition; it does not specifically probe self-related perception or judgments about the self.</td></tr>
<tr><td>26812250_analysis_2</td><td>(B) Unempathic (EN + CN) &gt; empathic (EE + CE)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The analysis targets processing of others&#x27; empathic/unempathic comments and social-cognitive responses, not judgments or representations about the self (no self-perception/self-knowledge contrast).</td></tr>
<tr><td>26812250_analysis_3</td><td>(C) Cognitive unempathic (CN) &gt; cognitive empathic (CE)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast targets processing of others&#x27; empathic comments (mentalizing/social cognition), not judgments or representations about the self, so it does not meet Perception/Self inclusion criteria.</td></tr>
<tr><td>26812250_analysis_4</td><td>(D) Empathic (EE + CE) &gt; high level baseline</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>Although participants provided valence ratings, the contrast targets processing of empathic comments (social processing) rather than tasks measuring self-perception or explicit self-related judgments; thus it does not meet the self-perception construct.</td></tr>
<tr><td>26812250_analysis_5</td><td>(E) Emotional empathic (EE) &gt; high level baseline</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The study focuses on processing others’ empathic/unempathic responses and mentalizing about the speaker, not on self-related judgment/agency or trait self-knowledge (participants’ valence ratings reflect affect but do not index self-perception constructs).</td></tr>
<tr><td>26812250_analysis_6</td><td>(F) Cognitive empathic (CE) &gt; high level baseline</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast focuses on processing received empathic comments and social cognition rather than explicit measures of self‑knowledge, agency, or self‑judgment.</td></tr>
<tr><td>26812250_analysis_7</td><td>(A) Emotional (EE + EN) &gt; cognitive (CE + CN)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>While participants rated their feelings, the contrast targets processing of others&#x27; empathic comments rather than judgments about the self or self-knowledge per se.</td></tr>
<tr><td>26812250_analysis_8</td><td>(B) Emotional empathic (EE) &gt; cognitive empathic (CE)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast examines processing of others&#x27; empathic comments and social-cognitive responses, not judgments or representation of the self, so it does not meet Perception/Understanding of Self criteria.</td></tr>
<tr><td>26812250_analysis_9</td><td>(C) Emotional unempathic (EN) &gt; cognitive unempathic (CN)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast concerns processing others&#x27; empathic/unempathic comments, not judgments or representations of the self.</td></tr>
<tr><td>26812250_analysis_10</td><td>(D) Cognitive unempathic (CN) &gt; emotional unempathic (EN)</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast targets processing of others&#x27; empathic/unempathic comments and associated social-cognitive responses rather than judgments about the self (agency/self-knowledge) as the primary construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>26812250_1</td><td>Cognitive empathic (CE) &gt; high level baseline; others</td><td>26812250_analysis_6</td><td>(F) Cognitive empathic (CE) &gt; high level baseline</td><td>0.957</td><td>1.000</td><td>0.987</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_10</td><td>Unempathic (EN + CN) &gt; empathic (EE + CE); others</td><td>26812250_analysis_2</td><td>(B) Unempathic (EN + CN) &gt; empathic (EE + CE)</td><td>0.953</td><td>1.000</td><td>0.986</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_11</td><td>negative vs positive performance; others</td><td>26812250_analysis_0</td><td>Negative vs. positive performance feedback.</td><td>0.853</td><td>1.000</td><td>0.956</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_2</td><td>Cognitive unempathic (CN) &gt; cognitive empathic (CE); others</td><td>26812250_analysis_3</td><td>(C) Cognitive unempathic (CN) &gt; cognitive empathic (CE)</td><td>0.962</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_3</td><td>Cognitive unempathic (CN) &gt; emotional unempathic (EN); others</td><td>26812250_analysis_10</td><td>(D) Cognitive unempathic (CN) &gt; emotional unempathic (EN)</td><td>0.964</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_4</td><td>Emotional (EE + EN) &gt; cognitive (CE + CN); others</td><td>26812250_analysis_7</td><td>(A) Emotional (EE + EN) &gt; cognitive (CE + CN)</td><td>0.953</td><td>1.000</td><td>0.986</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_5</td><td>Emotional Empathic (EE) &gt; emotional unempathic (EN); others</td><td>26812250_analysis_1</td><td>(A) Emotional empathic (EE) &gt; emotional unempathic (EN)</td><td>0.962</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_6</td><td>Emotional empathic (EE) &gt; cognitive empathic (CE); others</td><td>26812250_analysis_8</td><td>(B) Emotional empathic (EE) &gt; cognitive empathic (CE)</td><td>0.961</td><td>1.000</td><td>0.988</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_7</td><td>Emotional empathic (EE) &gt; high level baseline; others</td><td>26812250_analysis_5</td><td>(E) Emotional empathic (EE) &gt; high level baseline</td><td>0.957</td><td>1.000</td><td>0.987</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_8</td><td>Emotional unempathic (EN) &gt; cognitive unempathic (CN); others</td><td>26812250_analysis_9</td><td>(C) Emotional unempathic (EN) &gt; cognitive unempathic (CN)</td><td>0.964</td><td>1.000</td><td>0.989</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>26812250_9</td><td>Empathic (EE + CE) &gt; high level baseline; others</td><td>26812250_analysis_4</td><td>(D) Empathic (EE + CE) &gt; high level baseline</td><td>0.952</td><td>1.000</td><td>0.986</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 27095057</strong> | Pred included: 2 | Manual included (accepted matches only): 1 | Correct overlaps: 0 | Match statuses: accepted=1, uncertain=0, unmatched=1</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/27095057/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  <p><strong>Unmatched manual analyses:</strong> Social-Look &gt; Social-Reg; self</p>
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>27095057_analysis_0</td><td>Social-Reg &gt; Social-Look</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The primary contrast probes socially-induced regulation and social-cognitive processes (mentalizing, DMN) rather than processes of self-perception/agency or explicit self-judgment; it is not assessing Perception and Understanding of Self.</td></tr>
<tr><td>27095057_analysis_1</td><td>(Reg &gt; Look)_Social ∩ (Reg &gt; Look)_Self</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FP</span></td><td></td><td>The conjunction includes Self-Reg (self-induced reappraisal) and identifies common regions supporting regulation of one’s own emotions, thus engaging processes related to perception/understanding of self (self-monitoring/awareness) and meeting the inclusion criteria.</td></tr>
<tr><td>27095057_analysis_2</td><td>Distinct areas recruited by Social-Reg: (Reg &gt; Look)_Social &gt; (Reg &gt; Look)_Self ∩ (Reg &gt; Look)_social</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast isolates regions specific to Social-Reg relative to Self-Reg and is focused on social cognition/mentalizing rather than self-perception or self-knowledge processes.</td></tr>
<tr><td>27095057_analysis_3</td><td>Distinct areas recruited by Self-Reg: (Reg &gt; Look)_Self &gt; (Reg &gt; Look)_social ∩ (Reg &gt; Look)_Self</td><td class="decision-cell"><span class="decision-pill decision-include">+</span></td><td class="confusion-cell"><span class="confusion-pill confusion-na">-</span></td><td></td><td>The contrast isolates brain regions preferentially engaged during self-induced emotion regulation (Self-Reg), which involves awareness and control of one&#x27;s internal cognitive/emotional state (self-related processing). This fits the Perception and Understanding of Self construct.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>27095057_1</td><td>Social-Look &gt; Social-Reg; self</td><td>27095057_analysis_1</td><td>(Reg &gt; Look)_Social ∩ (Reg &gt; Look)_Self</td><td>0.406</td><td>0.000</td><td>0.122</td><td>unmatched</td><td>coord_count_mismatch, low_total_score</td></tr><tr><td>27095057_2</td><td>Social-Reg &gt; Social-Look; self</td><td>27095057_analysis_0</td><td>Social-Reg &gt; Social-Look</td><td>1.000</td><td>1.000</td><td>1.000</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>


<details class="doc-card">
  <summary><strong>PMID 28905269</strong> | Pred included: 0 | Manual included (accepted matches only): 2 | Correct overlaps: 0 | Match statuses: accepted=2, uncertain=0, unmatched=0</summary>
  <p><a href="https://pubmed.ncbi.nlm.nih.gov/28905269/" target="_blank" rel="noopener noreferrer">PubMed full text page</a></p>
  
  
  <details class="inner-accordion" open>
    <summary>Parsed analyses and annotation reasoning</summary>
    <div class="table-wrap">
      <table>
        <thead>
          <tr>
            <th>Analysis ID</th>
            <th>Parsed Analysis Name</th>
            <th>Model Decision</th>
            <th>Matched Outcome</th>
            <th>Tags</th>
            <th>Model Reasoning</th>
          </tr>
        </thead>
        <tbody>
          <tr><td>28905269_analysis_0</td><td>Regions with increased activation for monogamous men compared to non-monogamous men for the romantic &gt; neutral contrast</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast examines responses to others (romantic dyads) and group differences in affiliation-related reward, not self-related perception or self-reflection. No exclusion criteria are specified.</td></tr>
<tr><td>28905269_analysis_1</td><td>Regions with increased activation for non-monogamous men compared to monogamous men for the romantic &gt; sexual contrast</td><td class="decision-cell"><span class="decision-pill decision-exclude">-</span></td><td class="confusion-cell"><span class="confusion-pill confusion-bad">FN</span></td><td>manual+ (accepted)</td><td>The contrast examines perception of romantic vs sexual images of others, not self-referential processing or judgments about the self, so it does not meet Perception/Self criteria.</td></tr>
        </tbody>
      </table>
    </div>
  </details>
  <details class="inner-accordion" open>
    <summary>Manual-to-Auto Match Diagnostics</summary>
    <div class="table-wrap"><table><thead><tr><th>Manual ID</th><th>Manual Name</th><th>Matched Auto ID</th><th>Matched Auto Name</th><th>Name Score</th><th>Coord Score</th><th>Combined</th><th>Status</th><th>Reason Codes</th></tr></thead><tbody><tr><td>28905269_1</td><td>monogamous men compared to nonmonogamous, romantic &gt; neutral contrast; self</td><td>28905269_analysis_0</td><td>Regions with increased activation for monogamous men compared to non-monogamous men for the romantic &gt; neutral contrast</td><td>0.723</td><td>1.000</td><td>0.917</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr><tr><td>28905269_2</td><td>monogamous men compared to nonmonogamous, romantic &gt; sexual contrast; self</td><td>28905269_analysis_1</td><td>Regions with increased activation for non-monogamous men compared to monogamous men for the romantic &gt; sexual contrast</td><td>0.688</td><td>1.000</td><td>0.906</td><td>accepted</td><td>exact_coord_set, high_coord_match</td></tr></tbody></table></div>
  </details>
  
  
</details>
</details></section>
  
</body>
</html>
