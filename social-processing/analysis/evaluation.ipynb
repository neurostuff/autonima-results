{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Meta-Analysis Performance Evaluation\n",
    "\n",
    "This notebook analyzes the performance of an automated meta-analysis against three gold standards: liberal, unofficial, and official lists of PMIDs.\n",
    "\n",
    "For each automated meta-analysis there are three stages to focus on: search, abstract, and full-text screening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_82395/1118920014.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Define paths to evaluation results\n",
    "base_path = Path(\"../nt-rev2-search&all_pmids/evaluation\")\n",
    "standards = [\"liberal_all\", \"unofficial_all\", \"official_all\"]\n",
    "stages = [\"search\", \"abstract\", \"fulltext\"]\n",
    "\n",
    "# Metrics to visualize\n",
    "metrics_to_plot = [\n",
    "    \"precision\", \n",
    "    \"precision_ci_lower\", \n",
    "    \"precision_ci_upper\", \n",
    "    \"recall_all_meta\",\n",
    "    \"recall_all_meta_ci_lower\",\n",
    "    \"recall_all_meta_ci_upper\"\n",
    "]\n",
    "\n",
    "counts_to_plot = [\n",
    "    \"true_positives\", \n",
    "    \"false_negatives\",\n",
    "    \"false_positives\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all evaluation data\n",
    "evaluations = {}\n",
    "\n",
    "for standard in standards:\n",
    "    file_path = base_path / standard / \"performance_metrics.json\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        evaluations[standard] = json.load(f)\n",
    "        \n",
    "print(\"Loaded evaluation data for standards:\", list(evaluations.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract metrics for plotting\n",
    "def extract_metrics(evaluations, standards, stages, metrics):\n",
    "    data = []\n",
    "    \n",
    "    for standard in standards:\n",
    "        for stage in stages:\n",
    "            row = {\"standard\": standard.replace('_all', '').title(), \"stage\": stage.title()}\n",
    "            \n",
    "            # Extract metrics\n",
    "            for metric in metrics:\n",
    "                if stage in evaluations[standard] and \"metrics\" in evaluations[standard][stage]:\n",
    "                    if metric in evaluations[standard][stage][\"metrics\"]:\n",
    "                        row[metric] = evaluations[standard][stage][\"metrics\"][metric]\n",
    "                    else:\n",
    "                        row[metric] = None\n",
    "                else:\n",
    "                    row[metric] = None\n",
    "            \n",
    "            # Extract counts\n",
    "            if stage in evaluations[standard] and \"counts\" in evaluations[standard][stage]:\n",
    "                for count in [\"true_positives\", \"false_negatives\", \"false_positives\"]:\n",
    "                    if count in evaluations[standard][stage][\"counts\"]:\n",
    "                        row[count] = evaluations[standard][stage][\"counts\"][count]\n",
    "                    else:\n",
    "                        row[count] = None\n",
    "            \n",
    "            data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Extract all metrics\n",
    "df = extract_metrics(evaluations, standards, stages, metrics_to_plot)\n",
    "print(\"Dataframe shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization functions\n",
    "def plot_metric_with_ci(df, metric, title, ylabel):\n",
    "    \"\"\"Plot a metric with confidence intervals for all standards and stages\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    plot_data = df[[\"standard\", \"stage\", metric, f\"{metric}_ci_lower\", f\"{metric}_ci_upper\"]].copy()\n",
    "    plot_data = plot_data.dropna()\n",
    "    \n",
    "    # Create combinations of standard and stage for x-axis\n",
    "    plot_data[\"standard_stage\"] = plot_data[\"standard\"] + \"\\n\" + plot_data[\"stage\"]\n",
    "    \n",
    "    # Plot bars\n",
    "    x_pos = range(len(plot_data))\n",
    "    plt.bar(x_pos, plot_data[metric], color=['#1f77b4', '#ff7f0e', '#2ca02c']*3, alpha=0.8)\n",
    "    \n",
    "    # Add error bars for confidence intervals\n",
    "    errors_lower = plot_data[metric] - plot_data[f\"{metric}_ci_lower\"]\n",
    "    errors_upper = plot_data[f\"{metric}_ci_upper\"] - plot_data[metric]\n",
    "    plt.errorbar(x_pos, plot_data[metric], \n",
    "                 yerr=[errors_lower, errors_upper], \n",
    "                 fmt='none', ecolor='black', capsize=3, elinewidth=1)\n",
    "    \n",
    "    # Formatting\n",
    "    plt.xlabel(\"Standard and Stage\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.xticks(x_pos, plot_data[\"standard_stage\"], rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "def plot_counts(df, counts, title):\n",
    "    \"\"\"Plot counts (TP, FN, FP) as stacked bars\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Prepare data\n",
    "    plot_data = df[[\"standard\", \"stage\"] + counts].copy()\n",
    "    plot_data = plot_data.dropna()\n",
    "    plot_data[\"standard_stage\"] = plot_data[\"standard\"] + \"\\n\" + plot_data[\"stage\"]\n",
    "    \n",
    "    # Create stacked bar chart\n",
    "    x_pos = range(len(plot_data))\n",
    "    bottom = np.zeros(len(plot_data))\n",
    "    \n",
    "    colors = ['#2ca02c', '#d62728', '#ff7f0e']  # Green, red, orange\n",
    "    labels = ['True Positives', 'False Negatives', 'False Positives']\n",
    "    \n",
    "    for i, count in enumerate(counts):\n",
    "        plt.bar(x_pos, plot_data[count], bottom=bottom, \n",
    "                label=labels[i], color=colors[i], alpha=0.8)\n",
    "        bottom += plot_data[count]\n",
    "    \n",
    "    # Formatting\n",
    "    plt.xlabel(\"Standard and Stage\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(x_pos, plot_data[\"standard_stage\"], rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision metrics\n",
    "plot_metric_with_ci(df, \"precision\", \n",
    "                   \"Precision Across Standards and Stages with 95% Confidence Intervals\", \n",
    "                   \"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot recall metrics\n",
    "plot_metric_with_ci(df, \"recall_all_meta\", \n",
    "                   \"Recall Across Standards and Stages with 95% Confidence Intervals\", \n",
    "                   \"Recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot counts\n",
    "plot_counts(df, [\"true_positives\", \"false_negatives\", \"false_positives\"], \n",
    "           \"True Positives, False Negatives, and False Positives by Standard and Stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary table\n",
    "print(\"Performance Summary Table\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Select key metrics for display\n",
    "summary_cols = [\"standard\", \"stage\", \"precision\", \"recall_all_meta\", \n",
    "               \"true_positives\", \"false_negatives\", \"false_positives\"]\n",
    "summary_df = df[summary_cols].copy()\n",
    "\n",
    "# Format numeric columns\n",
    "for col in [\"precision\", \"recall_all_meta\"]:\n",
    "    summary_df[col] = summary_df[col].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"N/A\")\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Compare standards at each stage\n",
    "def compare_standards(df, stage):\n",
    "    \"\"\"Compare performance across standards for a specific stage\"\"\"\n",
    "    stage_data = df[df['stage'] == stage.title()]\n",
    "    \n",
    "    print(f\"\\n{stage.title()} Stage Performance Comparison:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for _, row in stage_data.iterrows():\n",
    "        print(f\"{row['standard']} Standard:\")\n",
    "        print(f\"  Precision: {row['precision']:.3f} (95% CI: {row['precision_ci_lower']:.3f} - {row['precision_ci_upper']:.3f})\")\n",
    "        print(f\"  Recall: {row['recall_all_meta']:.3f} (95% CI: {row['recall_all_meta_ci_lower']:.3f} - {row['recall_all_meta_ci_upper']:.3f})\")\n",
    "        print(f\"  TP: {row['true_positives']}, FN: {row['false_negatives']}, FP: {row['false_positives']}\")\n",
    "        print()\n",
    "\n",
    "# Compare performance at each stage\n",
    "for stage in stages:\n",
    "    compare_standards(df, stage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
